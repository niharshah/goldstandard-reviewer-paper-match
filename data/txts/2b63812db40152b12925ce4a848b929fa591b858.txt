arXiv:1703.01619v1 [cs.CL] 5 Mar 2017

Neural Machine Translation and Sequence-to-sequence Models: A Tutorial
Graham Neubig Language Technologies Institute, Carnegie Mellon University
1 Introduction
This tutorial introduces a new and powerful set of techniques variously called “neural machine translation” or “neural sequence-to-sequence models”. These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice.
1.1 Background
Before getting into the details, it might be worth describing each of the terms that appear in the title “Neural Machine Translation and Sequence-to-sequence Models”. Machine translation is the technology used to translate between human language. Think of the universal translation device showing up in sci-ﬁ movies to allow you to communicate eﬀortlessly with those that speak a diﬀerent language, or any of the plethora of online translation web sites that you can use to assimilate content that is not in your native language. This ability to remove language barriers, needless to say, has the potential to be very useful, and thus machine translation technology has been researched from shortly after the advent of digital computing.
We call the language input to the machine translation system the source language, and call the output language the target language. Thus, machine translation can be described as the task of converting a sequence of words in the source, and converting into a sequence of words in the target. The goal of the machine translation practitioner is to come up with an eﬀective model that allows us to perform this conversion accurately over a broad variety of languages and content.
The second part of the title, sequence-to-sequence models, refers to the broader class of models that include all models that map one sequence to another. This, of course, includes machine translation, but it also covers a broad spectrum of other methods used to handle other tasks as shown in Figure 1. In fact, if we think of a computer program as something that takes in a sequence of input bits, then outputs a sequence of output bits, we could say that every single program is a sequence-to-sequence model expressing some behavior (although of course in many cases this is not the most natural or intuitive way to express things).
1

Machine translation:
kare wa ringo wo tabeta → he ate an apple
Tagging:
he ate an apple → PRN VBD DET PP
Dialog:
he ate an apple → good, he needs to slim down
Speech Recognition:
→ he ate an apple
And just about anything...:
1010000111101 → 00011010001101
Figure 1: An example of sequence-to-sequence modeling tasks.
The motivation for using machine translation as a representative of this larger class of sequence-to-sequence models is many-fold:
1. Machine translation is a widely-recognized and useful instance of sequence-to-sequence models, and allows us to use many intuitive examples demonstrating the diﬃculties encountered when trying to tackle these problems.
2. Machine translation is often one of the main driving tasks behind the development of new models, and thus these models tend to be tailored to MT ﬁrst, then applied to other tasks.
3. However, there are also cases where MT has learned from other tasks as well, and introducing these tasks helps explain the techniques used in MT as well.
1.2 Structure of this Tutorial
This tutorial ﬁrst starts out with a general mathematical deﬁnition of statistical techniques for machine translation in Section 2. The rest of this tutorial will sequentially describe techniques of increasing complexity, leading up to attentional models, which represent the current state-of-the-art in the ﬁeld.
First, Sections 3-6 focus on language models, which calculate the probability of a target sequence of interest. These models are not capable of performing translation or sequence transduction, but will provide useful preliminaries to understand sequence-to-sequence models.
• Section 3 describes n-gram language models, simple models that calculate the probability of words based on their counts in a set of data. It also describes how we evaluate how well these models are doing using measures such as perplexity.
• Section 4 describes log-linear language models, models that instead calculate the probability of the next word based on features of the context. It describes how we can learn the parameters of the models through stochastic gradient descent – calculating derivatives and gradually updating the parameters to increase the likelihood of the observed data.
2

• Section 5 introduces the concept of neural networks, which allow us to combine together multiple pieces of information more easily than log-linear models, resulting in increased modeling accuracy. It gives an example of feed-forward neural language models, which calculate the probability of the next word based on a few previous words using neural networks.
• Section 6 introduces recurrent neural networks, a variety of neural networks that have mechanisms to allow them to remember information over multiple time steps. These lead to recurrent neural network language models, which allow for the handling of long-term dependencies that are useful when modeling language or other sequential data.
Finally, Sections 7 and 8 describe actual sequence-to-sequence models capable of performing machine translation or other tasks.
• Section 7 describes encoder-decoder models, which use a recurrent neural network to encode the target sequence into a vector of numbers, and another network to decode this vector of numbers into an output sentence. It also describes search algorithms to generate output sequences based on this model.
• Section 8 describes attention, a method that allows the model to focus on diﬀerent parts of the input sentence while generating translations. This allows for a more eﬃcient and intuitive method of representing sentences, and is often more eﬀective than its simpler encoder-decoder counterpart.

2 Statistical MT Preliminaries

First, before talking about any speciﬁc models, this chapter describes the overall framework
of statistical machine translation (SMT) [16] more formally.
First, we deﬁne our task of machine translation as translating a source sentence F = f1, . . . , fJ = f1|F | into a target sentence E = e1, . . . , eI = e|1E|.Thus, any type of translation system can be deﬁned as a function

Eˆ = mt(F ),

(1)

which returns a translation hypothesis Eˆ given a source sentence F as input. Statistical machine translation systems are systems that perform translation by cre-
ating a probabilistic model for the probability of E given F , P (E | F ; θ), and ﬁnding the target sentence that maximizes this probability:

Eˆ = argmax P (E | F ; θ),

(2)

E

where θ are the parameters of the model specifying the probability distribution. The parameters θ are learned from data consisting of aligned sentences in the source and target languages, which are called parallel corpora in technical terminology.Within this framework, there are three major problems that we need to handle appropriately in order to create a good translation system:

3

Modeling: First, we need to decide what our model P (E | F ; θ) will look like. What parameters will it have, and how will the parameters specify a probability distribution?
Learning: Next, we need a method to learn appropriate values for parameters θ from training data.
Search: Finally, we need to solve the problem of ﬁnding the most probable sentence (solving “argmax”). This process of searching for the best hypothesis and is often called decoding.1
The remainder of the material here will focus on solving these problems.

3 n-gram Language Models
While the ﬁnal goal of a statistical machine translation system is to create a model of the target sentence E given the source sentence F , P (E | F ), in this chapter we will take a step back, and attempt to create a language model of only the target sentence P (E). Basically, this model allows us to do two things that are of practical use.
Assess naturalness: Given a sentence E, this can tell us, does this look like an actual, natural sentence in the target language? If we can learn a model to tell us this, we can use it to assess the ﬂuency of sentences generated by an automated system to improve its results. It could also be used to evaluate sentences generated by a human for purposes of grammar checking or error correction.
Generate text: Language models can also be used to randomly generate text by sampling a sentence E from the target distribution: E ∼ P (E).2 Randomly generating samples from a language model can be interesting in itself – we can see what the model “thinks” is a natural-looking sentences – but it will be more practically useful in the context of the neural translation models described in the following chapters.
In the following sections, we’ll cover a few methods used to calculate this probability P (E).

3.1 Word-by-word Computation of Probabilities

As

mentioned

above,

we

are

interested

in

calculating

the

probability

of

a

sentence

E

=

e

T 1

.

Formally, this can be expressed as

P (E) = P (|E| = T, eT1 ),

(3)

the joint probability that the length of the sentence is (|E| = T ), that the identity of the
ﬁrst word in the sentence is e1, the identity of the second word in the sentence is e2, up until the last word in the sentence being eT . Unfortunately, directly creating a model of this probability distribution is not straightforward,3 as the length of the sequence T is not determined in advance, and there are a large number of possible combinations of words.4

1This is based on the famous quote from Warren Weaver, likening the process of machine translation to
decoding an encoded cipher. 2∼ means “is sampled from”. 3Although it is possible, as shown by whole-sentence language models in [88]. 4Question: If V is the size of the target vocabulary, how many are there for a sentence of length T ?

4

P(|E| = 3, e1=”she”, e2=”went”, e3=”home”) = P(e1=“she”) * P(e2=”went” | e1=“she”) * P(e3=”home” | e1=“she”, e2=”went”) * P(e4=”</s>” | e1=“she”, e2=”went”, e3=”home”)
Figure 2: An example of decomposing language model probabilities word-by-word.

As a way to make things easier, it is common to re-write the probability of the full sentence as the product of single-word probabilities. This takes advantage of the fact that a joint probability – for example P (e1, e2, e3) – can be calculated by multiplying together conditional probabilities for each of its elements. In the example, this means that P (e1, e2, e3) = P (e1)P (e2 | e1)P (e3 | e1, e2).
Figure 2 shows an example of this incremental calculation of probabilities for the sentence “she went home”. Here, in addition to the actual words in the sentence, we have introduced an implicit sentence end (“ /s ”) symbol, which we will indicate when we have terminated the sentence. Stepping through the equation in order, this means we ﬁrst calculate the probability of “she” coming at the beginning of the sentence, then the probability of “went” coming next in a sentence starting with “she”, the probability of “home” coming after the sentence preﬁx “she went”, and then ﬁnally the sentence end symbol “ /s ” after “she went home”. More generally, we can express this as the following equation:

T +1

P (E) = P (et | et1−1)

(4)

t=1

where eT +1 = /s . So coming back to the sentence end symbol /s , the reason why we introduce this symbol is because it allows us to know when the sentence ends. In other words, by examining the position of the /s symbol, we can determine the |E| = T term in our original LM joint probability in Equation 3. In this example, when we have /s as the 4th word in the sentence, we know we’re done and our ﬁnal sentence length is 3.
Once we have the formulation in Equation 4, the problem of language modeling now becomes a problem of calculating the next word given the previous words P (et | et1−1). This is much more manageable than calculating the probability for the whole sentence, as we now have a ﬁxed set of items that we are looking to calculate probabilities for. The next couple of sections will show a few ways to do so.

3.2 Count-based n-gram Language Models
The ﬁrst way to calculate probabilities is simple: prepare a set of training data from which we can count word strings, count up the number of times we have seen a particular string of words, and divide it by the number of times we have seen the context. This simple method,

5

i am from pittsburgh . i study at a university . my mother is from utah . P(e2=am | e1=i) = c(e1=i, e2=am)/c(e1=i) = 1 / 2 = 0.5 P(e2=study | e1=i) = c(e1=i, e2=study)/c(e1=i) = 1 / 2 = 0.5
Figure 3: An example of calculating probabilities using maximum likelihood estimation.

can be expressed by the equation below, with an example shown in Figure 3

t−1

cpreﬁx(et1)

PML(et | e1 ) = cpreﬁx(et1−1) . (5)

Here cpreﬁx(·) is the count of the number of times this particular word string appeared at the beginning of a sentence in the training data. This approach is called maximum likelihood estimation (MLE, details later in this chapter), and is both simple and guaranteed to create a model that assigns a high probability to the sentences in training data.
However, let’s say we want to use this model to assign a probability to a new sentence that we’ve never seen before. For example, if we want to calculate the probability of the sentence “i am from utah .” based on the training data in the example. This sentence is extremely similar to the sentences we’ve seen before, but unfortunately because the string “i am from utah” has not been observed in our training data, cpreﬁx(i, am, from, utah) = 0, P (e4 = utah | e1 = i, e2 = am, e3 = from) becomes zero, and thus the probability of the whole sentence as calculated by Equation 5 also becomes zero. In fact, this language model will assign a probability of zero to every sentence that it hasn’t seen before in the training corpus, which is not very useful, as the model loses ability to tell us whether a new sentence a system generates is natural or not, or generate new outputs.
To solve this problem, we take two measures. First, instead of calculating probabilities from the beginning of the sentence, we set a ﬁxed window of previous words upon which we will base our probability calculations, approximating the true probability. If we limit our context to n − 1 previous words, this would amount to:

P (et | et1−1) ≈ PML(et | ett−−1n+1).

(6)

Models that make this assumption are called n-gram models. Speciﬁcally, when models

where n = 1 are called unigram models, n = 2 bigram models, n = 3 trigram models, and

n ≥ 4 four-gram, ﬁve-gram, etc.

The parameters θ of n-gram models consist of probabilities of the next word given n − 1

previous words:

θet

= P (et | ett−−1n+1),

(7)

t−n+1

and in order to train an n-gram model, we have to learn these parameters from data.5 In the

simplest form, these parameters can be calculated using maximum likelihood estimation as

follows:

t−1

c(ett−n+1)

θett−n+1 = PML(et | et−n+1) = c(et−1

, )

(8)

t−n+1

5Question: How many parameters does an n-gram model with a particular n have?

6

where c(·) is the count of the word string anywhere in the corpus. Sometimes these equations will reference et−n+1 where t − n + 1 < 0. In this case, we assume that et−n+1 = s where s is a special sentence start symbol.
If we go back to our previous example and set n = 2, we can see that while the string “i am from utah .” has never appeared in the training corpus, “i am”, “am from”, “from utah”, “utah .”, and “. /s ” are all somewhere in the training corpus, and thus we can patch together probabilities for them and calculate a non-zero probability for the whole sentence.6
However, we still have a problem: what if we encounter a two-word string that has never appeared in the training corpus? In this case, we’ll still get a zero probability for that particular two-word string, resulting in our full sentence probability also becoming zero. ngram models ﬁx this problem by smoothing probabilities, combining the maximum likelihood estimates for various values of n. In the simple case of smoothing unigram and bigram probabilities, we can think of a model that combines together the probabilities as follows:

P (et | et−1) = (1 − α)PML(et | et−1) + αPML(et),

(9)

where α is a variable specifying how much probability mass we hold out for the unigram distribution. As long as we set α > 0, regardless of the context all the words in our vocabulary will be assigned some probability. This method is called interpolation, and is one of the standard ways to make probabilistic models more robust to low-frequency phenomena.
If we want to use even more context – n = 3, n = 4, n = 5, or more – we can recursively deﬁne our interpolated probabilities as follows:

P (et | ett−−1m+1) = (1 − αm)PML(et | ett−−1m+1) + αmP (et | ett−−1m+2).

(10)

The ﬁrst term on the right side of the equation is the maximum likelihood estimate for the model of order m, and the second term is the interpolated probability for all orders up to m − 1.
There are also more sophisticated methods for smoothing, which are beyond the scope of this section, but summarized very nicely in [19].

Context-dependent smoothing coeﬃcients: Instead of having a ﬁxed α, we condition the interpolation coeﬃcient on the context: αet−1 . This allows the model to give
t−m+1
more weight to higher order n-grams when there are a suﬃcient number of training examples for the parameters to be estimated accurately and fall back to lower-order n-grams when there are fewer training examples. These context-dependent smoothing coeﬃcients can be chosen using heuristics [118] or learned from data [77].

Back-oﬀ: In Equation 9, we interpolated together two probability distributions over the full vocabulary V . In the alternative formulation of back-oﬀ, the lower-order distribution only is used to calculate probabilities for words that were given a probability of zero in the higher-order distribution. Back-oﬀ is more expressive but also more complicated than interpolation, and the two have been reported to give similar results [41].

Modiﬁed distributions: It is also possible to use a diﬀerent distribution than PML. This can be done by subtracting a constant value from the counts before calculating probabilities, a method called discounting. It is also possible to modify the counts of
6Question: What is this probability?

7

lower-order distributions to reﬂect the fact that they are used mainly as a fall-back for when the higher-order distributions lack suﬃcient coverage.
Currently, Modiﬁed Kneser-Ney smoothing (MKN; [19]), is generally considered one of the standard and eﬀective methods for smoothing n-gram language models. MKN uses context-dependent smoothing coeﬃcients, discounting, and modiﬁcation of lower-order distributions to ensure accurate probability estimates.

3.3 Evaluation of Language Models
Once we have a language model, we will want to test whether it is working properly. The way we test language models is, like many other machine learning models, by preparing three sets of data:
Training data is used to train the parameters θ of the model.

Development data is used to make choices between alternate models, or to tune the hyperparameters of the model. Hyper-parameters in the model above could include the maximum length of n in the n-gram model or the type of smoothing method.

Test data is used to measure our ﬁnal accuracy and report results.

For language models, we basically want to know whether the model is an accurate model

of language, and there are a number of ways we can deﬁne this. The most straight-forward

way of deﬁning accuracy is the likelihood of the model with respect to the development

or test data. The likelihood of the parameters θ with respect to this data is equal to the

probability that the model assigns to the data. For example, if we have a test dataset Etest,

this is:

P (Etest; θ).

(11)

We often assume that this data consists of several independent sentences or documents E,

giving us

P (Etest; θ) =

P (E; θ).

(12)

E∈Etest

Another measure that is commonly used is log likelihood

log P (Etest; θ) =

log P (E; θ).

(13)

E∈Etest

The log likelihood is used for a couple reasons. The ﬁrst is because the probability of any particular sentence according to the language model can be a very small number, and the product of these small numbers can become a very small number that will cause numerical precision problems on standard computing hardware. The second is because sometimes it is more convenient mathematically to deal in log space. For example, when taking the derivative in gradient-based methods to optimize parameters (used in the next section), it is more convenient to deal with the sum in Equation 13 than the product in Equation 11.
It is also common to divide the log likelihood by the number of words in the corpus

length(Etest) =

|E|.

E∈Etest

(14)

8

This makes it easier to compare and contrast results across corpora of diﬀerent lengths. The ﬁnal common measure of language model accuracy is perplexity, which is deﬁned
as the exponent of the average negative log likelihood per word

ppl(Etest; θ) = e−(log P (Etest;θ))/length(Etest).

(15)

An intuitive explanation of the perplexity is “how confused is the model about its decision?” More accurately, it expresses the value “if we randomly picked words from the probability distribution calculated by the language model at each time step, on average how many words would it have to pick to get the correct one?” One reason why it is common to see perplexities in research papers is because the numbers calculated by perplexity are bigger, making the diﬀerences in models more easily perceptible by the human eye.7

3.4 Handling Unknown Words

Finally, one important point to keep in mind is that some of the words in the test set Etest will not appear even once in the training set Etrain. These words are called unknown words, and need to be handeled in some way. Common ways to do this in language models include:

Assume closed vocabulary: Sometimes we can assume that there will be no new words in the test set. For example, if we are calculating a language model over ASCII characters, it is reasonable to assume that all characters have been observed in the training set. Similarly, in some speech recognition systems, it is common to simply assign a probability of zero to words that don’t appear in the training data, which means that these words will not be able to be recognized.

Interpolate with an unknown words distribution: As mentioned in Equation 10, we

can interpolate between distributions of higher and lower order. In the case of un-

known words, we can think of this as a distribution of order “0”, and deﬁne the 1-gram

probability as the interpolation between the unigram distribution and unknown word

distribution

P (et) = (1 − α1)PML(et) + α1Punk(et).

(16)

Here, Punk needs to be a distribution that assigns a probability to all words Vall, not just ones in our vocabulary V derived from the training corpus. This could be done by, for example, training a language model over characters that “spells out” unknown words in the case they don’t exist in in our vocabulary. Alternatively, as a simpler approximation that is nonetheless fairer than ignoring unknown words, we can guess the total number of words |Vall| in the language where we are modeling, where |Vall| > |V |, and deﬁne Punk as a uniform distribution over this vocabulary: Punk(et) = 1/|Vall|.

Add an unk word: As a ﬁnal method to handle unknown words we can remove some of the words in Etrain from our vocabulary, and replace them with a special unk symbol representing unknown words. One common way to do so is to remove singletons, or words that only appear once in the training corpus. By doing this, we explicitly predict in which contexts we will be seeing an unknown word, instead of implicitly predicting it through interpolation like mentioned above. Even if we predict the unk symbol, we
7And, some cynics will say, making it easier for your research papers to get accepted.

9

will still need to estimate the probability of the actual word, so any time we predict unk at position i, we further multiply in the probability of Punk(et).
3.5 Further Reading
To read in more detail about n-gram language models, [41] gives a very nice introduction and comprehensive summary about a number of methods to overcome various shortcomings of vanilla n-grams like the ones mentioned above.
There are also a number of extensions to n-gram models that may be nice for the interested reader.
Large-scale language modeling: Language models are an integral part of many commercial applications, and in these applications it is common to build language models using massive amounts of data harvested from the web for other sources. To handle this data, there is research on eﬃcient data structures [48, 82], distributed parameter servers [14], and lossy compression algorithms [104].
Language model adaptation: In many situations, we want to build a language model for speciﬁc speaker or domain. Adaptation techniques make it possible to create large general-purpose models, then adapt these models to more closely match the target use case [6].
Longer-distance language count-based models: As mentioned above, n-gram models limit their context to n − 1, but in reality there are dependencies in language that can reach much farther back into the sentence, or even span across whole documents. The recurrent neural network language models that we will introduce in Section 6 are one way to handle this problem, but there are also non-neural approaches such as cache language models [61], topic models [13], and skip-gram models [41].
Syntax-based language models: There are also models that take into account the syntax of the target sentence. For example, it is possible to condition probabilities not on words that occur directly next to each other in the sentence, but those that are “close” syntactically [96].
3.6 Exercise
The exercise that we will be doing in class will be constructing an n-gram LM with linear interpolation between various levels of n-grams. We will write code to:
• Read in and save the training and testing corpora.
• Learn the parameters on the training corpus by counting up the number of times each n-gram has been seen, and calculating maximum likelihood estimates according to Equation 8.
• Calculate the probabilities of the test corpus using linearly interpolation according to Equation 9 or Equation 10.
10

To handle unknown words, you can use the uniform distribution method described in Section 3.4, assuming that there are 10,000,000 words in the English vocabulary. As a sanity check, it may be better to report the number of unknown words, and which portions of the per-word log-likelihood were incurred by the main model, and which portion was incurred by the unknown word probability log Punk.
In order to do so, you will ﬁrst need data, and to make it easier to start out you can use some pre-processed data from the German-English translation task of the IWSLT evaluation campaign8 here: http://phontron.com/data/iwslt-en-de-preprocessed.tar.gz.
Potential improvements to the model include reading [19] and implementing a better smoothing method, implementing a better method for handling unknown words, or implementing one of the more advanced methods in Section 3.5.
4 Log-linear Language Models
This chapter will discuss another set of language models: log-linear language models [87, 20], which take a very diﬀerent approach than the count-based n-grams described above.9
4.1 Model Formulation
Like n-gram language models, log-linear language models still calculate the probability of a particular word et given a particular context ett−−1n+1. However, their method for doing so is quite diﬀerent from count-based language models, based on the following procedure.
Calculating features: Log-linear language models revolve around the concept of features. In short, features are basically, “something about the context that will be useful in predicting the next word”. More formally, we deﬁne a feature function φ(ett−−1n+1) that takes a context as input, and outputs a real-valued feature vector x ∈ RN that describe the context using N diﬀerent features.10
For example, from our bi-gram models from the previous chapter, we know that “the identity of the previous word” is something that is useful in predicting the next word. If we want to express the identity of the previous word as a real-valued vector, we can assume that each word in our vocabulary V is associated with a word ID j, where 1 ≤ j ≤ |V |. Then, we deﬁne our feature function φ(ett−n+1) to return a feature vector x = R|V |, where if et−1 = j, then the jth element is equal to one and the remaining elements in the vector are equal to zero. This type of vector is often called a one-hot vector, an example of which is shown in Figure 4(a). For later user, we will also deﬁne a function onehot(i) which returns a vector
8http://iwslt.org 9It should be noted that the cited papers call these maximum entropy language models. This is because models in this chapter can be motivated in two ways: log-linear models that calculate un-normalized log-probability scores for each function and normalize them to probabilities, and maximum-entropy models that spread their probability mass as evenly as possible given the constraint that they must model the training data. While the maximum-entropy interpretation is quite interesting theoretically and interested readers can reference [11] to learn more, the explanation as log-linear models is simpler conceptually, and thus we will use this description in this chapter. 10Alternative formulations that deﬁne feature functions that also take the current word as input φ(ett−n+1) are also possible, but in this book, to simplify the transition into neural language models described in Section 5, we consider features over only the context.
11

Previous words: “giving a”

j=1: a

1

j=2: the

0

j=3: hat Φ(e )= 0

j=4: giving

i-1

0

...

…

0 0 Φ(ei-2)= 01 …

1 0

0 0

…

Φ(e ,e )= 0

i-1 i-2

0

0 1

…

Word IDs

Features for Features for Features for

the previous two words both words

word

ago

Figure 4: An example of feature values for a particular context.

where only the ith element is one and the rest are zero (assume the length of the vector is

the appropriate length given the context).

Of course, we are not limited to only considering one previous word. We could also

calculate one-hot vectors for both et−1 and et−2, then concatenate them together, which

would allow us to create a model that considers the values of the two previous words. In fact,

there are many other types of feature functions that we can think of (more in Section 4.4),

and the ability to ﬂexibly deﬁne these features is one of the advantages of log-linear language

models over standard n-gram models.

Calculating scores: Once we have our feature vector, we now want to use these features

to predict probabilities over our output vocabulary V . In order to do so, we calculate a score

vector s ∈ R|V | that corresponds to the likelihood of each word: words with higher scores in

the vector will also have higher probabilities. We do so using the model parameters θ, which

speciﬁcally come in two varieties: a bias vector b ∈ R|V |, which tells us how likely each word in the vocabulary is overall, and a weight matrix W = R|V |×N which describes the

relationship between feature values and scores. Thus, the ﬁnal equation for calculating our

scores for a particular context is:

s = W x + b.

(17)

One thing to note here is that in the special case of one-hot vectors or other sparse vectors where most of the elements are zero. Because of this we can also think about Equation 17 in a diﬀerent way that is numerically equivalent, but can make computation more eﬃcient. Speciﬁcally, instead of multiplying the large feature vector by the large weight matrix, we can add together the columns of the weight matrix for all active (non-zero) features as follows:

s=

W·,jxj + b,

{j:xj =0}

(18)

where W·,j is the jth column of W . This allows us to think of calculating scores as “look up the vector for the features active for this instance, and add them together”, instead of writing them as matrix math. An example calculation in this paradigm where we have two feature

12

Previous words: “giving a”

a

3.0

-6.0

-0.2

-3.2

the

2.5

-5.1

-0.3

-2.9

tgaiflkt b = -00..12 w1,a = 00..21 w2,giving = 12..00 s = 12..02

hat

1.2

0.6

-1.2

0.6

…

…

…

…

…

Words we're predicting

How likely

How likely given the

are they? previous

word is “a”?

How likely given two words before is “giving”?

Total score

Figure 5: An example of the weights for a log linear model in a certain context.

functions (one for the directly preceding word, and one for the word before that) is shown in Figure 5.
Calculating probabilities: It should be noted here that scores s are arbitrary real numbers, not probabilities: they can be negative or greater than one, and there is no restriction that they add to one. Because of this, we run these scores through a function that performs the following transformation:
pj = exp(sj) . (19) ˜j exp(s˜j)
By taking the exponent and dividing by the sum of the values over the entire vocabulary, these scores can be turned into probabilities that are between 0 and 1 and sum to 1.
This function is called the softmax function, and often expressed in vector form as follows:

p = softmax(s).

(20)

Through applying this to the scores calculated in the previous section, we now have a way to go from features to language model probabilities.

4.2 Learning Model Parameters
Now, the only remaining missing link is how to acquire the parameters θ, consisting of the weight matrix W and bias b. Basically, the way we do so is by attempting to ﬁnd parameters that ﬁt the training corpus well.
To do so, we use standard machine learning methods for optimizing parameters. First, we deﬁne a loss function (·) – a function expressing how poorly we’re doing on the training data. In most cases, we assume that this loss is equal to the negative log likelihood:

(Etrain, θ) = − log P (Etrain | θ) = −

log P (E | θ).

E∈Etrain

(21)

We assume we can also deﬁne the loss on a per-word level:

(ett−n+1, θ) = log P (et | ett−−1n+1).

(22)

13

Next, we optimize the parameters to reduce this loss. While there are many methods for doing so, in recent years one of the go-to methods is stochastic gradient descent (SGD). SGD is an iterative process where we randomly pick a single word et (or mini-batch, discussed in Section 5) and take a step to improve the likelihood with respect to et. In order to do so, we ﬁrst calculate the derivative of the loss with respect to each of the features in the full feature set θ:
d (ett−n+1, θ) . (23) dθ
We can then use this information to take a step in the direction that will reduce the loss according to the objective function
θ ← θ − η d (ett−n+1, θ) , (24) dθ
where η is our learning rate, specifying the amount with which we update the parameters every time we perform an update. By doing so, we can ﬁnd parameters for our model that reduce the loss, or increase the likelihood, on the training data.
This vanilla variety of SGD is quite simple and still a very competitive method for optimization in large-scale systems. However, there are also a few things to consider to ensure that training remains stable:
Adjusting the learning rate: SGD requires also requires us to carefully choose η: if η is too big, training can become unstable and diverge, and if η is too small, training may become incredibly slow or fall into bad local optima. One way to handle this problem is learning rate decay: starting with a higher learning rate, then gradually reducing the learning rate near the end of training. Other more sophisticated methods are listed below.
Early stopping: It is common to use a held-out development set, measure our log-likelihood on this set, and save the model that has achieved the best log-likelihood on this heldout set. This is useful in case the model starts to over-ﬁt to the training set, losing its generalization capability, we can re-wind to this saved model. As another method to prevent over-ﬁtting and smooth convergence of training, it is common to measure log likelihood on a held-out development set, and when the log likelihood stops improving or starts getting worse, reduce the learning rate.
Shuﬄing training order: One of the features of SGD is that it processes training data one at a time. This is nice because it is simple and can be eﬃcient, but it also causes problems if there is some bias in the order in which we see the data. For example, if our data is a corpus of news text where news articles come ﬁrst, then sports, then entertainment, there is a chance that near the end of training our model will see hundreds or thousands of entertainment examples in a row, resulting in the parameters moving to a space that favors these more recently seen training examples. To prevent this problem, it is common (and highly recommended) to randomly shuﬄe the order with which the training data is presented to the learning algorithm on every pass through the data.
There are also a number of other update rules that have been proposed to improve gradient descent and make it more stable or eﬃcient. Some representative methods are listed below:
14

SGD with momentum [90]: Instead of taking a single step in the direction of the current gradient, SGD with momentum keeps an exponentially decaying average of past gradients. This reduces the propensity of simple SGD to “jitter” around, making optimization move more smoothly across the parameter space.
AdaGrad [30]: AdaGrad focuses on the fact that some parameters are updated much more frequently than others. For example, in the model above, columns of the weight matrix W corresponding to infrequent context words will only be updated a few times for every pass through the corpus, while the bias b will be updated on every training example. Based on this, AdaGrad dynamically adjusts the training rate η for each parameter individually, with frequently updated (and presumably more stable) parameters such as b getting smaller updates, and infrequently updated parameters such as W getting larger updates.
Adam [60]: Adam is another method that computes learning rates for each parameter. It does so by keeping track of exponentially decaying averages of the mean and variance of past gradients, incorporating ideas similar to both momentum and AdaGrad. Adam is now one of the more popular methods for optimization, as it greatly speeds up convergence on a wide variety of datasets, facilitating fast experimental cycles. However, it is also known to be prone to over-ﬁtting, and thus, if high performance is paramount, it should be used with some caution and compared to more standard SGD methods.
[89] provides a good overview of these various methods with equations and notes a few other concerns when performing stochastic optimization.

4.3 Derivatives for Log-linear Models
Now, the ﬁnal piece in the puzzle is the calculation of derivatives of the loss function with respect to the parameters. To do so, ﬁrst we step through the full loss function in one pass as below:

x = φ(ett−−1m+1)

(25)

s=

W·,j xj + b

{j:xj !=0}

(26)

p = softmax(s)

(27)

= − log pet.

(28)

And thus, using the chain rule to calculate

d (ett−n+1, W, b) = d dp ds (29)

db

dp ds db

d (ett−n+1, W, b) = d dp ds (30)

dW·,j

dp ds dW·,j

15

we ﬁnd that the derivative of the loss function for the bias and each column of the weight matrix is:
d (ett−n+1, W, b) = p − onehot(et) (31) db
d (ett−n+1, W, b) = xj(p − onehot(et)) (32) dW·,j
Conﬁrming these equations is left as a (highly recommended) exercise to the reader. Hint: when performing this derivation, it is easier to work with the log probability log p than working with p directly.
4.4 Other Features for Language Modeling
One reason why log-linear models are nice is because they allow us to ﬂexibly design features that we think might be useful for predicting the next word. For example, these could include:
Context word features: As shown in the example above, we can use the identity of et−1 or the identity of et−2.
Context class: Context words can be grouped into classes of similar words (using a method such as Brown clustering [15]), and instead of looking up a one-hot vector with a separate entry for every word, we could look up a one-hot vector with an entry for each class [18]. Thus, words from the same class could share statistical strength, allowing models to generalize better.
Context suﬃx features: Maybe we want a feature that ﬁres every time the previous word ends with “...ing” or other common suﬃxes. This would allow us to learn more generalized patterns about words that tend to follow progressive verbs, etc.
Bag-of-words features: Instead of just using the past n words, we could use all previous words in the sentence. This would amount to calculating the one-hot vectors for every word in the previous sentence, and then instead of concatenating them simply summing them together. This would lose all information about what word is in what position, but could capture information about what words tend to co-occur within a sentence or document.
It is also possible to combine together multiple features (for example et−1 is a particular word and et−2 is another particular word). This is one way to create a more expressive feature set, but also has a downside of greatly increasing the size of the feature space. We discuss these features in more detail in Section 5.1.
4.5 Further Reading
The language model in this section was basically a featurized version of an n-gram language model. There are quite a few other varieties of linear featurized models including:
Whole-sentence language models: These models, instead of predicting words one-by-one, predict the probability over the whole sentence then normalize [88]. This can be conducive to introducing certain features, such as a probability distribution over lengths of sentences, or features such as “whether this sentence contains a verb”.
16

Discriminative language models: In the case that we want to use a language model to determine whether the output of a system is good or not, sometimes it is useful to train directly on this system output, and try to re-rank the outputs to achieve higher accuracy [86]. Even if we don’t have real negative examples, it can be possible to “hallucinate” negative examples that are still useful for training [80].
4.6 Exercise
In the exercise for this chapter, we will construct a log-linear language model and evaluate its performance. I highly suggest that you try to use the NumPy library to hold and perform calculations over feature vectors, as this will make things much easier. If you have never used NumPy before, you can take a look at this tutorial to get started: https://docs.scipy.org/ doc/numpy-dev/user/quickstart.html.
Writing the program will entail:
• Writing a function to read in the training and test corpora, and converting the words into numerical IDs.
• Writing the feature function φ(ett−−1n+1), which takes in a string and returns which features are active (for example, as a baseline these can be features with the identity of the previous two words).
• Writing code to calculate the loss function.
• Writing code to calculate gradients and perform stochastic gradient descent updates.
• Writing (or re-using from the previous exercise) code to evaluate the language models.
Similarly to the n-gram language models, we will measure the per-word log likelihood and perplexity on our text corpus, and compare it to n-gram language models. Handling unknown words will similarly require that you use the uniform distribution with 10,000,000 words in the English vocabulary.
Potential improvements to the model include designing better feature functions, adjusting the learning rate and measuring the results, and researching and implementing other types of optimizers such as AdaGrad or Adam.
5 Neural Networks and Feed-forward Language Models
In this chapter, we describe language models based on neural networks, a way to learn more sophisticated functions to improve the accuracy of our probability estimates with less feature engineering.
5.1 Potential and Problems with Combination Features
Before moving into the technical detail of neural networks, ﬁrst let’s take a look at a motivating example in Figure 6. From the example, we can see et−2 = “farmers” is compatible with et = “hay” (in the context “farmers grow hay”), and et−1 = “eat” is also compatible (in the context “cows eat hay”). If we are using a log-linear model with one set of features dependent
17

farmers eat farmers grow

steak → hay → steak → hay →

high low low high

cows eat cows grow

steak → low hay → high steak → low hay → low

Figure 6: An example of the eﬀect that combining multiple words can have on the probability of the next word.

on et−1, and another set of features dependent on et−2, neither set of features can rule out the unnatural phrase “farmers eat hay.”
One way we can ﬁx this problem is by creating another set of features where we learn one vector for each pair of words et−2, et−1. If this is the case, our vector for the context et−2 = “farmers”, et−1 = “eat” could assign a low score to “hay”, resolving this problem. However, adding these combination features has one major disadvantage: it greatly expands the parameters: instead of O(|V |2) parameters for each pair ei−1, ei, we need O(|V |3) parameters for each triplet ei−2, ei−1, ei. These numbers greatly increase the amount of memory used by the model, and if there are not enough training examples, the parameters may not be learned properly.
Because of both the importance of and diﬃculty in learning using these combination features, a number of methods have been proposed to handle these features, such as kernelized support vector machines [28] and neural networks [91, 39]. Speciﬁcally in this section, we will cover neural networks, which are both ﬂexible and relatively easy to train on large data, desiderata for sequence-to-sequence models.

5.2 A Brief Overview of Neural Networks
To understand neural networks in more detail, let’s take a very simple example of a function that we cannot learn with a simple linear classiﬁer like the ones we used in the last chapter: a function that takes an input x ∈ {−1, 1}2 and outputs y = 1 if both x1 and x2 are equal and y = −1 otherwise. This function is shown in Figure 7.
x2

-1

+1

x1

+1

-1

Figure 7: A function that cannot be solved by a linear transformation.

A ﬁrst attempt at solving this function might deﬁne a linear model (like the log-linear models from the previous chapter) that solves this problem using the following form:

y = W x + b.

(33)

18

x2

-1

+1

x1

+1

-1

Original input variables

Wh,0 x1 1 x2 1 step h1

1 -1 bh,0

x1

Wh,1 -1

x2 -1 step h2

1 -1 bh,1
Hidden layer

x = {1,1} → h2
h = {-1, 1}
+1
x = {1,-1} →
h = {-1, -1} -1
x = {-1,1} → h = {-1, -1}

h1
+1
x = {-1,-1} → h = {1, -1}

Wy h1 1 h2 1 y 11
by

Transformed variables

Output layer

Figure 8: A simple neural network that represents the nonlinear function of Figure 7.

However, this class of functions is not powerful enough to represent the function at hand.11 Thus, we turn to a slightly more complicated class of functions taking the following form:

h = step(Wxhx + bh)

y = whyh + by.

(34)

Computation is split into two stages: calculation of the hidden layer, which takes in input x and outputs a vector of hidden variables h, and calculation of the output layer, which takes in h and calculates the ﬁnal result y. Both layers consist of an aﬃne transform12 using weights W and biases b, followed by a step(·) function, which calculates the following:

1 if x > 0,

step(x) =

(35)

−1 otherwise.

This function is one example of a class of neural networks called multi-layer perceptrons (MLPs). In general, MLPs consist one or more hidden layers that consist of an aﬃne transform followed by a non-linear function (such as the step function used here), culminating in an output layer that calculates some variety of output.
Figure 8 demonstrates why this type of network does a better job of representing the non-linear function of Figure 7. In short, we can see that the ﬁrst hidden layer transforms the input x into a hidden vector h in a diﬀerent space that is more conducive for modeling our ﬁnal function. Speciﬁcally in this case, we can see that h is now in a space where we can deﬁne a linear function (using wy and by) that correctly calculates the desired output y.
As mentioned above, MLPs are one speciﬁc variety of neural network. More generally, neural networks can be thought of as a chain of functions (such as the aﬃne transforms and step functions used above, but also including many, many others) that takes some input and calculates some desired output. The power of neural networks lies in the fact that chaining together a variety of simpler functions makes it possible to represent more complicated functions
11Question: Prove this by trying to solve the system of equations. 12A fancy name for a multiplication followed by an addition.

19

y y y

2

2

2

1

1

1

0

0

0

-4 -3 -2 -1 -10 1 2 3 4 -4 -3 -2 -1 -10 1 2 3 4-4 -3 -2 -1 -10 1 2 3 4

-2

-2

-2

step(x)

tanh(x)

relu(x)

Figure 9: Types of non-linearities.

in an easily trainable, parameter-eﬃcient way. In fact, the simple single-layer MLP described above is a universal function approximator [51], which means that it can approximate any function to arbitrary accuracy if its hidden vector h is large enough.
We will see more about training in Section 5.3 and give some more examples of how these can be more parameter eﬃcient in the discussion of neural network language models in Section 5.5.

5.3 Training Neural Networks

Now that we have a model in Equation 34, we would like to train its parameters Wmh, bh, why, and by. To do so, remembering our gradient-based training methods from the last chapter, we need to deﬁne the loss function (·), calculate the derivative of the loss with respect to the
parameters, then take a step in the direction that will reduce the loss. For our loss function,
let’s use the squared-error loss, a commonly used loss function for regression problems which measures the diﬀerence between the calculated value y and correct value y∗ as follows

(y∗, y) = (y∗ − y)2.

(36)

Next, we need to calculate derivatives. Here, we run into one problem: the step(·) function is not very derivative friendly, with its derivative being:

dstep(x) undeﬁned if x = 0,

=

(37)

dx

0

otherwise.

Because of this, it is more common to use other non-linear functions, such as the hyperbolic tangent (tanh) function. The tanh function, as shown in Figure 9, looks very much like a softened version of the step function that has a continuous gradient everywhere, making it more conducive to training with gradient-based methods. There are a number of other alternatives as well, the most popular of which being the rectiﬁed linear unit (RelU)

x if x > 0,

RelU(x) =

(38)

0 otherwise.

shown in the left of Figure 9. In short, RelUs solve the problem that the tanh function gets “saturated” and has very small gradients when the absolute value of input x is very large (x is a large negative or positive number). Empirical results have often shown it to be an eﬀective alternative to tanh, including for the language modeling task described in this chapter [110].

20

Graph for the Function Itself x × + tanh × + y

Wh bh

wy by

Graph for the Training Objective x × + tanh × + sqr_err 

Wh bh

wy by y*

Figure 10: Computation graphs for the function itself, and the loss function.

So let’s say we swap in a tanh non-linearity instead of the step function to our network, we can now proceed to calculate derivatives like we did in Section 4.3. First, we perform the full calculation of the loss function:

h = Wxhx + bh

h = tanh(h )

y = whyh + by

= (y∗ − y)2.

(39)

Then, again using the chain rule, we calculate the derivatives of each set of parameters:

d d dy =
dby dy dby

d

d dy

=

dwhy dy dwhy

d d dy dh dh =
dbh dy dh dh dbh

d

d dy dh dh

=

.

(40)

dWxh dy dh dh dWxh

We could go through all of the derivations above by hand and precisely calculate the gradients of all parameters in the model. Interested readers are free to do so, but even for a simple model like the one above, it is quite a lot of work and error prone. For more complicated models, like the ones introduced in the following chapters, this is even more the case.
Fortunately, when we actually implement neural networks on a computer, there is a very useful tool that saves us a large portion of this pain: automatic diﬀerentiation (autodiﬀ) [116, 44]. To understand automatic diﬀerentiation, it is useful to think of our computation in Equation 39 as a data structure called a computation graph, two examples of which are shown in Figure 10. In these graphs, each node represents either an input to the network or the result of one computational operation, such as a multiplication, addition, tanh, or squared error. The ﬁrst graph in the ﬁgure calculates the function of interest itself and would be used when we want to make predictions using our model, and the second graph calculates the loss function and would be used in training.

21

Automatic diﬀerentiation is a two-step dynamic programming algorithm that operates over the second graph and performs:
• Forward calculation, which traverses the nodes in the graph in topological order, calculating the actual result of the computation as in Equation 39.
• Back propagation, which traverses the nodes in reverse topological order, calculating the gradients as in Equation 40.
The nice thing about this formulation is that while the overall function calculated by the graph can be relatively complicated, as long as it can be created by combining multiple simple nodes for which we are able to calculate the function f (x) and derivative f (x), we are able to use automatic diﬀerentiation to calculate its derivatives using this dynamic program without doing the derivation by hand.
Thus, to implement a general purpose training algorithm for neural networks, it is necessary to implement these two dynamic programs, as well as the atomic forward function and backward derivative computations for each type of node that we would need to use. While this is not trivial in itself, there are now a plethora of toolkits that either perform general-purpose auto-diﬀerentiation [7, 50], or auto-diﬀerentiation speciﬁcally tailored for machine learning and neural networks [1, 12, 26, 105, 78]. These implement the data structures, nodes, backpropogation, and parameter optimization algorithms needed to train neural networks in an eﬃcient and reliable way, allowing practitioners to get started with designing their models. In the following sections, we will take this approach, taking a look at how to create our models of interest in a toolkit called DyNet,13 which has a programming interface that makes it relatively easy to implement the sequence-to-sequence models covered here.14
5.4 An Example Implementation
Figure 11 shows an example of implementing the above neural network in DyNet, which we’ll step through line-by-line. Lines 1-2 import the necessary libraries. Lines 4-5 specify parameters of the models: the size of the hidden vector h and the number of epochs (passes through the data) for which we’ll perform training. Line 7 initializes a DyNet model, which will store all the parameters we are attempting to learn. Lines 8-11 initialize parameters Wxh, bh, why, and by to be the appropriate size so that dimensions in the equations for Equation 39 match. Line 12 initializes a “trainer”, which will update the parameters in the model according to an update strategy (here we use simple stochastic gradient descent, but trainers for AdaGrad, Adam, and other strategies also exist). Line 14 creates the training data for the function in Figure 7.
Lines 16-25 deﬁne a function that takes input x and creates a computation graph to calculate Equation 39. First, line 17 creates a new computation graph to hold the computation for this particular training example. Lines 18-21 take the parameters (stored in the model) and adds them to the computation graph as DyNet variables for this particular training example. Line 22 takes a Python list representing the current input and puts it into the computation graph as a DyNet variable. Line 23 calculates the hidden vector h, Line 24 calculates the value y, and Line 25 returns it.
13http://github.com/clab/dynet 14It is also developed by the author of these materials, so the decision might have been a wee bit biased.
22

1 import dynet as dy

2 import random

3 # Parameters of the model and training

4 HIDDEN_SIZE = 20

5 NUM_EPOCHS = 20

6 # Define the model and SGD optimizer

7 model = dy.Model()

8 W_xh_p = model.add_parameters((HIDDEN_SIZE, 2))

9 b_h_p = model.add_parameters(HIDDEN_SIZE)

10 W_hy_p = model.add_parameters((1, HIDDEN_SIZE))

11 b_y_p = model.add_parameters(1)

12 trainer = dy.SimpleSGDTrainer(model)

13 # Define the training data, consisting of (x,y) tuples

14 data = [([1,1],1), ([-1,1],-1), ([1,-1],-1), ([-1,-1],1)]

15 # Define the function we would like to calculate

16 def calc_function(x):

17 dy.renew_cg()

18 w_xh = dy.parameter(w_xh_p)

19 b_h = dy.parameter(b_h_p)

20 W_hy = dy.parameter(W_hy_p)

21 b_y = dy.parameter(b_y_p)

22 x_val = dy.inputVector(x)

23 h_val = dy.tanh(w_xh * x_val + b_h)

24 y_val = W_hy * h_val + b_y

25 return y_val

26 # Perform training

27 for epoch in range(NUM_EPOCHS):

28 epoch_loss = 0

29 random.shuffle(data)

30 for x, ystar in data:

31

y = calc_function(x)

32

loss = dy.squared_distance(y, dy.scalarInput(ystar))

33

epoch_loss += loss.value()

34

loss.backward()

35

trainer.update()

36 print("Epoch %d: loss=%f" % (epoch, epoch_loss))

37 # Print results of prediction

38 for x, ystar in data:

39 y = calc_function(x)

40 print("%r -> %f" % (x, y.value()))

Figure 11: An example of training a neural network for a multi-layer perceptron using the toolkit DyNet.
23

lookup(et-1)

Wh bh

Wp bp

M

concat x + tanh x + softmax p

lookup(et-2)

Figure 12: A computation graph for a tri-gram feed-forward neural language model.

Lines 27-36 perform training for NUM EPOCHS passes over the data (one pass through the training data is usually called an “epoch”). Line 28 creates a variable to keep track of the loss for this epoch for later reporting. Line 29 shuﬄes the data, as recommended in Section 4.2. Lines 30-35 perform stochastic gradient descent, looping over each of the training examples. Line 31 creates a computation for the function itself, and Line 32 adds computation for the loss function. Line 33 runs the forward calculation to calculate the loss and adds it to the loss for this epoch. Line 34 runs back propagation, and Line 35 updates the model parameters. At the end of the epoch, we print the loss for the epoch in Line 36 to make sure that the loss is going down and our model is converging.
Finally, at the end of training in Lines 38-40, we print the output results. In an actual scenario, this would be done on a separate set of test data.

5.5 Neural-network Language Models

Now that we have the basics down, it is time to apply neural networks to language modeling [76, 9]. A feed-forward neural network language model is very much like the log-linear language model that we mentioned in the previous section, simply with the addition of one or more non-linear layers before the output.
First, let’s recall the tri-gram log-linear language model. In this case, assume we have two sets of features expressing the identity of et−1 (represented as W (1)) and et−2 (as W (2)), the equation for the log-linear model looks like this:

s = W·(,e1t)−1 + W·(,e2t)−2 + b p = softmax(s),

(41)

where we add the appropriate columns from the weight matricies to the bias to get the score, then take the softmax to turn it into a probability.
Compared to this, a tri-gram neural network model with a single layer is structured as shown in Figure 12 and described in equations below:

m = concat(M·,et−2 , M·,et−1 )

h = tanh(Wmhm + bh)

s = Whsh + bs

p = softmax(s)

(42)

In the ﬁrst line, we obtain a vector m representing the context eii−−1n+1 (in the particular case above, we are handling a tri-gram model so n = 3). Here, M is a matrix with |V | columns,
and Lm rows, where each column corresponds to an Lm-length vector representing a single

24

1 # Define the lookup parameters at model definition time 2 # VOCAB_SIZE is the number of words in the vocabulary 3 # EMBEDDINGS_SIZE is the length of the word embedding vector 4 M_p = model.add_lookup_parameters((VOCAB_SIZE, EMBEDDING_SIZE)) 5 # Load the parameters into the computation graph 6 M = dy.lookup(M_p) 7 # And look up the vector for word i 8 m = M[i]

Figure 13: Code for looking things up in DyNet.

word in the vocabulary. This vector is called a word embedding or a word representation, which is a vector of real numbers corresponding to particular words in the vocabulary.15 The interesting thing about expressing words as vectors of real numbers is that each element of the vector could reﬂect a diﬀerent aspect of the word. For example, there may be an element in the vector determining whether a particular word under consideration could be a noun, or another element in the vector expressing whether the word is an animal, or another element that expresses whether the word is countable or not.16 Figure 13 shows an example of how to deﬁne parameters that allow you to look up a vector in DyNet.
The vector m then results from the concatenation of the word vectors for all of the words in the context, so |m| = Lm ∗ (n − 1). Once we have this m, we run the vectors through a hidden layer to obtain vector h. By doing so, the model can learn combination features that reﬂect information regarding multiple words in the context. This allows the model to be expressive enough to represent the more diﬃcult cases in Figure 6. For example, given a context is “cows eat”, and some elements of the vector M·,cows identify the word as a “large farm animal” (e.g. “cow”, “horse”, “goat”), while some elements of M·,eat corresponds to “eat” and all of its relatives (“consume”, “chew”, “ingest”), then we could potentially learn a unit in the hidden layer h that is active when we are in a context that represents “things farm animals eat”.
Next, we calculate the score vector for each word: s ∈ R|V |. This is done by performing an aﬃne transform of the hidden vector h with a weight matrix Whs ∈ R|V |×|h| and adding a bias vector bs ∈ R|V |. Finally, we get a probability estimate p by running the calculated scores through a softmax function, like we did in the log-linear language models. For training, if we know et we can also calculate the loss function as follows, similarly to the log-linear model:

= − log(pet).

(43)

DyNet has a convenience function that, given a score vector s, will calculate the negative log likelihood loss:
15For the purposes of the model in this chapter, these vectors can basically be viewed as one set of tunable parameters in the neural language model, but there has also been a large amount of interest in learning these vectors for use in other tasks. Some methods are outlined in Section 5.6.
16In reality, it is rare that single elements in the vector have such an intuitive meaning unless we impose some sort of constraint, such as sparsity constraints [75].

25

1 loss = dy.pickneglogsoftmax(s, e_t)
The reasons why the neural network formulation is nice becomes apparent when we compare this to n-gram language models in Section 3:
Better generalization of contexts: n-gram language models treat each word as its own discrete entity. By using input embeddings M , it is possible to group together similar words so they behave similarly in the prediction of the next word. In order to do the same thing, n-gram models would have to explicitly learn word classes and using these classes eﬀectively is not a trivial problem [15].
More generalizable combination of words into contexts: In an n-gram language model, we would have to remember parameters for all combinations of {cow, horse, goat} × {consume, chew, ingest} to represent the context “things farm animals eat”. This would be quadratic in the number of words in the class, and thus learning these parameters is diﬃcult in the face of limited training data. Neural networks handle this problem by learning nodes in the hidden layer that can represent this quadratic combination in a feature-eﬃcient way.
Ability to skip previous words: n-gram models generally fall back sequentially from longer contexts (e.g. “the two previous words ett−−12”) to shorter contexts (e.g. “the previous words et−1”), but this doesn’t allow them to “skip” a word and only reference for example, “the word two words ago et−2”. Log-linear models and neural networks can handle this skipping naturally.
5.6 Further Reading
In addition to the methods described above, there are a number of extensions to neuralnetwork language models that are worth discussing.
Softmax approximations: One problem with the training of log-linear or neural network language models is that at every training example, they have to calculate the large score vector s, then run a softmax over it to get probabilities. As the vocabulary size |V | grows larger, this can become quite time-consuming. As a result, there are a number of ways to reduce training time. One example are methods that sample a subset of the vocabulary V ∈ V where |V | V , then calculate the scores and approximate the loss over this smaller subset. Examples of these include methods that simply try to get the true word et to have a higher score (by some margin) than others in the subsampled set [27] and more probabilistically motivated methods, such as importance sampling [10] or noise-contrastive estimation (NCE; [74]). Interestingly, for other objective functions such as linear regression and special variety of softmax called the spherical softmax, it is possible to calculate the objective function in ways that do not scale linearly with the vocabulary size [111].
Other softmax structures: Another interesting trick to improve training speed is to create a softmax that is structured so that its loss functions can be computed eﬃciently. One
26

way to do so is the class-based softmax [40], which assigns each word et to a class ct,

then divides computation into two steps: predicting the probability of class ct given the

context, then predicting the probability of the word et given the class and the current

context

P (et

|

ct

,

ett−−

1 n+1

)P

(ct

|

ett−−1n+1).

The advantage of this method is that we

only need to calculate scores for the correct class ct out of |C| classes, then the correct

word et out of the vocabulary for class ct, which is size |Vct|. Thus, our computational complexity becomes O(|C| + |Vct|) instead of O(|V |).17 The hierarchical softmax [73]

takes this a step further by predicting words along a binary-branching tree, which results

in a computational complexity of O(log2|V |).

Other models to learn word representations: As mentioned in Section 5.5, we learn word embeddings M as a by-product of training our language models. One very nice feature of word representations is that language models can be trained purely on raw text, but the resulting representations can capture semantic or syntactic features of the words, and thus can be used to eﬀectively improve down-stream tasks that don’t have a lot of annotated data, such as part-of-speech tagging or parsing [107].18 Because of their usefulness, there have been an extremely large number of approaches proposed to learn diﬀerent varieties of word embeddings,19 from early work based on distributional similarity and dimensionality reduction [93, 108] to more recent models based on predictive models similar to language models [107, 71], with the general current thinking being that predictive models are the more eﬀective and ﬂexible of the two [5].The most wellknown methods are the continuous-bag-of-words and skip-gram models implemented in the software word2vec,20 which deﬁne simple objectives for predicting words using the immediately surrounding context or vice-versa. word2vec uses a sampling-based approach and parallelization to easily scale up to large datasets, which is perhaps the primary reason for its popularity. One thing to note is that these methods are not language models in themselves, as they do not calculate a probability of the sentence P (E), but many of the parameter estimation techniques can be shared.

5.7 Exercise
In the exercise for this chapter, we will use DyNet to construct a feed-forward language model and evaluate its performance.
Writing the program will entail:
• Writing a function to read in the data and (turn it into numerical IDs).
• Writing a function to calculate the loss function by looking up word embeddings, then running them through a multi-layer perceptron, then predicting the result.
• Writing code to perform training using this function.
17Question: What is the ideal class size to achieve the best computational eﬃciency? 18Manning (2015) called word embeddings the “Sriracha sauce of NLP”, because you can add them to anything to make it better http://nlp.stanford.edu/~manning/talks/ NAACL2015-VSM-Compositional-Deep-Learning.pdf 19So many that Daum´e III (2016) called word embeddings the “Sriracha sauce of NLP: it sounds like a good idea, you add too much, and now you’re crying” https://twitter.com/haldaume3/status/ 706173575477080065 20https://code.google.com/archive/p/word2vec/

27

• Writing evaluation code that measures the perplexity on a held-out data set.
Language modeling accuracy should be measured in the same way as previous exercises and compared with the previous models.
Potential improvements to the model include tuning the various parameters of the model. How big should h be? Should we add additional hidden layers? What optimizer with what learning rate should we use? What happens if we implement one of the more eﬃcient versions of the softmax explained in Section 5.6?
6 Recurrent Neural Network Language Models
The neural-network models presented in the previous chapter were essentially more powerful and generalizable versions of n-gram models. In this section, we talk about language models based on recurrent neural networks (RNNs), which have the additional ability to capture long-distance dependencies in language.
6.1 Long Distance Dependencies in Language
He doesn't have very much confidence in himself She doesn't have very much confidence in herself
Figure 14: An example of long-distance dependencies in language.
Before speaking about RNNs in general, it’s a good idea to think about the various reasons a model with a limited history would not be suﬃcient to properly model all phenomena in language.
One example of a long-range grammatical constraint is shown in Figure 14. In this example, there is a strong constraint that the starting “he” or “her” and the ﬁnal “himself” or “herself” must match in gender. Similarly, based on the subject of the sentence, the conjugation of the verb will change. These sorts of dependencies exist regardless of the number of intervening words, and models with a ﬁnite history eii−−1n+1, like the one mentioned in the previous chapter, will never be able to appropriately capture this. These dependencies are frequent in English but even more prevalent in languages such as Russian, which has a large number of forms for each word, which must match in case and gender with other words in the sentence.21
Another example where long-term dependencies exist is in selectional preferences [85]. In a nutshell, selectional preferences are basically common sense knowledge of “what will do what to what”. For example, “I ate salad with a fork” is perfectly sensible with “a fork” being a tool, and “I ate salad with my friend” also makes sense, with “my friend” being a companion. On the other hand, “I ate salad with a backpack” doesn’t make much sense because a backpack is neither a tool for eating nor a companion. These selectional preference violations lead to nonsensical sentences and can also span across an arbitrary length due to the fact that subjects, verbs, and objects can be separated by a great distance.
21See https://en.wikipedia.org/wiki/Russian_grammar for an overview.
28

(a) A single RNN time step

(b) An unrolled RNN

Whh

bh

Whh bh

ht-1

×

+ tanh ht h0 ×

+ tanh ×

+ tanh ×

+ tanh

Wxh ×

×

×

×

x1

x2

x3

xt

W

xh

(c) A simplified view h0 RNN RNN

RNN

x1

x2

x3

Figure 15: Examples of computation graphs for neural networks. (a) shows a single time step. (b) is the unrolled network. (c) is a simpliﬁed version of the unrolled network, where gray boxes indicate a function that is parameterized (in this case by Wxh, Whh, and bh).

Finally, there are also dependencies regarding the topic or register of the sentence or document. For example, it would be strange if a document that was discussing a technical subject suddenly started going on about sports – a violation of topic consistency. It would also be unnatural for a scientiﬁc paper to suddenly use informal or profane language – a lack of consistency in register.
These and other examples describe why we need to model long-distance dependencies to create workable applications.

6.2 Recurrent Neural Networks

Recurrent neural networks (RNNs; [33]) are a variety of neural network that makes it

possible to model these long-distance dependencies. The idea is simply to add a connection

that references the previous hidden state ht−1 when calculating hidden state h, written in

equations as:

ht = tanh(Wxhxt + Whhht−1 + bh) t ≥ 1, (44)

0

otherwise.

As we can see, for time steps t ≥ 1, the only diﬀerence from the hidden layer in a standard neural network is the addition of the connection Whhht−1 from the hidden state at time step t − 1 connecting to that at time step t. As this is a recursive equation that uses ht−1 from the previous time step. This single time step of a recurrent neural network is shown visually in the computation graph shown in Figure 15(a).
When performing this visual display of RNNs, it is also common to “unroll” the neural network in time as shown in Figure 15(b), which makes it possible to explicitly see the information ﬂow between multiple time steps. From unrolling the network, we can see that we are still dealing with a standard computation graph in the same form as our feed-forward networks, on which we can still do forward computation and backward propagation, making

29

it possible to learn our parameters. It also makes clear that the recurrent network has to start somewhere with an initial hidden state h0. This initial state is often set to be a vector full of zeros, treated as a parameter hinit and learned, or initialized according to some other information (more on this in Section 7).
Finally, for simplicity, it is common to abbreviate the whole recurrent neural network step with a single block “RNN” as shown in Figure 15. In this example, the boxes corresponding to RNN function applications are gray, to show that they are internally parameterized with Wxh, Whh, and bh. We will use this convention in the future to represent parameterized functions.
RNNs make it possible to model long distance dependencies because they have the ability to pass information between timesteps. For example, if some of the nodes in ht−1 encode the information that “the subject of the sentence is male”, it is possible to pass on this information to ht, which can in turn pass it on to ht+1 and on to the end of the sentence. This ability to pass information across an arbitrary number of consecutive time steps is the strength of recurrent neural networks, and allows them to handle the long-distance dependencies described in Section 6.1.
Once we have the basics of RNNs, applying them to language modeling is (largely) straightforward [72]. We simply take the feed-forward language model of Equation 42 and enhance it with a recurrent connection as follows:

mt = M·,et−1

ht = tanh(Wmhmt + Whhht−1 + bh) t ≥ 1,

0

otherwise.

pt = softmax(Whsht + bs).

(45)

One thing that should be noted is that compared to the feed-forward language model, we are only feeding in the previous word instead of the two previous words. The reason for this is because (if things go well) we can expect that information about et−2 and all previous words are already included in ht−1, making it unnecessary to feed in this information directly.
Also, for simplicity of notation, it is common to abbreviate the equation for ht with a function RNN(·), following the simpliﬁed view of drawing RNNs in Figure 15(c):

mt = M·,et−1

ht = RNN(mt, ht−1)

pt = softmax(Whsht + bs).

(46)

6.3 The Vanishing Gradient and Long Short-term Memory
However, while the RNNs in the previous section are conceptually simple, they also have problems: the vanishing gradient problem and the closely related cousin, the exploding gradient problem.
A conceptual example of the vanishing gradient problem is shown in Figure 16. In this example, we have a recurrent neural network that makes a prediction after several times steps, a model that could be used to classify documents or perform any kind of prediction over a sequence of text. After it makes its prediction, it gets a loss that it is expected to backpropagate over all time steps in the neural network. However, at each time step, when we run

30

dl =tiny dh
0

dl =small dh
1

dl =med. dl =large

dh

dh

2

3

h0 RNN h1 RNN h2 RNN h3 square_err



x1

x2

x3

y*

Figure 16: An example of the vanishing gradient problem.

the back propagation algorithm, the gradient gets smaller and smaller, and by the time we get back to the beginning of the sentence, we have a gradient so small that it eﬀectively has no ability to have a signiﬁcant eﬀect on the parameters that need to be updated. The reason why this eﬀect happens is because unless dhdht−t1 is exactly one, it will tend to either diminish or amplify the gradient ddht , and when this diminishment or ampliﬁcation is done repeatedly, it will have an exponential eﬀect on the gradient of the loss.22
One method to solve this problem, in the case of diminishing gradients, is the use of a neural network architecture that is speciﬁcally designed to ensure that the derivative of the recurrent function is exactly one. A neural network architecture designed for this very purpose, which has enjoyed quite a bit of success and popularity in a wide variety of sequential processing tasks, is the long short-term memory (LSTM; [49]) neural network architecture. The most fundamental idea behind the LSTM is that in addition to the standard hidden state h used by most neural networks, it also has a memory cell c, for which the gradient dcdtc−t 1 is exactly one. Because this gradient is exactly one, information stored in the memory cell does not suﬀer from vanishing gradients, and thus LSTMs can capture long-distance dependencies more eﬀectively than standard recurrent neural networks.
So how do LSTMs do this? To understand this, let’s take a look at the LSTM architecture in detail, as shown in Figure 17 and the following equations:

ut = tanh(Wxuxt + Whuht−1 + bu)

(47)

it = σ(Wxixt + Whiht−1 + bi)

(48)

ot = σ(Wxoxt + Whoht−1 + bo)

(49)

ct = it ut + ct−1

(50)

ht = ot tanh(ct).

(51)

Taking the equations one at a time: Equation 47 is the update, which is basically the same as the RNN update in Equation 44; it takes in the input and hidden state, performs an aﬃne transform and runs it through the tanh non-linearity.
Equation 48 and Equation 49 are the input gate and output gate of the LSTM respectively. The function of “gates”, as indicated by their name, is to either allow information to pass through or block it from passing. Both of these gates perform an aﬃne transform
22This is particularly detrimental in the case where we receive a loss only once at the end of the sentence, like the example above. One real-life example of such a scenario is document classiﬁcation, and because of this, RNNs have been less successful in this task than other methods such as convolutional neural networks, which do not suﬀer from the vanishing gradient problem [59, 63]. It has been shown that pre-training an RNN as a language model before attempting to perform classiﬁcation can help alleviate this problem to some extent [29].

31

ct-1

+

ct

ht-1 u: tanh(×+) ⊙

tanh ⊙

ht

i: σ(×+)

o: σ(×+)

xt
update u: what value do we try to add to the memory cell? input i: how much of the update do we allow to go through? output o: how much of the cell do we reflect in the next state?

Figure 17: A single time step of long short-term memory (LSTM). The information ﬂow between the h and cell c is modulated using parameterized input and output gates.

followed by the sigmoid function, also called the logistic function23

1

σ(x) =

,

(52)

1 + exp(−x)

which squashes the input between 0 (which σ(x) will approach as x becomes more negative) and 1 (which σ(x) will approach as x becomes more positive). The output of the sigmoid is then used to perform a componentwise multiplication

z=x y zi = xi ∗ yi

with the output of another function. This results in the “gating” eﬀect: if the result of the
sigmoid is close to one for a particular vector position, it will have little eﬀect on the input
(the gate is “open”), and if the result of the sigmoid is close to zero, it will block the input,
setting the resulting value to zero (the gate is “closed”).
Equation 50 is the most important equation in the LSTM, as it is the equation that implements the intuition that dcdtc−t 1 must be equal to one, which allows us to conquer the vanishing gradient problem. This equation sets ct to be equal to the update ut modulated by the input gate it plus the cell value for the previous time step ct−1. Because we are directly adding ct−1 to ct, if we consider only this part of Equation 50, we can easily conﬁrm that the gradient will indeed be one.24
Finally, Equation 51 calculates the next hidden state of the LSTM. This is calculated by
using a tanh function to scale the cell value between -1 and 1, then modulating the output
23 To be more accurate, the sigmoid function is actually any mathematical function having an s-shaped curve, so the tanh function is also a type of sigmoid function. The logistic function is also a slightly broader class of functions f (x) = 1+exp(−Lk(x−x0)) . However, in the machine learning literature, the “sigmoid” is usually used to refer to the particular variety in Equation 52.
24In actuality it ut is also aﬀected by ct−1, and thus dcdtc−t 1 is not exactly one, but the eﬀect is relatively indirect. Especially for vector elements with it close to zero, the eﬀect will be minimal.

32

using the output gate value ot. This will be the value actually used in any downstream calculation, such as the calculation of language model probabilities.

pt = softmax(Whsht + bs).

(53)

6.4 Other RNN Variants
Because of the importance of recurrent neural networks in a number of applications, many variants of these networks exist. One modiﬁcation to the standard LSTM that is used widely (in fact so widely that most people who refer to “LSTM” are now referring to this variant) is the addition of a forget gate [38]. The equations for the LSTM with a forget gate are shown below:

ut = tanh(Wxuxt + Whuht−1 + bu)

it = σ(Wxixt + Whiht−1 + bi)

ft = σ(Wxf xt + Whf ht−1 + bf )

(54)

ot = σ(Wxoxt + Whoht−1 + bo)

ct = it ut + ft ct−1

(55)

ht = ot tanh(ct).

Compared to the standard LSTM, there are two changes. First, in Equation 54, we add an additional gate, the forget gate. Second, in Equation 55, we use the gate to modulate the passing of the previous cell ct−1 to the current cell ct. This forget gate is useful in that it allows the cell to easily clear its memory when justiﬁed: for example, let’s say that the model has remembered that it has seen a particular word strongly correlated with another word, such as “he” and “himself” or “she” and “herself” in the example above. In this case, we would probably like the model to remember “he” until it is used to predict “himself”, then forget that information, as it is no longer relevant. Forget gates have the advantage of allowing this sort of ﬁne-grained information ﬂow control, but they also come with the risk that if ft is set to zero all the time, the model will forget everything and lose its ability to handle long-distance dependencies. Thus, at the beginning of neural network training, it is common to initialize the bias bf of the forget gate to be a somewhat large value (e.g. 1), which will make the neural net start training without using the forget gate, and only gradually start forgetting content after the net has been trained to some extent.
While the LSTM provides an eﬀective solution to the vanishing gradient problem, it is also rather complicated (as many readers have undoubtedly been feeling). One simpler RNN variant that has nonetheless proven eﬀective is the gated recurrent unit (GRU; [24]), expressed in the following equations:

rt = σ(Wxrxt + Whrht−1 + br)

(56)

zt = σ(Wxzxt + Whzht−1 + bz)

(57)

h˜ t = tanh(Wxhxt + Whh(rt ht−1) + bh)

(58)

ht = (1 − zt)ht−1 + zth˜ t.

(59)

The most characteristic element of the GRU is Equation 59, which interpolates between a candidate for the updated hidden state h˜ t and the previous state h˜ t−1. This interpolation is

33

modulated by an update gate zt (Equation 57), where if the update gate is close to one, the GRU will use the new candidate hidden value, and if the update is close to zero, it will use the previous value. The candidate hidden state is calculated by Equation 58, which is similar to a standard RNN update but includes an additional modulation of the hidden state input by a reset gate rt calculated in Equation 56. Compared to the LSTM, the GRU has slightly fewer parameters (it performs one less parameterized aﬃne transform) and also does not have a separate concept of a “cell”. Thus, GRUs have been used by some to conserve memory or computation time.

(a) A stacked RNN

(b) With residual connections

h3,0 RNN RNN RNN

h2,0 RNN RNN RNN

h1,0 RNN RNN RNN

x1

x2

x3

+

+

+

h3,0 RNN RNN RNN

+

+

+

h2,0 RNN RNN RNN

+

+

+

h1,0 RNN RNN RNN

x1

x2

x3

Figure 18: An example of (a) stacked RNNs and (b) stacked RNNs with residual connections.
One other important modiﬁcation we can do to RNNs, LSTMs, GRUs, or really any other neural network layer is simple but powerful: stack multiple layers on top of each other (stacked RNNs Figure 18(a)). For example, in a 3-layer stacked RNN, the calculation at time step t would look as follows:
h1,t = RNN1(xt, h1,t−1) h2,t = RNN2(h1,t, h2,t−1) h3,t = RNN3(h2,t, h3,t−1),
where hn,t is the hidden state for the nth layer at time step t, and RNN(·) is an abbreviation for the RNN equation in Equation 44. Similarly, we could substitute this function for LSTM(·), GRU(·), or any other recurrence step. The reason why stacking multiple layers on top of each other is useful is for the same reason that non-linearities proved useful in the standard neural networks introduced in Section 5: they are able to progressively extract more abstract features of the current words or sentences. For example, [98] ﬁnd evidence that in a two-layer stacked LSTM, the ﬁrst layer tends to learn granular features of words such as part of speech tags, while the second layer learns more abstract features of the sentence such as voice or tense.
While stacking RNNs has potential beneﬁts, it also has the disadvantage that it suﬀers from the vanishing gradient problem in the vertical direction, just as the standard RNN did in the horizontal direction. That is to say, the gradient will be back-propagated from the layer

34

close to the output (RNN3) to the layer close to the input (RNN1), and the gradient may vanish in the process, causing the earlier layers of the network to be under-trained. A simple solution to this problem, analogous to what the LSTM does for vanishing gradients over time, is residual networks (Figure 18(b)) [47]. The idea behind these networks is simply to add the output of the previous layer directly to the result of the next layer as follows:
h1,t = RNN1(xt, h1,t−1) + xt h2,t = RNN2(h1,t, h2,t−1) + h1,t h3,t = RNN3(h2,t, h3,t−1) + h2,t.
As a result, like the LSTM, there is no vanishing of gradients due to passing through the RNN(·) function, and even very deep networks can be learned eﬀectively.

6.5 Online, Batch, and Minibatch Training
As the observant reader may have noticed, the previous sections have gradually introduced more and more complicated models; we started with a simple linear model, added a hidden layer, added recurrence, added LSTM, and added more layers of LSTMs. While these more expressive models have the ability to model with higher accuracy, they also come with a cost: largely expanded parameter space (causing more potential for overﬁtting) and more complicated operations (causing much greater potential computational cost). This section describes an eﬀective technique to improve the stability and computational eﬃciency of training these more complicated networks, minibatching.
Up until this point, we have used the stochastic gradient descent learning algorithm introduced in Section 4.2 that performs updates according to the following iterative process. This type of learning, which performs updates a single example at a time is called online learning.

Algorithm 1 A fully online training algorithm

1: procedure Online

2: for several epochs of training do

3:

for each training example in the data do

4:

Calculate gradients of the loss

5:

Update the parameters according to this gradient

6:

end for

7: end for

8: end procedure

In contrast, we can also think of a batch learning algorithm, which treats the entire data set as a single unit, calculates the gradients for this unit, then only performs update after making a full pass through the data.
These two update strategies have trade-oﬀs.
• Online training algorithms usually ﬁnd a relatively good solution more quickly, as they don’t need to make a full pass through the data before performing an update.
• However, at the end of training, batch learning algorithms can be more stable, as they are not overly inﬂuenced by the most recently seen training examples.

35

Algorithm 2 A batch learning algorithm

1: procedure Batch

2: for several epochs of training do

3:

for each training example in the data do

4:

Calculate and accumulate gradients of the loss

5:

end for

6:

Update the parameters according to the accumulated gradient

7: end for

8: end procedure

Operations w/o Minibatching W x1 b

W x2 b

W x3 b

tanh(

+ ) tanh(

+ ) tanh(

+)

Operations with Minibatching x1 x2 x3 concat WX

broadcast b B

tanh(

+

)

Figure 19: An example of combining multiple operations together when minibatching.
• Batch training algorithms are also more prone to falling into local optima; the randomness in online training algorithms often allows them to bounce out of local optima and ﬁnd a better global solution.
Minibatching is a happy medium between these two strategies. Basically, minibatched training is similar to online training, but instead of processing a single training example at a time, we calculate the gradient for n training examples at a time. In the extreme case of n = 1, this is equivalent to standard online training, and in the other extreme where n equals the size of the corpus, this is equivalent to fully batched training. In the case of training language models, it is common to choose minibatches of n = 1 to n = 128 sentences to process at a single time. As we increase the number of training examples, each parameter update becomes more informative and stable, but the amount of time to perform one update increases, so it is common to choose an n that allows for a good balance between the two.
One other major advantage of minibatching is that by using a few tricks, it is actually possible to make the simultaneous processing of n training examples signiﬁcantly faster than processing n diﬀerent examples separately. Speciﬁcally, by taking multiple training examples and grouping similar operations together to be processed simultaneously, we can realize large gains in computational eﬃciency due to the fact that modern hardware (particularly GPUs, but also CPUs) have very eﬃcient vector processing instructions that can be exploited with appropriately structured inputs. As shown in Figure 19, common examples of this in neural networks include grouping together matrix-vector multiplies from multiple examples into a single matrix-matrix multiply or performing an element-wise operation (such as tanh) over

36

Input:

<s> <s>
lookup

that this
lookup

is is
lookup

an example another </s>
lookup lookup

Recurrence: Estimate:
Loss:
Masking:

RNN softmax

RNN softmax

RNN softmax

RNN softmax

RNN softmax

loss( that this
) ⊙

loss( is is
) ⊙

loss( an
another )

loss(
example
</s> )

loss( </s> </s>
)

⊙

⊙

⊙

Sum Time Steps:

+
Final Loss

Figure 20: An example of minibatching in an RNN language model.

multiple vectors at the same time as opposed to processing single vectors individually. Luckily, in DyNet, the library we are using, this is relatively easy to do, as much of the machinery for each elementary operation is handled automatically. We’ll give an example of the changes that we need to make when implementing an RNN language model below.
The basic idea in the batched RNN language model (Figure 20) is that instead of processing a single sentence, we process multiple sentences at the same time. So, instead of looking up a single word embedding, we look up multiple word embeddings (in DyNet, this is done by replacing the lookup function with the lookup batch function, where we pass in an array of word IDs instead of a single word ID). We then run these batched word embeddings through the RNN and softmax as normal, resulting in two separate probability distributions over words in the ﬁrst and second sentences. We then calculate the loss for each word (again in DyNet, replacing the pickneglogsoftmax function with the pickneglogsoftmax batch function and pass word IDs). We then sum together the losses and use this as the loss for our entire sentence.
One sticking point, however, is that we may need to create batches with sentences of diﬀerent sizes, also shown in the ﬁgure. In this case, it is common to perform sentence padding and masking to make sure that sentences of diﬀerent lengths are treated properly. Padding works by simply adding the “end-of-sentence” symbol to the shorter sentences until they are of the same length as the longest sentence in the batch. Masking works by multiplying all loss functions calculated over these padded symbols by zero, ensuring that the losses for sentence end symbols don’t get counted twice for the shorter sentences.
By taking these two measures, it becomes possible to process sentences of diﬀerent lengths, but there is still a problem: if we perform lots of padding on sentences of vastly diﬀerent

37

lengths, we’ll end up wasting a lot of computation on these padded symbols. To ﬁx this problem, it is also common to sort the sentences in the corpus by length before creating mini-batches to ensure that sentences in the same mini-batch are approximately the same size.
6.6 Further Reading
Because of the prevalence of RNNs in a number of tasks both on natural language and other data, there is signiﬁcant interest in extensions to them. The following lists just a few other research topics that people are handling:
What can recurrent neural networks learn?: RNNs are surprisingly powerful tools for language, and thus many people have been interested in what exactly is going on inside them. [57] demonstrate ways to visualize the internal states of LSTM networks, and ﬁnd that some nodes are in charge of keeping track of length of sentences, whether a parenthesis has been opened, and other salietn features of sentences. [65] show ways to analyze and visualize which parts of the input are contributing to particular decisions made by an RNN-based model, by back-propagating information through the network.
Other RNN architectures: There are also quite a few other recurrent network architectures. [42] perform an interesting study where they ablate various parts of the LSTM and attempt to ﬁnd the best architecture for particular tasks. [123] take it a step further, explicitly training the model to ﬁnd the best neural network architecture.
6.7 Exercise
In the exercise for this chapter, we will construct a recurrent neural network language model using LSTMs.
Writing the program will entail:
• Writing a function such as lstm step or gru step that takes the input of the previous time step and updates it according to the appropriate equations. For reference, in DyNet, the componentwise multiply and sigmoid functions are dy.cmult and dy.logistic respectively.
• Adding this function to the previous neural network language model and measuring the eﬀect on the held-out set.
• Ideally, implement mini-batch training by using the functionality implemented in DyNet, lookup batch and pickneglogsoftmax batch.
Language modeling accuracy should be measured in the same way as previous exercises and compared with the previous models.
Potential improvements to the model include: Measuring the speed/stability improvements achieved by mini-batching. Comparing the diﬀerences between recurrent architectures such as RNN, GRU, or LSTM.
38

7 Neural Encoder-Decoder Models
From Section 3 to Section 6, we focused on the language modeling problem of calculating the probability P (E) of a sequence E. In this section, we return to the statistical machine translation problem (mentioned in Section 2) of modeling the probability P (E | F ) of the output E given the input F .

7.1 Encoder-decoder Models

The ﬁrst model that we will cover is called an encoder-decoder model [22, 36, 53, 101]. The basic idea of the model is relatively simple: we have an RNN language model, but before starting calculation of the probabilities of E, we ﬁrst calculate the initial state of the language model using another RNN over the source sentence F . The name “encoder-decoder” comes from the idea that the ﬁrst neural network running over F “encodes” its information as a vector of real-valued numbers (the hidden state), then the second neural network used to predict E “decodes” this information into the target sentence.

Encoder

Decoder

p(e) 1

p(e) 2

p(e) |E|

softmax(e) softmax(e)

softmax(e)

… … 0

RNN(f)

RNN(f)

RNN(f)

h|F| RNN(e)

RNN(e)

RNN(e)

lookup(f) lookup(f)

lookup(f)

lookup(e) lookup(e)

lookup(e)

f1

f2

f|F|

e0

e1

e|E|-1

Figure 21: A computation graph of the encoder-decoder model.

If the encoder is expressed as RNN(f)(·), the decoder is expressed as RNN(e)(·), and we have a softmax that takes RNN(e)’s hidden state at time step t and turns it into a probability,
then our model is expressed as follows (also shown in Figure 21):

m(tf ) = M·(,fft)

h(tf) =

RNN

(f

)

(m

(f t

)

,

h

(f ) t−1

)

0

t ≥ 1, otherwise.

m(te) = M·(,ee)t−1

h(te) =

RNN(e)(m(te), h(t−e)1) h(|Ff )|

p(te) = softmax(Whsh(te) + bs)

t ≥ 1, otherwise.

(60)

In the ﬁrst two lines, we look up the embedding m(tf) and calculate the encoder hidden state h(tf) for the tth word in the source sequence F . We start with an empty vector h(0f) = 0, and

39

by h(|Ff)|, the encoder has seen all the words in the source sentence. Thus, this hidden state should theoretically be able to encode all of the information in the source sentence.
In the decoder phase, we predict the probability of word et at each time step. First, we similarly look up m(te), but this time use the previous word et−1, as we must condition the probability of et on the previous word, not on itself. Then, we run the decoder to calculate h(te). This is very similar to the encoder step, with the important diﬀerence that h(0e) is set to the ﬁnal state of the encoder h(|Ff)|, allowing us to condition on F . Finally, we calculate the probability p(te) by using a softmax on the hidden state h(te).
While this model is quite simple (only 5 lines of equations), it gives us a straightforward
and powerful way to model P (E | F ). In fact, [101] have shown that a model that follows
this basic pattern is able to perform translation with similar accuracy to heavily engineered
systems specialized to the machine translation task (although it requires a few tricks over
the simple encoder-decoder that we’ll discuss in later sections: beam search (Section 7.2), a
diﬀerent encoder (Section 7.3), and ensembling (Section 7.4)).

7.2 Generating Output
At this point, we have only mentioned how to create a probability model P (E | F ) and haven’t yet covered how to actually generate translations from it, which we will now cover in the next section. In general, when we generate output we can do so according to several criteria:
Random Sampling: Randomly select an output E from the probability distribution P (E | F ). This is usually denoted Eˆ ∼ P (E | F ).
1-best Search: Find the E that maximizes P (E | F ), denoted Eˆ = argmax P (E | F ).
E
n-best Search: Find the n outputs with the highest probabilities according to P (E | F ).
Which of these methods we will choose will depend on our application, so we will discuss some use cases along with the algorithms themselves.

7.2.1 Random Sampling

First, random sampling is useful in cases where we may want to get a variety of outputs for

a particular input. One example of a situation where this is useful would be in a sequence-

to-sequence model for a dialog system, where we would prefer the system to not always give

the same response to a particular user input to prevent monotony. Luckily, in models like the

encoder-decoder above, it is simple to exactly generate samples from the distribution P (E | F )

using a method called ancestral sampling. Ancestral sampling works by sampling variable

values one at a time, gradually conditioning on more context, so at time step t, we will sample a word from the distribution P (et | eˆt1−1). In the encoder-decoder model, this means we simply have to calculate pt according to the previously sampled inputs, leading to the

simple generation algorithm in Algorithm 3.

One thing to note is that sometimes we also want to know the probability of the sentence that we sampled. For example, given a sentence Eˆ generated by the model, we might want to

know how certain the model is in its prediction. During the sampling process, we can calculate

P (Eˆ | F ) =

|Eˆ| t

P

(eˆt

|

F, Eˆ1t−1)

incrementally

by

stepping

along

and

multiplying

together

40

the probabilities of each sampled word. However, as we remember from the discussion of probability vs. log probability in Section 3.3, using probabilities as-is can result in very small numbers that cause numerical precision problems on computers. Thus, when calculating the full-sentence probability it is more common to instead add together log probabilities for each word, which avoids this problem.

Algorithm 3 Generating random samples from a neural encoder-decoder

1: procedure Sample

2: for t from 1 to |F | do

3:

Calculate m(tf) and h(tf)

4: end for

5: Set eˆ0 =“ s ” and t ← 0

6: while eˆt =“ /s ” do

7:

t←t+1

8:

Calculate m(te), h(te), and p(te) from eˆt−1

9:

Sample eˆt according to pt(e)

10: end while

11: end procedure

7.2.2 Greedy 1-best Search
Next, let’s consider the problem of generating a 1-best result. This variety of generation is useful in machine translation, and most other applications where we simply want to output the translation that the model thought was best. The simplest way of doing so is greedy search, in which we simply calculate pt at every time step, select the word that gives us the highest probability, and use it as the next word in our sequence. In other words, this algorithm is exactly the same as Algorithm 3, with the exception that on Line 9, instead of sampling eˆt randomly according to p(te), we instead choose the max: eˆt = argmax p(t,ei).
i
Interestingly, while ancestral sampling exactly samples outputs from the distribution according to P (E | F ), greedy search is not guaranteed to ﬁnd the translation with the highest probability. An example of a case in which this is true can be found in the graph in Figure 22, which is an example of search graph with a vocabulary of {a, b, /s }.25 As an exercise, I encourage readers to ﬁnd the true 1-best (or n-best) sentence according to the probability P (E | F ) and the probability of the sentence found according to greedy search and conﬁrm that these are diﬀerent.
7.2.3 Beam Search
One way to solve this problem is through the use of beam search. Beam search is similar to greedy search, but instead of considering only the one best hypothesis, we consider b best hypotheses at each time step, where b is the “width” of the beam. An example of beam search where b = 2 is shown in Figure 23 (note that we are using log probabilities here because they
25In reality, we will never have a probability of exactly P (et = /s | F, et1−1) = 1.0, but for illustrative purposes, we show this here.
41

e0 P(e1|F) e1 P(e2|F,e1) e2 P(e3|F,e1,e2) e3
“a” 1.0 </s> 0.15
0.8 “b” 1.0 </s>

0.35 “a”

0.05 </s>

0.4

0.4 “a” 1.0 </s>

<s>

“b”

0.5

0.25

0.1 “b” 1.0 </s>

</s>

</s>

Figure 22: A search graph where greedy search fails.

log P(e1|F)

log P(e2|F,e1) log P(e3|F,e1,e2)
X-2.95 “a”
-1.9-00.22-1“.b2”7 0 -1<./2s>7

X -1.05

-1.05
“a”

-3.00 -4</.s0>5

-0.92
X <s> -0.92 “b”

-0.92 -1“.a8”4 -0.69

-1.39 -1.39

-1.61 0 -1.61

</s>

“b”

</s>

X -2.30 -3.22

X</s>

Figure 23: An example of beam search with b = 2. Numbers next to arrows are log probabilities for a single word log P (et | F, et1−1), while numbers above nodes are log probabilities for the entire hypothesis up until this point.

42

are more conducive to comparing hypotheses over the entire sentence, as mentioned before).

In the ﬁrst time step, we expand hypotheses for e1 corresponding to all of the three words in

the vocabulary, then keep the top two (“b” and “a”) and delete the remaining one (“ /s ”).

In the second time step, we expand hypotheses for e2 corresponding to the continuation of the

ﬁrst hypotheses for all words in the vocabulary, temporarily creating b ∗ |V | active hypotheses.

These active hypotheses are also pruned down to the b active hypotheses (“a b” and “b b”).

This process of calculating scores for b ∗ |V | continuations of active hypotheses, then pruning

back down to the top b, is continued until the end of the sentence.

One thing to be careful about when generating sentences using models, such as neural

machine translation, where P (E | F ) =

|E| t

P

(et

|

F

,

e

t− 1

1

)

is

that

they

tend

to

prefer

shorter sentences. This is because every time we add another word, we multiply in another

probability, reducing the probability of the whole sentence. As we increase the beam size, the

search algorithm gets better at ﬁnding these short sentences, and as a result, beam search

with a larger beam size often has a signiﬁcant length bias towards these shorter sentences.

There have been several attempts to ﬁx this length bias problem. For example, it is

possible to put a prior probability on the length of the sentence given the length of the

previous sentence P (|E| | |F |), and multiply this with the standard sentence probability

P (E | F ) at decoding time [34]:

Eˆ = argmax log P (|E| | |F |) + log P (E | F ).

(61)

E

This prior probability can be estimated from data, and [34] simply estimate this using a multinomial distribution learned on the training data:

c(|E|, |F |)

P (|E| | |F |) =

.

(62)

c(|F |)

A more heuristic but still widely used approach normalizes the log probability by the length of the target sentence, eﬀectively searching for the sentence that has the highest average log probability per word [21]:

Eˆ = argmax log P (E | F )/|E|.

(63)

E

7.3 Other Ways of Encoding Sequences
In Section 7.1, we described a model that works by encoding sequences linearly, one word at a time from left to right. However, this may not be the most natural or eﬀective way to turn the sentence F into a vector h. In this section, we’ll discuss a number of diﬀerent ways to perform encoding that have been reported to be eﬀective in the literature.

7.3.1 Reverse and Bidirectional Encoders

First, [101] have proposed a reverse encoder. In this method, we simply run a standard linear encoder over F , but instead of doing so from left to right, we do so from right to left.

←−(f )

← R−N−−N(f ) (m(f ) ,

←−(f ) h

)

t ≤ |F |,

ht =

t

t+1

(64)

0

otherwise.

43

(a) Dependency Distances in Forward Encoder

|F|

|F|

|F|

e1

e2

e|E|

0 enc enc … enc dec dec … enc

f1

f2

f|F|

e0

e1

e|E|-1

(b) Dependency Distances in Reverse Encoder

|F|+|E|-1 3 1 e1 e2 e|E|

0 enc enc … enc dec dec … enc

f|F|

f|F|-1

f1

e0

e1

e|E|-1

Figure 24: The distances between words with the same index in the forward and reverse decoders.

The motivation behind this method is that for pairs of languages with similar ordering (such as English-French, which the authors experimented on), the words at the beginning of F will generally correspond to words at the beginning of E. Assuming the extreme case that words with identical indices correspond to each-other (e.g. f1 corresponds to e1, f2 to e2, etc.), the distance between corresponding words in the linear encoding and decoding will be |F |, as shown in Figure 24(a). Remembering the vanishing gradient problem from Section 6.3, this means that the RNN has to propagate the information across |F | time steps before making a prediction, a diﬃcult feat. At the beginning of training, even RNN variants such as LSTMs have trouble, as they have to essentially “guess” what part of the information encoded in their hidden state is being used without any prior bias.
Reversing the encoder helps solve this problem by reducing the length of dependencies for a subset of the words in the sentence, speciﬁcally the ones at the beginning of the sentences. As shown in Figure 24(b), the length of the dependency for f1 and e1 is 1, and subsequent pairs of ft and et have a distance of 2t − 1. During learning, the model can “latch on” to these short-distance dependencies and use them as a way to bootstrap the model training, after which it becomes possible to gradually learn the longer dependencies for the words at the end of the sentence. In [101], this proved critical to learn eﬀective models in the encoder-decoder framework.
However, this approach of reversing the encoder relies on the strong assumption that the order of words in the input and output sequences are very similar, or at least that the words at the beginning of sentences are the same. This is true for languages like English and French, which share the same “subject-verb-object (SVO)” word ordering, but may not be true for more typologically distinct languages. One type of encoder that is slightly more robust to these diﬀerences is the bi-directional encoder [4]. In this method, we use two diﬀerent

44

(a) Convolutional Neural Net
h1 h2 h3 … h|F|-2 pool h

filt filt filt

filt

m1 m2 m3 m4 m5 … m|F|

(b) Tree-structured Net
h

comp

comp

comp

…

comp

comp

m1 m2 m3 m4 m5 … m|F|

Figure 25: Examples of convolutional and tree-structured networks.

encoders: one traveling forward and one traveling backward over the input sentence

→− (f)

−R−N−→N(f

)

(m

(f

)

,

→− h

(f

)

)

t ≥ 1,

ht =

t

t+−

(65)

0

otherwise.

←−(f )

← R−N−−N(f ) (m(f ) ,

←−(f ) h

)

t ≤ |F |,

ht =

t

t+1

(66)

0

otherwise.

which are then combined into the initial vector h(0e) for the decoder RNN. This combination

→−

←−

can be done by simply concatenating the two ﬁnal vectors h |F | and h 1. However, this also

requires that the size of the vectors for the decoder RNN be exactly equal to the combined

size of the two encoder RNNs. As a more ﬂexible alternative, we can add an additional

parameterized hidden layer between the encoder and decoder states, which allows us to convert

the bidirectional encoder states into an appropriately-sized state for the decoder:

(e)

→−

←−

h0

=

tanh(W−→
fe

h

|F

|

+

W←−
fe

h

1

+

be).

(67)

7.3.2 Convolutional Neural Networks

In addition, there are also methods for decoding that move beyond a simple linear view of

the input sentence. For example, convolutional neural networks (CNNs; [37, 114, 62]),

Figure 25(a)) are a variety of neural net that combines together information from spatially

or temporally local segments. They are most widely applied to image processing but have

also been used for speech processing, as well as the processing of textual sequences. While

there are many varieties of CNN-based models of text (e.g. [55, 63, 54]), here we will show

an example from [59]. This model has n ﬁlters with a width w that are passed incrementally

over w-word segments of the input. Speciﬁcally, given an embedding matrix M of width |F |,

we generate a hidden layer matrix H of width |F | − w + 1, where each column of the matrix

is equal to

ht = W concat(mt, mt+1, . . . , mt+w−1)

(68)

45

S
NP NP'

VP NP NP'

DET JJ NN VBD DET JJ NN the red cat chased the little bird

Figure 26: An example of a syntax tree for a sentence showing the sentence structure and phrase types (DET=“determiner”, JJ=“adjective”, NN=“noun”, VBD=“past tense verb”, NP=“noun phrase”, NP’=“part of a noun phrase”, VP=“verb phrase”, S=“sentence”).

where W ∈ Rn×w|m| is a matrix where the ith row represents the parameters of ﬁlter i that will be multiplied by the embeddings of w consecutive words. If w = 3, we can interpret this as h1 extracting a vector of features for f13, h2 as extracting a vector of features for f24, etc. until the end of the sentence.
Finally, we perform a pooling operation that converts this matrix H (which varies in width according to the sentence length) into a single vector h (which is ﬁxed-size and can thus be used in down-stream processing). Examples of pooling operations include average, max, and k-max [55].
Compared to RNNs and their variants, CNNs have several advantages and disadvantages:
• On the positive side, CNNs provide a relatively simple way to detect features of short word sequences in sentence text and accumulate them across the entire sentence.
• Also on the positive side, CNNs do not suﬀer as heavily from the vanishing gradient problem, as they do not need to propagate gradients across multiple time steps.
• On the negative side, CNNs are not quite as expressive and are a less natural way of expressing complicated patterns that move beyond their ﬁlter width.
In general, CNNs have been found to be quite eﬀective for text classiﬁcation, where it is more important to pick out the most indicative features of the text and there is less of an emphasis on getting an overall view of the content [59]. There have also been some positive results reported using speciﬁc varieties of CNNs for sequence-to-sequence modeling [54].
7.3.3 Tree-structured Networks
Finally, one other popular form of encoder that is widely used in a number of tasks are treestructured networks ([83, 100], Figure 25(b)). The basic idea behind these networks is that the way to combine the information from each particular word is guided by some sort of structure, usually the syntactic structure of the sentence, an example of which is shown in Figure 26. The reason why this is intuitively useful is because each syntactic phrase usually also corresponds to a coherent semantic unit. Thus, performing the calculation and manipulation of vectors over these coherent units will be more appropriate compared to using random substrings of words, like those used by CNNs.

46

For example, let’s say we have the phrase “the red cat chased the little bird” as shown

in the ﬁgure. In this case, following a syntactic tree would ensure that we calculate vectors

for coherent units that correspond to a grammatical phrase such as “chased” and “the little

bird”, and combine these phrases together one by one to obtain the meaning of larger coherent

phrase such as “chased the little bird”. By doing so, we can take advantage of the fact

that language is compositional, with the meaning of a more complex phrase resulting from

regular combinations and transformation of smaller constituent phrases [102]. By taking

this linguistically motivated and intuitive view of the sentence, we hope will help the neural

networks learn more generalizable functions from limited training data.

Perhaps the most simple type of tree-structured network is the recursive neural net-

work proposed by [100]. This network has very strong parallels to standard RNNs, but

instead of calculating the hidden state ht at time t from the previous hidden state ht−1 as

follows:

ht = tanh(Wxhxt + Whhht−1 + bh),

(69)

we instead calculate the hidden state of the parent node hp from the hidden states of the left and right children, hl and hr respectively:

hp = tanh(Wxpxt + Wlphl + Wrphr + bp).

(70)

Thus, the representation for each node in the tree can be calculated in a bottom-up fashion. Like standard RNNs, these recursive networks suﬀer from the vanishing gradient problem.
To ﬁx this problem there is an adaptation of LSTMs to tree-structured networks, ﬁttingly called tree LSTMs [103], which ﬁxes this vanishing gradient problem. There are also a wide variety of other kinds of tree-structured composition functions that interested readers can explore [99, 31, 32]. Also of interest is the study by [66], which examines the various tasks in which tree structures are necessary or unnecessary for NLP.

7.4 Ensembling Multiple Models
One other method that is widely used in encoder-decoders, or other models of translation is ensembling: the combination of the prediction of multiple independently trained models to improve the overall prediction results. The intuition behind ensembling is that diﬀerent models will make diﬀerent mistakes, and that on average it is more common for models to agree when the answer is correct than when it is mistaken. Thus, if we combine multiple models together, it becomes possible to smooth over these mistakes, ﬁnding the correct answer more often.
The ﬁrst step in ensembling encoder-decoder models is to independently train N diﬀerent models P1(·), P2(·), . . . , PN (·), for example, by randomly initializing the weights of the neural network diﬀerently before training. Next, during search, at each time step we calculate the probability of the next word as the average probability of the N models:

t−1

1N

t−1

P (et | F, e1

)= N

Pi(et | F, e1 ).

(71)

i=1

This probability is used in searching for our hypotheses.

47

7.5 Exercise
In the exercise for this chapter, we will create an encoder-decoder translation model and make it possible to generate translations.
Writing the program will entail:
• Extend your RNN language model code to ﬁrst read in a source sentence to calculate the initial hidden state.
• On the training set, write code to calculate the loss function and perform training.
• On the development set, generate translations using greedy search.
• Evaluate your generated translations by comparing them to the reference translations to see if they look good or not. Translations can also be evaluated by automatic means, such as BLEU score [81]. A reference implementation of a BLEU evaluation script can be found here: https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl.
Potential improvements to the model include: Implementing beam search and comparing the results with greedy search. Implementing an alternative encoder. Implementing ensembling.

8 Attentional Neural MT
In the past chapter, we described a simple model for neural machine translation, which uses an encoder to encode sentences as a ﬁxed-length vector. However, in some ways, this view is overly simpliﬁed, and by the introduction of a powerful mechanism called attention, we can overcome these diﬃculties. This section describes the problems with the encoder-decoder architecture and what attention does to ﬁx these problems.

8.1 Problems of Representation in Encoder-Decoders

Theoretically, a suﬃciently large and well-trained encoder-decoder model should be able to

perform machine translation perfectly. As mentioned in Section 5.2, neural networks are

universal function approximators, meaning that they can express any function that we wish

to model, including a function that accurately predicts our predictive probability for the next

word

P (et

|

F

,

e

t− 1

1

).

However,

in

practice,

it

is

necessary

to

learn

these

functions

from

limited

data, and when we do so, it is important to have a proper inductive bias – an appropriate

model structure that allows the network to learn to model accurately with a reasonable amount

of data.

There are two things that are worrying about the standard encoder-decoder architecture.

The ﬁrst was described in the previous section: there are long-distance dependencies between

words that need to be translated into each other. In the previous section, this was alleviated

to some extent by reversing the direction of the encoder to bootstrap training, but still, a

large number of long-distance dependencies remain, and it is hard to guarantee that we will

learn to handle these properly.

48

The second, and perhaps more, worrying aspect of the encoder-decoder is that it attempts to store information sentences of any arbitrary length in a hidden vector of ﬁxed size. In other words, even if our machine translation system is expected to translate sentences of lengths from 1 word to 100 words, it will still use the same intermediate representation to store all of the information about the input sentence. If our network is too small, it will not be able to encode all of the information in the longer sentences that we will be expected to translate. On the other hand, even if we make the network large enough to handle the largest sentences in our inputs, when processing shorter sentences, this may be overkill, using needlessly large amounts of memory and computation time. In addition, because these networks will have large numbers of parameters, it will be more diﬃcult to learn them in the face of limited data without encountering problems such as overﬁtting.
The remainder of this section discusses a more natural way to solve the translation problem with neural networks: attention.

8.2 Attention

The basic idea of attention is that instead of attempting to learn a single vector representation for each sentence, we instead keep around vectors for every word in the input sentence, and reference these vectors at each decoding step. Because the number of vectors available to reference is equivalent to the number of words in the input sentence, long sentences will have many vectors and short sentences will have few vectors. As a result, we can express input sentences in a much more eﬃcient way, avoiding the problems of ineﬃcient representations for encoder-decoders mentioned in the previous section.
First we create a set of vectors that we will be using as this variably-lengthed representation. To do so, we calculate a vector for every word in the source sentence by running an RNN in both directions:

→− (f)

→− (f)

h j = RNN(embed(fj), h j−1)

←−(f )

←−(f )

h j = RNN(embed(fj), h j+1).

→− (f)

←−(f )

(f )

Then we concatenate the two vectors h j and h j into a bidirectional representation hj

(f) ←−(f) →− (f) hj = [ h j ; h j ].

We can further concatenate these vectors into a matrix:

H (f )

=

concat

col(

h

(f 1

)

,

.

.

.

,

h

(f |F

) |

).

This will give us a matrix where every column corresponds to one word in the input sentence.

However, we are now faced with a diﬃculty. We have a matrix H(f) with a variable

number of columns depending on the length of the source sentence, but would like to use this

to compute, for example, the probabilities over the output vocabulary, which we only know

how to do (directly) for the case where we have a vector of input. The key insight of attention

is that we calculate a vector αt that can be used to combine together the columns of H into

a vector ct

ct = H(f)αt.

(72)

49

Figure 27: An example of attention from [4]. English is the source, French is the target, and a higher attention weight when generating a particular target word is indicated by a lighter color in the matrix.

αt is called the attention vector, and is generally assumed to have elements that are between zero and one and add to one.
The basic idea behind the attention vector is that it is telling us how much we are “focusing” on a particular source word at a particular time step. The larger the value in αt, the more impact a word will have when predicting the next word in the output sentence. An example of how this attention plays out in an actual translation example is shown in Figure 27, and as we can see the values in the alignment vectors generally align with our intuition.

8.3 Calculating Attention Scores

The next question then becomes, from where do we get this αt? The answer to this lies in
the decoder RNN, which we use to track our state while we are generating output. As before, the decoder’s hidden state h(te) is a ﬁxed-length continuous vector representing the previous target words et1−1, initialized as h(0e) = h(|Ff)|+1. This is used to calculate a context vector ct that is used to summarize the source attentional context used in choosing target word et, and
initialized as c0 = 0. First, we update the hidden state to h(te) based on the word representation and context
vectors from the previous target time step

h(te) = enc([embed(et−1); ct−1], h(t−e)1).

(73)

Based on this h(te), we calculate an attention score at, with each element equal to

at,j

=

attn

score(

h

(f j

)

,

h

(e) t

).

(74)

50

this
lookup

is
lookup

a
lookup

pen
lookup

0 RNN

RNN

RNN

RNN

RNN

RNN

RNN

RNN 0

concat concat concat concat

concat_col ht attn_score softmax x
concat softmax(x+) pt

Figure 28: A computation graph for attention.

attn score(·) can be an arbitrary function that takes two vectors as input and outputs a score

about how much we should focus on this particular input word encoding h(jf) at the time step h(te). We describe some examples at a later point in Section 8.4.
We then normalize this into the actual attention vector itself by taking a softmax over the

scores:

αt = softmax(at).

(75)

This attention vector is then used to weight the encoded representation H(f) to create a
context vector ct for the current time step, as mentioned in Equation 72. We now have a context vector ct and hidden state h(te) for time step t, which we can pass
on down to downstream tasks. For example, we can concatenate both of these together when
calculating the softmax distribution over the next words:

p(te) = softmax(Whs[h(te); ct] + bs).

(76)

It is worth noting that this means that the encoding of each source word h(jf) is considered much more directly in the calculation of output probabilities. In contrast to the encoderdecoder, where the encoder-decoder will only be able to access information about the ﬁrst encoded word in the source by passing it over |F | time steps, here the source encoding is accessed (in a weighted manner) through the context vector Equation 72.
This whole, rather involved, process is shown in Figure 28.

8.4 Ways of Calculating Attention Scores
As mentioned in Equation 74, the ﬁnal missing piece to the puzzle is how to calculate the attention score at,j.
[68] test three diﬀerent attention functions, all of which have their own merits:

51

Dot product: This is the simplest of the functions, as it simply calculates the similarity between h(te) and h(jf) as measured by the dot product:

attn

score(

h(jf

)

,

h

(e t

)

)

:=

h(jf )

h(te).

(77)

This model has the advantage that it adds no additional parameters to the model.
However, it also has the intuitive disadvantage that it forces the input and output encodings to be in the same space (because similar h(te) and h(jf) must be close in space in order for their dot product to be high). It should also be noted that the dot product
can be calculated eﬃciently for every word in the source sentence by instead deﬁning the attention score over the concatenated matrix H(f) as follows:

attn

score(

H

(

f

)

,

h

(e t

)

)

:=

Hj(f )

h(te).

(78)

Combining the many attention operations into one can be useful for eﬃcient impemen-
tation, especially on GPUs. The following attention functions can also be calculated like this similarly.26

Bilinear functions: One slight modiﬁcation to the dot product that is more expressive is the bilinear function. This function helps relax the restriction that the source and target embeddings must be in the same space by performing a linear transform parameterized by Wa before taking the dot product:

attn

score(

h

(f j

)

,

h(t

e)

)

:=

h(jf )

Wah(te).

(79)

This has the advantage that if Wa is not a square matrix, it is possible for the two vectors to be of diﬀerent sizes, so it is possible for the encoder and decoder to have
diﬀerent dimensions. However, it does introduce quite a few parameters to the model (|h(f)| × |h(e)|), which may be diﬃcult to train properly.

Multi-layer perceptrons: Finally, it is also possible to calculate the attention score using a multi-layer perceptron, which was the method employed by [4] in their original implementation of attention:

attn

score(

h

(e t

)

,

h

(f j

)

)

:=

wa2

tanh(W

a1

[h

(e) t

;

h

(f j

)

]),

(80)

where Wa1 and wa2 are the weight matrix and vector of the ﬁrst and second layers of the MLP respectively. This is more ﬂexible than the dot product method, usually has fewer parameters than the bilinear method, and generally provides good results.

In addition to these methods above, which are essentially the defacto-standard, there are a few more sophisticated methods for calculating attention as well. For example, it is possible to use recurrent neural networks [120], tree-structured networks based on document structure [121], convolutional neural networks [2], or structured models [58] to calculate attention.
26Question: What do the equations look like for the combined versions of the following functions?

52

8.5 Copying and Unknown Word Replacement
One pleasant side-eﬀect of attention is that it not only increases translation accuracy, but also makes it easier to tell which words are translated into which words in the output. One obvious consequence of this is that we can draw intuitive graphs such as the one shown in Figure 27, which aid error analysis.
Another advantage is that it also becomes possible to handle unknown words in a more elegant way, performing unknown word replacement [67]. The idea of this method is simple, every time our decoder chooses the unknown word token unk in the output, we look up the source word with the highest attention weight at this time step, and output that word instead of the unknown token unk . If we do so, at least the user can see which words have been left untranslated, which is better than seeing them disappear altogether or be replaced by a placeholder.
It is also common to use alignment models such as those described in [16] to obtain a translation dictionary, then use this to aid unknown word replacement even further. Specifically, instead of copying the word as-is into the output, if the chosen source word is f , we output the word with the highest translation probability Pdict(e | f ). This allows words that are included in the dictionary to be mapped into their most-frequent counterpart in the target language.
8.6 Intuitive Priors on Attention
Because of the importance of attention in modern NMT systems, there have also been a number of proposals to improve accuracy of estimating the attention itself through the introduction of intuitively motivated prior probabilities. [25] propose several methods to incorporate biases into the training of the model to ensure that the attention weights match our belief of what alignments between languages look like.
These take several forms, and are heavily inspired by the alignment models used in more traditional SMT systems such as those proposed by [16]. These models can be brieﬂy summarized as:
Position Bias: If two languages have similar word order, then it is more likely that alignments should fall along the diagonal. This is demonstrated strongly in Figure 27. It is possible to encourage this behavior by adding a prior probability over attention that makes it easier for things near the diagonal to be aligned.
Markov Condition: In most languages, we can assume that most of the time if two words in the target are contiguous, the aligned words in the source will also be contiguous. For example, in Figure 27, this is true for all contiguous pairs of English words except “the, European” and “Area, was”. To take advantage of this property, it is possible to impose a prior that discourages large jumps and encourages local steps in attention. A model that is similar in motivation, but diﬀerent in implementation, is the local attention model [68], which selects which part of the source sentence to focus on using the neural network itself.
Fertility: We can assume that some words will be translated into a certain number words in the other langauge. For example, the English word “cats” will be translated into two words “les chats” in French. Priors on fertility takes advantage of this fact by giving
53

the model a penalty when particular words are not attended too much, or attended to too much. In fact one of the major problems with poorly trained neural MT systems is that they repeat the same word over and over, or drop words, a violation of this fertility constraint. Because of this, several other methods have been proposed to incorporate coverage in the model itself [106, 69], or as a constraint during the decoding process [119].
Bilingual Symmetry: Finally, we expect that words that are aligned when performing translation from F to E should also be aligned when performing translation from E to F . This can be enforced by training two models in parallel, and enforcing constraints that the alignment matrices look similar in both directions.
[25] experiment extensively with these approaches, and ﬁnd that the bilingual symmetry constraint is particularly eﬀective among the various methods.
8.7 Further Reading
This section outlines some further directions for reading more about improvements to attention:
Hard Attention: As shown in Equation 75, standard attention uses a soft combination of various contents. There are also methods for hard attention that make a hard binary decision about whether to focus on a particular context, with motivations ranging from learning explainable models [64], to processing text incrementally [122, 45].
Supervised Training of Attention: In addition, sometimes we have hand-annotated data showing us true alignments for a particular language pair. It is possible to train attentional models using this data by deﬁning a loss function that penalizes the model when it does not predict these alignments correctly [70].
Other Ways of Memorizing Input: Finally, there are other ways of accessing relevant information other than attention. [115] propose a method using memory networks, which have a separate set of memory that can be written to or read from as the processing continues.
8.8 Exercise
In the exercise for this chapter, we will create code to train and generate translations with an attentional neural MT model.
Writing the program will entail extending your encoder-decoder code to add attention. You can then generate translations and compare them to others.
• Extend your encoder-decoder code to add attention.
• On the training set, write code to calculate the loss function and perform training.
• On the development set, generate translations using greedy search.
• Evaluate these translations, either manually or automatically.
54

It is also highly recommended, but not necessary, that you attempt to implement unknown word replacement.
Potential improvements to the model include implementing any of the improvements to attention mentioned in Section 8.6 or Section 8.7.
9 Conclusion
This tutorial has covered the basics of neural machine translation and sequence-to-sequence models. It gradually stepped through models of increasing sophistication, starting with ngram language models, and culminating in attention, which now represents the state-of-the-art in many sequence-to-sequence modeling tasks.
It should be noted that this is a very active reserach ﬁeld, and there are a number of advanced research topics that are beyond the scope of this tutorial, but may be of interest to readers who have mastered the basics and would like to learn more.
Handling large vocabularies: One diﬃculty of neural MT models is that they perform badly when using large vocabularies; it is hard to learn how to properly translate rare words with limited data, and computation becomes a burden. One method to handle this is to break words into smaller units such as characters [23] or subwords [94]. It is also possible to incorporate translation dictionaries with broad coverage to handle low-frequency phenomena [3].
Optimizing translation performance: While the models presented in this tutorial are trained to maximize the likelihood of the target sentence given the source P (E | F ), in reality what we actually care about is the accuracy of the generated sentences. There have been a number of works proposed to resolve this disconnect by directly considering the accuracy of the generated results when training our models. These include methods that sample translation results from the current model and move towards parameters that result in good translations [84, 97], methods that optimize parameters towards partially mistaken hypotheses to try to improve robustness to mistakes in generation [8, 79], or methods that try to prevent mistakes that may occur during the search process [117].
Multi-lingual learning: Up until now we assumed that we were training a model between two languages F and E. However, in reality there are many languages in the world, and some work has shown that we can beneﬁt by using data from all these languages to learn models together [35, 52, 46]. It is also possible to perform transfer across languages, training a model ﬁrst on one language pair, then ﬁne-tuning it to others [124].
Other applications: Similar sequence-to-sequence models have been used for a wide variety of tasks, from dialog systems [112, 95] to text summarization [92], speech recognition [17], speech synthesis [109], image captioning [56, 113], image generation [43], and more.
This is just a small sampling of topics from this exciting and rapidly expanding ﬁeld, and hopefully this tutorial gave readers the tools to strike out on their own and apply these models to their applications of interest.
55

Acknowledgements
I am extremely grateful to Qinlan Shen and Dongyeop Kang for their careful reading of these materials and useful comments about unclear parts. I also thank the students in the Machine Translation and Sequence-to-sequence Models class at CMU for pointing out various bugs in the materials when a preliminary version was used in the class.
References
[1] Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
[2] Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention network for extreme summarization of source code. arXiv preprint arXiv:1602.03001, 2016.
[3] Philip Arthur, Graham Neubig, and Satoshi Nakamura. Incorporating discrete translation lexicons into neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.
[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
[5] Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 238–247, Baltimore, Maryland, June 2014. Association for Computational Linguistics.
[6] Jerome R Bellegarda. Statistical language model adaptation: review and perspectives. Speech communication, 42(1):93–108, 2004.
[7] Claus Bendtsen and Ole Stauning. Fadbad, a ﬂexible c++ package for automatic differentiation. Department of Mathematical Modelling, Technical University of Denmark, 1996.
[8] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS), pages 1171–1179, 2015.
[9] Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neural probabilistic language models. In Innovations in Machine Learning, volume 194, pages 137–186. 2006.
[10] Yoshua Bengio and Jean-S´ebastien Sen´ecal. Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4):713–722, 2008.
56

[11] Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22, 1996.
[12] James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A CPU and GPU math compiler in Python. In Proc. 9th Python in Science Conf, pages 1–7, 2010.
[13] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3, 2003.
[14] Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeﬀrey Dean. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858–867, 2007.
[15] Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. Class-based n-gram models of natural language. Comput. Linguist., 18(4):467–479, 1992.
[16] Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–312, 1993.
[17] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 4960–4964. IEEE, 2016.
[18] Stanley Chen. Shrinking exponential language models. In Proceedings of the Human Language Technologies: The 2009 Conference of the North American Chapter of the Association for Computational Linguistics, pages 468–476, 2009.
[19] Stanley F. Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 310–318, 1996.
[20] Stanley F. Chen and Roni Rosenfeld. A survey of smoothing techniques for me models. Speech and Audio Processing, IEEE Transactions on, 8(1):37–50, Jan 2000.
[21] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder–decoder approaches. In Proceedings of the Workshop on Syntax and Structure in Statistical Translation, pages 103–111, 2014.
[22] Lonnie Chrisman. Learning recursive distributed representations for holistic computation. Connection Science, 3(4):345–366, 1991.
[23] Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. A character-level decoder without explicit segmentation for neural machine translation. In Proceedings of the
57

54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1693–1703, 2016.
[24] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
[25] Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, and Gholamreza Haﬀari. Incorporating structural alignment biases into an attentional neural translation model. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 876–885, 2016.
[26] Ronan Collobert, Samy Bengio, and Johnny Mari´ethoz. Torch: a modular machine learning software library. Technical report, Idiap, 2002.
[27] Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.
[28] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.
[29] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS), pages 3079–3087, 2015.
[30] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121– 2159, 2011.
[31] Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. Transition-based dependency parsing with stack long short-term memory. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 334–343, 2015.
[32] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 199–209, 2016.
[33] Jeﬀrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.
[34] Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. Tree-to-sequence attentional neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 823–833, 2016.
[35] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine translation with a shared attention mechanism. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 866–875, 2016.
58

[36] Mikel L Forcada and Ram´on P N˜ eco. Recursive hetero-associative memories for translation. In International Work-Conference on Artiﬁcial Neural Networks, pages 453–462. Springer, 1997.
[37] Kunihiko Fukushima. Neocognitron: A hierarchical neural network capable of visual pattern recognition. Neural networks, 1(2):119–130, 1988.
[38] Felix A Gers, Ju¨rgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. Neural computation, 12(10):2451–2471, 2000.
[39] Yoav Goldberg. A primer on neural network models for natural language processing. arXiv preprint arXiv:1510.00726, 2015.
[40] Joshua Goodman. Classes for fast maximum entropy training. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 561–564. IEEE, 2001.
[41] Joshua T. Goodman. A bit of progress in language modeling. Computer Speech & Language, 15(4):403–434, 2001.
[42] Klaus Greﬀ, Rupesh Kumar Srivastava, Jan Koutn´ık, Bas R. Steunebrink, and Ju¨rgen Schmidhuber. LSTM: A search space odyssey. CoRR, abs/1503.04069, 2015.
[43] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
[44] Andreas Griewank. Automatic diﬀerentiation of algorithms: theory, implementation, and application. In proceedings of the ﬁrst SIAM Workshop on Automatic Diﬀerentiation, 1991.
[45] Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor OK Li. Learning to translate in real-time with neural machine translation. 2017.
[46] Thanh-Le Ha, Jan Niehues, and Alexander Waibel. Toward multilingual neural machine translation with universal encoder and decoder. arXiv preprint arXiv:1611.04798, 2016.
[47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
[48] Kenneth Heaﬁeld. Kenlm: Faster and smaller language model queries. In Proceedings of the 6th Workshop on Statistical Machine Translation (WMT), pages 187–197, 2011.
[49] Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[50] Robin J Hogan. Fast reverse-mode automatic diﬀerentiation using expression templates in c++. ACM Transactions on Mathematical Software (TOMS), 40(4):26, 2014.
[51] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.
59

[52] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi´egas, Martin Wattenberg, Greg Corrado, et al. Google’s multilingual neural machine translation system: Enabling zero-shot translation. arXiv preprint arXiv:1611.04558, 2016.
[53] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700–1709, 2013.
[54] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.
[55] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 655–665, 2014.
[56] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. pages 3128–3137, 2015.
[57] Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.
[58] Y. Kim, C. Denton, L. Hoang, and A. M. Rush. Structured Attention Networks. ArXiv e-prints, February 2017.
[59] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, 2014.
[60] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[61] Roland Kuhn and Renato De Mori. A cache-based natural language model for speech recognition. IEEE transactions on pattern analysis and machine intelligence, 12(6):570– 583, 1990.
[62] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[63] Tao Lei, Regina Barzilay, and Tommi Jaakkola. Molding cnns for text: non-linear, nonconsecutive convolutions. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1565–1575, 2015.
[64] Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 107–117, 2016.
[65] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp. arXiv preprint arXiv:1506.01066, 2015.
60

[66] Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard Hovy. When are tree structures necessary for deep learning of representations? In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2304–2314, 2015.
[67] Minh-Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. Addressing the rare word problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 11–19, 2015.
[68] Thang Luong, Hieu Pham, and Christopher D. Manning. Eﬀective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1412–1421, 2015.
[69] Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. Coverage embedding models for neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 955–960, 2016.
[70] Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. Supervised attentions for neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2283–2288, 2016.
[71] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
[72] Tomas Mikolov, Martin Karaﬁa´t, Lukas Burget, Jan Cernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (InterSpeech), pages 1045–1048, 2010.
[73] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS), pages 3111–3119, 2013.
[74] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012.
[75] Brian Murphy, Partha Talukdar, and Tom Mitchell. Learning eﬀective and interpretable semantic models using non-negative sparse embedding. In Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 1933–1950, 2012.
[76] Masami Nakamura, Katsuteru Maruyama, Takeshi Kawabata, and Kiyohiro Shikano. Neural network approach to word category prediction for English texts. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), 1990.
[77] Graham Neubig and Chris Dyer. Generalizing and hybridizing count-based and neural language models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.
61

[78] Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, and Pengcheng Yin. Dynet: The dynamic neural network toolkit. arXiv preprint arXiv:1701.03980, 2017.
[79] Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, et al. Reward augmented maximum likelihood for neural structured prediction. In Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NIPS), pages 1723–1731, 2016.
[80] Daisuke Okanohara and Jun’ichi Tsujii. A discriminative language model with pseudonegative samples. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 73–80, 2007.
[81] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, 2002.
[82] Adam Pauls and Dan Klein. Faster and smaller n-gram language models. pages 258–267, 2011.
[83] Jordan B Pollack. Recursive distributed representations. Artiﬁcial Intelligence, 46(1):77–105, 1990.
[84] MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. Proceedings of the International Conference on Learning Representations (ICLR), 2016.
[85] Philip Resnik. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How, pages 52–57. Washington, DC, 1997.
[86] Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. Discriminative language modeling with conditional random ﬁelds and the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 47–54, 2004.
[87] Ronald Rosenfeld. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10(3):187 – 228, 1996.
[88] Ronald Rosenfeld, Stanley F Chen, and Xiaojin Zhu. Whole-sentence exponential language models: a vehicle for linguistic-statistical integration. Computer Speech & Language, 15(1):55–73, 2001.
[89] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.
62

[90] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Learning Internal Representations by Error Propagation, pages 318–362. MIT Press, 1986.
[91] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.
[92] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 379–389, 2015.
[93] Hinrich Sch utze. Word space. 5:895–902, 1993.
[94] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1715–1725, 2016.
[95] Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for shorttext conversation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 1577–1586, 2015.
[96] Libin Shen, Jinxi Xu, and Ralph Weischedel. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 577–585, 2008.
[97] Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum risk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1683– 1692, 2016.
[98] Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1526–1534, 2016.
[99] Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 455–465, 2013.
[100] Richard Socher, Cliﬀ C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 129–136, 2011.
[101] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS), pages 3104–3112, 2014.
[102] Zolt´an Gendler Szab´o. Compositionality. Stanford encyclopedia of philosophy, 2010.
63

[103] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), 2015.

[104] David Talbot and Thorsten Brants. Randomized language models via perfect hash functions. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 505–513, 2008.

[105] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.

[106] Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. Modeling coverage for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 76–85, 2016.

[107] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 384–394. Association for Computational Linguistics, 2010.

[108] Peter D Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal of Artiﬁcial Intelligence Research, 37:141–188, 2010.

[109] A¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. CoRR abs/1609.03499, 2016.

[110] Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. Decoding with large-scale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1387–1392, 2013.

[111] Pascal Vincent, Alexandre de Br´ebisson, and Xavier Bouthillier. Eﬃcient exact gradient update for training deep networks with very large sparse targets. In Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS), pages 1108–1116, 2015.

[112] Oriol Vinyals and Quoc Le. arXiv:1506.05869, 2015.

A neural conversational model.

arXiv preprint

[113] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. pages 3156–3164, 2015.

[114] Alex Waibel, Toshiyuki Hanazawa, Geoﬀrey Hinton, Kiyohiro Shikano, and Kevin J Lang. Phoneme recognition using time-delay neural networks. 37(3):328–339, 1989.

[115] Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. Memory-enhanced decoder for neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278–286, 2016.

64

[116] R.E. Wengert. A simple automatic derivative evaluation program. Communications of the ACM, 7(8):463–464, 1964.
[117] Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search optimization. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1306, 2016.
[118] I.H. Witten and T.C. Bell. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. Information Theory, IEEE Transactions on, 37(4):1085–1094, 1991.
[119] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[120] Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, and Alex Smola. Neural machine translation with recurrent attention modeling. arXiv preprint arXiv:1607.05108, 2016.
[121] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classiﬁcation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1480–1489, San Diego, California, June 2016. Association for Computational Linguistics.
[122] Lei Yu, Jan Buys, and Phil Blunsom. Online segment to segment neural transduction. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1307–1316, 2016.
[123] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.
[124] Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for lowresource neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1568–1575, 2016.
65

