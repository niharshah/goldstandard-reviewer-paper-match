arXiv:2006.10627v2 [cs.AI] 24 Oct 2020

Compositional Generalization by Learning Analytical Expressions
Qian Liuâ€ âˆ—, Shengnan Anâ™¦âˆ—, Jian-Guang LouÂ§, Bei ChenÂ§, Zeqi LinÂ§, Yan GaoÂ§, Bin Zhouâ€ , Nanning Zhengâ™¦, Dongmei ZhangÂ§
â€ Beihang University, Beijing, China;â™¦Xiâ€™an Jiaotong University, Xiâ€™an, China; Â§Microsoft Research, Beijing, China
â€ {qian.liu, zhoubin}@buaa.edu.cn; â™¦{an1006634493@stu, nnzheng@mail}.xjtu.edu.cn; Â§{jlou, beichen, Zeqi.Lin, Yan.Gao, dongmeiz}@microsoft.com
Abstract
Compositional generalization is a basic and essential intellective capability of human beings, which allows us to recombine known parts readily. However, existing neural network based models have been proven to be extremely deï¬cient in such a capability. Inspired by work in cognition which argues compositionality can be captured by variable slots with symbolic functions, we present a refreshing view that connects a memory-augmented neural model with analytical expressions, to achieve compositional generalization. Our model consists of two cooperative neural modules, Composer and Solver, ï¬tting well with the cognitive argument while being able to be trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on the well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, solving all challenges addressed by previous works with 100% accuracies.
1 Introduction
When using language, humans have a remarkable ability to recombine known parts to understand novel sentences they have never encountered before [8, 12]. For example, once humans have learned the meanings of â€œwalkâ€, â€œjumpâ€ and â€œwalk twiceâ€, it is effortless for them to understand the meaning of â€œjump twiceâ€. This kind of ability relies on the compositionality that characterizes languages. The principle of compositionality refers to the idea that the meaning of a complex expression (e.g. a sentence) is determined by the meanings of its constituents (e.g. the verb â€œjumpâ€ and the adverb â€œtwiceâ€) together with the way these constituents are combined (e.g. an adverb modiï¬es a verb) [34]. Understanding language compositionality is a basic and essential capacity for human beings, which is argued to be one of the key skills towards human-like machine intelligence [25].
Recently, Lake and Baroni [19] made a step towards exploring and benchmarking compositional generalization of neural networks. They argued that leveraging compositional generalization was an essential ability for neural networks to understand out-of-domain sentences. The test suite, their proposed Simpliï¬ed version of the CommAI Navigation (SCAN) dataset, contains compositional navigation commands, such as â€œwalk twiceâ€, and corresponding action sequences, like WALK WALK. Such a task lies in the category of machine translation, and thus is expected to be well solved by current state-of-the-art translation models (e.g. sequence to sequence with attention [32, 3]). However, experiments on SCAN demonstrated that modern translation models dramatically fail to obtain a satisfactory performance on compositional generalization. For example, although the meanings of â€œwalkâ€, â€œwalk twiceâ€ and â€œjumpâ€ have been seen, current models fail to generalize
âˆ— Work done during an internship at Microsoft Research. The ï¬rst two authors contributed equally.
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

to understand â€œjump twiceâ€. Subsequent works veriï¬ed that it was not an isolated case, since convolutional encoder-decoder model [10] and Transformer [17] met the same problem. There have been several attempts towards SCAN, but so far no neural based model can successfully solve all the compositional challenges on SCAN without extra resources [21, 18, 13].
In this paper, we propose a memory-augmented neural model to achieve compositional generalization by Learning Analytical Expressions (LANE). Motivated by work in cognition which argues compositionality can be captured by variable slots with symbolic functions [4], our memory-augmented architecture is devised to contain two cooperative neural modules accordingly: Composer and Solver. Composer aims to ï¬nd structured analytical expressions from unstructured sentences, while Solver focuses on understanding these expressions with accessing Memory (Sec. 3). These two modules are trained to learn analytical expressions together in an end-to-end manner via a hierarchical reinforcement learning algorithm (Sec. 4). Experiments on a well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, reaching 100% accuracies in all tasks (Sec. 5). As far as we know, our model is the ï¬rst neural model to pass all compositional challenges addressed by previous works on SCAN without extra resources. We open-source our code at https://github.com/microsoft/ContextualSP.
2 Compositional Generalization Assessment
Since the study on compositional generalization of deep neural models is still in its infancy, the overwhelming majority of previous works employ artiï¬cial datasets to conduct assessment. As one of the most important benchmarks, the SCAN dataset is proposed to evaluate the compositional generalization ability of translation models [19]. As mentioned above, SCAN describes a simple navigation task that aims to translate compositional navigation sentences into executed action sequences. However, due to the open nature of compositional generalization, there is disagreement about which aspect should be addressed [34, 20, 15, 17]. To conduct a comprehensive assessment, we consider both systematicity and productivity, two important arguments for compositional generalization.
Systematicity evaluates if models can recombine known parts. To assess it, Lake and Baroni [19] proposed three tasks: (i) Add Jump. The pairs of train and test are split in terms of the primitive JUMP. All commands that contain, but are not exactly, the word â€œjumpâ€ form the test set. The rest forms the train set. (ii) Around Right. Any compositional command whose constitutes include â€œaround rightâ€ is excluded from the train test. This task is proposed to evaluate whether the model can generalize the experience about â€œleftâ€ to â€œrightâ€, especially on â€œaround rightâ€. (iii) Length. All commands with long outputs (i.e. output length is longer than 24), such as â€œaround âˆ— twice âˆ— aroundâ€ and â€œaround âˆ— thriceâ€, are never seen in training, where â€œâˆ—â€ indicates a wildcard. More recently, Keysers et al. [17] proposed another assessment, the distribution-based systematicity. It aims to measure compositional generalization by using a setup where there is a large compound distribution divergence between train and test sets (Maximum Compound Divergence, MCD) [17].
Productivity is thought to be another key argument. It not only requires models to recombine known parts, but also evaluates if they can productively generalize to inputs beyond the length they have seen in training. It relates itself to the unboundedness of languages, which means languages license a theoretically inï¬nite set of possible sentences [4]. To evaluate it, we re-create the SCAN dataset (SCAN-ext). Compared with SCAN using up to one â€œandâ€ in a sentence, SCAN-ext roughly controls the distribution of input lengths by the number of â€œandâ€ (e.g. â€œjump and walk twice and turn leftâ€). Input sentences in the train set consist of at most 2 â€œandâ€, while the test set allows at most 9. Except for â€œandâ€, the generation of other parts follows the procedure in SCAN.
3 Methodology
In this section, we ï¬rst show the intrinsic connection between language compositionality and analytical expressions. We then describe how these expressions are learned through our model.
3.1 Problem Statement
Cognitive scientists argue that the compositionality of language indeed constitutes an algebraic system, of the sort that can be captured by variable slots with symbolic functions [4, 12]. As an
2

6

$x after $y

5

$x after $y twice

4

$x after walk twice

3 $x opposite $y after walk twice

2 $x opposite left after walk twice

1 run opposite left after walk twice

symbolic function

$x after $y

$Y $X

variable assignment
WALK WALK LTURN LTURN RUN

$y twice

$Y $Y

WALK WALK

walk

WALK

$x opposite $y

$Y $Y $X LTURN LTURN RUN

left

LTURN

run

RUN

Figure 1: The schematic illustration of learning analytical expressions. The understanding of â€œrun opposite left after walk twiceâ€ can be regarded as a hierarchical application of symbolic functions.

illustrative example, any adjective attached with the preï¬x â€œsuper-â€ can be regarded as applying a symbolic function (i.e. â€œsuper-adjâ€) on a variable slot (e.g. â€œgoodâ€), and will be mapped to a new adjective (e.g. â€œsuper-goodâ€) [4]. Such a formulation frees the symbolic function from speciï¬c adjectives and makes it able to generalize on new adjectives (e.g. â€œsuper-badâ€).
Taking a more complicated case from SCAN, as shown in Fig. 1, â€$xâ€ and â€$yâ€ are variables deï¬ned in the source domain, and â€$Xâ€ and â€$Yâ€ are variables deï¬ned in the destination domain. We call a sequence of source domain variables or words (e.g. run) a source analytical expression (SrcExp), while we call a sequence of destination domain variables or action words (e.g. RUN) a destination analytical expression (DstExp). If there is no variable in an SrcExp (or DstExp), it is also a constant SrcExp (or DstExp). From bottom to top, each phrase marked blue represents an SrcExp which will be superseded by a source domain variable (e.g. $x) when moving to the next hierarchy of understanding. These SrcExps can be recognized and translated into their corresponding DstExps by a set of symbolic functions. We call such SrcExps as recognizable SrcExps, and their corresponding DstExps as recognizable DstExps. By iterative recognizing and translating recognizable SrcExps, we can construct a tree hierarchy with a set of recognizable DstExps. By assigning values to the destination variables in recognizable DstExps recursively (dotted red arrows in Fig. 1), we can ï¬nally obtain a constant DstExp as the ï¬nal resulted sequence.
It is well known that, variables are pieces of memory in computers, and a memory mechanism can be used to support variable-related operations. Thus we propose a memory-augmented neural model to achieve compositional generalization by automatically learning the above analytical expressions.
3.2 Model Design
Our model takes several steps to understand a sentence. Fig. 2 presents the overall procedure of our model at step t and t + 1 in detail (corresponding to step 5 and 6 in Fig. 1). At the beginning of step t,

Source Analytical Expression ð°ð‘¡
Recognizable SrcExp ð°à·¥ ð‘¡ Recognizable DstExp ð’à·¥ð‘¡
Constant DstExp ð’ð‘¡

$x after $y twice
Composer
$y twice
Solver
$Y $Y WALK WALK

ð°ð‘¡+1
ð°à·¥ ð‘¡+1 ð’à·¥ð‘¡+1 ð’ð‘¡+1

$x after $y
Composer
$x after $y
Solver
$Y $X WALK WALK LTURN LTURN RUN

Memory Memory Memory

SrcVec DesVec Value Slot

$x

$X

LTURN LTURN RUN

$y

$Y

WALK

SrcVec DesVec

Value Slot

$x

$X

LTURN LTURN RUN

$y

$Y

WALK WALK

SrcVec DesVec

Value Slot

$x

$X

WALK WALK LTURN LTURN RUN

$y

$Y

Figure 2: The illustration of our model. Colored neurons are learnable vectors. Composer accepts an SrcExp as input, and aims to ï¬nd a recognizable SrcExp inside it. Solver ï¬rst translates a recognizable SrcExp into a recognizable DstExp, and then assigns values to destination variables in the recognizable DstExp, obtaining a constant DstExp. To support variable-related operations in a differentiable manner [31], Memory is designed to include a number of items, each of which contains a source vector (SrcVec) to represent source variables (e.g. $x, $y), a destination vector (DesVec) to represent destination variables (e.g. $X, $Y), and a value slot to temporarily store a constant DstExp.

3

$y twice

Check Process
Merge Process

0.9

Linear

merging score

0.2

0.1

0.7

q

$x after $y twice
(a) The illustration of Composer

ðœ‹ðœƒ ðœ‹ðœƒ â€¦ ðœ‹ðœƒ

ðœ‹ðœ‘ ðœ‹ðœ‘ â€¦ ðœ‹ðœ‘

ðš1

ðš2

ðšð‘‡

ð¬1

ð¬2

ð¬3

ð‘…

(b) The illustration of HRL algorithm

Figure 3: (a) Composer ï¬nds a recognizable SrcExp via the cooperation of the merge process and the check process. (b) Our HRL algorithm contains a high-level policy Ï€Î¸ and a low-level policy Ï€Ï•.

an SrcExp â€œ$x after $y twiceâ€ is fed into Composer. Then Composer ï¬nds a recognizable SrcExp â€œ$y twiceâ€ and sends it to Solver. Receiving â€œ$y twiceâ€, Solver ï¬rst translates it into â€œ$Y $Yâ€. Using â€œ$Y $Yâ€ as the skeleton, Solver obtains WALK WALK by replacing â€œ$Yâ€ with its corresponding constant DstExp WALK stored in Memory. Meanwhile, since WALK has been used, the value slot which stores WALK is set to empty. Next, Solver applies for one item with an empty value slot in Memory, i.e. the item containing $y and $Y, and then writes WALK WALK into its value slot (gray background in Fig.2). Finally, the recognizable SrcExp â€œ$y twiceâ€ in wt is superseded by â€œ$yâ€, producing â€œ$x after $yâ€ as wt+1 for the next step. Such a procedure is repeated until the SrcExp fed into Composer is a recognizable SrcExp. Assuming the step at this point is T , the constant DstExp oT is actually the ï¬nal output action sequence.
Composer Given an SrcExp wt, Composer aims to ï¬nd a recognizable SrcExp wËœ t. There are several ways to implement it, and we choose to gradually merge elements of wt until a recognizable SrcExp appears. As shown in Fig. 3a, given â€œ$x after $y twiceâ€, at ï¬rst Composer merges â€œ$yâ€ and â€œtwiceâ€. Then it checks if â€œ$y twiceâ€ is a recognizable SrcExp. In this case the answer is YES, and thus Composer triggers Solver to translate â€œ$y twiceâ€. Otherwise, the overall procedure would be iterative, which means that Composer would continue to merge until a recognizable SrcExp appears. Viewing the procedure as building a binary tree from bottom to top, Composer iteratively merges two neighboring elements of wt into a parent node at each layer (i.e. the merge process), and checks if the parent node represents a recognizable SrcExp (i.e. the check process).
The merge process is implemented by ï¬rst enumerating all possible parent nodes of the current layer, and then selecting the one which has the highest merging score. Assuming i-th and (i + 1)-th node at layer l are represented by rli and rli+1 respectively, their parent representation rli+1 can be obtained via a standard Tree-LSTM encoding [35] using rli and rli+1 as input. As shown in Fig. 3a, given all parent node representations (blue neurons), Composer selects the parent node (solid lines with arrows) whose merging score is the maximum. In fact, the merging score measures the merging priority of rli+1 using a learnable query vector q by q, rli+1 , where Â·, Â· represents the inner product. Once the parent node for layer l is determined, the check process begins.
The check process is to check if a parent node represents a recognizable SrcExp. Concretely, denoting rli+1 the parent node representation, an afï¬ne transformation is built based on it to obtain the probability pc = Ïƒ(Wcrli+1 + bc) where Wc and bc are learned parameters and Ïƒ is the sigmoid function. pc > 0.5 means that the parent node represents a recognizable SrcExp, and thus Composer triggers Solver to translate it. Otherwise, the parent node and other unmerged nodes enter a new layer l + 1, based on which Composer restarts the merge process.
Solver Given a recognizable SrcExp wËœ t, Solver ï¬rst translates it into a recognizable DstExp oËœt, and then obtains a constant DstExp ot via variable assignment through interacting with Memory. To achieve this, Solver is designed to be an LSTM-based sequence to sequence network with an attention mechanism [3]. It generates the recognizable DstExp via decoding it step by step. At each step, Solver either generates an action word, or a destination variable. Using the recognizable DstExp as the skeleton, Solver obtains a constant DstExp by replacing each destination variable with its corresponding constant DstExp stored in Memory.
4

4 Model Training
Training our proposed model is non-trivial for two reasons: (i) since the identiï¬cation of wËœ t is discrete, it is hard to optimize Composer and Solver via back propagation; (ii) since there is no supervision about wËœ t and oËœt, Composer and Solver cannot be trained separately. Recalling the procedure of these two modules in Fig. 2, it is natural to model the problem via Hierarchical Reinforcement Learning (HRL) [5]: a high-level agent to ï¬nd recognizable SrcExps (Composer), and a low-level agent to obtain constant DstExps conditioned on these recognizable SrcExps (Solver).

4.1 Hierarchical Reinforcement Learning
We begin by introducing some preliminary formulations for our HRL algorithm. Denoting st as the state at step t, it contains both wt and Memory. The action of Composer, denoted by Gt, is the recognizable SrcExp to be found at step t. Given st as observation, the parameter of Composer Î¸ deï¬nes a high-level policy Ï€Î¸(Gt | st). Once a high-level action Gt is produced, the low-level agent Solver is triggered to react following a low-level policy conditioned on Gt. In this sense, the high-level action can be viewed as a sub-goal for the low-level agent. Denoting at the action of Solver, the low-level policy Ï€Ï•(at | Gt, st) is parameterized by the parameter of Solver Ï•. at is the constant DstExp output by Solver at step t. More implementation details about Ï€Î¸ and Ï€Ï• can be found in the supplementary material.

Policy Gradient As illustrated in Fig. 3b, in our HRL algorithm, Composer and Solver take actions in turn. When it is Composerâ€™s turn to act, it picks a sub-goal Gt according to Ï€Î¸. Once Gt is set, Solver is triggered to pick a low-level action at according to Ï€Ï•. These two modules alternately act until they reach the endpoint (i.e. step T ) and predict the output action sequence, forming a trajectory Ï„ = (s1G1a1 Â· Â· Â· sT GT aT ). Once Ï„ is determined, the reward is collected to optimize Î¸ and Ï• using
policy gradient [33]. Denoting R(Ï„ ) as the reward of a trajectory Ï„ (elaborated in Sec. 4.2), the
training objective of our model is to maximize the expectation of rewards as:

max J (Î¸, Ï•) = max EÏ„âˆ¼Ï€Î¸,Ï• R(Ï„ ).

(1)

Î¸,Ï•

Î¸,Ï•

Applying the likelihood ratio trick, Î¸ and Ï• can be optimized by ascending the following gradient:

âˆ‡Î¸,Ï•J (Î¸, Ï•) = EÏ„âˆ¼Ï€Î¸,Ï• R(Ï„ )âˆ‡Î¸,Ï• log Ï€Î¸,Ï• (Ï„ ) .

(2)

Expanding the above equation via the chain rule2, we can obtain:

âˆ‡Î¸,Ï•J (Î¸, Ï•) = EÏ„âˆ¼Ï€Î¸,Ï• tR(Ï„ ) âˆ‡Î¸,Ï• log Ï€Î¸ Gt|st + âˆ‡Î¸,Ï• log Ï€Ï• at|Gt, st . (3)
Considering the search space of Ï„ is huge, the REINFORCE algorithm [39] is leveraged to approximate Eq. 3 by sampling Ï„ from Ï€Î¸,Ï• for N times. Furthermore, the technique of subtracting a baseline [38] is employed to reduce variance, where the baseline is the mean reward over sampled Ï„ .

Differential Update Unlike standard Reinforcement Learning (RL) algorithms, we introduce a differential update strategy to optimize Composer and Solver via different learning rates. It is motivated by an intuition that actions of a high-level agent cannot be changed quickly. According to Eq. 3, simplifying EÏ„âˆ¼Ï€Î¸,Ï• as E, the parameters of Composer and Solver are optimized as:

Î¸ â† Î¸ + Î± Â· E R(Ï„ ) âˆ‡Î¸ log Ï€Î¸ Gt|st , Ï• â† Ï• + Î² Â· E R(Ï„ ) âˆ‡Ï• log Ï€Ï• at|Gt, st , (4)

t

t

where Solverâ€™s learning rate Î² is greater than Composerâ€™s learning rate Î±.

4.2 Reward Design
The design of the reward function is critical to an RL based algorithm. Bearing this in mind, we design our reward from two aspects: similarity and simplicity. It is worth noting that both rewards work globally, i.e., all actions share the same reward, as indicated by dotted lines in Fig. 3b.
2More details can be found in the supplementary material.

5

Similarity-based Reward It is based on the similarity between the modelâ€™s output and the ground-
truth. Since the output of our model is an action sequence, a kind of sequence similarity, the Intersection over Union (IoU) similarity, is employed as the similarity-based reward function. Given the sampled output aT and the ground-truth o, the similarity-based reward is computed by:

Rs (Ï„ ) = aT âˆ© o / aT + |o| âˆ’ aT âˆ© o ,

(5)

where aT âˆ© o means the longest common substring between aT and o, and | Â· | represents the length of a sequence. Compared with exact matching, such a reward alleviates the reward sparsity issue.

Simplicity-based Reward Inspired by Occamâ€™s Razor principle that â€œthe simplest solution is
most likely the right oneâ€, we try to encourage our model to have the fewest kinds of learned
recognizable DstExps overall. In other words, we encourage the model to fully utilize variables and be more generalizable. Taking an illustration of â€œjump twiceâ€, [ jump twice â†’ JUMP JUMP] and [ jump â†’ JUMP, $x twice â†’ $X $X] both result in correct outputs. Intuitively, the latter is
more generalizable as it enables Solver to reuse learned recognizable DstExps, more in line with
the Occamâ€™s Razor principle. Concretely, when understanding a novel input like â€œwalk twiceâ€, $x twice â†’ $X $X can be reused. Denoting T âˆ— as the number of steps where the recognizable DstExp only contains destination variables, we design a reward Ra(Ï„ ) = T âˆ— / T as a measure of the simplicity. The ï¬nal reward function R(Ï„ ) is a linear summation as R(Ï„ ) = Rs(Ï„ ) + Î³Â·Ra(Ï„ ), where Î³ is a hyperparameter.

4.3 Curriculum Learning
One typical strategy for improving model generalization capacity is to use curriculum learning, which arranges examples from easy to hard in training [24, 1]. Inspired by it, we divide the training into different lessons according to the length of the input sequence. Our model starts training on the simplest lesson, with lesson complexity gradually increasing. Besides, as done in literature [7], we accumulate training data from previous lessons to avoid catastrophic forgetting.

5 Experiments
In this section, we conduct a series of experiments to evaluate our model on various compositional tasks mentioned in Sec. 2. We then verify the importance of each component via a thorough ablation study. Finally we present two real cases to illustrate our model concretely.
5.1 Experimental Setup
Task In this section, we introduce Tasks used in our experiments. Systematicity is evaluated on Add Jump, Around Right and Length of SCAN [19], while distribution-based systematicity is assessed on MCD splits of SCAN [17]. MCD uses a nondeterministic algorithm to split examples into the train set and the test set. By using different random seeds, it introduces three tasks MCD1, MCD2, and MCD3. Productivity is evaluated on the SCAN-ext dataset. In addition, we also conduct experiments on the Simple task of SCAN which requires no compositional generalization capacity, and the Limit task of MiniSCAN [20] which evaluates if models can learn compositional generalization when given limited (i.e. 14) training data. We follow previous works to split datasets for all tasks.
Baselines We consider a range of state-of-the-art models on SCAN compositional tasks as our baselines. In terms of the usage of extra resources, we divide them into two groups: (i) No Extra Resources includes vanilla sequence to sequence with attention (Seq2Seq) [19, 23], convolutional sequence to sequence (CNN) [10], Transformer [36], Universal Transformer [9], Syntactic Attention [29] and Compositional Generalization for Primitive Substitutions (CGPS) [21]. (ii) Using Extra Resources consists of Good Enough Compositional Data Augmentation (GECA) [2], meta sequence to sequence (Meta Seq2seq) [18], equivariant sequence to sequence (Equivariant Seq2seq) [13] and Program Synthesis [27]. Here we deï¬ne â€œextra resourcesâ€ as â€œdata or data speciï¬c rules other than original training dataâ€. GECA and Meta Seq2Seq lie in â€œextra resourcesâ€ since they both utilize extra data. Speciï¬cally, GECA recombines real examples to construct extra data, while Meta Seq2Seq employs random assignment of the primitive instructions (e.g. â€œjumpâ€) to their meaning (e.g. JUMP) to synthesize extra data. Regarding data-speciï¬c rules, Equivariant Seq2Seq requires predeï¬ned local

6

Table 1: Test accuracies of systematicity assessment on the SCAN dataset. All results of LANE are obtained by averaging over 5 runs, the same for Tab. 2 and Tab. 3.

Extra Resources

Model

Simple Add Jump Around Right Length

None

Seq2Seq [19, 23] CNN [10] Syntactic Attention [29] CGPS [21] LANE (Ours)

99.7 100.0 100.0 99.9 100.0

1.2 69.2 Â± 9.2 91.0 Â± 27.4 98.8 Â± 1.4
100.0

2.5 Â± 2.7 56.7 Â± 10.2 28.9 Â± 34.8 83.2 Â± 13.2
100.0

13.8 0.0
15.2 Â± 0.7 20.3 Â± 1.1
100.0

Data Augmentation Permutation-based Augmentation Manually Designed Local Groups Manually Designed Meta Grammar

GECA [2] Meta Seq2Seq [18] Equivariant Seq2Seq [13] Program Synthesis [27]

100.0 100.0

87.0 99.9 99.1 Â± 0.0 100.0

82.0 99.9 92.0 Â± 0.2 100.0

16.6 15.9 Â± 3.2 100.0

Table 2: Test accuracies of the distribution-based systematicity assessment on the SCAN dataset (left) and the Limit task on the MiniSCAN dataset (right).

Model
Seq2Seq [17] Transformer [17] Universal Transformer [17]
CGPS LANE (Ours)

MCD1
6.5 Â± 3.0 0.4 Â± 0.2 0.5 Â± 0.1 1.2 Â± 1.0
100.0

MCD2
4.2 Â± 1.4 1.6 Â± 0.3 1.5 Â± 0.2 1.7 Â± 2.0
100.0

MCD3
1.4 Â± 0.2 0.8 Â± 0.4 1.1 Â± 0.4 0.6 Â± 0.3
100.0

Model
Human [20] Seq2Seq CGPS
Meta Seq2Seq LANE (Ours)

Limit
84.3 2.5 76.0 100.0 100.0

groups to make the model aware of equivariance between verbs or directions (e.g. â€œjumpâ€ and â€œrunâ€ are verbs). Similarly, Program Synthesis needs a predeï¬ned meta grammar, which heavily relies on the grammar of a dataset, and hence we think it also falls into the group of using â€œextra resourcesâ€. Details of these baselines can be found in Sec. 6.
5.2 Implementation Details
Our model is implemented in PyTorch [28]. All experiments use the same hyperparameters. Dimensions of word embeddings, hidden states, key vectors and value vectors are set as 128. Hyperparameters Î³ and N are set as 0.5 and 10 respectively. All parameters are randomly initialized and updated via the AdaDelta [40] optimizer, with a learning rate of 0.1 for Composer and 1.0 for Solver. Meanwhile, as done in previous works [14], we introduce a regularization term to prevent our model from overï¬tting in the early stage of training. Its weight is set to 0.1 at the beginning, and exponentially anneals with a rate 0.5 as the lesson increases. Our model is trained on a single Tesla-P100 (16GB) and the training time for a single run is about 20 âˆ¼ 25 hours.
5.3 Experimental Results
Experiment 1: Systematicity on SCAN As shown in Tab. 1, LANE achieves stunning 100% test accuracies on all tasks. Compared with state-of-the-art baselines without extra resources, LANE achieves a signiï¬cantly higher performance. Even compared to baselines with extra resources, LANE is highly competitive, suggesting that to some extent LANE is capable of learning human prior knowledge. Although program synthesis [27] also achieves perfect accuracies, it heavily depends on a predeï¬ned meta-grammar where decent task-related knowledge is encoded. As far as we know, LANE is the ï¬rst neural model to pass all tasks without extra resources.
Experiment 2: Distribution-based Systematicity on SCAN LANE also achieves 100% accuracies on the more challenging distribution-based systematicity tasks (see Tab. 2). By comparing Tab. 1 and Tab. 2, one can ï¬nd LANE maintains a stable and perfect performance regardless of the task, while a strong baseline CGPS shows a sharp drop. Furthermore, to the best of our knowledge, LANE is also the ï¬rst one to pass the assessment of distribution-based systematicity on SCAN.
Experiment 3: Productivity As shown in Fig. 4a, there is a sharp divergence between input lengths of train and test set on SCAN-ext, suggesting it is a feasible benchmark for productivity. From the results (right), one can ï¬nd that test accuracies of baselines are mainly ruled by the frequency of input lengths in the train set. In contrast, LANE maintains a perfect trend as the input length
7

Frequency Test Accuracy Train Accuracy Test Accuracy

train

0.10

test

0.05

0.00 0

10 20 30 40
Input Length

1.0
0.5 Seq2Seq CGPS LAnE
0.0 10 20 30 40
Input Length

high=0.1 low=1.0 high=0.1 low=0.5
100
10âˆ’1
10âˆ’2
10âˆ’3
10âˆ’4 0 100 200 300 400 500 600
# of Trajectory (k)

high=0.1 low=0.1 high=0.2 low=0.1
100
10âˆ’1
10âˆ’2
10âˆ’3
10âˆ’4 0 100 200 300 400 500 600
# of Trajectory (k)

(a) Experiments on input length distributions

(b) Experiments on learning rate combinations

Figure 4: (a) Input length distributions on train set and test set of SCAN-ext (left) and test accuracies of various method on different input lengths (right). (b) Accuracies on train set (left) and test set (right) under different learning rate combinations.

Table 3: Test accuracies of different variants in all tasks on the SCAN dataset.

Variant

Simple Add Jump Length Around Right MCD1 MCD2 MCD3

w/o Composer

98.5 Â± 0.6

0.0

11.1 Â± 13.1

0.0

5.3 Â± 2.4 0.7 Â± 0.3 2.6 Â± 0.9

w/o Curriculum Learning

0.0

0.0

0.0

0.0

0.0

0.0

0.0

w/o Simplicity-based Reward 100.0

100.0

100.0

0.0

100.0

100.0 78.8 Â± 4.2

increases, indicating it has productive generalization capabilities. Furthermore, the trend suggests the potential of LANE on tackling inputs with unbounded length.
Experiment 4: Compositional Generalization on MiniSCAN Tab. 2 (right) shows the performance of various methods given limited training data, and LANE remains highly effective. Without extra resources such as permutation-based augmentation employed by Meta Seq2Seq, our model performs perfectly, i.e. 100% on the Limit task. Compared with the human performance 84.3% [20], to a certain extent, our model is close to the human ability at learning compositional generalization from few examples. However, it does not imply that either our model or Meta Seq2Seq triumphs over humans in terms of compositional generalization, as the Limit task is relatively simple.
5.4 Closer Analysis
We conduct a thorough ablation study in Tab. 3 to verify the effectiveness of each component in our model. â€œw/o Composerâ€ ablates the check process of Composer, making our model degenerate into a tree to sequence model, which employs a Tree-LSTM to build trees and encode input sequences dynamically. â€œw/o Curriculum Learningâ€ means training our model on the full train set from the beginning. As the result shows, ablating each of above causes an enormous performance drop, indicating the necessity of Composer and the curriculum learning. Especially, without the curriculum learning, our model shows no sign of convergence even after training for several days, and thus all results are directly dropped to 0. We suppose that our model shows such non-convergence since its action space is exponentially large, which is due to the indeï¬nite length of output sequences in Solver, and the huge number of possible trees in Composer. Such a huge space means that rewards are very sparse, especially for harder examples. So without curriculum learning, our randomly initialized model receives zero rewards on most examples. In comparison, by arranging examples from easy to hard, curriculum learning alleviates the sparse reward issue. On the one hand, easy examples are more likely to provide non-zero rewards to help our model converge; on the other hand, models trained on easy examples have a greater possibility to receive non-zero rewards on hard examples. â€œw/o Simplicity-based Rewardâ€, which only considers the similarity-based reward, fails on several tasks such as Around Right. We attribute its failure to its inability to learn sufï¬ciently general recognizable DstExps from the data. As for the differential update, we compare the results of several learning rate combinations in Fig. 4b. As indicated, our designed differential update strategy is essential for successful convergence and high test accuracies. Last, we present learned tree structures of two real cases in Fig. 5. Observing that â€œtwiceâ€ behaves differently under different contexts, it is non-trivial to produce such trees.
8

(a) walk

(b) and look twice look opposite left twice after turn around right

Recognizable Expression
Normal Parent Node

Figure 5: Learned tree structures in Composer of two real cases.

6 Related Work
The most related work is the line of exploring compositional generalization on neural networks, which has attracted a large attention on different topics in recent years. Under the topic of mathematical reasoning, Veldhoen et al. [37] explored the algebraic compositionality of neural networks via simple arithmetic expressions, and Saxton et al. [30] pushed the area forward by probing if the standard Seq2Seq model can resolve complex mathematical problems. Under the topic of logical inference, previous works devoted to testing the ability of neural networks on inferring logical relations between pairs of artiï¬cial language utterances [6, 26]. Our work differently focuses more on the compositionality in languages, benchmarked by the SCAN compositional tasks [19].
As for the SCAN compositional tasks, there have been several attempts. Inspired by work in neuroscience which suggests a disjoint processing on syntactic and semantic, Russin et al. [29] proposed the Syntactic Attention model. Analogously, Li et al. [21] employed different representations for primitives and functions respectively (CGPS). Unlike their separate representations, our proposed Composer and Solver can be seen as separate at the module level. There are also some works which impose prior knowledge of compositionality via extra resources. Andreas [2] presented a data augmentation technique to enhance standard approaches (GECA). Lake [18] argued to achieve compositional generalization by meta learning, and thus they employed a Meta Seq2Seq model with a memory mechanism. Regarding the memory mechanism, our work is similar to theirs. However, their training process, namely permutation training, requires handcrafted data augmentation. In a follow-up paper [27], they argued to generalize via the paradigm of program synthesis. Despite the nearly perfect performance, it also requires a predeï¬ned meta-grammar, where decent knowledge is encoded. Meanwhile, based on the group-equivariance theory, Gordon et al. [13] predeï¬ned local groups to enable models aware of equivariance between verbs or directions (Equivariant Seq2Seq). The biggest difference between our work and theirs is that we do not utilize any extra resource.
Our work is also related to those which apply RL on language. In this sense, using language as the abstraction for HRL [16] is the most related work. They proposed to use sentences as the sub-goal for the low-level policy in vision-based tasks, while we employ recognizable SrcExps as the sub-goal. In addition, the applications of RL on language involves topics such as natural language generation [11], conversational semantic parsing [22] and text classiï¬cation [41].
7 Conclusion & Future Work
In this paper, we propose to achieve compositional generalization by learning analytical expressions. Motivated by work in cognition, we present a memory-augmented neural model which contains two cooperative neural modules Composer and Solver. These two modules are trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on a well-known benchmark demonstrate that our model solves all challenges addressed by previous works with 100% accuracies, surpassing existing baselines signiï¬cantly. For future work, we plan to extend our model to a recently proposed compositional task CFQ [17] and more realistic applications.
Acknowledgments
We thank all the anonymous reviewers for their valuable comments. This work was supported in part by National Natural Science Foundation of China (U1736217 and 61932003), and National Key R&D Program of China (2019YFF0302902).
9

Broader Impact
This work explores the topic of compositional generalization capacities in neural networks, which is a fundamental problem in artiï¬cial intelligence but not involved in real applications at now. Therefore, there will be no foreseeable societal consequences nor ethical aspects.
References
[1] S. E. Ada, E. Ugur, and H. L. Akin. Generalization in transfer learning. arXiv preprint arXiv:1909.01331, 2019.
[2] J. Andreas. Good-enough compositional data augmentation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556â€“7566, Online, July 2020. Association for Computational Linguistics.
[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
[4] M. Baroni. Linguistic generalization and compositionality in modern artiï¬cial neural networks. CoRR, abs/1904.00157, 2019.
[5] A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete event dynamic systems, 13(1-2):41â€“77, 2003.
[6] S. R. Bowman, C. D. Manning, and C. Potts. Tree-structured composition in neural networks without tree-structured architectures. In T. R. Besold, A. S. dâ€™Avila Garcez, G. F. Marcus, and R. Miikkulainen, editors, Proceedings of the NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches co-located with the 29th Annual Conference on Neural Information Processing Systems (NIPS 2015), Montreal, Canada, December 11-12, 2015, volume 1583 of CEUR Workshop Proceedings, 2015.
[7] X. Chen, C. Liu, and D. Song. Towards synthesizing complex programs from input-output examples. In International Conference on Learning Representations, 2018.
[8] N. Chomsky. Syntactic Structures. Mouton and Co., The Hague, 1957.
[9] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
[10] R. DessÃ¬ and M. Baroni. CNNs found to jump around more skillfully than RNNs: Compositional generalization in seq2seq convolutional networks. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3919â€“3923, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1381.
[11] N. Dethlefs and H. CuayÃ¡huitl. Combining hierarchical reinforcement learning and Bayesian networks for natural language generation in situated dialogue. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 110â€“120, Nancy, France, Sept. 2011. Association for Computational Linguistics.
[12] J. A. Fodor and E. Lepore. The compositionality papers. Oxford University Press, 2002.
[13] J. Gordon, D. Lopez-Paz, M. Baroni, and D. Bouchacourt. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations, 2020.
[14] S. Havrylov, G. Kruszewski, and A. Joulin. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1118â€“1128, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1115.
10

[15] D. Hupkes, V. Dankers, M. Mul, and E. Bruni. Compositionality decomposed: How do neural networks generalise? J. Artif. Intell. Res., 67:757â€“795, 2020. doi: 10.1613/jair.1.11674.
[16] Y. Jiang, S. Gu, K. Murphy, and C. Finn. Language as an abstraction for hierarchical deep reinforcement learning. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. dâ€™AlchÃ©-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 9414â€“9426, 2019.
[17] D. Keysers, N. SchÃ¤rli, N. Scales, H. Buisman, D. Furrer, S. Kashubin, N. Momchev, D. Sinopalnikov, L. Staï¬niak, T. Tihon, D. Tsarkov, X. Wang, M. van Zee, and O. Bousquet. Measuring compositional generalization: A comprehensive method on realistic data. In International Conference on Learning Representations, 2020.
[18] B. M. Lake. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural Information Processing Systems 32, pages 9791â€“9801. 2019.
[19] B. M. Lake and M. Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, StockholmsmÃ¤ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2879â€“2888. PMLR, 2018.
[20] B. M. Lake, T. Linzen, and M. Baroni. Human few-shot learning of compositional instructions. In Proceedings of the 41th Annual Meeting of the Cognitive Science Society, CogSci 2019: Creativity + Cognition + Computation, Montreal, Canada, July 24-27, 2019, pages 611â€“617, 2019.
[21] Y. Li, L. Zhao, J. Wang, and J. Hestness. Compositional generalization for primitive substitutions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4293â€“4302, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1438.
[22] Q. Liu, B. Chen, H. Liu, J.-G. Lou, L. Fang, B. Zhou, and D. Zhang. A split-and-recombine approach for follow-up query analysis. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5316â€“5326, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1535.
[23] J. Loula, M. Baroni, and B. Lake. Rearranging the familiar: Testing compositional generalization in recurrent networks. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 108â€“114, Brussels, Belgium, Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5413.
[24] Y. Lyu and I. W. Tsang. Curriculum loss: Robust learning and generalization against label corruption. In International Conference on Learning Representations, 2019.
[25] T. Mikolov, A. Joulin, and M. Baroni. A roadmap towards machine intelligence. In International Conference on Intelligent Text Processing and Computational Linguistics, pages 29â€“61. Springer, 2016.
[26] M. Mul and W. H. Zuidema. Siamese recurrent networks learn ï¬rst-order logic reasoning and exhibit zero-shot compositional generalization. CoRR, abs/1906.00180, 2019.
[27] M. I. Nye, A. Solar-Lezama, J. B. Tenenbaum, and B. M. Lake. Learning compositional rules via neural program synthesis. CoRR, abs/2003.05562, 2020.
[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8026â€“8037. 2019.
11

[29] J. Russin, J. Jo, R. C. Oâ€™Reilly, and Y. Bengio. Compositional generalization in a deep seq2seq model by separating syntax and semantics. CoRR, abs/1904.09708, 2019.
[30] D. Saxton, E. Grefenstette, F. Hill, and P. Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019.
[31] S. Sukhbaatar, a. szlam, J. Weston, and R. Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440â€“2448. 2015.
[32] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3104â€“3112. 2014.
[33] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and K. MÃ¼ller, editors, Advances in Neural Information Processing Systems 12, pages 1057â€“1063. MIT Press, 2000.
[34] Z. G. SzabÃ³. Compositionality. In E. N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, summer 2017 edition, 2017.
[35] K. S. Tai, R. Socher, and C. D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1556â€“1566. The Association for Computer Linguistics, 2015. doi: 10.3115/v1/p15-1150.
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998â€“6008, 2017.
[37] S. Veldhoen, D. Hupkes, and W. H. Zuidema. Diagnostic classiï¬ers revealing how neural networks process hierarchical structure. In CoCo@NIPS, 2016.
[38] L. Weaver and N. Tao. The optimal reward baseline for gradient-based reinforcement learning. In J. S. Breese and D. Koller, editors, UAI â€™01: Proceedings of the 17th Conference in Uncertainty in Artiï¬cial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, pages 538â€“545, 2001.
[39] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8:229â€“256, 1992. doi: 10.1007/BF00992696.
[40] M. D. Zeiler. AdaDelta: An adaptive learning rate method. CoRR, abs/1212.5701, 2012.
[41] T. Zhang, M. Huang, and L. Zhao. Learning structured representation for text classiï¬cation via reinforcement learning. In S. A. McIlraith and K. Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artiï¬cial Intelligence, (AAAI-18), the 30th innovative Applications of Artiï¬cial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiï¬cial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 6053â€“6060. AAAI Press, 2018.
12

A Tree-LSTM Encoding

As mentioned in Sec. 3, a Tree-LSTM [35] model is employed to accomplish the merge process in
Composer. Similar to LSTM, Tree-LSTM uses gate mechanisms to control the ï¬‚ow of information
from child nodes to parent nodes. Meanwhile, it maintains a hidden state and a cell state analogously. Denoting rli as the node representation of i-th node at layer l, it consists of the hidden state vector hli and the cell state vector cli. For any parent node, its node representation rli (l > 1) can be obtained by merging its left child node representation rliâˆ’1 = (hliâˆ’1, cliâˆ’1) and right child node representation rliâˆ’+11 = (hliâˆ’+11, cliâˆ’+11) as:

ï£®o ï£¹ ï£®Ïƒ ï£¹

ï£¯ filâˆ’1 ï£º ï£¯ Ïƒ ï£º

hlâˆ’1

ï£¯ï£¯ï£° feil+âˆ’11 ï£ºï£ºï£» = ï£¯ï£¯ï£° ÏƒÏƒ ï£ºï£ºï£» Wtree hiliâˆ’+11 + btree , (6)

g

tanh

cli = filâˆ’1 cliâˆ’1 + fil+âˆ’11 cliâˆ’+11 + e g, hli = o tanh cli ,

where Wtree âˆˆ R5DhÃ—2Dh is a learnable matrix, btree âˆˆ R5Dh is a learnable vector, Ïƒ and tanh
are activation functions, and represents the element-wise product. As for leaf nodes, their representations rli (l = 1) can be obtained by applying leaf transformation on the embeddings of their corresponding elements wit (e.g. $x, after) as:

r1 = h1i = W Emb wt + b ,

(7)

i

c1i

leaf

i

leaf

where Wleaf âˆˆ R2DhÃ—Dh is a learnable matrix, bleaf âˆˆ R2Dh is a learnable vector, wit is the i-th element of wt, and Emb(wit) âˆˆ RDh represents the word embedding if wit is a word, otherwise the key vector of the source domain variable wit.

B Details about Policy

In the following, we will explain the high-level policy Ï€Î¸ and the low-level policy Ï€Ï• in detail. For the sake of clarity, we simplify st, Gt and at as s, G and a, respectively.
High-level policy Given s, the high-level agent picks G according to the high-level policy Ï€Î¸(G | s) parameterized by Î¸. As mentioned in Sec. 3, G is obtained by applying in turn the merge and check process. Denoting the decisions made in the merge and check process at layer l as Ml and Cl, they are governed by parameters Î¸M and Î¸C, respectively. A high-level action G is indeed a sequence of M and C as (M1C1 Â· Â· Â· MLCL), where L represents the highest layer. Therefore, Ï€Î¸(G | s) is expanded as:

L
Ï€Î¸ (G = (M1C1 Â· Â· Â· MLCL) | s) = Ï€Î¸M (Ml | s, M<l, C<l) Ï€Î¸C (Cl | s, M<l+1, C<l) , (8)
l=1

where Ï€Î¸M is implemented by a Tree-LSTM with a learnable query vector q (mentioned in Sec. 3.2). Assuming there are K parent node candidates for layer l, Ml is a one-hot vector drawn from a
K-dimensional categorical distribution Ï€Î¸M (Ml | s, M<l, C<l) with the weight (p1, Â· Â· Â· , pK ). For the k-th parent node candidate, represented by rlk+1, its selection probability pk is computed by normalizing over all merging scores (mentioned in Sec. 3.2) as:

exp q, rlk+1

pk = K

l+1 .

(9)

k=1 exp q, rk

As for Ï€Î¸C (Cl | s, M<l+1, C<l) in the check process, it follows a Bernoulli distribution with expectation plc = Ïƒ(Wcrlk+1 + bc), where Î¸C = {Wc, bc} are learned parameters. plc is indeed the trigger probability pc mentioned in Sec. 3.2.

13

Dataset
Train Size Test Size

Simple
16728 4182

Table 4: The dataset splits for all tasks.

SCAN Add Jump Around Right Length MCD (1/2/3)

14670 7706

15225 4476

16990 3920

8365 1045

SCAN-ext Extend
20506 4000

MiniSCAN Limit
14 8

Low-level policy When the high-level action G is determined, the low-level agent is triggered to output a according to the low-level policy Ï€Ï•(a | G, s). The policy Ï€Ï•(a | G, s) is implemented by an LSTM-based sequence to sequence network with an attention mechanism, i.e.,

M

Ï€Ï•(a = (a1 Â· Â· Â· aM ) | G, s) = Ï€Ï• (am | G, s, a<m) ,

(10)

m=1

where M is the number of decoding steps and am represents an action word (e.g. JUMP), or a destination variable (e.g. $Y) which will be replaced by its corresponding constant DstExp stored in Memory. At each decoding step, am is sampled from a categorical distribution, whose sample space consists of all action words and destination variables with non-empty value slots.

C Chain Rule Derivation

Looking back to Eq. 2, the parameters Î¸ and Ï• can be optimized by ascending the following gradient:

âˆ‡Î¸,Ï•J (Î¸, Ï•) = EÏ„âˆ¼Ï€Î¸,Ï• R(Ï„ )âˆ‡Î¸,Ï• log Ï€Î¸,Ï• (Ï„ ) ,

(11)

where the policy Ï€Î¸,Ï• can be further decomposed into a sequence of actions and state transitions:

Ï€Î¸ , Ï•(Ï„ ) = p(s1G1a1 Â· Â· Â· sT GT aT )

T

(12)

= p(s1) Ï€Î¸,Ï•(at, Gt | st) p(st+1 | st, Gt, at).

t=1

Consider that the low-level action at is conditioned on the high-level action Gt, which means that Ï€Î¸,Ï•(at, Gt | st) = Ï€Î¸(Gt | st)Ï€Ï•(at | Gt, st), and thus Ï€Î¸ , Ï•(Ï„ ) can be expanded as:

T

Ï€Î¸ , Ï•(Ï„ ) = p(s1) Ï€Î¸(Gt|st)Ï€Ï•(at|Gt, st)p(st+1|st, Gt, at).

(13)

t=1

Since the state at step t + 1 is fully determined by the state and actions at step t, not dependent on the policy parameters Î¸ and Ï•, the gradients of p(st+1 | st, Gt, at) and p(s1) with respect to Î¸ and Ï• are
0. Therefore, âˆ‡Î¸,Ï•J (Î¸, Ï•) can be expanded as:

âˆ‡Î¸,Ï•J (Î¸, Ï•) = EÏ„âˆ¼Ï€Î¸,Ï• R(Ï„ )âˆ‡Î¸,Ï• log Ï€Î¸,Ï•(Ï„ ),

T

= EÏ„ âˆ¼Ï€Î¸,Ï• R(Ï„ )âˆ‡Î¸,Ï•

log Ï€Î¸ Gt|st + log Ï€Ï• at|Gt, st ,

t=1

(14)

T

= EÏ„ âˆ¼Ï€Î¸,Ï• R(Ï„ )

âˆ‡Î¸,Ï• log Ï€Î¸ Gt|st + âˆ‡Î¸,Ï• log Ï€Ï• at|Gt, st .

t=1

D Data Splits

As for data splits, we split each dataset into the train set and the test set for all tasks according to previous works. More details about train and test sizes can be seen in Tab. 4. More speciï¬cally, except for the task Limit, we further randomly take 20% training data as the development set to tune the hyperparameters, with the rest being the train set.

14

