Hierarchical Coding for Distributed Computing
Hyegyeong Park, Kangwook Lee, Jy-yong Sohn, Changho Suh and Jaekyun Moon School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST)
Email: {parkh, kw1jjang, jysohn1108, chsuh}@kaist.ac.kr, jmoon@kaist.edu

arXiv:1801.04686v1 [cs.DC] 15 Jan 2018

Abstractâ€”Coding for distributed computing supports lowlatency computation by relieving the burden of straggling workers. While most existing works assume a simple master-worker model, we consider a hierarchical computational structure consisting of groups of workers, motivated by the need to reï¬‚ect the architectures of real-world distributed computing systems. In this work, we propose a hierarchical coding scheme for this model, as well as analyze its decoding cost and expected computation time. Speciï¬cally, we ï¬rst provide upper and lower bounds on the expected computing time of the proposed scheme. We also show that our scheme enables efï¬cient parallel decoding, thus reducing decoding costs by orders of magnitude over non-hierarchical schemes. When considering both decoding cost and computing time, the proposed hierarchical coding is shown to outperform existing schemes in many practical scenarios.
I. INTRODUCTION
Enabling large-scale computations for big data analytics, distributed computing systems have received signiï¬cant attention in recent years [1]. The distributed computing system divides a computational task to a number of subtasks, each of which is allocated to a different worker. This helps reduce computing time by exploiting parallel computing options and thus enables handling of large-scale computing tasks.
In a distributed computing system, the â€œstragglersâ€, which refers to the computing nodes that slow down in some random fashion due to a variety of factors, may increase the total runtime of the computing system. To address this problem, the notion of coded computation is introduced in [2] where an (n, k) maximum distance separable (MDS) code is employed to speed up distributed matrix multiplications. The authors show that for linear computing tasks, one can design n distributed computing tasks such that any k out of n tasks sufï¬ce to complete the assigned task. Since then, coded computation has been applied to a wide variety of task scenarios such as matrix-matrix multiplication [3], [4], distributed gradient computation [5]â€“[8], convolution [9], Fourier transform [10] and matrix sparsiï¬cation [11], [12].
While the idea of coded computation has been studied in various settings, existing works have not taken into account the underlying hierarchical nature of practical distributed systems [13]â€“[15]. In modern distributed computing systems, each group of workers is collocated in the same rack, which contains a Top of Rack (ToR) switch, and cross-rack communication is available only via these ToR switches. Surveys on real cloud computing systems show that cross-rack communication through the ToR switches is highly unstable due to the limited bandwidth, whereas intra-rack communication is faster and more reliable [14], [15]. A natural question is whether one

Group-master M
communication

SM

SM

SM

Completion time

Î¼1

of the worker

...

...

WWW

WWW

WWW

Group

Group

Group

Fig. 1. Illustration of the hierarchical computing system

can devise a coded computation scheme that exploits such hierarchical structure.

A. Contribution
In this work, we ï¬rst model a distributed computing system with a tree-like hierarchical structure illustrated in Fig. 1, which is inspired by the practical computing systems in [13]â€“ [15]. The workers (denoted by â€œWâ€) are divided into groups, each of which has a submaster (denoted by â€œSMâ€). Each submaster sends the computational result of its group to the master (denoted by â€œMâ€). The suggested model can be viewed as a generalization of the existing non-hierarchical coded computation.
In this framework, we propose a hierarchical coding scheme which employs an (n1(i), k1(i)) MDS code within group i and another (n2, k2) outer MDS code across the groups as depicted in Fig. 2. We also develop a parallel decoding algorithm which exploits the concatenated code structure and allows low complexity.
Moreover, we analyze the latency performance of our proposed solution. It turns out that the latency performance of our scheme cannot be analyzed via simple order statistics as in other existing schemes. Here we resort to ï¬nd lower and upper bounds on the average latency performance: Our upper bound relies on concentration inequalities, and our lower bound is obtained via constructing and analyzing an auxiliary Markov chain (to be detailed later).

B. Related Work
Previous works on coded computation have rarely considered the inherent hierarchical structure of most real-world systems. Whereas a very recent work [16] deals with the multirack computing system reï¬‚ecting imbalance between intraand cross-rack communications, it is based on the settings of the coded MapReduce architecture which do not include general linear computation tasks that we focus on in this work. Another distinction is that the analysis of [16] includes only

(n1(i), k1(i)) MDS coding within group i

(n2, k2) MDS coding
across groups

â€¦

Group 1 w(1, 1) w(1, 2)

â€¦ w(1, n1(1))

Group 2

â€¦
w(2, 1) w(2, 2) w(1, n1(2))

Group n2

â€¦

w(n2, 1) w(n2, 2)

w(1, n1(n2))

Master Submaster

Fig. 2. Illustration of the proposed coding scheme applied to the hierarchical computing system. An (n1(i), k1(i)) MDS code is employed within group i, and an (n2, k2) MDS code is applied across the groups. w(i, j) denotes
worker j in group i.

the cross-rack redundancy whereas our analysis considers both intra- and cross-group coding.

C. Notations

We use boldface uppercase letters for matrices and boldface

lowercase letters for vectors. The transpose of a matrix A is

denoted

by

AT .

For

a

matrix

A

satisfying

AT

=

[AT1

A

T 2

]

,

we write A = [A1; A2]. For a positive integer n, the set {1, 2, . . . , n} is denoted by [n]. The jth worker in group i is represented by w(i, j) for i âˆˆ [n2] and j âˆˆ [n(1i)]. The symbol

r indicates the largest integer less than or equal to a real

number r.

II. HIERARCHICAL CODED COMPUTATION

A. Proposed Coding Scheme

Consider a matrix-vector multiplication task, i.e., computing

Ax for a matrix A âˆˆ RmÃ—d and a vector x âˆˆ RdÃ—1.

The input matrix A is split into k2 submatrices as A = [A1; A2; . . . ; Ak2 ], where Ai âˆˆ R km2 Ã—d for i âˆˆ [k2]. Here

we assume that m is divisible by k2 for simplicity. Then, we

apply an (n2, k2) MDS code to set {Ai}iâˆˆ[k2] in obtaining

{Ai}iâˆˆ[n2]. Then, each coded matrix Ai is further divided

into k1(i) submatrices as Ai = [Ai,1; Ai,2; . . . ; Ai,k(i) ] where

m Ã—d

1

A âˆˆ R (i)

i,j

k1 k2

for j âˆˆ [k1(i)] and m divisible by k1(i)k2.

Afterwards, for each i âˆˆ [n2], we apply an (n(1i), k1(i)) MDS

code to set {Ai,j }jâˆˆ[k(i)] to obtain {Ai,j }jâˆˆ[n(i)]. Then, for

1

1

each i âˆˆ [n2] and j âˆˆ [n(1i)], worker w(i, j) computes

Ai,jx. Fig. 2 illustrates the proposed coding scheme for

the hierarchical computing system with a different number of workers in each group. In the case of n(1i) = n1 and k1(i) = k1 for all i âˆˆ [n2], we will refer this coding scheme as

(n1, k1) Ã— (n2, k2) coded computation.

We present our code in Fig. 3 via a toy example. In this

example, (n1, k1) = (n2, k2) = (3, 2). That is, the input

matrix A = [A1; A2] is encoded via (n2, k2) = (3, 2) MDS

code, yielding [A1; A2; A1 + A2]. Afterwards, the matrix

Ai = [Ai,1; Ai,2] is encoded via an (n1, k1) = (3, 2)

MDS code, producing [Ai,1; Ai,2; Ai,1 + Ai,2]. For notational

simplicity, we deï¬ne A3 = A1 + A2 and Ai,3 = Ai,1 + Ai,2.

Group 1

ğ‘¨ğ‘¨ = ğ‘¨ğ‘¨1; ğ‘¨ğ‘¨2 ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘– = ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘–,1; ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘–,2 ğ‘¨ï¿½ğ‘¨1,1 x

(3,2) MDS (3,2) MDS

ğ‘¨ï¿½ğ‘¨1; ğ‘¨ï¿½ğ‘¨2; ğ‘¨ï¿½ğ‘¨3 â‰” ğ‘¨ï¿½ğ‘¨1 + ğ‘¨ï¿½ğ‘¨2 ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘–,1; ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘–,2; ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘–,3 â‰” ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘–,1 + ğ‘¨ï¿½ğ‘¨ğ‘–ğ‘–,2

ğ‘¨ï¿½ğ‘¨1,2 x

ğ‘¨ï¿½ğ‘¨1,3 x = (ğ‘¨ï¿½ğ‘¨1,1 +ğ‘¨ï¿½ğ‘¨1,2) x

Group 2

ğ‘¨ï¿½ğ‘¨2,1 x

ğ‘¨ï¿½ğ‘¨2,2 x

ğ‘¨ï¿½ğ‘¨2,3 x = (ğ‘¨ï¿½ğ‘¨2,1 +ğ‘¨ï¿½ğ‘¨2,2) x

Group 3 ğ‘¨ï¿½ğ‘¨3,1 x = (ğ‘¨ï¿½ğ‘¨1,1 +ğ‘¨ï¿½ğ‘¨2,1) x

ğ‘¨ï¿½ğ‘¨3,2 x = (ğ‘¨ï¿½ğ‘¨1,2 +ğ‘¨ï¿½ğ‘¨2,2) x ğ‘¨ï¿½ğ‘¨3,3 x = (âˆ‘ğ‘–2ğ‘–=1 âˆ‘ğ‘—2ğ‘—=1 ğ‘¨ï¿½ğ‘¨i,j) x

Fig. 3. Allocation of the computational task to workers in a (3, 2) Ã— (3, 2) coded computation

For i, j âˆˆ {1, 2, 3}, worker w(i, j) computes Ai,jx. Note that group i is assigned a subtask with respect to Ai.
We now describe the decoding algorithm for our proposed coding scheme. When a worker completes its task, it sends the result to its submaster. With the aid of the (n1, k1) MDS code, submaster i (in group i) can compute Aix as soon as the task results from any k1 workers within group i are collected. Once Aix is computed, it is sent to the master. The master can obtain Ax by retrieving Aix from any k2 submasters. For each worker, we deï¬ne completion time as the sum of the runtime of the worker and the time required for delivering its computation result to the submaster. For each group, we further deï¬ne intragroup latency as the time for completing its assigned subtask. The total computation time is deï¬ned as the time from when the workers start to run until the master completes computing Ax. The proposed computation framework can be applied to practical multi-rack systems where the input data A is coded and distributed into n2 racks; the ith rack contains Ai. For instance, in the Facebookâ€™s warehouse cluster, data is encoded with a (14, 10) MDS code, and then the 14 encoded chunks are stored across different racks [17]. Once x is given from the master, the ith rack can compute Aix using the coded data Ai that it contains.
B. Application: Matrix-Matrix Multiplications
Our scheme can be also applied to matrix-matrix multiplications. More speciï¬cally, consider computing AT B for given matrices A and B = [b1 b2 Â· Â· Â· bk2 ]. After applying an (n2, k2) MDS code to B, we have BË‡ = [bË‡1 bË‡2 Â· Â· Â· bË‡n2 ]. Moreover, group i divides A into k1(i) equal-sized submatrices as A = [Ai,1 Ai,2 Â· Â· Â· Ai,k(i) ], and we apply an (n(1i), k1(i)) MDS code, resulting in AË‡ 1i = [AË‡ i,1 AË‡ i,2 Â· Â· Â· AË‡ (i) ]. The
i,n1
computation AË‡ Ti,jbË‡i is assigned to worker w(i, j). Using an (n(1i), k1(i)) MDS code, submaster i can compute AT bË‡i when any k1(i) workers within its group delivered their computation results. The master can calculate AT B by gathering AT bË‡i results from any k2 submasters, using the (n2, k2) MDS code. Under the homogeneous setting of n(1i) = n1 and k1(i) = k1 for all i âˆˆ [n2], the encoding algorithm of the proposed scheme reduces to that of the product coded scheme [3]. However, the suggested scheme with the homogeneous setting is shown to reduce the decoding cost compared to the product coded

Group 1 Group 2 Group 3

S1 S2 S3 T(2) T(4) T(6)

Time

Fig. 4. Illustration of obtaining L in a (3, 2) Ã— (3, 2) coded computation

4, 2

5, 2

6, 2 v = k2

Âµ2

Âµ2

2Âµ2

7Âµ1

6Âµ1

5Âµ1

4Âµ1

2, 1

3, 1

4, 1

5, 1

6, 1

Âµ2

Âµ2

2Âµ2

2Âµ2

3Âµ2

9Âµ1

8Âµ1

7Âµ1

6Âµ1

5Âµ1

4Âµ1

0, 0

1, 0

2, 0

3, 0

4, 0

5, 0

6, 0

v=0

scheme under the hierarchical computing structure: a detailed analysis is in Sec. IV.

III. LATENCY ANALYSIS

We start by providing some preliminaries for the order

statistics. For n random variables, the kth order statistic is

deï¬ned by the kth smallest one of n. From the known results

from the order statistics [18], the expected value of the kth

order statistics out of n (n > k) exponential random variables

with rate Âµ is (Hn âˆ’ Hnâˆ’k)/Âµ, where Hk =

k1 l=1 l

log k + Î³ as k grows for a ï¬xed constant Î³. This leads to

(Hn âˆ’ Hnâˆ’k)/Âµ Âµ1 log nâˆ’nk . For n = k, the expected

latency is given by Hn/Âµ (log n)/Âµ. Further, deï¬ne

H0 := 0 for ease of exposition. Consider the hierarchical computing system1 of Fig. 2.

Assume that for i âˆˆ [n2] and j âˆˆ [n1], the completion time

Ti,j of worker w(i, j) is exponentially distributed with rate Âµ1 (i.e., Pr[Ti,j â‰¤ t] = 1 âˆ’ eâˆ’Âµ1t). Further, the communication time Ti(c) from the ith group to the master is also exponentially distributed with rate Âµ2 (i.e., Pr[Ti(c) â‰¤ t] = 1 âˆ’ eâˆ’Âµ2t).
Here, we assume that all latencies are independent with one

another. Given the assumptions, the total computation time of

the (n1, k1) Ã— (n2, k2) coded computation is written as

T = k2th min Ti(c) + Si

(1)

iâˆˆ[n2 ]

where

Si = k1th min Ti,j

(2)

j âˆˆ[n1 ]

denotes the time to wait for the k1 fastest workers in group
i. The group index i is relabeled such that S1 â‰¤ S2 â‰¤ Â· Â· Â· â‰¤
Sn2 . In other words, the fastest group that ï¬nishes its assigned subtask is relabeled as the 1st group, while the slowest group is relabeled as the n2th group. Here we provide upper and lower
bounds on E[T ].

A. Lower Bound
Let T(m) be the mth smallest element of {Ti,j }iâˆˆ[n2],jâˆˆ[n1]. Then, T(1) â‰¤ T(2) â‰¤ Â· Â· Â· â‰¤ T(n1n2) holds. Using this notation, we derive a lower bound on E[T ], formally stated below.
Theorem 1: The expected total computation time of the (n1, k1) Ã— (n2, k2) coded computation is lower bounded as

E[T ] â‰¥ E k2th min Ti(c) + T(ik1) := L .

(3)

iâˆˆ[n2 ]

1For simplicity of analysis, we only consider the homogeneous setting of n(1i) = n1 and k1(i) = k1 for all i âˆˆ [n2].

u=0

u = k1

u = k2k1

u = n2k1

Fig. 5. State transition diagram producing the lower bound L on the expected latency in a (3, 2) Ã— (3, 2) coded computation. Each state is labeled with (u, v), where u is the number of completed workers and v is the number of groups that have sent their computation results.

Proof: Consider a realization of {Ti,j }iâˆˆ[n2],jâˆˆ[n1] and {Ti(c)}iâˆˆ[n2]. Recall that a group ï¬nishes its assigned subtask if k1 workers within the group complete their tasks. Hence, it is impossible for the ith group to ï¬nish its work if the total
number of completed workers in the system is less than ik1.
In other words, it must hold that

T(ik1) â‰¤ Si for all i âˆˆ [n2] .

(4)

Thus, the total computation time in (1) should be:

T = k2th min(Ti(c) + Si) â‰¥ k2th min(Ti(c) + T(ik1)) .

iâˆˆ[n2 ]

iâˆˆ[n2 ]

Averaging over all possible realizations, we complete the proof.
To further illustrate the proof, we provide a schematic example in Fig. 4. Consider a (3, 2) Ã— (3, 2) coded computation. The yellow circles denote the completion times of the workers. After k1 = 2 workers in a group ï¬nish their computations, the group-master communication, shown as the red arrows, starts from each group. As can be seen, T(2) â‰¤ S1, T(4) â‰¤ S2 and T(6) â‰¤ S3, which concur with (4). The following lemma shows that L can be computed by analyzing the hitting time of an auxiliary Markov chain.
Lemma 1: Let C be the continuous-time Markov chain deï¬ned over the state space (u, v) âˆˆ {0, 1, . . . , n2k1} Ã— {0, 1, . . . , k2}. The state transition rates of C are deï¬ned as follows:
â€¢ From state (u, v) to state (u + 1, v) at rate (n1n2 âˆ’ i)Âµ1, if vk1 â‰¤ u < n2k1,
â€¢ From state (u, v) to state (u, v + 1) at rate ( u/k1 âˆ’ v) Âµ2, if 0 â‰¤ v < min { u/k1 , k2}.
Then, the expected hitting time of C from state (0, 0) to the set of states {(u, k2)}nu=2kk12k1 is equal to L.
Proof: See Appendix A for the proof. Markov chain C deï¬ned in Lemma 1 consists of the states (u, v), where u represents the number of completed workers and v indicates the number of groups which have delivered their computation results to the master. For an illustrative example, the state transition diagram for a (3, 2) Ã— (3, 2) coded computation yielding a lower bound is shown in Fig. 5. The overall computation is terminated when

Expected total computation time Expected total computation time

4

Upper bound (Lemma 2)

Upper bound (Theorem 2)

3

Simulation

Lower bound

2

4

Upper bound (Lemma 2)

Upper bound (Theorem 2)

3

Simulation

Lower bound

2

1

1

0 1 2 3 4 5 6 7 8 9 10 k
2
(a) n1 = 10, k1 = 5, n2 = 10

0 1 2 3 4 5 6 7 8 9 10 k
2
(b) n1 = 600, k1 = 300, n2 = 10

Fig. 6. The expected total computation time of the (n1, k1)Ã—(n2, k2) coded computing with its bounds for varying k2

the k2 = 2 groups ï¬nish conveying their computational results to the master, i.e., when the Markov chain visits the states with v = 2 for the ï¬rst time. We see that u increases by one when a worker completes its computation, and v increases by one when master receives the computation result from a group. The rightward transition (to increase u) rate is determined by the product of Âµ1 and the number of remaining workers. The upward transition (to increase v) rate is the product of Âµ2 and the number of groups that have not delivered their computation results to the master. The proposed lower bound L can be easily computed from the ï¬rst-step analysis [19] of the Markov chain produced by Lemma 1.
B. Upper Bound
We here provide two upper bounds on the expected total computation time. The ï¬rst bound in the following lemma is applicable for all values of n1 and k1.
Lemma 2: The expected total computation time of the (n1, k1) Ã— (n2, k2) coded computation is upper bounded as E[T ] â‰¤ Hn1n2 /Âµ1 + (Hn2 âˆ’ Hn2âˆ’k2 )/Âµ2 .
Proof: See Appendix B for the proof. We now establish another upper bound using the following two steps. First we ï¬nd an upper bound on the maximum intragroup latency among n2 groups. Afterwards, adding this value to the expected latency of the group-master communication yields an upper bound on the expected total computation time. For given n1 and k1, we use Î´1 > 0 which satisï¬es n1 = (1 + Î´1)k1. We now present the asymptotic upper bound as follows. Theorem 2: For a ï¬xed constant Î´1 > 0, the expected total computation time of the (n1, k1) Ã— (n2, k2) coded computing system is upper bounded as E[T ] â‰¤ [log(1 + Î´1)/Î´1]/Âµ1 + (Hn2 âˆ’ Hn2âˆ’k2 )/Âµ2 + o(1) in the limit of k1.
Proof: See Appendix C for the proof.
C. Evaluation of Bounds
Fig. 6 shows the behavior of the expected total computation time and its upper/lower bounds with varying k2. Here we consider two upper bounds proposed in Lemma 2 and Theorem 2. To see the impact of k1, the values of k1 are ï¬xed to 5 and 300 in Figs. 6a and 6b, respectively. The other code parameters are set to n1 = (1 + Î´1)k1, n2 = 10 for both ï¬gures, where Î´1 is ï¬xed to 1. The rates of the completion

time of the worker and group-master communication are set to Âµ1 = 10 and Âµ2 = 1. For a relatively small values of k1, the upper bound in Lemma 2 is a tighter upper bound than the upper bound in Theorem 2. As can be seen in Fig. 6b, the asymptotic upper bound in Theorem 2 becomes tighter as k1 grows, which concurs with Theorem 2. We also have numerically conï¬rmed that the proposed lower bound is tight.
IV. DECODING COMPLEXITY
In this section, we compare decoding complexity of our hierarchical coding with the replication and non-hierarchical coding schemes including the (n1, k1)Ã—(n2, k2) product code [3] and the (n, k) polynomial code [4]. For fair comparison, we set n = n1n2 and k = k1k2. We further assume that the decoding complexity of the (n, k) MDS code is O(kÎ²) for some Î² > 1.2 In our framework, the n2 intra-group codes can be decoded in parallel, followed by decoding of the crossgroup code using the k2 fastest results. Thus, the overall decoding procedure consists of 1) parallel decoding of (n1, k1) intra-group MDS codes and 2) decoding of the (n2, k2) crossgroup MDS code, resulting in the total decoding cost of O(k1Î² + k1k2Î²). Similarly, one can show that the decoding cost of polynomial codes is O(kÎ²), and that of product code is O(k1k2Î² + k2k1Î²). We note that the hierarchical code can have a substantial improvement, sometimes by an order of magnitude, in decoding complexity, compared to the product code. For instance, if Î² = 2 and k1 = k22, the decoding cost of hierarchical code becomes O(k24) while that of the product code is O(k25); if k1 = k21.5, the decoding costs are O(k23.5) and O(k24), respectively. In general, if k1 = k2p, one can show that the relative gain of the hierarchical codes in decoding cost monotonically increases as p increases, providing a guideline for efï¬cient code designs. Table I summarizes the computing times and decoding costs of various coding schemes.
We now compare the expected total execution time deï¬ned as Texec := Tcomp + Î±Tdec, where Tcomp is the computing time, Tdec is the decoding cost, and Î± â‰¥ 0 is the relative weight of the decoding cost. We note that Î± is a systemspeciï¬c parameter that depends on 1) the relative CPU speed of the master compared to the workers and 2) dimension of the input data. Shown in Fig. 7 are the expected total execution times for parameters of (n1, k1) = (800, 400), (n2, k2) = (40, 20), (Âµ1, Âµ2) = (10, 1) and Î² = 2.
We ï¬rst observe that with all tested practical values of Âµ1 and Âµ2, the hierarchical code strictly outperforms the product code for all values of Î±. Further, we observe that the optimal choice of coding scheme depends on the value of Î± as follows:
â€¢ (moderate Î±) when both Tcomp and Tdec have to be minimized, the hierarchical code achieves the lowest Texec by striking a balance between them;
â€¢ (low Î±) when Tdec is negligible, the polynomial code achieves the lowest Texec; and
â€¢ (high Î±) when Tdec dominates Texec, the replication code is the best.
2Note that this is the case for most practical decoding algorithms [20], [21]. Decoding with Î² = 1 requires a large ï¬eld size [4].

TABLE I COMPARISONS OF VARIOUS CODING SCHEMES

Coding scheme Replication
Hierarchical code
Product code [3]
Polynomial code [4]

Computing time (Tcomp)

kHk /(nÂµ2 )

Âµ12 log

âˆšE[T ] âˆš nâˆš/k+ 4 n/k
n/kâˆ’1

(Hn âˆ’ Hnâˆ’k)/Âµ2

Decoding cost (Tdec) 0
O(k1k2Î² + k1Î² ) O(k1k2Î² + k2k1Î² )
O(k1Î² k2Î² )

ğ‘»ğ‘»ğŸ(ğŸğ’„ğ’„) ğ‘»ğ‘»ğŸ(ğŸğ’„ğ’„)

The lower bound ğ“›ğ“› : the time when the ğ’Œğ’ŒğŸğ­ğ­ğŸğ­ğ­ fastest
red dashed arrow finishes

â‹¯

ğ‘»ğ‘»ğ’(ğ’ğ’„ğŸğ’„ğŸ)
0 ğ‘‡ğ‘‡(1) ğ‘‡ğ‘‡(2) â‹¯ ğ‘‡ğ‘‡(ğ‘˜ğ‘˜1) â‹¯ ğ‘‡ğ‘‡(2ğ‘˜ğ‘˜1) â‹¯ ğ‘‡ğ‘‡(ğ‘›ğ‘›2ğ‘˜ğ‘˜1)

Time

Fig. 8. Illustration of the lower bound L

variables {T(lk1)}l=u/1k1 , only v random variables satisfy t â‰¥ Tc(l) + T(lk1). Therefore, the transition from state (u, v) to state (u, v + 1) occurs with rate ( u/k1 âˆ’ v)Âµ2, for v âˆˆ {0, 1, Â· Â· Â· , min{ u/k1 , k2}âˆ’1}. Fig. 9 shows the consequent state transition diagram. This Markov chain is identical to C,
which completes the proof.

Fig. 7. E[Texec] of various coding schemes for parameters of (n1, k1) = (800, 400), (n2, k2) = (40, 20), (Âµ1, Âµ2) = (10, 1) and Î² = 2

Note that the shaded area in Fig. 7 represents the additional achievable (Î±, E[Texec]) region thanks to introducing the hierarchical code.

APPENDIX A
PROOF OF LEMMA 1
Note that the lower bound L in Theorem 1 can be illustrated
as in Fig. 8. The lower bound depends on two types of variables: {T(m)}m n2=k11, the set of n2k1 smallest realizations of n2n1 exponentially distributed random variables with rate Âµ1 and {Tc(l)}nl=21, the set of n2 exponentially distributed random variables with rate Âµ2. Consider arbitrary realizations of {T(m)} and {Tc(l)}. For a given time t, deï¬ne

u := max {t â‰¥ T(m)},

(5)

mâˆˆ[n2 k1 ]

v := {l âˆˆ [n2] : t â‰¥ Tc(l) + T(lk1)} .

(6)

Thus, each time slot t can be assigned to a state (u, v) for
u âˆˆ {0, 1, Â· Â· Â· , n2k1} and v âˆˆ {0, 1, Â· Â· Â· , n2}. From the deï¬nition of L in (3), the lower bound corresponds to the
expected time t to achieve v = k2. Thus, we consider the state space of (u, v) âˆˆ {0, 1, Â· Â· Â· , n2k1} Ã— {0, 1, Â· Â· Â· , k2}, and ï¬nd the expected time to arrive at states (u, v) with v = k2 from state (0, 0).
We now examine the state transition rates. From the def-
initions of T(m) and (5), the transition from state (u, v) to state (u + 1, v) occurs with rate (n1n2 âˆ’ u)Âµ1, since there are n1n2 âˆ’ u remaining {T(m)}nm1=nu2+1 such that t < T(m) holds. Moreover, for a given time t and the corresponding state (u, v), we have {T(lk1)}l=u/1k1 which satisï¬es t â‰¥ T(lk1), and v in (6) is expressed as

v = {l âˆˆ {1, 2, Â· Â· Â· , u/k1 } : t â‰¥ Tc(l) + T(lk1)} (7)

since Tc(l) is a random variable with nonnegative values. Thus, out of u/k1 activated (i.e., t â‰¥ T(lk1)) random

APPENDIX B PROOF OF LEMMA 2
Hn1n2 /Âµ1 is the maximum intra-group latency, which comes from waiting for all n1n2 workers. Assuming that every group starts the group-master communication at time Hn1n2 /Âµ1, the expected total computation time can be obtained by summing up the group-master communication time to Hn1n2 /Âµ1. The group-master communication time is calculated from the time that the k2th fastest group ï¬nishes communication to the master, which is given by (Hn2 âˆ’ Hn2âˆ’k2 )/Âµ2. This completes the proof.

APPENDIX C PROOF OF THEOREM 2

A part of the proof generalizes the idea of [3], which

analyzes the latency of the product code. First, we focus on the

intra-group latency of each group. The expected latency of the

k1th fastest worker out of n1 is given by (Hn1 âˆ’ Hn1âˆ’k1 )/Âµ1,

where the latency of a worker assumes an exponential distribu-

tion with rate Âµ1. Noticing that the expected completion time of a worker (Hn1 âˆ’ Hn1âˆ’k1 )/Âµ1 is rewritten as Âµ11 log 1+Î´1Î´1 for a ï¬xed constant Î´1 > 0 and a sufï¬ciently large n1, we

deï¬ne

t0 := 1 log 1 + Î´1 + Î±

Âµ1

Î´1

log k1 k1

for some constant Î± > 0.
Consider group i0 with n1 workers. Then, for worker w(i0, j), assume a Bernoulli random variable Xi0,j which takes 0 when worker w(i0, j) has completed its computation by time t0, and takes 1 otherwise. Then, probability p0 that Xi0,j takes 1 is:

p0 := Pr[Ti ,j > t0] = eâˆ’Âµ1t0 = Î´1 eâˆ’Âµ1Î±

0

1 + Î´1

log k1 k1

Î´1 1 + Î´1

1 âˆ’ Âµ1Î±

log k1 k1

, (8)

(n2 âˆ’ k2)Âµ2 (n2 âˆ’ k2)Âµ2

v = k2 (n2 âˆ’ k2 + 1)Âµ2

(n1n2 âˆ’ 2k1)Âµ1

... ... ...

(n2 âˆ’ 3)Âµ2 (n2 âˆ’ 3)Âµ2 (n1n2 âˆ’ 2k1 âˆ’ 1)Âµ1
...

(n2 âˆ’ 2)Âµ2 (n2k1 âˆ’ 1)Âµ1 ...

v=2

Âµ2

Âµ2

(n2 âˆ’ 2)Âµ2 (n2 âˆ’ 2)Âµ2

(n1n2 âˆ’ k1)Âµ1

(n1n2 âˆ’ k1 âˆ’ 1)Âµ1

(n1n2 âˆ’ 2k1 + 1)Âµ1

...

(n1n2 âˆ’ 2k1)Âµ1

(n1n2 âˆ’ 2k1 âˆ’ 1)Âµ1 ...

(n2 âˆ’ 1)Âµ2 (n2k1 âˆ’ 1)Âµ1 ...

v=1

Âµ2

Âµ2

2Âµ2

2Âµ2

(n2 âˆ’ 1)Âµ2 (n2 âˆ’ 1)Âµ2

n2 Âµ2

n1 n2 Âµ1

(n1n2 âˆ’ 1)Âµ1

(n1n2 âˆ’ k1 + 1)Âµ1

...

(n1n2 âˆ’ k1)Âµ1

(n1n2 âˆ’ k1 âˆ’ 1)Âµ1

(n1n2 âˆ’ 2k1 + 1)Âµ1

...

(n1n2 âˆ’ 2k1)Âµ1

(n1n2 âˆ’ 2k1 âˆ’ 1)Âµ1 ...

(n2k1 âˆ’ 1)Âµ1 ...

v=0

u=0

u=1

u = k1

u = k1 + 1

u = 2k1

u = 2k1 + 1

u = (n2 âˆ’ 1)k1

u = (n2 âˆ’ 1)k1 + 1

u = n2k1

u, v

Fig. 9. State transition diagram for the (n1, k1) Ã— (n2, k2) coded computation producing a lower bound. Any state is denoted by (u, v), where u describes the number of completed workers and v is the number of groups that sent their computation results.

where (8) follows because Âµ1Î± lokg1k1 is quite small with a sufï¬ciently large k1. Out of n1 workers in group i0, the set
of workers not completed by time t0 is represented as

St0 = {j âˆˆ [n1] : Ti0,j > t0} = {j âˆˆ [n1] : Xi0,j = 1}

with |St0 | representing the number of workers not completed by time t0, where |Â·| denotes the cardinality of a set.
Since Xi0,j is a Bernoulli random variable with parameter p0, the expected number of workers noâˆšt completed in group i0 is calculated as n1p0 = Î´1(k1 âˆ’ Âµ1Î± k1 log k1) for a given n1 = (1 + Î´1)k1. Recall that a group ï¬nishes its assigned subtask when k1 out of n1 workers in the group completed their works. At time t0, we thus denote a case where the number of stragglers in group i is greater than Î´1k1 = n1 âˆ’ k1 by an error event Ei for i âˆˆ [n2]. For group i0, we wish to ï¬nd an upper bound on the probability that Ei0 occurs, which is equivalent to the probability that group i0 has not ï¬nished its assigned subtask by time t0. We establish such a bound using Hoeffdingâ€™s inequality [22] to bound the deviation of
|St0 | from the mean:

âˆ’ 2t2
Pr[|St0 | âˆ’ Î´1(k1 âˆ’ Âµ1Î± k1 log k1) â‰¥ t] â‰¤ e (1+Î´1)k1 . âˆš
By setting t = Î´1Âµ1Î± k1 log k1, we obtain

âˆ’ 2Î´12Âµ21Î±2 log k

âˆ’ 2Î´12Âµ21Î±2
1+Î´

Pr [|St0 | â‰¥ Î´1k1] â‰¤ e 1+Î´1

1 = k1

1.

Combining all n2 groups, the upper bound on the probability that n2 groups not ï¬nished their assigned subtasks by time t0 is obtained by the union bound. Let TI be the time when all n2 groups ï¬nish their assigned subtasks. Hence, we have

Pr[TI > t0] = Pr[E1 âˆª E2 âˆª Â· Â· Â· âˆª En2 ]

(9)

n2

âˆ’ 2Î´12Âµ21Î±2

â‰¤ Pr[Ei] = n2k1 1+Î´1 = o(k1âˆ’1) ,

(10)

i=1

where the last equality holds since Î± can be made arbitrarily large. Then the expected intra-group latency E[TI ] satisï¬es

E[TI ] â‰¤ Pr[TI â‰¤ t0]t0 + Pr[TI > t0] Hn1n2 + t0 (11) Âµ1

= 1 âˆ’ o(kâˆ’1) t0 + o(kâˆ’1) Hn1n2 + t0

(12)

1

1

Âµ1

= 1 log 1 + Î´1 + o(1) , (13)

Âµ1

Î´1

where (11) is due to the fact that t0 is the worst case latency for all events satisfying TI â‰¤ t0, and Hn1n2 /Âµ1 + t0 is an upper bound on E[TI |TI > t0].
From (13), we conclude that all the n2 groups embark on the group-master communication before time Âµ11 log 1+Î´1Î´1 , as k1 grows large. Hence, adding the latency of the group-master
communication (Hn2 âˆ’ Hn2âˆ’k2 )/Âµ2 to (13) gives an upper bound on the expected total computation time. This completes
the proof of the case where n2 > k2. When n2 = k2, the group-master communication time is represented by Hn2 /Âµ2. Thus, adding this value to (13) completes the proof, using
H0 = 0.

REFERENCES
[1] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker, K. Yang, Q. V. Le et al., â€œLarge scale distributed deep networks,â€ in Proc. NIPS, 2012, pp. 1223â€“1231.
[2] K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran, â€œSpeeding up distributed machine learning using codes,â€ IEEE Trans. Inf. Theory, vol. PP, no. 99, pp. 1â€“1, 2017.
[3] K. Lee, C. Suh, and K. Ramchandran, â€œHigh-dimensional coded matrix multiplication,â€ in Proc. IEEE ISIT, June 2017, pp. 2418â€“2422.
[4] Q. Yu, M. Maddah-Ali, and S. Avestimehr, â€œPolynomial codes: An optimal design for high-dimensional coded matrix multiplication,â€ in Proc. NIPS, 2017, pp. 4406â€“4416.
[5] R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis, â€œGradient coding: Avoiding stragglers in distributed learning,â€ in Proc. ICML, 2017, pp. 3368â€“3376.
[6] W. Halbawi, N. Azizan-Ruhi, F. Salehi, and B. Hassibi, â€œImproving distributed gradient descent using Reed-Solomon codes,â€ arXiv:1706.05436, 2017.

[7] N. Raviv, I. Tamo, R. Tandon, and A. G. Dimakis, â€œGradient coding from cyclic MDS codes and expander graphs,â€ arXiv:1707.03858, 2017.
[8] Z. Charles, D. Papailiopoulos, and J. Ellenberg, â€œApproximate gradient coding via sparse random graphs,â€ arXiv:1711.06771, 2017.
[9] S. Dutta, V. Cadambe, and P. Grover, â€œCoded convolution for parallel and distributed computing within a deadline,â€ in Proc. IEEE ISIT, June 2017, pp. 2403â€“2407.
[10] Q. Yu, M. A. Maddah-Ali, and A. S. Avestimehr, â€œCoded Fourier transform,â€ Proc. Allerton Conf., 2017.
[11] S. Dutta, V. Cadambe, and P. Grover, â€œShort-Dot: Computing large linear transforms distributedly using coded short dot products,â€ in Proc. NIPS, 2016, pp. 2100â€“2108.
[12] G. Suh, K. Lee, and C. Suh, â€œMatrix sparsiï¬cation for coded matrix multiplication,â€ in Proc. Allerton Conf., 2017.
[13] J. Dean and S. Ghemawat, â€œMapReduce: Simpliï¬ed data processing on large clusters,â€ Commun. ACM, vol. 51, no. 1, pp. 107â€“113, 2008.
[14] F. Ahmad, S. T. Chakradhar, A. Raghunathan, and T. Vijaykumar, â€œShufï¬‚eWatcher: Shufï¬‚e-aware scheduling in multi-tenant MapReduce clusters.â€ in Proc. USENIX ATC, 2014, pp. 1â€“12.
[15] A. Vahdat, M. Al-Fares, N. Farrington, R. N. Mysore, G. Porter, and S. Radhakrishnan, â€œScale-out networking in the data center,â€ IEEE Micro, vol. 30, no. 4, pp. 29â€“41, 2010.
[16] S. Gupta and V. Lalitha, â€œLocality-aware hybrid coded MapReduce for server-rack architecture,â€ arXiv:1709.01440, 2017.
[17] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran, â€œA solution to the network challenges of data recovery in erasure-coded distributed storage systems: A study on the Facebook warehouse cluster.â€ in Proc. USENIX HotStorage, 2013.
[18] H. A. David and H. N. Nagaraja, Order Statistics. Wiley, New York, 2003.
[19] P. BrÃ©maud, Markov chains: Gibbs ï¬elds, Monte Carlo simulation, and queues. Springer Science & Business Media, 2013, vol. 31.
[20] W. Halbawi, Z. Liu, and B. Hassibi, â€œBalanced Reed-Solomon codes,â€ in Proc. IEEE ISIT, July 2016, pp. 935â€“939.
[21] â€”â€”, â€œBalanced Reed-Solomon codes for all parameters,â€ in Proc. IEEE ITW, Sept. 2016, pp. 409â€“413.
[22] W. Hoeffding, â€œProbability inequalities for sums of bounded random variables,â€ J. Am. Stat. Assoc., vol. 58, no. 301, pp. 13â€“30, 1963.

