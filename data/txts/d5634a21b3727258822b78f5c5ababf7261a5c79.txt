INVESTIGATING SELF-SUPERVISED LEARNING FOR SPEECH ENHANCEMENT AND SEPARATION
Zili Huang1, Shinji Watanabe2, Shu-wen Yang3, Paola GarcÂ´Ä±a1, Sanjeev Khudanpur1
1 Center for Language and Speech Processing and HLTCOE, Johns Hopkins University, USA 2 Carnegie Mellon University, USA 3 National Taiwan University, Taiwan

arXiv:2203.07960v1 [eess.AS] 15 Mar 2022

ABSTRACT
Speech enhancement and separation are two fundamental tasks for robust speech processing. Speech enhancement suppresses background noise while speech separation extracts target speech from interfering speakers. Despite a great number of supervised learning-based enhancement and separation methods having been proposed and achieving good performance, studies on applying self-supervised learning (SSL) to enhancement and separation are limited. In this paper, we evaluate 13 SSL upstream methods on speech enhancement and separation downstream tasks. Our experimental results on Voicebank-DEMAND and Libri2Mix show that some SSL representations consistently outperform baseline features including the short-time Fourier transform (STFT) magnitude and log Mel ï¬lterbank (FBANK). Furthermore, we analyze the factors that make existing SSL frameworks difï¬cult to apply to speech enhancement and separation and discuss the representation properties desired for both tasks. Our study is included as the ofï¬cial speech enhancement and separation downstreams for SUPERB.
Index Termsâ€” Self-Supervised Learning, Speech Enhancement, Speech Separation
1. INTRODUCTION
Speech enhancement and separation are two fundamental tasks for speech processing. The former suppresses background noises to improve speech quality and intelligibility while the latter extracts target speech from interfering speakers [1, 2]. Both techniques are commonly used as preprocessing steps for tasks like automatic speech recognition (ASR) and speaker diarization, especially under noisy conditions [3, 4, 5].
Over the past few years, deep learning-based methods have developed rapidly and become the mainstream for speech enhancement and separation, among which the supervised learning-based approaches are the most widely used ones. The supervised learningbased methods design objectives to approximate target signals either by estimating the spectral mask [6, 7, 8, 9, 10, 11, 12] or directly predicting the waveform [13, 14, 15, 16, 17, 18, 19].
Despite good performance, the supervised learning-based approaches are data-hungry and require a sufï¬cient amount of labeled data to perform well, which is expensive. Self-supervised learning has been proposed to address this issue. Unlike supervised learning which directly optimizes for a speciï¬c task, SSL ï¬rst pretrains models on unlabeled data to extract task-agnostic representations and then ï¬netune models on the target domain. SSL has drawn massive attention due to its great performance and generalization ability. Inspired by the great success of SSL in natural language

processing (NLP) [20, 21] and computer vision (CV) [22, 23], an increasing number of SSL frameworks for speech have been proposed [24, 25, 26] and successfully applied to various downstream tasks including ASR [27], speaker recognition [28], emotion recognition [29], spoken language understanding [30] etc.
To systematically explore the SSL paradigm for speech-related tasks, the Speech processing Universal PERformance Benchmark (SUPERB) [31] is proposed. It evaluates the performance of a shared model across a wide range of speech processing tasks including phoneme recognition, ASR, speaker identiï¬cation, automatic speaker veriï¬cation, speaker diarization, intent classiï¬cation, slot ï¬lling and emotion recognition, with minimal architecture changes and labeled data.
In this paper, we follow the principles of SUPERB [31] and further investigate SSL for speech enhancement and separation. We aim to 1) Compare existing SSL models for speech enhancement and separation tasks. 2) Figure out the desired representation properties and proper pretraining setups for both tasks. We hope our study could cast light on the future design of SSL frameworks for speech enhancement and separation.
2. RELATED WORK
2.1. Speech separation
With the advance of deep learning techniques, speech separation has witnessed rapid improvement. Most of the existing speech separation frameworks are based on supervised learning and can be divided into frequency-domain and time-domain methods. The former estimates time-frequency (T-F) mask for each source based on the STFT features and reconstructs individual sources using inverse short-time Fourier transform (iSTFT). Typical systems include Deep Clustering [10], uPIT [11], Deep Attractor Network [12] etc. The timedomain methods [15, 16, 17, 18, 19] take the waveform of mixtures as input and directly predict the waveform of different sources using an encoder-decoder architecture. They are achieving state-of-the-art results in recent years. For SSLâ€™s applications in speech separation, in [32], the authors ï¬nd self-supervised pretraining on enhancement data can stabilize the label assignment during separation training and improve separation performance.
2.2. Speech enhancement
Over the past few years, deep learning-based enhancement models have dominated this ï¬eld. Same as speech separation, speech enhancement methods can be divided into frequency and time domain. Among frequency-domain methods, [6] uses a recurrent neural network (RNN) to predict T-F masks. MMSE-GAN [7] generates

T-F masks using a generative adversarial network (GAN). MetricGAN [8] and MetricGAN+ [9] propose a method to train the generator with respect to enhancement evaluation metrics. Among timedomain methods, SEGAN [13] uses a GAN to directly generate the clean waveform. DEMUCS [14] use an encoder-decoder architecture with skip-connections to predict the clean waveform. Unlike separation, where time-domain methods are dominating, frequencydomain methods are still competitive for speech enhancement [9].

Noisy waveform

SSL representation ğ‘­ SSL model

RNN

Estimated masks ğ‘´ğŸ

Estimated STFT
ğ‘ºğŸ = ğ‘º âŠ— ğ‘´ğŸ

iSTFT

Clean waveform

STFT feature ğ‘º

Fig. 1. T-F mask-based speech enhancement downstream model. For speech separation, we estimate multiple masks from the RNN.

3. METHODOLOGY
3.1. Self-supervised pretrained models
In this paper, we evaluate 13 SSL upstream models from the S3PRL toolkit [31] on speech enhancement and separation downstream tasks. These SSL models can be categorized into generative and contrastive models [33].
Generative models train an encoder to transform input x to representation z, and try to reconstruct x with representation z [33]. The generative models we studied include APC [34], VQ-APC [35], NPC [36], Mockingjay [37] and TERA [38]. APC [34] follows a language model training style, and it uses a RNN to predict the future spectrum. VQ-APC [35] adds a vector quantization (VQ) layer on top of APC model to better control the model capacity. NPC [36] is proposed as a non-autoregressive alternative to APC. It uses convolution architectures and predicts the center frame based on left and right context. Inspired by BERT [21], Mockingjay [37] pretrains a Transformer encoder by predicting masked time frames. TERA [38] extends Mockingjay by also predicting masked frequency bins.
Contrastive models also train an encoder to transform input x to representation z but to measure similarity [33]. Among the contrastive models we use, CPC [39] combines predicting future observations with a contrastive loss InfoNCE. Modiï¬ed CTC [40] proposes several changes to the model architecture to improve training stability and model performance. wav2vec [24] uses the same InfoNCE objective but a larger CNN architecture. vq-wav2vec [41] adds a VQ layer to wav2vec, enabling the direct use of NLP models like BERT on top of it. wav2vec 2.0 [25] incorporates the vqwav2vec and BERT model into one end-to-end framework. Unlike BERT which predicts the masked tokens, wav2vec 2.0 still uses the contrastive loss as the objective. Inspired by DeepCluster [42], HuBERT [26] performs ofï¬‚ine clustering on representations, enabling it to avoid contrastive loss through directly predicting the cluster labels of the masked positions. UniSpeech-SAT [43] and WavLM [44] models are variants of the HuBERT model. The UniSpeech-SAT model adds an utterance-wise contrastive loss to enhance speaker information modeling and mixes original speech with interfering speakers as data augmentation. The WavLM model adds gated relative position bias to the Transformer structure and also uses utterance mixing augmentation (both interfering speech and noises are added).
In addition to these models, PASE+ [45] borrows ideas from both generative and contrastive models. It performs multiple SSL tasks including feature generation and contrastive learning to learn robust speech representations.

input and extracts speech representations F. Based on F, the RNN predicts the STFT mask M1 of the clean signal. The estimated mask M1 is multiplied with the STFT features S and transformed back to the time domain using iSTFT. The pipeline of separation is almost the same. The only difference is that the RNN will estimate multiple masks for different speakers. We use a three-layer bidirectional long short-term memory network (BLSTM) as the network architecture and the mean square error between the predicted mask and Ideal Non-negative Phase Sensitive Mask (INPSM) [11] is chosen as the objective. INPSM is deï¬ned as

Msinpsm = max 0, |Xs(t, f )| cos|Y(Î¸(yt,(tf,)f| ) âˆ’ Î¸s(t, f ))

where Y is the mixture signal, Xs is the signal from source s, |X(t, f )| is the STFT magnitude of signal X for time frame t and frequency bin f , Î¸y and Î¸s are the phases of the mixture and source s. For speech separation, permutation invariant training (PIT) [46] is utilized to address the speaker permutation problem.
During ï¬netuning, we follow SUPERBâ€™s setup [31] to freeze the parameters of the SSL models. Instead of extracting representations from the last hidden layer, we weighted-sum the embeddings from all layers as the ï¬nal representation F similar to ELMo [20].

Kâˆ’1

F=

wiFi

i=0

where K is the total number of layers, Fi is the representation extracted from the ith layer, wi is the weight for the ith layer. The layer weights w = [w0, w1, w2, ..., wKâˆ’1] are learned during the ï¬netuning stage.
We did not use time-domain methods for the following two reasons. 1) Stride difference: Most of the existing SSL frameworks are using a stride size of 10 or 20ms, which corresponds to 160 to 320 samples for 16kHz audios. Such stride sizes are suitable for phoneme-level and sentence-level tasks such as ASR and speaker recognition but excessively large for time-domain speech enhancement and separation. As a comparison, most time-domain speech separation models are using a stride size smaller than 10 samples. We discuss the effect of stride size in Section 5.2.1. 2) Model complexity: We could not ï¬nd an appropriate time-domain method that is light enough for enhancement and separation. Putting a huge downstream model on top of SSL representations violates SSLâ€™s principle of simple ï¬netuning.

3.2. Downstream models for enhancement and separation
Following the principles of SUPERB [31], we constrain our downstream models to be as lightweight as possible. After balancing between computational cost and performance, we choose a T-F maskbased model [11] as our downstream model. As shown in Figure 1, for speech enhancement, the SSL model takes the noisy waveform as

4. EXPERIMENTAL SETUP
4.1. Dataset
For speech enhancement, we use the Voicebank-DEMAND [47], a synthetic dataset created by mixing up clean speech and noise. The clean speech is extracted from the Voice Bank corpus [48], and

the noise is from the Diverse Environments Multichannel Acoustic Noise Database (DEMAND) [49]. The training set contains 28 speakers with 4 signal-to-noise ratios (SNRs) (15, 10, 5, and 0 dB) and the test set contains 2 speakers with 4 SNRs (17.5, 12.5, 7.5, and 2.5 dB). The training set contains 11,572 utterances (9.4h) and the test set contains 824 utterances (0.6h). The lengths of utterances range from 1.1s to 15.1s with an average of 2.9s.
For speech separation, we experiment on the LibriMix [50] dataset. The LibriMix dataset is simulated from the clean speech in LibriSpeech [51] and noise in WHAM! [52]. Since most SSL models only support 16kHz audios as input, we choose the â€œ16kHz minâ€ version of the data. The speech mixtures are created by mixing speech segments from different speakers. The loudness of each utterance is uniformly sampled between -25 and -33 loudness units relative to full scale (LUFS). Random noise samples with loudness between -38 and -30 LUFS are added to the speech mixtures. The training set contains 13,900 utterances with 43.3 hours of speech. In our experiments, we evaluate both â€œsep cleanâ€ and â€œsep noisyâ€ conditions (separating speech from clean/noisy mixtures).

4.2. Evaluation metric

Speech enhancement requires both speech quality and intelligibility. In our experiment, we report two commonly used metrics: perceptual evaluation of speech quality (PESQ) [53, 54] and short-time objective intelligibility (STOI) [55]. PESQ measures the speech quality, and it predicts the subjective opinion scores of a degraded signal. We use the wide-band version of PESQ implemented in pythonpesq [56]. STOI is a human-designed metric that shows a high correlation with the intelligibility of noisy speech. The range of STOI is from 0 to 100. For both metrics, a higher value indicates better performance.
For speech separation, we use Scale-Invariant Signal-to-Noise Ratio improvement (SI-SNRi) as the evaluation metric. It is a simpler and more robust alternative to Source-to-Distortion Ratio (SDR). SI-SNR is deï¬ned as

(Ë†s s)s

starget =

2

s

enoise = Ë†s âˆ’ starget

SI-SNR(s, Ë†s) = 10 log10

starget 2 enoise 2

where s âˆˆ RL is tâˆšhe ground truth signal, Ë†s âˆˆ RL is the estimated signal and s = s s denotes the L2 norm of s. SI-SNRi is the SI-SNR improvement against the mixtures, deï¬ned as

SI-SNRi = SI-SNR(s, Ë†s) âˆ’ SI-SNR(s, m)

where m âˆˆ RL is the mixture signal.

4.3. Model architecture and ï¬netuning details
During ï¬netuning for speech enhancement and separation tasks, we use a three-layer BLSTM as the downstream model. Each BLSTM layer contains 896 hidden units. The output of the BLSTM is further processed by a linear layer and a ReLU activation.
The downstream models are ï¬netuned for 150k steps with a batch size of 8. We use the Adam optimizer with a learning rate of 1eâˆ’4. Following SUPERBâ€™s [31] setup, we donâ€™t decay the learning rate during ï¬netuning. We choose the model with the best performance (highest PESQ for enhancement and SI-SNRi for separation) on the development set.

5. EXPERIMENTAL RESULTS
5.1. Main experiment
We present the speech enhancement and separation results for 13 SSL upstream models in Table 1. For the STFT features, we use a frame size of 512, a frame shift of 160, and perform a 512-point FFT on each frame. The FBANK features are extracted using the torchaudio [57] toolkit with a frame size of 400 and a frame shift of 160. The number of Mel-frequency bins is set to 80. Delta and delta-delta coefï¬cients are appended, and cepstral mean and variance normalization (CMVN) is applied. The extracted FBANK features have 240 dimensions.
Among the SSL models, wav2vec2, HuBERT, UniSpeechSAT, and WavLM use a stride of 320 samples (20ms) while other models use 160 samples (10ms). Among these SSL models, UniSpeech-SAT/WavLM Base+/Large, wav2vec2 Robust have seen noisy speech in real scenarios while other models are pretrained on the clean speech from audiobooks (LibriSpeech [51] and LibriLight [58]). Our ï¬ndings are as follows.
Compared with other tasks such as ASR, the improvement of SSL is not as large for enhancement and separation. For enhancement, only the HuBERT/UniSpeech-SAT/WavLM Large and UniSpeech-SAT Base+ achieve more than 0.05 PESQ improvement over the FBANK baseline. Other SSL models have comparable or even slightly worse performance. For separation, only the UniSpeech-SAT/WavLM Large can consistently outperform (>0.5dB SI-SNRi improvement) the STFT baseline for both sep clean and sep noisy conditions. The possible reasons for some SSL models donâ€™t perform well include 1) Domain mismatch. Most of the SSL models above are pretrained on the clean speech from audiobooks, and they have never seen noise and speaker overlaps before, making representations less robust to such conditions. For example, the Modiï¬ed CPC and HuBERT Large achieve more than 0.5dB SI-SNRi improvement over the STFT baseline for the sep clean condition. However, their performance largely degrades when separating noisy mixtures. 2) Information Loss. The objectives of some SSL models encourage the systems to focus on global structures and build long-term dependencies. Some local information necessary for signal reconstruction is lost during pretraining.
Pretraining with audios from real scenarios seems to improve the enhancement and separation performance in some cases. The wav2vec2 Robust and WavLM Base+ largely improve the PESQ value for enhancement and slightly improve the SI-SNRi for separation. The UniSpeech-SAT Base+ performs almost the same as the UniSpeech-SAT Base for both tasks. The utterance mixing augmentation doesnâ€™t seem useful for enhancement, but it improves the separation performance. Combining both techniques and other small modiï¬cations, the UniSpeech-SAT and WavLM Large models consistently outperform the HuBERT Large model. The UniSpeechSAT Large model has achieved the best results for enhancement and separation tasks (except for the STOI metric). It improves the STFT and FBANK baselines by 0.15 PESQ, 0.8 STOI, 1.24/1.18 dB SISNRi on sep clean/sep noisy conditions.
Vector quantization seems to degrade the separation performance. VQ-APC and vq-wav2vec achieve worse separation performance compared to APC and wav2vec. A potential explanation is that converting continuous speech representations to discrete ones is detrimental to continuous sequence generation tasks like speech separation. Besides this, the TERA model improves both enhancement and separation performance over the Mockingjay, which shows that masked frequency bin prediction is useful for both tasks.

Table 1. Evaluating 13 SSL upstream models on speech enhance-

ment and separation downstream tasks. We measure speech en-

hancement performance with PESQ and STOI on the Voicebank-

DEMAND [48] dataset. For speech separation, we evaluate on

the Libri2Mix [50] dataset and report SI-SNRi for sep clean and

sep noisy conditions.

Enhancement

Separation

Model

PESQâ†‘ STOIâ†‘ SI-SNRi (dB)â†‘

/

sep c sep n

FBANK

2.55

STFT

2.51

PASE+ [45]

2.56

APC [34]

2.56

VQ-APC [35]

2.56

NPC [36]

2.52

Mockingjay [37]

2.53

TERA [38]

2.54

Modiï¬ed CPC [40]

2.57

wav2vec [24]

2.53

vq-wav2vec [41]

2.48

wav2vec2 Base [25]

2.55

wav2vec2 Large

2.52

wav2vec2 Robust [59]

2.59

HuBERT Base [26]

2.58

HuBERT Large

2.64

UniSpeech-SAT Base [43] 2.60

UniSpeech-SAT Base+

2.61

UniSpeech-SAT Large

2.70

WavLM Base [44]

2.56

WavLM Base+

2.60

WavLM Large

2.68

93.6 9.23 7.18 93.6 9.89 8.26 93.9 9.87 8.01 93.4 8.92 7.16 93.4 8.44 6.86 93.1 8.04 6.75 93.4 9.38 7.74 93.6 10.19 8.28 93.7 10.40 8.15 93.8 9.30 7.09 93.6 8.16 6.22 93.9 9.77 7.52 94.0 10.02 8.01 94.1 10.35 8.22 93.9 9.36 7.46 94.2 10.45 8.45 94.0 10.33 8.28 94.2 10.25 8.30 94.4 11.13 9.44 94.0 10.10 7.97 94.0 10.58 8.68 94.5 10.97 9.14

Table 2. Speech enhancement and separation performance of the

STFT and HuBERT Base/Large upstreams with different stride

sizes. For separation, we only consider the sep clean condition.

Upstream

Stride PESQ STOI SI-SNRi (dB)

STFT

160

320

HuBERT Base 160 320

HuBERT Large 160 320

2.51 93.6 2.42 93.3 2.68 94.1 2.58 93.9 2.80 94.5 2.64 94.2

9.89 8.79 10.47 9.36 11.26 10.45

5.2. Ablation studies
In this section, we use the HuBERT model as an example to study the factors that inï¬‚uence the SSL modelâ€™s performance on speech enhancement and separation tasks.
5.2.1. Effect of stride size
As shown in Table 2, the stride size has a huge impact on speech enhancement and separation performance. For STFT, after we increase the stride size from 160 (10ms) to 320 (20ms), the PESQ, STOI, SI-SNRi (dB) degrade by 0.1, 0.3, and 1.1 respectively. The original stride of HuBERT Base/Large model is 320 (20ms). We upsample the representations by reducing the stride of the last convolution layer from 2 to 1. After upsampling, the HuBERT Base/Large models signiï¬cantly outperform the original results. For all strides and metrics, the HuBERT models consistently outperform the STFT baseline. Note that even after upsampling, the stride size we use is still much larger than most time-domain enhancement and separation

Table 3. The separation performance of Conv-Tasnet [15] on the

16kHz min Libri2Mix (sep clean condition) with different stride

sizes. We use the Conv-Tasnet implementation from Asteroid [60]

and adjust the stride size in the 1d convolution encoder

Stride

8

40 160 320

SI-SNRi (dB) 14.34 13.63 9.64 8.22

systems. As a comparison, we present the correlation between stride size and SI-SNRi for Conv-Tasnet [15] in Table 3. The vanilla ConvTasnet (with a stride of 8) achieves 14.34dB SI-SNRi on Libri2Mix. However, the performance degrades a lot as the stride size increases. When the stride size is larger than 160, the SI-SNRi of Conv-Tasnet is even lower than our STFT baseline.

5.2.2. Effect of layer weighting
For SSL models, different layers usually capture different speech information, which is used for different tasks. In this section, we extract speech representations from different layers of the HuBERT Large model and perform speech enhancement and separation on top of them. As shown in Table 4, the performance gap between different layers is signiï¬cant. For speech enhancement, the embeddings from the 12th layer obtain the best PESQ and STOI numbers. It achieves around 0.1 PESQ and 0.6 STOI improvements compared to the last hidden layer. For speech separation, the performance declines as the layer becomes deeper, and the ï¬rst layer outperforms the last layer by 4.21dB. The weighted-sum representations further improve the enhancement and separation results, and we observe that for most SSL models lower layers generally obtain higher weights. One possible explanation is that some local signal information necessary for speech reconstruction tasks is lost in deeper layers because it is restricted to local speech areas and less useful for objectives like contrastive learning and masked/future context prediction. Fully exploiting the information captured in different layers is important for speech enhancement and separation downstreams.

Table 4. Speech enhancement and separation performance of differ-

ent layer embeddings from the HuBERT Large model. For separa-

tion, we only consider the sep clean conition.

Upstream

Layer PESQ STOI SI-SNRi (dB)

0

2.52 93.9

HuBERT Large

12

24

2.58 94.0 2.49 93.4

weighted 2.64 94.2

9.96 8.58 5.75 10.45

6. CONCLUSION
In this paper, we investigate SSL for speech enhancement and separation. We evaluate 13 SSL upstream models on speech enhancement and separation with a T-F mask prediction downstream. Our experimental results reveal that 1) Although SSL models are not designed for waveform generation tasks like enhancement and separation, some of them achieve remarkable improvements over the STFT magnitudes and FBANKs. 2) Pretraining with audios from real scenarios and utterance mixing augmentation can increase the robustness of speech representations and improve the enhancement and separation performances. 3) Enhancement and separation require ï¬ne-grained waveform information to reconstruct the clean signal, which is often lost in deeper layers of SSL models. In the future, we will study SSL representations for time-domain methods.

7. REFERENCES
[1] DeLiang Wang and Jitong Chen, â€œSupervised speech separation based on deep learning: An overview,â€ IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1702â€“1726, 2018.
[2] Daniel Michelsanti et al., â€œAn overview of deep-learning-based audio-visual speech enhancement and separation,â€ TASLP, 2021.
[3] Shinji Watanabe et al., â€œChime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,â€ arXiv preprint arXiv:2004.09249, 2020.
[4] Ivan Medennikov et al., â€œThe stc system for the chime-6 challenge,â€ in CHiME 2020 Workshop on Speech Processing in Everyday Environments, 2020.
[5] Yuxuan Wang et al., â€œUstc-nelslip system description for dihard-iii challenge,â€ arXiv preprint arXiv:2103.10661, 2021.
[6] Felix Weninger et al., â€œSpeech enhancement with lstm recurrent neural networks and its application to noise-robust asr,â€ in International conference on latent variable analysis and signal separation. Springer, 2015, pp. 91â€“99.
[7] Meet H Soni, Neil Shah, and Hemant A Patil, â€œTime-frequency masking-based speech enhancement using generative adversarial network,â€ in ICASSP. IEEE, 2018, pp. 5039â€“5043.
[8] Szu-Wei Fu et al., â€œMetricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement,â€ in International Conference on Machine Learning. PMLR, 2019, pp. 2031â€“2041.
[9] Szu-Wei Fu et al., â€œMetricgan+: An improved version of metricgan for speech enhancement,â€ arXiv preprint arXiv:2104.03538, 2021.
[10] John R Hershey et al., â€œDeep clustering: Discriminative embeddings for segmentation and separation,â€ in ICASSP. IEEE, 2016, pp. 31â€“35.
[11] Morten KolbÃ¦k et al., â€œMultitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,â€ TASLP, vol. 25, no. 10, pp. 1901â€“1913, 2017.
[12] Zhuo Chen, Yi Luo, and Nima Mesgarani, â€œDeep attractor network for singlemicrophone speaker separation,â€ in ICASSP. IEEE, 2017, pp. 246â€“250.
[13] Santiago Pascual, Antonio Bonafonte, and Joan Serra, â€œSEGAN: Speech enhancement generative adversarial network,â€ arXiv preprint arXiv:1703.09452, 2017.
[14] Alexandre Defossez, Gabriel Synnaeve, and Yossi Adi, â€œReal time speech enhancement in the waveform domain,â€ in Interspeech, 2020.
[15] Yi Luo and Nima Mesgarani, â€œConv-tasnet: Surpassing ideal timeâ€“frequency magnitude masking for speech separation,â€ IEEE/ACM transactions on audio, speech, and language processing, vol. 27, no. 8, pp. 1256â€“1266, 2019.
[16] Yi Luo, Zhuo Chen, and Takuya Yoshioka, â€œDual-path rnn: efï¬cient long sequence modeling for time-domain single-channel speech separation,â€ in ICASSP. IEEE, 2020, pp. 46â€“50.
[17] Jingjing Chen, Qirong Mao, and Dong Liu, â€œDual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation,â€ in Interspeech, 2020.
[18] Cem Subakan et al., â€œAttention is all you need in speech separation,â€ in ICASSP 2021. IEEE, 2021, pp. 21â€“25.
[19] Neil Zeghidour and David Grangier, â€œWavesplit: End-to-end speech separation by speaker clustering,â€ TASLP, vol. 29, pp. 2840â€“2849, 2021.
[20] Matthew Peters et al., â€œDeep contextualized word representations,â€ in NAACL, 2018, pp. 2227â€“2237.
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, â€œBert: Pre-training of deep bidirectional transformers for language understanding,â€ in NAACL-HLT (1), 2019.
[22] Ishan Misra and Laurens van der Maaten, â€œSelf-supervised learning of pretextinvariant representations,â€ in CVPR, 2020, pp. 6707â€“6717.
[23] Kaiming He et al., â€œMomentum contrast for unsupervised visual representation learning,â€ in CVPR, 2020, pp. 9729â€“9738.
[24] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli, â€œwav2vec: Unsupervised pre-training for speech recognition,â€ in Interspeech, 2019.
[25] Alexei Baevski et al., â€œwav2vec 2.0: A framework for self-supervised learning of speech representations,â€ in NeurIPS, 2020.
[26] Wei-Ning Hsu et al., â€œHuBERT: Self-supervised speech representation learning by masked prediction of hidden units,â€ arXiv preprint arXiv:2106.07447, 2021.
[27] Alexei Baevski and Abdelrahman Mohamed, â€œEffectiveness of self-supervised pre-training for asr,â€ in ICASSP. IEEE, 2020, pp. 7694â€“7698.
[28] Zhiyun Fan, Meng Li, Shiyu Zhou, and Bo Xu, â€œExploring wav2vec 2.0 on speaker veriï¬cation and language identiï¬cation,â€ arXiv preprint arXiv:2012.06185, 2020.
[29] Leonardo Pepino, Pablo Riera, and Luciana Ferrer, â€œEmotion recognition from speech using wav2vec 2.0 embeddings,â€ arXiv preprint arXiv:2104.03502, 2021.

[30] Cheng-I Lai et al., â€œSemi-supervised spoken language understanding via selfsupervised speech and language model pretraining,â€ in ICASSP. IEEE, 2021, pp. 7468â€“7472.
[31] Shu wen Yang et al., â€œSUPERB: Speech Processing Universal PERformance Benchmark,â€ in Proc. Interspeech 2021, 2021, pp. 1194â€“1198.
[32] Sung-Feng Huang et al., â€œSelf-supervised pre-training reduces label permutation instability of speech separation,â€ arXiv preprint arXiv:2010.15366, 2020.
[33] Xiao Liu et al., â€œSelf-supervised learning: Generative or contrastive,â€ IEEE Transactions on Knowledge and Data Engineering, 2021.
[34] Yu-An Chung et al., â€œAn unsupervised autoregressive model for speech representation learning,â€ in INTERSPEECH, 2019.
[35] Yu-An Chung, Hao Tang, and James Glass, â€œVector-quantized autoregressive predictive coding,â€ in INTERSPEECH, 2020.
[36] Alexander H Liu et al., â€œNon-autoregressive predictive coding for learning speech representations from local dependencies,â€ arXiv preprint arXiv:2011.00406, 2020.
[37] Andy T Liu et al., â€œMockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders,â€ in ICASSP. IEEE, 2020, pp. 6419â€“ 6423.
[38] Andy T Liu, Shang-Wen Li, and Hung-yi Lee, â€œTera: Self-supervised learning of transformer encoder representation for speech,â€ TASLP, vol. 29, pp. 2351â€“2366, 2021.
[39] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, â€œRepresentation learning with contrastive predictive coding,â€ arXiv preprint arXiv:1807.03748, 2018.
[40] Morgane Riviere et al., â€œUnsupervised pretraining transfers well across languages,â€ in ICASSP. IEEE, 2020, pp. 7414â€“7418.
[41] Alexei Baevski et al., â€œvq-wav2vec: Self-supervised learning of discrete speech representations,â€ in ICLR, 2020.
[42] Mathilde Caron et al., â€œDeep clustering for unsupervised learning of visual features,â€ in ECCV, 2018, pp. 132â€“149.
[43] Sanyuan Chen et al., â€œUnispeech-sat: Universal speech representation learning with speaker aware pre-training,â€ arXiv preprint arXiv:2110.05752, 2021.
[44] Sanyuan Chen et al., â€œWavlm: Large-scale self-supervised pre-training for full stack speech processing,â€ arXiv preprint arXiv:2110.13900, 2021.
[45] Mirco Ravanelli et al., â€œMulti-task self-supervised learning for robust speech recognition,â€ in ICASSP. IEEE, 2020, pp. 6989â€“6993.
[46] Dong Yu et al., â€œPermutation invariant training of deep models for speakerindependent multi-talker speech separation,â€ in ICASSP. IEEE, 2017, pp. 241â€“ 245.
[47] Cassia Valentini-Botinhao et al., â€œNoisy speech database for training speech enhancement algorithms and tts models,â€ 2017.
[48] Christophe Veaux et al., â€œThe voice bank corpus: Design, collection and data analysis of a large regional accent speech database,â€ in O-COCOSDA/CASLRE. IEEE, 2013, pp. 1â€“4.
[49] Joachim Thiemann et al., â€œThe diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,â€ in Proceedings of Meetings on Acoustics ICA2013. Acoustical Society of America, 2013, vol. 19, p. 035081.
[50] Joris Cosentino et al., â€œLibrimix: An open-source dataset for generalizable speech separation,â€ arXiv preprint arXiv:2005.11262, 2020.
[51] Vassil Panayotov et al., â€œLibrispeech: an asr corpus based on public domain audio books,â€ in ICASSP. IEEE, 2015, pp. 5206â€“5210.
[52] Gordon Wichern et al., â€œWham!: Extending speech separation to noisy environments,â€ arXiv preprint arXiv:1907.01160, 2019.
[53] Antony W Rix et al., â€œPerceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,â€ in ICASSP. IEEE, 2001, vol. 2, pp. 749â€“752.
[54] Recommendation ITU-T P ITU, â€œ862.2: Wideband extension to recommendation p. 862 for the assessment of wideband telephone networks and speech codecs,â€ 2007.
[55] Cees H Taal et al., â€œA short-time objective intelligibility measure for timefrequency weighted noisy speech,â€ in ICASSP. IEEE, 2010, pp. 4214â€“4217.
[56] Miao Wang et al., â€œpython-pesq,â€ https://github.com/ludlows/python-pesq, 2019.
[57] Yao-Yuan Yang et al., â€œTorchaudio: Building blocks for audio and speech processing,â€ arXiv preprint arXiv:2110.15018, 2021.
[58] Jacob Kahn et al., â€œLibri-light: A benchmark for asr with limited or no supervision,â€ in ICASSP. IEEE, 2020, pp. 7669â€“7673.
[59] Wei-Ning Hsu et al., â€œRobust wav2vec 2.0: Analyzing domain shift in selfsupervised pre-training,â€ arXiv preprint arXiv:2104.01027, 2021.
[60] Manuel Pariente et al., â€œAsteroid: the PyTorch-based audio source separation toolkit for researchers,â€ in Proc. Interspeech, 2020.

