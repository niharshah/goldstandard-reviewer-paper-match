{"title": "An Investigation of Machine Translation Evaluation Metrics in Cross-lingual Question Answering", "abstract": "Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accuracy. However, it is not clear whether an MT system that is better for human consumption is also better for CLQA. In this paper, we investigate the relationship between manual and automatic translation evaluation metrics and CLQA accuracy by creating a data set using both manual and machine translations and perform CLQA using this created data set. 1 As a result, we find that QA accuracy is closely related with a metric that considers frequency of words, and as a result of manual analysis, we identify 3 factors of translation results that affect CLQA accuracy.", "year": 2015, "ssId": "51c2321244b0a489970e1b52c59b049fdcc5cd46", "arXivId": null, "link": null, "openAccess": false, "authors": ["Kyoshiro Sugiyama", "M. Mizukami", "Graham Neubig", "Koichiro Yoshino", "S. Sakti", "T. Toda", "Satoshi Nakamura"]}