arXiv:1710.00162v4 [math.OC] 11 Feb 2019

Ускоренный спуск по случайному направлению с неевклидовой прокс-структурой *
Е.А. ВОРОНЦОВА, канд. ф.-м. наук (vorontsovaea@gmail.com) (Дальневосточный федеральный университет, Владивосток), А.В. ГАСНИКОВ, д-р ф.-м. наук (gasnikov@yandex.ru), Э.А. ГОРБУНОВ (ed-gorbunov@yandex.ru) (Московский физико-технический институт)
Рассматриваются задачи гладкой выпуклой оптимизации,. для численного решения которых полный градиент недоступен. В 2011 г. Ю.Е. Нестеровым были предложены ускоренные безградиентные методы решения таких задач. Поскольку рассматривались только задачи безусловной оптимизации, то использовалась евклидова прокс-структура. Однако если заранее знать, например, что решение задачи разреженно, а точнее, что расстояние от точки старта до решения в 1-норме и в 2-норме близки, то более выгодно выбирать не евклидову прокс-структуру, связанную с 2-нормой, а прокс-структуру, связанную с 1-нормой. Полное обоснование этого утверждения проводится в статье. Предлагается ускоренный метод спуска по случайному направлению с неевклидовой проксструктурой для решения задачи безусловной оптимизации (в дальнейшем подход предполагается расширить на ускоренный безградиентный метод). Получены оценки скорости сходимости метода. Показаны сложности переноса описанного подхода на задачи условной оптимизации.
Ключевые слова: ускоренные методы первого порядка, выпуклая оптимизация, метод линейного каплинга, концентрация равномерной меры на единичной евклидовой сфере, неевклидова проксструктура.
1 Введение
В [1] были предложены ускоренные оракульные 1 методы нулевого порядка (безградиентные методы) решения задач гладкой выпуклой безусловной оптимизации.
В рассуждениях [1] существенным образом использовалось то, что была выбрана евклидова прокс-структура (выпуклая гладкая функция, порождающая расстояние, и 1-сильно выпуклая относительно какой-то нормы (строгое определение см. в разделе 4)). Такой выбор прокс-структуры для задач безусловной оптимизации является вполне естественным (см., например, [3]). Однако в ряде задач имеется дополнительная информация, которая, например, позволяет рассчитывать на разреженность решения (в решении большая часть компонент нулевые). В таких случаях использование других прокс-структур бывает более выгодным. Для негладких задач стохастической условной оптимизации с оракулом нулевого порядка недавно было показано (см. [4, 5]), что в определенных ситуациях ускорение метода за счет перехода от евклидовой прокс-структуры, связанной с 2-нормой, к прокс-структурам, связанным с 1-нормой, может давать ускорение методу, по порядку равное размерности пространства, в котором происходит оптимизация. К сожалению, техника, использованная в [4, 5] существенным образом использовала неускоренную природу оптимальных методов для негладких задач. Другими словами, из [4, 5] непонятно, как получать аналогичные оценки для гладких задач. В настоящей статье на базе специального варианта
*Работа А.В. Гасникова по разделам 3 и 4 поддержана Российским фондом фундаментальных исследований (проект № 18-31-20005 мол_а_вед). Работа Э.А. Горбунова и Е.А. Воронцовой поддержана грантом Президента РФ МД-1320.2018.1. Работа А.В. Гасникова и Е.А. Воронцовой поддержана Российским фондом фундаментальных исследований (проект № 18-29-03071 мк).
1Здесь и далее под оракулом понимается подпрограмма расчета значений целевой функции и/или градиента (его части), а оптимальность метода на классе задач понимается в смысле Бахвалова–Немировского [2] как число обращений (по ходу работы метода) к оракулу для достижения заданной точности (по функции).
1

быстрого (ускоренного) градиентного метода [6] строится ускоренный метод спуска по случайному направлению. Особенностью метода из [6] является представление быстрого градиентного метода как специальной выпуклой комбинации градиентного спуска и зеркального спуска. В [6], как и во всех известных авторам вариантах быстрого градиентного метода с двумя и более “проекциями”, обе проекции осуществлялись в одной норме/прокс-структуре. Главной идеей настоящей статьи является использование разных норм/прокс-структур в этих проекциях, а именно: в градиентном шаге всегда используется обычная евклидова проекция, а вот в зеркальном шаге выбор проксструктуры обусловлен априорной информацией о свойствах решения.
В настоящей статье на базе описанной конструкции для детерминированных задач безусловной гладкой выпуклой оптимизации строится ускоренный метод спуска по случайному направлению 2 (раздел 4).
В классе детерминированных спусков по направлению (к ним можно отнести и циклический координатный спуск) для получения лучших оценок необходимо вводить рандомизацию (доказательство см. в. [8]), поэтому рассматриваются сразу спуски по случайному направлению.
Показано, какие возникают сложности при попытке перенесения описанного подхода на задачи оптимизации на множествах простой структуры (раздел 4).

2 Постановка задачи

Рассматривается задача гладкой выпуклой оптимизации

𝑓 (𝑥) → min,

(1)

𝑥∈𝑄

где функция 𝑓 (𝑥), заданная на выпуклом замкнутом множестве 𝑄 ⊆ R𝑛, имеет липшицев градиент с константой 𝐿2 (т.е. 𝑓 (𝑥) — 𝐿2-гладкая функция)

‖∇𝑓 (𝑦) − ∇𝑓 (𝑥)‖2 ≤ 𝐿2 ‖𝑦 − 𝑥‖2

и является 𝜇-сильно выпуклой в 𝑝-норме (1 ≤ 𝑝 ≤ 2) функцией (далее будем использовать и обозначение 𝜇𝑝), при этом в точке минимума 𝑥* выполнено равенство ∇𝑓 (𝑥*) = 0, а итерационный процесс стартует с точки 𝑥0.
В данной статье для решения задачи (1) вместо обычного градиента используется его стохастическая аппроксимация, построенная на базе производной по случайно выбранному направлению [9]
𝑔 (𝑥, 𝑒) d=ef 𝑛 ⟨∇𝑓 (𝑥) , 𝑒⟩ 𝑒,
где 𝑒 — случайный вектор, равномерно распределенный на 𝑆2𝑛 (1) — единичной сфере в 2-норме в пространстве R𝑛 (𝑒 ∼ 𝑅𝑆2𝑛 (1); под этой записью будем понимать, что случайный вектор 𝑒 имеет равномерное распределение на 𝑛-мерной единичной евклидовой сфере), а угловые скобки ⟨·, ·⟩ обозначают скалярное произведение 3.
Имеет место следующая лемма (доказательство см. в [10]), являющаяся следствием явления концентрации равномерной меры на сфере вокруг экватора (см. также [11]; северный полюс задается градиентом ∇𝑓 (𝑥)).

Л е м м а 1 Пусть 𝑒 ∼ 𝑅𝑆2𝑛(1), 𝑛 8, 𝑠 ∈ R𝑛, тогда

E[||𝑒||2𝑞 ]

min{𝑞

−

1,

16

ln

𝑛

−

8}𝑛

2 𝑞

−

1

,

2

𝑞

∞,

(2)

√

2

E[⟨𝑠, 𝑒⟩2||𝑒||2𝑞] 3||𝑠||22 min{2𝑞 − 1, 32 ln 𝑛 − 8}𝑛 𝑞 −2, 2 𝑞 ∞,

(3)

где под знаком || · ||𝑞 понимается векторная 𝑞-норма (норма Гельдера с показателем 𝑞).

Из (3) (см. также [4]) вытекает следующий факт.

2Подробно о разнице в подходах в случае детерминированной постановки, но с введением рандомизации
и в случае задач стохастической оптимизации см. в [7]. 3Отметим, что E𝑒[𝑔(𝑥, 𝑒)] = ∇𝑓 (𝑥), что следует из факта: E𝑒[|𝑒𝑖|2] = 𝑛1 , где 𝑒𝑖 — 𝑖-я компонента вектора
𝑒.

2

У т в е р ж д е н и е Пусть 𝑒 ∼ 𝑅𝑆2𝑛 (1) и 𝑔 (𝑥, 𝑒) = 𝑛 ⟨∇𝑓 (𝑥) , 𝑒⟩ 𝑒, тогда

[︁

2]︁ √

2

2

E𝑒 ‖𝑔 (𝑥, 𝑒)‖𝑞

3min{2𝑞 − 1, 32 ln 𝑛 − 8}𝑛 𝑞 ‖∇𝑓 (𝑥)‖2.

Используя данное утверждение, можно в сильно выпуклом случае при условии ∇𝑓 (𝑥*) = 0 получить оценку необходимого числа обращений к оракулу за производной по направлению для достижения по функции точности 𝜀 в среднем [12]:

𝑁 (𝜀) = O (︂𝑛 𝑞2 ln 𝑛 𝐿2 ln (︂ Δ𝑓 0 )︂)︂ ,

𝜇

𝜀

где Δ𝑓 0 = 𝑓 (𝑥0) − 𝑓 (𝑥*). Имеется гипотеза (см., например, [3]), что, используя специальные ускоренные методы (типа
Катюши (Katusha) из [13]), можно получить оценку

(︃

√︃

)︃

𝑁 (𝜀) = O 𝑛 𝑞1 + 12 ln 𝑛 𝐿2 ln (︂ Δ𝑓 0 )︂ .

𝜇

𝜀

Насколько известно авторам, эта гипотеза на данный момент не доказана и не опровергнута. В разделе 4 данной статьи для случая 𝑄 = R𝑛 доказывается оценка

2 +1

𝐿2𝑅2

E[𝑓 (𝑥𝑁 )] − 𝑓 (𝑥*) ≤ 𝐶𝑛 𝑞 ln 𝑛 𝑁 2 .

(4)

Эта оценка получается из приведенной выше оценки с помощью регуляризации 𝜇 = 𝜀/𝑅2 [7], где (с точностью до корня из логарифмического по 𝑛 множителя) 𝑅 — расстояние в 𝑝-норме от точки старта до решения.

3 Задача А.С. Немировского

Рассмотрим задачу (1) минимизации гладкого выпуклого функционала 𝑓 (𝑥) с константой Липшица градиента 𝐿2 в 2-норме на множестве 𝑄 = 𝐵1𝑛 (𝑅) (шар в пространстве R𝑛 радиуса 𝑅 в 1-норме). Тогда на рассматриваемом классе функции для любого итерационного метода, на каждой
итерации которого только один раз можно обратиться к оракулу за градиентом функции, можно
так подобрать функцию 𝑓 (𝑥) из этого класса, что имеет место оценка скорости сходимости [14] в
виде

𝐶˜1𝐿2𝑅2

𝑓 (𝑥𝑁 ) − 𝑓 (𝑥*) ≥ 𝑁 3 ,

(5)

где 𝐶˜1 — некоторая числовая константа 4, 𝑥* — ближайшая к 𝑥0 точка минимума функции 𝑓 (𝑥). Поскольку 𝑓 (𝑥𝑁 ) − 𝑓 (𝑥*) должно быть меньше или равно 𝜀, из (5) можно получить оценку на 𝑁 (𝜀).
С другой стороны, если использовать обычный быстрый градиентный метод с KL-прокс-

структурой для этой же задачи, то [15]:

𝐶˜2𝐿1𝑅2 ln 𝑛

𝑓 (𝑥𝑁 ) − 𝑓 (𝑥*) ≤

𝑁2 ,

где константа Липшица градиента 𝐿1 в 1-норме удовлетворяет условию: 𝐿2/𝑛 ≤ 𝐿1 ≤ 𝐿2 5. Отсюда нельзя сделать вывод, что нижняя оценка достигается. Достигается ли эта нижняя оценка и если достигается, то на каком методе? Насколько авторам известно, это пока открытый вопрос, поставленный А.С. Немировским в 2015 г. (см. также [14]). Однако если оценивать не число итераций, а общее число арифметических операций и если ограничиться рассмотрением класса функций, для которых “стоимость” расчета производной по направлению примерно в 𝑛 раз меньше

4Как и 𝐶˜2 далее. Здесь и далее все числовые константы не зависят от 𝑁 и 𝑛. 5Здесь под 𝐿1 понимается такое положительное число, что ||∇𝑓 (𝑥) − ∇𝑓 (𝑦)||∞

𝐿1||𝑥 − 𝑦||1.

3

“стоимости” расчета полного градиента 6, то при 𝑁 ≤ 𝑛 (точнее даже при 𝑁 ≃ 𝑛) выписанная оценка (4) (в варианте для общего числа арифметических операций, необходимых для достижения заданной точности в среднем) с точностью до логарифмического множителя будет соответствовать нижней оценке (5), если последнюю понимать как

𝐶˜1𝐿2𝑅2 𝑓 (𝑥𝑁 ) − 𝑓 (𝑥*) ≥ 𝑛𝑁 2 ,

т.е.

(︃√︂

)︃

𝑁 (𝜀) = O 𝐿2𝑅2 .

𝜀𝑛

Действительно (𝑞 = ∞ ), общее число арифметических операций равно

(︃

√︂

)︃

(︃√︂

)︃

O (𝑛) · O 𝑛 12 ln 𝑛 𝐿2𝑅2 ≈ O (︀𝑛2)︀ · O 𝐿2𝑅2 .

𝜀

𝜀𝑛

⏟

⏞

число итераций

4 Обоснование формулы (4) в случае 𝑄 = R𝑛

Введем дивергенцию Брэгмана [17] 𝑉𝑧 (𝑦), связанную с 𝑝-нормой (1 𝑝 2)

𝑉𝑧(𝑦) d=ef 𝑑(𝑦) − 𝑑(𝑧) − ⟨∇𝑑(𝑧), 𝑦 − 𝑧⟩,

где функция 𝑑(𝑥) является непрерывно дифференцируемой сильно и выпуклой с константой сильной выпуклости, равной единице. Например, для 𝑝 = 1 функцию 𝑑(𝑥) можно выбрать так:
𝑑(𝑥) = 2(𝑎1− 1) ||𝑥||2𝑎,

где 𝑎 = 2 2lolgog𝑛−𝑛 1 . Функцию 𝑑(𝑥) будем называть прокс-функцией (или прокс-структурой), связанной с 𝑝-нормой. Кроме того, пусть 𝑞 — такое число, что 𝑝1 + 1𝑞 = 1. Далее будем следовать обозначениям из [12]. Пусть случайный вектор 𝑒 равномерно распределен на поверхности евклидовой сферы единичного радиуса (𝑒 ∼ 𝑅𝑆2𝑛 (1)). Положим, что

def

1

Grad𝑒 (𝑥) = 𝑥 − ⟨∇𝑓 (𝑥) , 𝑒⟩ 𝑒,

𝐿

Mirr𝑒 (𝑥, 𝑧, 𝛼) d=ef argmin {𝛼 ⟨𝑛 ⟨∇𝑓 (𝑥) , 𝑒⟩ 𝑒, 𝑦 − 𝑧⟩ + 𝑉𝑧 (𝑦)} ,
𝑦∈R𝑛

где 𝐿 — константа Липшица градиента функции 𝑓 (𝑥) в 2-норме (индекс 2 не пишем, так как везде далее интересуемся константой Липшица в 2-норме).
Опишем ускоренный неевклидов спуск (английское название метода — Accelerated by Coupling Directional Search, ACDS), построенный на базе специальной комбинации спусков по направлению в форме градиентного спуска (Grad) и метода зеркального спуска (Mirr).

Т е о р е м а Пусть 𝑓 (𝑥) — выпуклая дифференцируемая функция на 𝑄 = R𝑛 с константой Липшица для градиента, равной 𝐿 в 2-норме, 𝑑(𝑥) — 1-сильно выпуклая в 𝑝-норме функция на 𝑄, 𝑁 – число итераций метода, 𝑥* — точка минимума функции 𝑓 (𝑥). Тогда ускоренный неевклидов
спуск (ACDS) на выходе даст точку 𝑦𝑁 , удовлетворяющую неравенству

E𝑒1,𝑒2,...,𝑒𝑁 [𝑓 (𝑦𝑁 )] − 𝑓 (𝑥*)

4Θ𝐿𝐶𝑛,𝑞 , 𝑁2

где Θ d=ef 𝑉𝑥0 (𝑥*), 𝐶𝑛,𝑞 d=ef √3 min{2𝑞 − 1, 32 ln 𝑛 − 8}𝑛 𝑞2 +1, 1𝑞 + 𝑝1 = 1.

6Из-за быстрого автоматического дифференцирования [16] это предположение довольно обременительное; но если функция задана моделью черного ящика, выдающего только значение функции, а градиент восстанавливается при 𝑛 + 1 таком обращении, то сделанное предположение кажется вполне естественным.

4

Алгоритм 1. Ускоренный неевклидов спуск (ACDS)

Вход: 𝑓 — выпуклая дифференцируемая функция на R𝑛 с липшицевым градиентом с

константой 𝐿 по отношению к 2-норме; 𝑥0 — некоторая стартовая точка; 𝑁 — количество

итераций. Выход: точка 𝑦𝑁 , для которой выполняется E𝑒1,𝑒2,...,𝑒𝑁 [𝑓 (𝑦𝑁 )] − 𝑓 (𝑥*)
1: 𝑦0 ← 𝑥0, 𝑧0 ← 𝑥0

4Θ𝐿𝑁𝐶2𝑛,𝑞 .

2: for 𝑘 = 0, . . . , 𝑁 − 1 3: 𝛼𝑘+1 ← 2𝐿𝑘𝐶+𝑛2,𝑞 , 𝜏𝑘 ← 𝛼𝑘+11𝐿𝐶𝑛,𝑞 = 𝑘+2 2 4: Генерируется 𝑒𝑘+1 ∼ 𝑅𝑆2𝑛 (1) независимо от предыдущих итераций 5: 𝑥𝑘+1 ← 𝜏𝑘𝑧𝑘 + (1 − 𝜏𝑘)𝑦𝑘

6: 𝑦𝑘+1 ← Grad𝑒𝑘+1 (𝑥𝑘+1) 7: 𝑧𝑘+1 ← Mirr𝑒𝑘+1 (𝑥𝑘+1, 𝑧𝑘, 𝛼𝑘+1) 8: end for

9: return 𝑦𝑁

Сформулируем две ключевые леммы, которые понадобятся для доказательства теоремы (доказательства приведены в Приложении).

Л е м м а 2 Если 𝜏𝑘 = 𝛼𝑘+11𝐿𝐶𝑛,𝑞 , то для всех 𝑢 ∈ 𝑄 = R𝑛 верны неравенства

𝛼𝑘+1⟨∇𝑓 (𝑥𝑘+1), 𝑧𝑘 − 𝑢⟩

𝛼𝑘2+1

·

𝐶𝑛,𝑞 2𝑛

||∇𝑓 (𝑥𝑘+1)||22

+

𝑉𝑧𝑘 (𝑢) −

−E𝑒𝑘+1 [𝑉𝑧𝑘+1 (𝑢) | 𝑒1, 𝑒2, . . . , 𝑒𝑘]

(6)

𝛼𝑘2+1𝐿𝐶𝑛,𝑞 · (𝑓 (𝑥𝑘+1) − E𝑒𝑘+1 [𝑓 (𝑦𝑘+1) | 𝑒1, 𝑒2, . . . , 𝑒𝑘]) +

+ 𝑉𝑧𝑘 (𝑢) − E𝑒𝑘+1 [𝑉𝑧𝑘+1 (𝑢) | 𝑒1, 𝑒2, . . . , 𝑒𝑘].

Л е м м а 3 Для всех 𝑢 ∈ 𝑄 = R𝑛 выполнено неравенство

𝛼𝑘2+1𝐿𝐶𝑛E𝑒𝑘+1 [𝑓 (𝑦𝑘+1) | 𝑒1, . . . , 𝑒𝑘] − (𝛼𝑘2+1𝐿𝐶𝑛 − 𝛼𝑘+1)𝑓 (𝑦𝑘) +

(7)

+ E𝑒𝑘+1 [𝑉𝑧𝑘+1 (𝑢) | 𝑒1, . . . , 𝑒𝑘] − 𝑉𝑧𝑘 (𝑢) 𝛼𝑘+1𝑓 (𝑢).

Сложности возникают при попытке перенесения этого результата на случай 𝑄 ̸= R𝑛. Ограничимся рассмотрением случая 𝑝 = 𝑞 = 2, так как даже для него не удается обобщить рассуждения из [6]. Введем обозначение

{︂
def

𝐿

}︂
2

Prog𝑠(𝑥) = −min
𝑦∈𝑄

⟨𝑠, 𝑦 − 𝑥⟩ + 2 ‖𝑦 − 𝑥‖2

.

Заметим, что вторая часть леммы 1 в евклидовом случае упрощается, а именно

E𝑒[⟨𝑠, 𝑒⟩2] = ||𝑠||22 , 𝑛
где 𝑒 ∼ 𝑅𝑆2𝑛 (см. лемму B.10 из [18]; формулировка данной леммы из [18] есть в Приложении — см. лемму П.1). Отсюда следует, что 𝐶𝑛,𝑞 = 𝑛2. Если положить

{︂
def

𝐿

}︂
2

Grad𝑒 (𝑥) = argmin
𝑦∈𝑄

⟨⟨∇𝑓 (𝑥), 𝑒⟩𝑒, 𝑥 − 𝑦⟩ − 2 ‖𝑦 − 𝑥‖2

и Mirr𝑒 (𝑥, 𝑧, 𝛼) d=ef argmin {𝛼 ⟨𝑛 ⟨∇𝑓 (𝑥) , 𝑒⟩ 𝑒, 𝑦 − 𝑧⟩ + 𝑉𝑧 (𝑦)} ,
𝑦∈𝑄
то чтобы обобщить приведенные рассуждения на условный случай, нужно оценить подходящим образом Prog𝑛⟨∇𝑓(𝑥), 𝑒⟩ 𝑒(𝑥𝑘+1) (точнее, его математическое ожидание по 𝑒𝑘+1), т.е., исходя из техники, используемой в [6], хотелось бы доказать оценку

[︁

]︁

E𝑒𝑘+1 Prog𝑛⟨∇𝑓(𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 (𝑥𝑘+1) 𝑛2 (𝑓 (𝑥𝑘+1) − E [𝑓 (𝑦𝑘+1)]) , (8)

5

чтобы получить оценку скорости сходимости, как и в случае безусловной минимизации. К сожалению, существует пример (будет приведен далее) выпуклой 𝐿-гладкой функции и замкнутого выпуклого множества, для которых (8) не выполняется.
Сначала рассмотрим более детально Prog𝜉(𝑥):

{︂ ⃒⃒

⃒⃒2

}︂

Prog𝜉(𝑥) = −min 𝐿2 ⃒⃒⃒⃒𝑦 − 𝑥⃒⃒⃒⃒ + ⟨𝜉, 𝑦 − 𝑥⟩ =

𝑦∈𝑄

2

{︂ ⃒⃒

⃒⃒2

⃒⃒ ⃒⃒2}︂

⃒⃒ ⃒⃒2

= −min 𝐿 ⃒⃒𝑦 − 𝑥⃒⃒ + ⟨𝜉, 𝑦 − 𝑥⟩ + 1 ⃒⃒𝜉⃒⃒ + 1 ⃒⃒𝜉⃒⃒ =

𝑦∈𝑄 2 ⃒⃒

⃒⃒2

2𝐿 ⃒⃒ ⃒⃒2 2𝐿 ⃒⃒ ⃒⃒2

{︂ ⃒⃒

√︁

√︁ ⃒⃒2}︂

⃒⃒ ⃒⃒2

= −min ⃒⃒ √1 𝜉 + 𝐿 · 𝑦 − 𝐿 · 𝑥⃒⃒ + 1 ⃒⃒𝜉⃒⃒ =

𝑦∈𝑄 ⃒⃒ 2𝐿

2

2 ⃒⃒2 2𝐿 ⃒⃒ ⃒⃒2

{︂ ⃒⃒

⃒⃒2}︂

⃒⃒ ⃒⃒2

= − 𝐿 min ⃒⃒𝑦 − (︀𝑥 − 1 𝜉)︀ ⃒⃒ + 1 ⃒⃒𝜉⃒⃒ ,

2 𝑦∈𝑄 ⃒⃒

𝐿 ⃒⃒2 2𝐿 ⃒⃒ ⃒⃒2

т.е. точка, в которой достигается этот минимум7,

(︂ 1 )︂ 𝑦^ = 𝜋𝑄 𝑥 − 𝐿 𝜉 .

Тогда

(︂ 1 )︂ 𝑦𝑘+1 = 𝜋𝑄 𝑥 − 𝐿 𝑠𝑘+1 ,

def

1

𝑠𝑘+1 = ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 = 𝑔(𝑥𝑘+1, 𝑒𝑘+1).

𝑛

Кроме того, обозначим через 𝑦̃︀𝑘+1 точку множества 𝑄, в которой достигается минимум в формуле для Prog𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 (𝑥𝑘+1). Тогда

(︁ 𝑛 )︁ 𝑦̃︀𝑘+1 = 𝜋𝑄 𝑥 − 𝐿 𝑠𝑘+1 .

Также для удобства рассмотрим следующие представления для 𝑦𝑘+1 и 𝑦̃︀𝑘+1:

1 𝑦𝑘+1 = 𝑥𝑘+1 − 𝐿 𝑠𝑘+1 + 𝑟𝑘+1, (9)
𝑛 𝑦̃︀𝑘+1 = 𝑥𝑘+1 − 𝐿 𝑠𝑘+1 + 𝑟̃︀𝑘+1, где 𝑟𝑘+1 и 𝑟̃︀𝑘+1 будем называть векторами невязок. Рассмотрим функцию

𝐿 ⃒⃒

⃒⃒2

𝑓 (𝑦) = 𝑓 (𝑥𝑘+1) + ⟨∇𝑓 (𝑥𝑘+1), 𝑦 − 𝑥𝑘+1⟩ + 2 ⃒⃒⃒⃒𝑦 − 𝑥𝑘+1⃒⃒⃒⃒2 (10)

и множество, изображенное на рис. 1 (в качестве ∇𝑓 (𝑥𝑘+1) можно выбрать любой ненулевой вектор, а в качестве 𝑄 — прямоугольный параллелепипед с достаточно длинными сторонами, в центре одной из гиперграней которого размещена точка 𝑥𝑘+1).
Подставим в (10) значение 𝑦 = 𝑦𝑘+1 и воспользуемся представлением 𝑦𝑘+1 из (9):

1

𝐿 ⃒⃒

1 ⃒⃒2

−⟨∇𝑓 (𝑥𝑘+1), − 𝐿 𝑠𝑘+1 + 𝑟𝑘+1⟩ − 2 ⃒⃒⃒⃒𝑟𝑘+1 − 𝐿 𝑠𝑘+1⃒⃒⃒⃒2 = 𝑓 (𝑥𝑘+1) − 𝑓 (𝑦𝑘+1).

Далее воспользуемся тем, что 𝑠𝑘+1 = ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1:

⃒⃒ ⃒⃒2 𝐿1 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩2 − ⟨∇𝑓 (𝑥𝑘+1), 𝑟𝑘+1⟩ − 𝐿2 ⃒⃒⃒⃒𝑟𝑘+1⃒⃒⃒⃒ + ⟨𝑟𝑘+1, 𝑠𝑘+1⟩ −
2
− 21𝐿 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩2 = 𝑓 (𝑥𝑘+1) − 𝑓 (𝑦𝑘+1), или в более компактной форме

⃒⃒ ⃒⃒2 21𝐿 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩2 − 𝐿2 ⃒⃒⃒⃒𝑟𝑘+1⃒⃒⃒⃒ + ⟨𝑟𝑘+1, 𝑠𝑘+1 − ∇𝑓 (𝑥𝑘+1)⟩ =
2
= 𝑓 (𝑥𝑘+1) − 𝑓 (𝑦𝑘+1).

(11)

7См. [7].

6

Рис. 1: Пример ситуации, когда ключевое неравенство не выполнено

При таком выборе функции и множества получаем, что 𝑛2 · ||𝑟𝑘+1||22 = ||𝑟˜𝑘+1||22 для всех единичных 𝑒. Действительно, если

Prog⟨∇𝑓(𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 (𝑥𝑘+1) = 21𝐿 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩2 − 𝐿2 ⃒⃒⃒⃒⃒⃒𝑟𝑘+1⃒⃒⃒⃒⃒⃒22

и

𝑛2

2 𝐿 ⃒⃒ ⃒⃒2

Prog𝑛⟨∇𝑓(𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 (𝑥𝑘+1) = 2𝐿 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ − 2 ⃒⃒⃒⃒𝑟˜𝑘+1⃒⃒⃒⃒2,

то

Prog𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 (𝑥𝑘+1) = 𝑛2 Prog⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 (𝑥𝑘+1).

Отсюда и из (11) следует, что

1 𝑛2

Prog𝑛⟨∇𝑓

(𝑥𝑘+1

),

𝑒𝑘+1

⟩

𝑒𝑘+1

(𝑥𝑘+1

)

+

⟨𝑟𝑘+1,

𝑠𝑘+1

−

∇𝑓 (𝑥𝑘+1)⟩

=

= 𝑓 (𝑥𝑘+1) − 𝑓 (𝑦𝑘+1).

Заметим, что вектор 𝑠𝑘+1 всегда короче (точнее, не длиннее) вектора ∇𝑓 (𝑥𝑘+1) и направлен “вниз” (т.е. в то же полупространство, образованное гранью 𝑄, на которой лежит точка 𝑥𝑘+1), как и ∇𝑓 (𝑥𝑘+1). Значит, разность 𝑠𝑘+1 − ∇𝑓 (𝑥𝑘+1) будет направлена в противоположную часть пространства. А вектор 𝑟𝑘+1 тоже направлен вниз. Следовательно, всегда выполняется ⟨𝑟𝑘+1, 𝑠𝑘+1 − ∇𝑓 (𝑥𝑘+1)⟩ 0, причем с ненулевой вероятностью выполнено строгое неравенство. Это означает, что

E𝑒𝑘+1 [⟨𝑟𝑘+1, 𝑠𝑘+1 − ∇𝑓 (𝑥𝑘+1)⟩] < 0.

Поэтому

[︁

]︁

E𝑒𝑘+1 Prog𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1 (𝑥𝑘+1) =

= 𝑛2(𝑓 (𝑥𝑘+1) − E𝑒𝑘+1 [𝑓 (𝑦𝑘+1)]) − E𝑒𝑘+1 [⟨𝑟𝑘+1, 𝑠𝑘+1 − ∇𝑓 (𝑥𝑘+1)⟩] > > 𝑛2 (︀𝑓 (𝑥𝑘+1) − E𝑒𝑘+1 [𝑓 (𝑦𝑘+1)])︀ .

Представленный контр-пример показывает трудности в перенесении предлагаемого в статье

метода на задачи условной оптимизации. Теорема утверждает, что ускоренный неевклидов спуск

(алгоритм ACDS) через 𝑁 итераций выдаст точку 𝑦𝑁 , удовлетворяющую неравенству E[𝑓 (𝑦𝑁 )] −

𝑓 (𝑥*)

(︂√︁

)︂

Θ𝐿2 𝐶𝑛,𝑞

√

2 +1

𝜀, 𝜀 > 0, если 𝑁 = 𝑂

𝜀 . По определению 𝐶𝑛,𝑞 = 3 min{2𝑞 − 1, 32 ln 𝑛 − 8}𝑛 𝑞 ,

7

а в случае 𝑝 = 𝑞 = 2 можно взять 𝐶𝑛,𝑞 = 𝑛2, что видно из леммы 1 и леммы B.10 из [18] (приведена

(︂√︁ )︂

в Приложении как лемма П.1), поэтому 𝑁 = 𝑂

Θ𝐿2 𝑛2 𝜀

. Если же 𝑝 = 1 и 𝑞 = ∞, то 𝐶𝑛,𝑞 =

𝑛√3 (32 ln 𝑛 − 8) и 𝑁 = 𝑂 (︂√︁ Θ𝐿2𝑛 ln 𝑛 )︂.
𝜀

5 Численные эксперименты

Для практического применения предложенный ускоренный неевклидов спуск по случайному
направлению ACDS был реализован на языке программирования Python. Код метода и демонстра-
ция вычислительных свойств метода с построением графиков сходимости доступны как Jupyter
Notebook и выложены в свободном доступе на Github [19].
Рассмотрим следующую з а д а ч у. Пусть 𝐴 — матрица размеров 𝑛 × 𝑛 со случайными независимыми элементами, равномерно распределенными на отрезке [0, 1], а матрица 𝐵 = 𝜆ma𝐴x(⊤𝐴𝐴⊤𝐴) , где 𝜆max(𝐴⊤𝐴) — максимальное собственное значение матрицы 𝐴⊤𝐴.
Необходимо минимизировать функцию

1 𝑓 = ⟨𝑥 − 𝑥*, 𝐵(𝑥 − 𝑥*)⟩ ,

𝑥 ∈ R𝑛,

(12)

2

где 𝑥* = (1, 0, 0, . . . , 0)⊤. Решение этой задачи известно и равно 𝑥*, 𝑓 (𝑥*) = 0. Начальная точка 𝑥0 для всех экспериментов выбиралась как (0, 0, 0, . . . , 1)⊤. Константа Липшица градиента целевой функции 𝐿 = 1.
Следует отметить важность достаточно точного решения вспомогательной задачи минимизации для нахождения 𝑧𝑘+1 на шаге 7 алгоритма 1 (зеркальный спуск). В рассматриваемом случае эту задачу с помощью метода множителей Лагранжа можно свести к задаче одномерной минимизации, подробности с формулами см. в [19]. В реализации метода одномерная минимизация выполняется с помощью обычной дихотомии с точностью, на один порядок превышающей заданную.
Для различных 𝑛 и заданной точности 𝜀 были рассчитаны теоретически требуемые значения числа итераций по теореме и проведена проверка сходимости на практике. Для данной задачи во всех случаях практическая скорость сходимости по функции была выше. Так, например, для 𝑛 = 10 и 𝜀 = 10−3 заданная точность была достигнута за 729 итераций (см. рис. 2), а теоретическая оценка числа итераций дает 2537 итераций. Далее, для 𝑛 = 103 и 𝜀 = 10−4 теоретическая оценка числа итераций дает не более чем 255972 итерации. По факту алгоритм завершил работу за 141643 итерации (см. рис. 3). Медленный спуск в начале работы метода объясняется близостью начального значения целевой функции к оптимальному именно в данном примере (𝑓 (𝑥0) = 0, 00032983).
При проведении численных экспериментов было обнаружено, что преимущество выбора проксструктуры, связанной с 1-нормой, возникает только в пространствах от средней размерности (от 𝑛 = 1000). Будет ли иметь преимущество предложенный метод, можно определить, сравнив теоретические оценки числа итераций предложенного метода для разных 𝑝. На рис. 3 именно показан случай, когда ускоренный спуск по направлению с неевклидовой прокс-структурой оказывается оптимальнее спуска по направлению с евклидовой прокс-структурой.
В целом, численные эксперименты с ускоренным спуском по случайному направлению подтверждают теоретические результаты.

6 Заключение
В статье предложен ускоренный неевклидов спуск по направлению для решения задачи выпуклой безусловной оптимизации. В отличие от известных вариантов методов спуска по направлению (см., например [1]) в данной статье рассматривается ускоренный спуск по направлению с неевклидовой прокс-структурой. В случае когда 1-норма решения близка к 2-норме решения (это имеет место, например, если решение задачи разрежено — имеет много нулевых компонент), предлагаемый подход улучшает оц√енку на необходимое число итераций, полученную оптимальным методом из [1], приблизительно в 𝑛 раз, где 𝑛 – размерность пространства, в котором происходит оптимизация.

8

Рис. 2: Сходимость ускоренного неевклидового спуска ACDS для функции (12), размерность
𝑛 = 10. Показана практическая зависимость точности нахождения минимума 𝑓 (𝑥𝑁 ) − 𝑓 (𝑥*) от числа итераций 𝑁 алгоритма (график 1) и теоретическая оценка 𝑂 (︁ 4Θ𝐿𝑁𝐶2𝑛,𝑞 )︁ (график 2)

Данная статья открывает цикл работ, в которых планируется привести полные доказательства утверждений, полученных авторами в 2014–2016 гг. и приведенных (без доказательств) в [20]. В частности, далее планируется распространить приведенные в настоящей статье результаты на безградиентные методы, на задачи стохастической оптимизации и распространить все эти результаты на случай сильно выпуклой функции.
Открытой проблемой остается распространение полученных здесь результатов на случай задач оптимизации на множествах простой структуры. Напомним, что в статье существенным образом использовалось то, что оптимизация происходит на всем пространстве. Тем не менее в будущем планируется показать, что приведенные здесь результаты распространяются на задачи оптимизации на множествах простой структуры в случае, если градиент функционала в точке решения равен нулю (принцип Ферма).
Авторы выражают благодарность Павлу Двуреченскому и Александру Тюрину за помощь в работе.

ПРИЛОЖЕНИЕ

Д о к а з а т е л ь с т в о л е м м ы 2. Докажем сначала первую часть неравенства:

𝛼𝑘+1⟨𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑢⟩ =

= ⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ +

x
+ ⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘+1 − 𝑢⟩

x

y

⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ + ⟨−∇𝑉𝑧𝑘 (𝑧𝑘+1), 𝑧𝑘+1 − 𝑢⟩ =

y

z

= ⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ + 𝑉𝑧𝑘 (𝑢) − 𝑉𝑧𝑘+1 (𝑢) − 𝑉𝑧𝑘 (𝑧𝑘+1)

z
(︀⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ − 12 ||𝑧𝑘 − 𝑧𝑘+1||2𝑝)︀ +

+ 𝑉𝑧𝑘 (𝑢) − 𝑉𝑧𝑘+1 (𝑢),

(13)

9

Рис. 3: Сходимость ускоренного неевклидового спуска ACDS для функции (12), размерность 𝑛 = 103. Показана практическая зависимость точности нахождения минимума 𝑓 (𝑥𝑁 ) − 𝑓 (𝑥*) от числа итераций 𝑁 алгоритма (сплошная линия — график 1). Также для сравнения приведены результаты работы метода при других 𝑝 (евклидова норма — график 2; 𝑝 = 1, 8 —
график 3; 𝑝 = 1, 9 — график 4) при тех же генерируемых векторах 𝑒 и точке старта 𝑥0

где x выполнено в силу того, что 𝑧𝑘+1 = argmin {𝑉𝑧𝑘 (𝑧) + 𝛼𝑘+1⟨𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧⟩}, откуда
𝑧∈R𝑛
следует, что ⟨∇𝑉𝑧𝑘 (𝑧𝑘+1) + 𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑢 − 𝑧𝑘+1⟩ 0 для всех 𝑢 ∈ 𝑄 = R𝑛, y выполнено в силу равенства треугольника для дивергенции Брэгмана 8, z выполнено, так как 𝑉𝑥(𝑦) 21 ||𝑥 − 𝑦||2𝑝 в силу сильной выпуклости прокс-функции 𝑑(𝑥).
Теперь покажем, что
⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ − 21 ||𝑧𝑘 − 𝑧𝑘+1||2𝑝 𝛼2𝑘+21𝑛2 |⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩|2 · ||𝑒𝑘+1||2𝑞 .
Действительно, в силу неравенства Гельдера

⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ 𝛼𝑘+1𝑛||⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1||𝑞 · ||𝑧𝑘 − 𝑧𝑘+1||𝑝 = = 𝛼𝑘+1𝑛|⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩| · ||𝑒𝑘+1||𝑞 · ||𝑧𝑘 − 𝑧𝑘+1||𝑝,

откуда

⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ − 12 ||𝑧𝑘 − 𝑧𝑘+1||2𝑝 𝛼𝑘+1𝑛|⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩| · ||𝑒𝑘+1||𝑞 · ||𝑧𝑘 − 𝑧𝑘+1||𝑝 − 12 ||𝑧𝑘 − 𝑧𝑘+1||2𝑝.

(14)

Положим

𝑡

=

||𝑧𝑘

− 𝑧𝑘+1||𝑝,

𝑎

=

1 2

и

𝑏

=

𝛼𝑘+1𝑛|⟨∇𝑓 (𝑥𝑘+1),

𝑒𝑘+1⟩| · ||𝑒𝑘+1||𝑞,

тогда

правая

часть

в

(14)

имеет вид

𝑏𝑡 − 𝑎𝑡2.

8Действительно,

∀𝑥, 𝑦 ∈ R𝑛 ⟨−∇𝑉𝑥(𝑦), 𝑦 − 𝑢⟩ = ⟨∇𝑑(𝑥) − ∇𝑑(𝑦), 𝑦 − 𝑢⟩ = (𝑑(𝑢) − 𝑑(𝑥) − ⟨∇𝑑(𝑥), 𝑢 − 𝑥⟩) − − (𝑑(𝑢) − 𝑑(𝑦) − ⟨∇𝑑(𝑦), 𝑢 − 𝑦⟩) − (𝑑(𝑦) − 𝑑(𝑥) − ⟨∇𝑑(𝑥), 𝑦 − 𝑥⟩) = 𝑉𝑥(𝑢) − 𝑉𝑦(𝑢) − 𝑉𝑥(𝑦).

10

Если рассматривать полученное выражение как функцию от 𝑡 ∈ R, то ее максимум при 𝑡 ∈ R равен (а значит, при 𝑡 ∈ R+ не превосходит) 4𝑏2𝑎 . Отсюда и из (14) следует неравенство

⟨𝛼𝑘+1𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑧𝑘+1⟩ − 12 ||𝑧𝑘 − 𝑧𝑘+1||2𝑝 𝛼2𝑘+21𝑛2 |⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩|2 · ||𝑒𝑘+1||2𝑞 .

(15)

Итак, учитывая (13) и (15), получаем, что

𝛼𝑘+1⟨𝑛⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, 𝑧𝑘 − 𝑢⟩ 𝛼2𝑘+21𝑛2 |⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩|2 · ||𝑒𝑘+1||2𝑞 + 𝑉𝑧𝑘 (𝑢) − 𝑉𝑧𝑘+1 (𝑢).
Беря условное математическое ожидание E𝑒𝑘+1 [ · | 𝑒1, 𝑒2, . . . , 𝑒𝑘] от левой и правой частей последнего неравенства и пользуясь вторым неравенством из леммы 1, получаем, что

𝛼𝑘+1⟨∇𝑓 (𝑥𝑘+1), 𝑧𝑘 − 𝑢⟩

𝛼𝑘2+1

·

𝐶𝑛,𝑞 2𝑛

||∇𝑓 (𝑥𝑘+1)||22

+

𝑉𝑧𝑘 (𝑢)

−

E𝑒𝑘+1 [𝑉𝑧𝑘+1 (𝑢)

|

𝑒1,

𝑒2,

...,

𝑒𝑘 ],

где 𝐶𝑛,𝑞=√3 min{2𝑞 − 1, 32 ln 𝑛 − 8}𝑛 𝑞2 +1. Чтобы доказать вторую часть неравенства (6), покажем,

что

||∇𝑓 (𝑥𝑘+1)||22 2𝑛𝐿 (︀𝑓 (𝑥𝑘+1) − E𝑒𝑘+1 [𝑓 (𝑦𝑘+1) | 𝑒1, 𝑒2, . . . , 𝑒𝑘])︀ .

(16)

Во-первых, для всех 𝑥, 𝑦 ∈ R

1
𝑓 (𝑦) − 𝑓 (𝑥) = ∫︀ ⟨∇𝑓 (𝑥 + 𝜏 (𝑦 − 𝑥)), 𝑦 − 𝑥⟩𝑑𝜏 =
0 1
= ⟨∇𝑓 (𝑥), 𝑦 − 𝑥⟩ + ∫︀ ⟨∇𝑓 (𝑥 + 𝜏 (𝑦 − 𝑥)) − ∇𝑓 (𝑥), 𝑦 − 𝑥⟩𝑑𝜏
0 1
⟨∇𝑓 (𝑥), 𝑦 − 𝑥⟩ + ∫︀ ||∇𝑓 (𝑥 + 𝜏 (𝑦 − 𝑥)) − ∇𝑓 (𝑥)||2 · ||𝑦 − 𝑥||2𝑑𝜏
0 1
⟨∇𝑓 (𝑥), 𝑦 − 𝑥⟩ + ∫︀ 𝜏 𝐿||𝑦 − 𝑥||2 · ||𝑦 − 𝑥||2𝑑𝜏 =
0
= ⟨∇𝑓 (𝑥), 𝑦 − 𝑥⟩ + 𝐿2 ||𝑦 − 𝑥||22,
т.е. −⟨∇𝑓 (𝑥), 𝑦 − 𝑥⟩ − 𝐿2 ||𝑦 − 𝑥||22 𝑓 (𝑥) − 𝑓 (𝑦).
Беря в последнем неравенстве 𝑥 = 𝑥𝑘+1, 𝑦 = Grad𝑒𝑘+1 (𝑥𝑘+1) = 𝑥𝑘+1 − 𝐿1 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩ 𝑒𝑘+1, получим, что

𝑓 (𝑥𝑘+1) − 𝑓 (𝑦𝑘+1)

𝐿1 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩2 − 21𝐿 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩2 · ||𝑒𝑘+1||22 = = 21𝐿 ⟨∇𝑓 (𝑥𝑘+1), 𝑒𝑘+1⟩2.

Возьмем от этого неравенства условное математическое ожидание E𝑒𝑘+1 [ · | 𝑒1, 𝑒2, . . . , 𝑒𝑘], используя лемму B.10 из [18] (см. лемму П.1), и получим неравенство (16). Лемма 2 доказана.
Д о к а з а т е л ь с т в о л е м м ы 3. Доказательство состоит в выписывании цепочки неравенств:

𝛼𝑘+1(𝑓 (𝑥𝑘+1) − 𝑓 (𝑢))
𝛼𝑘+1⟨∇𝑓 (𝑥𝑘+1), 𝑥𝑘+1 − 𝑢⟩ =
= 𝛼𝑘+1⟨∇𝑓 (𝑥𝑘+1), 𝑥𝑘+1 − 𝑧𝑘⟩ + 𝛼𝑘+1⟨∇𝑓 (𝑥𝑘+1), 𝑧𝑘 − 𝑢⟩ =x
=x (1−𝜏𝑘𝜏𝑘)𝛼𝑘+1 ⟨∇𝑓 (𝑥𝑘+1), 𝑦𝑘 − 𝑥𝑘+1⟩ + 𝛼𝑘+1⟨∇𝑓 (𝑥𝑘+1), 𝑧𝑘 − 𝑢⟩ y y (1−𝜏𝑘𝜏𝑘)𝛼𝑘+1 (𝑓 (𝑦𝑘) − 𝑓 (𝑥𝑘+1)) + 𝛼𝑘+1⟨∇𝑓 (𝑥𝑘+1), 𝑧𝑘 − 𝑢⟩ z z (1−𝜏𝑘𝜏𝑘)𝛼𝑘+1 (𝑓 (𝑦𝑘) − 𝑓 (𝑥𝑘+1)) + + 𝛼𝑘2+1𝐿𝐶𝑛,𝑞 · (𝑓 (𝑥𝑘+1) − E𝑒𝑘+1 [𝑓 (𝑦𝑘+1) | 𝑒1, 𝑒2, . . . , 𝑒𝑘]) + + 𝑉𝑧𝑘 (𝑢) − E𝑒𝑘+1 [𝑉𝑧𝑘+1 (𝑢) | 𝑒1, 𝑒2, . . . , 𝑒𝑘] ={
={ (𝛼𝑘2+1𝐿𝐶𝑛,𝑞 − 𝛼𝑘+1)𝑓 (𝑦𝑘) − 𝛼𝑘2+1𝐿𝐶𝑛,𝑞E𝑒𝑘+1 [𝑓 (𝑦𝑘+1) | 𝑒1, 𝑒2, . . . , 𝑒𝑘] + + 𝛼𝑘+1𝑓 (𝑥𝑘+1) + 𝑉𝑧𝑘 (𝑢) − E𝑒𝑘+1 [𝑉𝑧𝑘+1 (𝑢) | 𝑒1, 𝑒2, . . . , 𝑒𝑘].

11

Действительно, x выполнено, так как 𝑥𝑘+1 d=ef 𝜏𝑘𝑧𝑘 +(1−𝜏𝑘)𝑦𝑘 ⇔ 𝜏𝑘(𝑥𝑘+1 −𝑧𝑘) = (1−𝜏𝑘)(𝑦𝑘 −𝑥𝑘+1),
y следует из выпуклости 𝑓 (·) и неравенства 1 − 𝜏𝑘 0, z справедливо в силу леммы 2 и в { используется равенство 𝜏𝑘 = 𝛼𝑘+11𝐿𝐶𝑛,𝑞 . Лемма 3 доказана.
Д о к а з а т е л ь с т в о т е о р е м ы. Заметим, что при 𝛼𝑘+1 = 2𝐿𝑘𝐶+𝑛2,𝑞 выполнено равенство

𝛼2 𝐿𝐶𝑛,𝑞 = 𝛼2 𝐿𝐶𝑛,𝑞 − 𝛼𝑘+1 +

1 .

𝑘

𝑘+1

4𝐿𝐶𝑛,𝑞

Возьмем для 𝑘 = 0, 1, . . . , 𝑁 − 1 от каждого неравенства (7) леммы 3 математическое ожидание по 𝑒1, 𝑒2, . . . , 𝑒𝑁 , просуммируем полученные неравенства и получим

𝑁 −1
∑︁

1

𝛼𝑁2 𝐿𝐶𝑛,𝑞E[𝑓 (𝑦𝑁 )] +

E[𝑓 (𝑦𝑘)] + E[𝑉𝑧𝑁 (𝑢)] − 𝑉𝑧0 (𝑢)

𝑘=1 4𝐿𝐶𝑛,𝑞

𝑁
∑︁ 𝛼𝑘𝑓 (𝑢),
𝑘=1

где E[·] = E𝑒1, 𝑒2, ..., 𝑒𝑁 [·]. Положим 𝑢 = 𝑥*. Так как ∑︀𝑁𝑘=1 𝛼𝑘 = 𝑁4𝐿(𝑁𝐶+𝑛,3𝑞) , E[𝑓 (𝑦𝑘)] 𝑓 (𝑥*), 𝑉𝑧𝑁 (𝑢) 0 и 𝑉𝑧0 (𝑥*) = 𝑉𝑥0 (𝑥*) Θ, то выполняется неравенство

(𝑁 + 1)2 4𝐿𝐶𝑛,𝑞 E[𝑓 (𝑦𝑁 )]

(︂ 𝑁 (𝑁 + 3) −

𝑁 − 1 )︂ 𝑓 (𝑥*) + Θ,

4𝐿𝐶𝑛,𝑞

4𝐿𝐶𝑛,𝑞

откуда следует, что E[𝑓 (𝑦𝑁 )] 𝑓 (𝑥*) + 4(Θ𝑁𝐿+𝐶1𝑛),2𝑞 . Теорема доказана. Приводим формулировку леммы B.10 из [18]. Отметим, что в доказательстве нигде не исполь-
зовалось, что второй вектор в скалярном произведении (помимо 𝑒) есть градиент функции 𝑓 (𝑥) (поэтому утверждение леммы П.1 остается верным для произвольного вектора 𝑠 ∈ R𝑛 вместо ∇𝑓 (𝑥)).

Л е м м а П.1 Пусть 𝑒 ∼ 𝑅𝑆2𝑛(1) и вектор 𝑠 ∈ R𝑛 — некоторый вектор. Тогда E𝑒[⟨𝑠, 𝑒⟩2] = ||𝑠||22 . 𝑛

Список литературы

[1] Nesterov Yu. Random gradient-free minimization of convex functions // CORE Discussion Paper 2011/1. 2011.
[2] Немировский А.С., Юдин Д.Б. Сложность задач и эффективность методов оптимизации. М.: Наука, 1979.
[3] Гасников А.В., Двуреченский П.Е., Нестеров Ю.Е. Стохастические градиентные методы с неточным оракулом // Тр. МФТИ. 2016. Т. 8. № 1. С. 41–91. arXiv preprint arXiv:1411.4218.
[4] Гасников А.В., Лагуновская А.А., Усманова И.Н., Федоренко Ф.А. Безградиентные проксметоды с неточным оракулом для негладких задач выпуклой стохастической оптимизации на симплексе // АиТ. 2016. № 10. С. 57–77. Gasnikov A.V., Lagunovskaya A.A., Usmanova I.N., Fedorenko F.A. Gradient-free proximal methods with inexact oracle for convex stochastic nonsmooth optimization problems on the simplex // Autom. Remote Control. 2016. V. 77. No. 11. P. 2018–2034.
[5] Гасников А.В., Крымова Е.А., Лагуновская А.А., Усманова И.Н., Федоренко Ф.А. Стохастическая онлайн оптимизация. Одноточечные и двухточечные нелинейные многорукие бандиты. Выпуклый и сильно выпуклый случаи // АиТ. 2017. № 2. C. 36–49. Gasnikov A.V., Krymova E.A., Lagunovskaya A.A., Usmanova I.N., Fedorenko F.A. Stochastic online optimization. Singlepoint and multi-point non-linear multi-armed bandits. Convex and strongly-convex case // Autom. Remote Control. 2017. V. 78. No. 2. P. 224–234.
[6] Allen-Zhu Z., Orecchia L. Linear coupling: An ultimate unification of gradient and mirror descent // arXiv preprint arXiv:1407.1537.
[7] Гасников А.В. Современные численные методы оптимизации. Универсальный градиентный спуск. Уч. пос. М.: МФТИ, 2018. URL: https://arxiv.org/ftp/arxiv/papers/1711/1711. 00394.pdf.

12

[8] Nesterov Yu. Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems // SIAM J. Optim. 2012. № 22 (2). P. 341–362.
[9] Dvurechensky P., Gasnikov A., Tiurin A. Randomized Similar Triangles Method: A Unifying Framework for Accelerated Randomized Optimization Methods (Coordinate Descent, Directional Search, Derivative-Free Method) // arXiv preprint arXiv:1707.08486.
[10] Горбунов Э.А., Воронцова Е.А., Гасников А.В. О верхней оценке математического ожидания нормы равномерно распределенного на сфере вектора и явлении концентрации равномерной меры на сфере // arXiv preprint arXiv:1804.03722.
[11] Баяндина А.С., Гасников А.В., Лагуновская А.А. Безградиентные двухточечные методы решения задач стохастической негладкой выпуклой оптимизации при наличии малых шумов не случайной природы // АиТ. 2018. № 8. С. 38–49. Bayandina A.S., Gasnikov A. V., Lagunovskaya A.A. Gradient-Free Two-Point Methods for Solving Stochastic Nonsmooth Convex Optimization Problems with Small Non-Random Noises // Autom. Remote Control. 2018. V. 79. No. 8. P. 1399– 1408.
[12] Гасников А.В., Двуреченский П.Е., Усманова И.Н. О нетривиальности быстрых (ускоренных) рандомизированных методов // Тр. МФТИ. 2016. Т. 8. № 2. С. 67–100. arXiv preprint arXiv:1508.02182.
[13] Allen-Zhu Z. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods // arXiv preprint arXiv:1603.05953.
[14] Guzman C., Nemirovski A. On Lower Complexity Bounds for Large-Scale Smooth Convex Optimization // J. Complexity. February 2015. V. 31. Iss. 1. P. 1–14.
[15] Гасников А.В., Нестеров Ю.Е. Универсальный метод для задач стохастической композитной оптимизации // ЖВМ и МФ. 2018. Т. 58. № 1. C. 52–69. arXiv preprint arXiv:1604.05275.
[16] Baydin A.G., Pearlmutter B.A., Radul A.A., Siskand J.M. Automatic Differentiation in Machine Learning: a Survey // arXiv preprint arXiv:1502.05767.
[17] Брэгман Л.М. Релаксационный метод нахождения общей точки выпуклых множеств и его применение для решения задач выпуклого программирования // ЖВМ и МФ. 1967. Т. 7. № 3. С. 200–217.
[18] Bogolubsky L., Dvurechensky P., Gasnikov A., Gusev G., Raigorodskii A., Tikhonov A., Zhukovskii M. Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods // 13th Annual Conf. on Neural Information Processing Systems (NIPS). 2016. arXiv preprint arXiv:1603.00717.
[19] ACDS method Python code. URL: https://github.com/evorontsova/ACDS. [20] Гасников А.В. Эффективные численные методы поиска равновесий в больших транспортных
сетях. Дисс. на соискание уч. степ. д.ф.-м.н. по специальности 05.13.18 – Математическое моделирование, численные методы, комплексы программ. М.: МФТИ, 2016. arXiv preprint arXiv:1607.03142.
13

