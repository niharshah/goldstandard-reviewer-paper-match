Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations
Aditi Chaudhary, Chunting Zhou, Lori Levin, Graham Neubig, David R. Mortensen, Jaime G. Carbonell
Language Technologies Institute, Carnegie Mellon University {aschaudh,ctzhou,lsl,gneubig,dmortens,jgc}@cs.cmu.edu

Abstract
Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to lowresourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a signiï¬cant gain in performance over previous methods relying on these resources. We demonstrate the eï¬€ectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.
1 Introduction
Continuous word representations have demonstrated utility in state-of-the-art neural models for several NLP tasks, such as named entity recognition (NER; Ma and Hovy (2016)), machine reading (Tan et al., 2017), sentiment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (HammarstrÃ¶m et al., 2018), and corpora with suï¬ƒcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting.

Disheartening though this high dependence on resources sounds, several eï¬€orts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diï¬€erent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncertain quality.
In this paper, we take a diï¬€erent task: focusing instead on the similarity of the surface forms, phonology, or morphology of the two transfer languages. Speciï¬cally, inspired by Ling et al. (2015), who demonstrate the eï¬€ectiveness of character-level modeling for knowledge sharing in multilingual scenarios, we propose two approaches to transfer word embeddings using diï¬€erent types of linguistically-inspired subword-level information. Both approaches focus on mapping the low resource language embeddings closer to those of the high resource language and are executed using two diï¬€erent training regimes. We explore the eï¬€ect of diï¬€erent subword unitsâ€” characters, lemmas, inï¬‚ectional properties, and phonemesâ€” as each one oï¬€ers a unique linguistic insight, discussed more in Section 3. Our proposed approaches do require language speciï¬c resources, but importantly do not depend on crosslingual resources and achieve considerable performance gains over existing methods which do.
We evaluate our proposed approach on two downstream tasks: NER, which deals with detecting and classifying Named Entities (NEs) into pre-

deï¬ned categories (Nadeau and Sekine, 2007), and MT to English. For the purposes of error analysis and discussion, we focus on the NER task in particular. NEs are typically noun phrases and occur rarely in the corpus, making the generalization across types and domains diï¬ƒcult. We chose NER as our test bed because word vectors have a direct impact on NER model performanceâ€” as suggested by (Ruder, 2017) and observed by us in Table 3, where the model without any pre-trained embeddings scores an average of 18 F1 points less. It thus provides a transparent way to measure the effectiveness of diï¬€erent subword units.
This paper makes the following contributions:
1. We show that embeddings trained on subword representations yield better task performance than those trained only on whole words. This is especially true in a transfer setting, where subword representations also outperform a word alignment based method. We further show that embeddings trained on morphological representations often outperform those trained only on whole words.
2. We demonstrate that training embeddings on character-based phonemic representations presents substantial performance advantages over training on orthographic characters in some transfer settings, e.g. when there are script diï¬€erences across languages. These advantages are in addition to those from morphological representations (lemmas and morphological properties).
3. We produce continuous representations for each subword unit, giving researchers the ability to use them in their own tasks as they see ï¬t. The code 1 for training word embeddings and the embeddings 2 which produced the best results are publicly available. We also release morphological analyzers for Hindi and Bengali3.
2 Skipgram Objective
The two most popular training objectives for monolingual word embeddings are the skipgram and continuous-bag-of-words (CBOW), introduced by Mikolov et al. (2013a). The skipgram
1https://github.com/Aditi138/Embeddings 2https://github.com/Aditi138/Embeddings/ tree/master/embeddings_released 3https://github.com/dmort27/mstem

model attempts to predict the context surrounding a word, given the word itself whereas CBOW predicts the word given its context. Formally, given a corpus having a sequence of words ğ‘¤1, ğ‘¤2, â‹¯ , ğ‘¤ğ‘‡, the skip-gram model maximizes the following loglikelihood:

ğ‘‡

âˆ‘ âˆ‘ log ğ‘(ğ‘£|ğ‘¤ğ‘–)

(1)

ğ‘–=1 ğ‘£âˆˆğ¶ğ‘–

where ğ¶ğ‘– are the context tokens, within a speciï¬ed window of the focus word ğ‘¤ğ‘– and ğ‘(ğ‘£|ğ‘¤ğ‘–) is the probability of observing context word ğ‘£ given fo-
cus word ğ‘¤ğ‘–. The skipgram was originally deï¬ned using the softmax function:

ğ‘’ğ‘ (ğ‘£,ğ‘¤ğ‘–)

ğ‘(ğ‘£|ğ‘¤ğ‘–) = âˆ‘ğ‘Š ğ‘’ğ‘ (ğ‘¤ğ‘–,ğ‘—)

(2)

ğ‘—=1

where ğ‘  is a scoring function mapping ğ‘£ and ğ‘¤ğ‘– to â„. The summation in the denominator is over the entire vocabulary ğ‘Š which makes this formulation computationally ineï¬ƒcient as cost of gradient computation is proportional to ğ‘Š which is quite large (âˆ¼ 106). Mikolov et al. (2013b) hence employ negative sampling to make this computation eï¬ƒcient and robust (Levy et al., 2015) and give better representations for infrequent words4, which is crucial for the low resource settings. Negative sampling represents the above objective function (Equation 1) using a binary logistic loss as shown below:

ğ‘‡

âˆ‘ ( âˆ‘ ğ‘™(ğ‘ (ğ‘¤ğ‘–, ğ‘¤ğ‘)) + âˆ‘ ğ‘™(âˆ’ğ‘ (ğ‘¤ğ‘–, ğ‘¤ğ‘›))) (3)

ğ‘–=1 ğ‘¤ğ‘âˆˆğ¶ğ‘–

ğ‘¤ğ‘› âˆˆğ‘ğ‘–

where ğ‘ğ‘– are the negative words sampled randomly from vocabulary and ğ‘™ is the log-sigmoid
function. The scoring function ğ‘  is a dot product similarity function given by ğ‘ (ğ‘¤ğ‘–, ğ‘¤ğ‘) = uâŠ¤ğ‘¤ğ‘–vğ‘¤ğ‘ where uğ‘¤ğ‘– and vğ‘¤ğ‘ are the embeddings of the focus word and its context word respectively.

3 Subword Representation
Mikolov et al. (2013b)â€™s model fails to capture internal structure of words and does not generalize for out of vocabulary words that may share morphemes with in-vocabulary words. The problems of this method are particularly salient for
4https://code.google.com/archive/p/ word2vec/

graphemes phonemes morphemes lemma+tag
gloss

â€«âŸ¨Ù‚Ø§Ø±Ù‰ÙŠØ§Ù„Ù…Ø§ÙŠØ¯Û‡âŸ©â€¬ /qarijalmajdu/ /qari-jal-ma-jdu/ qari+Verb+Pot+Neg+Pres+A3sg â€˜s/he canâ€™t care forâ€™

phoneme ngrams lemma
morphemes

ğ‘¥<ğ‘ğ‘ + ğ‘¥ğ‘ğ‘ğ‘Ÿ + ... + ğ‘¥ğ‘šğ‘ğ‘—ğ‘‘ğ‘¢> ğ‘¥qari ğ‘¥ğ‘‰ğ‘’ğ‘Ÿğ‘ + ğ‘¥ğ‘ƒğ‘œğ‘¡ + ... + ğ‘¥ğ´3ğ‘ ğ‘”

Figure 2: Vector representations of a word in Uyghur

Figure 1: Representations of a word in Uyghur

morphologically rich languages such as Turkish, Uyghur, Hindi, and Bengali. Although, given a large enough training corpus, most or all morphological forms of a lexeme (of which there may be many) could theoretically learn to have similar vector representations, it will be vastly more data efï¬cient if we can take into account regularities of their form to model morphology explicitly. We explore the following methods for doing so:

Orthographic units: Wieting et al. (2016) and

Bojanowski et al. (2016) show the utility of

character-level modeling by representing the focus

word ğ‘¤ğ‘– as a set of its character ngrams, denoted

by uğ‘¤ğ‘–

=

1 |ğº|

âˆ‘ğ‘”âˆˆğº

xğ‘”,

where ğº is the set of char-

acter ngrams and xğ‘” is the vector representation of

ngram ğ‘”. Such representations capture morpho-

logical information in a brute-force but principled

fashionâ€”words that share the same morpheme are

more likely to share the same character ngrams

than words that do not.

Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and SchÃ¼tze, 2015). Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called â€œtagsâ€) for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags capture information about the syntactic context of a word. See Figure 1 for an example. We take inspiration from the above work in adapting these subword units for cross-lingual transfer.

Phonological units: Subword units other than tags might seem to be of no use in closely-related languages with diï¬€erent scripts (such as Serbian and Croatian). Following Bharadwaj et al. (2016), we convert text from its orthographic form into a

phonemic representation, stated in terms of the International Phonetic Alphabet (IPA). We then train embeddings on this representation. This means that, roughly speaking, morphemes that sound the same will be represented in the same way across languages.

4 Cross-lingual Transfer
In this section we discuss in detail both our approaches for cross-lingual transfer along with the relevant baselines.

4.1 Proposed Approach

We propose to use phoneme ngrams, represented using IPA, in addition to the lemma and morphological tags, to enable eï¬€ective transfer across languages. Tsvetkov and Dyer (2016) demonstrate the eï¬€ectiveness of projecting words from orthographic space to phonemic space as related languages often share similar phonological patterns. More formally, let ğ‘ƒğ‘¤ be the set of linguistic properties of a word consisting of the phoneme ngrams (Iğ‘”) , lemma (L) and individual morphological tags (Mğ‘š). The focus word is then represented as the average sum of its linguistically motivated subword units:

v =1

x

ğ‘¤ğ‘

|ğ‘ƒğ‘¤ğ‘ |

âˆ‘
ğ‘âˆˆğ‘ƒ

ğ‘

ğ‘¤ğ‘

where xğ‘ is the vector representation of subword unit ğ‘ of word ğ‘¤ğ‘. The average operation is important to remove any bias towards words having too many or too few subword units. For instance, the Uyghur word in Figure 1 is represented using its phoneme-ngrams ranging from 3-grams to 6grams, lemma and morphological tags as shown in Figure 2. Avraham and Goldberg (2017) instead encode the diï¬€erent morphological inï¬‚ections as one tag, so that Verb+Pot+Neg+Pres+A3sg would be encoded as ğ‘¥ğ‘‰ğ‘’ğ‘Ÿğ‘+ğ‘ƒğ‘œğ‘¡+ğ‘ğ‘’ğ‘”+ğ‘ƒğ‘Ÿğ‘’ğ‘ +ğ´3ğ‘ ğ‘”. We encode each property in a tag separately to avoid data sparsity issues and empirically ï¬nd this approach to perform better.
We present two training regimes for transferring knowledge from a related language, namely CT-

Joint and CT-FineTune by explicitly incorporating the subword units. We hypothesize that having word representations of both languages lying in a similar space will aid the low resource language in leveraging resources from the high resource language, including annotations for the downstream task. These two regimes are described below:
CT-Joint: This model explicitly maps the word representations of the two languages into the same space by training simultaneously on both. This is achieved simply by combining the corpora of both the high-resource and the low-resource language and training jointly using the skip-gram objective, discussed above. The central intuition is as follows: once two related languages are placed in the same phonological and morphological space, they will share many subword units in common and this will make joint training proï¬table. Duong et al. (2016) and Gouws et al. (2015) have previously shown the advantages of joint training and we observe this to be true in our case as well.
CT-FineTune: This model implicitly maps the word representations of the two languages into the same space. The model attempts this by taking the learned continuous representations of the high resource subword units, referred to by xğ»ğ‘†ğ‘Šğ‘– ğ‘ˆ, and uses them to initialize the model for the low resource language. The model is ï¬rst trained using all subword units on the high resource language and the learned representations are then used for initializing the subword units for the low resource language. To elucidate which pretrained subword helped the most on the low resource language, we use the same model for diï¬€erent experiments, which is trained using all subword unitsâ€”phoneme-ngrams, lemma and morphological properties. The linguistic intuition behind CTFineTune is similar to that behind CT-Joint. This idea of transferring parameters from high resource language has been previously explored by Zoph et al. (2016) for low resource neural machine translation which showed considerable improvement.
5 Evaluation
In this section, we ï¬rst describe the model setup for training word embeddings followed by details on NER and MT experiments.

5.1 Implementation details
We base our model on the C++ implementation of fasttext5(Bojanowski et al., 2016) with modiï¬cations as described above.
Data: We represent a word in the training corpus using the format presented by Avraham and Goldberg (2017). For instance, the Uyghur word in Figure 1 is represented as follows: phoneme ipa: qarijalmajdu, lemma l:qari, and morphological inï¬‚ections m:Verb+Pot+Neg+Pres+A3sg. We consider phoneme-ngrams ranging from 3-grams to 6grams and append a special start symbol < and end symbol > to the word. We discard unigrams and bigram ngrams on the assumption that they donâ€™t contribute much to the word.
Linguistic properties: We experiment with different subword units for both the transfer setting and the monolingual setting. We use the orthography-to-IPA tool Epitran (Mortensen et al., 2018) to obtain the phonemic representations. The lemmas and morphological properties for a word in context are obtained using a rule-based morphological analyzer in such a fashion as to produce tags similar to the high resource language. For Turkish we use the morphological disambiguator developed by (Shen et al., 2016), which in turn is based on an FST-based morphological analyzer developed by Oï¬‚azer (1994). For Uyghur, we took a (parser combinator based) morphological analyzer that had been developed for a DARPA LORELEI evaluation and modiï¬ed it to output part-of-speech tags and to use a property set that was as close as possible to that of the Oï¬‚azer Turkish analyzer. The analyzer for Turkish produces 116 inï¬‚ectional properties and for Uyghur we get 54 properties, of which 64% are shared with Turkish. Unfortunately, we did not have access to existing morphological analyzers for Hindi or Bengali. Many Hindi morphological analyzers exist, but they are not typically released publicly (Malladi and Mannem, 2013; Goyal and Lehal, 2008). We developed our own analyzers using a stemmer-like framework6 over a span of few weeks (2-3), which gave 8 unique morphological tags for Hindi and 10 for Bengali (for both languages, noun inï¬‚ection only) of which just 2 were shared with Hindi.
Morphologically speaking, we only use inï¬‚ec-
5https://github.com/facebookresearch/ fastText/
6https://github.com/dmort27/mstem

tional properties. For most languages, we considered derivational aï¬ƒxes to be part of the stem, since they change the meaning and grammatical category of the word rather than simply expressing syntactic information. An exception to this was Turkish, where the available morphological analyzer segments all aï¬ƒxes oï¬€ from the root. However, even there we conï¬ned our use of morphological properties to inï¬‚ectional properties. Derivational aï¬ƒxes display scopal behavior; since we wanted to treat the morphological properties of a word as a set, rather than a sequence, we were required to choose this option.
Hyperparameters: During training, we consider context tokens within a window size 3 of the focus word and we sample 5 negative examples from the vocabulary. We chose a window size of only 3 based on the fact that we are working with morphologically rich languages with a relatively high information to token ratio (otherwise a window size of 5 may be more appropriate). Subword units are initialized with uniform samples from [ ğ‘‘âˆ’ğ‘–ğ‘š1 , ğ‘‘1ğ‘–ğ‘š ] where ğ‘‘ğ‘–ğ‘š = 100. We use the same training regime as Bojanowski et al. (2016). For CT-FineTune, instead of uniform samples we initialize the subword units of the low resource language from the learnt xğ»ğ‘†ğ‘Šğ‘– ğ‘ˆ.

Lang. Train Dev
Turkish 3376 1126 Uyghur 1822 240 Hindi 3974 497 Bengali 1908 53

Test
1126 2448* 497 7012

Table 1: Sentences in train/dev/test set for NER. (*Unsequestered set. The full test set has 12,546 sentences.)

5.2 Baselines
For comparison, we train multilingual embeddings using MultiCCA (Ammar et al., 2016) as our baseline. It employs canonical correlation analysis by projecting multiple languages in the same shared space of one language, also referred to as multilanguage space. This method learns linear projections for each language into this common language space using bilingual lexicons. English is used as a common vector space due to availability of corresponding bilingual lexicons between English and each of our languages. For a fair comparison, we run MultiCCA on monolingual embed-

dings trained with diï¬€erent subword units. We use 100 dimension (Bojanowski et al., 2016) embeddings for English.
For NER, we also compare with Bharadwaj et al. (2016) who use a neural attention model over phonological features and report the best performance for Turkish using transfer from Uzbek and Uyghur, and Mayhew et al. (2017) who use a cheap translation method to translate training data from high-resource language into the lowresource language and report best NER results for Uyghur, as part of the LORELEI program. Our work diï¬€ers from these primarily on two fronts: a) it is independent of the downstream task and can easily be adapted across various tasks, and b) it doesnâ€™t require parallel corpora or bilingual dictionaries. For our monolingual experiments, we compare our proposed approach with models using subword representationsâ€”Bojanowski et al. (2016) and Avraham and Goldberg (2017).
5.3 Named Entity Recognition Task
We use state-of-the-art NER architecture (Ma and Hovy, 2016) as our model for evaluation. The task is to identify NEs and categorize them into four types. Since this is a supervised model, the performance is highly contingent on the quality of labeled data. F1 scores are used as the evaluation metric.
5.3.1 Experiments
We conduct the two main sets of NER experiments,
1. Transfer experiments on the low resource languagesâ€”Uyghur and Bengaliâ€”using Turkish and Hindi as the high resource languages respectively. We show results using both our proposed models, CT-Joint and CT-FineTune.
2. Monolingual experiments on all four languages: Uyghur, Turkish, Bengali and Hindi. We do an ablation study using diï¬€erent combinations of subword units.
These language pairs were chosen partly out of convenienceâ€”the data were available to us as part of the DARPA LORELEI programâ€”and partly because they satisï¬ed certain deeper desiderata. Turkish and Uyghur are fairly closely related to one another, as are Hindi and Bengali. Despite this relationship, the members of both pairs are written

Model CT-Joint
CT-FineTune
MultiCCA (Baseline)

subword units
phoneme-ngrams + lemma + morph phoneme-ngrams + lemma phoneme-ngrams phoneme char-ngrams + lemma + morph char-ngrams + lemma char-ngrams word
phoneme-ngrams + lemma + morph lemma + morph phoneme-ngrams + lemma phoneme-ngrams phoneme
char-ngrams + lemma + morph char-ngrams + lemma char-ngrams word

Uyghur
55.00 56.20 54.90 51.30 50.20 48.20 49.60 51.80
48.60 52.80 51.00 50.50 49.20
41.00 43.10 45.80 42.70

Bengali
60.33 59.63 58.50 53.75 55.10 53.83 52.77 53.69
56.19 57.72 56.83 57.69 59.86
50.63 50.63 38.06 45.86

Table 2: Transfer experiments on NER. Metric F1 (out of 100%). Uyghur transfer is from Turkish; Bengali transfer is from Hindi

in diï¬€erent scripts (Roman and Perso-Arabic; Devanagari and Bengali). Finally, all four languages are morphologically rich, especially Turkish and Uyghur. These qualities allow us to showcase the value of embeddings with subword units.
Data Preprocessing: We use data, comprised of unlabeled corpora, English bilingual dictionaries, annotations, from the Linguistic Data Consortium (LDC) language packsâ€”Turkish and Hindi 7, Bengali8, from which we generate train-devtest splits. Uyghur data was released as part of LoReHLT16 task, organized by NIST 9 under the aegis of DARPA, and training annotations were acquired using native speakers as part of the task. For Uyghur we evaluate on an unsequestered set consisting of 199 annotated evaluation documents, released by NIST. For Turkish, Hindi and Bengali, we create our own train-dev-test splits (Table 1). The exact documents from which Mayhew et al. (2017) and Bharadwaj et al. (2016) created their test set is not apparent. The Uyghur corpus has 27 million tokens and the Turkish corpus has about 40 million tokens. Although Bengali is widely-spoken and the unlabeled corpus
7LDC2014E115,LDC2017E62,http://www.cfilt. iitb.ac.in/iitb_parallel/
8LDC2017E60, LDC2015E13 9https://www.nist.gov/

contains more than 140 million tokens, there are very few named entity annotations available, making it a low-resource language for the purposes of this exercise. To have a fair experimental setup across language pairs, we sub-sample the Bengali and Hindi corpora to have comparable corpus sizes with Uyghur and Turkish respectively. We also up-sample the low resource data for both unlabeled corpora and NER annotations, so the model doesnâ€™t become biased towards the high resource language.
NER model setup: We train the model using 100-dimensional word embeddings, pre-trained using the above discussed strategies, and use hidden dimension of size 100 for each direction of the LSTM. Stochastic gradient descent was used as the optimizer with a learning rate of 0.015. Dropout of 0.5 was used in the LSTM layer to prevent over-ï¬tting. Uyghur and Turkish were trained for 100 epochs, Bengali and Hindi converged after 70 epochs.
5.3.2 Results and Discussion
Transfer Experiments: From Table 2 we note that our CT-Joint model trained with phonemengrams, lemma, and morphological tags outperforms the MultiCCA baseline by a signiï¬cant margin. MultiCCA strongly depends on

Model Ours
prop2vec
fastText word2vec Random

subword units
Char-ngrams + Lemma + Morph Char-ngrams + Lemma Char-ngrams + Morph
Word + Lemma Word + Morph Word + Lemma + Morph
Char-ngrams
Word
No embedding

Turkish
68.06 68.61 67.97
66.52 64.45 68.46
66.81
62.85
58.94

Uyghur
52.50 52.40 47.80
46.00 46.00 47.70
50.80
46.80
31.30

Hindi
73.15 73.37 73.46
71.82 71.52 70.51
72.67
72.04
59.89

Table 3: NER results for monolingual experiments. Metric F1 (out of 100%)

Bengali
52.77 52.09 52.06
50.03 49.27 48.16
52.10
49.83
21.25

Model
Ours Bharadwaj et al. (2016) Mayhew et al. (2017)

Uyghur* (unseq.)
56.20 â€“ 51.32

Uyghur*
56.00 51.2 55.6

Turkish
68.61 66.47 53.44

Bengali
60.33 â€“ 45.70

Table 4: Comparison with previous work using data released by DARPA LORELEI. Metric F1 (out of 100%) *Oï¬ƒcial NIST scores.

Figure 3: Recall for all languages (monolingual) W:Word, NG:Char-ngrams, L:Lemma, M:Morph
bilingual dictionaries which is possibly why it performs poorly in our low resource setting, where these dictionaries are not of high quality. The advantage of phoneme-ngrams over char-ngrams is quite apparent here. phonemengrams+lemma+morph performs +5.2 F1 points better than char-ngram+lemma+morph for both Uyghur and Bengali, and similar increase is observed across other combinations, the only exception being the phoneme case for Uyghur which performed -0.5 F1 with respect to its counterpart word.

We ï¬nd CT-Joint to be consistently better performing than CT-FineTune. Interestingly, the performance of CT-FineTune model converges to the monolingual performance. We hypothesize that the model forgets the pre-trained subword units as training progresses.
For CT-FineTune, the column subword units in Table 2 refers to the subword units which were pretrained on a high resource related language. For example lemma + morph means lemma and morph embeddings are ï¬rst pre-trained on the resoucerich language and then used to initialize the respective lemma and morph representations for the low resource language.
Monolingual Experiments: Table 3 shows our results on all languages. We get +5.8 F1 points for Turkish, +4.8 F1 for Uyghur, +0.8 F1 for Hindi and +0.7 F1 for Bengali over the existing methods. We observe that a combination of character-ngrams, lemma and morphological properties gives the best performance for Uyghur and Bengali. Adding morph hurts in Turkish, in contrast to Hindi, where it helps. Section 5.3.3 discuses plausible reasons for this.
We report oï¬ƒcial NIST scores on the full evaluation set for Uyghur, as part of LORELEI Oï¬ƒcal Retest. Additionally, we compare our results with

Model CT-Joint
CT-FineTune

subword units
phoneme-ngrams + lemma + morph phoneme-ngrams + lemma phoneme-ngrams
lemma + morph

Uyghur
23.04 23.24 23.25
23.71

Bengali
7.88 7.62 7.45
7.58

Table 5: Transfer experiments for MT. Metric: BLEU. Uyghur transfer is from Turkish; Bengali transfer is from Hindi

Model Ours
fastText word2vec Random

subword units
Char-ngrams + Lemma + Morph Char-ngrams + Lemma Char-ngrams + Morph
Char-ngrams
Word
No embedding

Uyghur
23.59 23.91 23.27
23.24
23.31
23.51

Bengali 7.96 7.77 7.88
7.91
6.64
6.23

Table 6: MT results for monolingual experiments. Metric: BLEU

the best results reported on the same LORELEI dataset. Results are seen in Table 4.
5.3.3 Error analysis
We plot recall curves for all languages. As seen in Figure 3, adding subword units boosts the recall consistently across all languages, more so for Uyghur. For Turkish, lemma performs better than lemma+morph, perhaps because the morphological analyzer outputs so many redundant properties which reduce the distance between words that are not particularly similar. In contrast, morph helps and lemma hurts in Hindi, perhaps because the morph analyzer outputs only a small number of highly informative properties, but is a poor general-purpose lemmatizer.
We analyze our results for Uyghur language, as it was part of the LORELEI challenge and presents a situation close to a real-life application. We base our analysis on the unsequestered set since annotations for full test data are not released. There are 1,341 NEâ€™s in this set, 396 of which are covered by the word embeddings when trained with just monolingual corpus. One obvious advantage of jointly training with a resource-rich corpus is that coverage of NEs increases, as validated in our case where jointly training with Turkish corpus adds 114 more NEs.
Figure 4 shows ten named entities in two diï¬€erent embeddings (CT-Joint: phoneme-

150

birazilijÉ›dÉ› GPE

afÊanistanGPE

Noun+Pnon+Nom

Noun+Pass+Pnon+Nom

100 50 0 -50 -100

elidÊ’Í¡ an PER

Noun+Pass+Pnon+Nom

dawalaÊƒ ORG

birazilijÉ›dÉ› GPE

Noun+Pnon+Nom Noun+Pnon+Nom

afÊanistan GPE

adilÍ¡ Ê’an PER Noun+Pass+Pnon+Nom

gowujunniÅ‹ GPE Noun+P2sg+Nom

adilÍ¡ Ê’an PER Noun+Pass+Pnon+Nom

Noun+Pass+Pnon+Nom

jawropaniÅ‹ GPE

amerikida GPE

Noun+P2sg+Nom

Noun+Pnon+Nom

amerikida GPE

Noun+Pnon+Nom

afriqidin LOC

afriqidin LOC

elidÊ’Í¡ an PER Noun+P3pl+Abl Noun+P3pl+Abl

dawalaÊƒ ORG Noun+Pass+Pnon+Nom Noun+Pnon+Nom

gowujunniÅ‹ GPE Noun+P2sg+Nom

tygmÉ›nbeÊƒi GPE

tygmÉ›nbeÊƒi GPE

Noun+P3sg+Nom

Noun+P3sg+Nom jawropaniÅ‹ GPE

Noun+P2sg+Nom

-100

-50

0

50

100

150

Figure 4: Two-dimensional PCA projection of select NEs from word embeddings for Uyghurâ€”CT-Joint model trained with phoneme-ngrams+lemma+morph (blue) and monolingual model trained with charngrams+lemma+morph
ngrams+lemma+morph and monolingual: char-ngram+lemma+morph). The diï¬€erence is strikingâ€”in the monolingual condition, the NEs are widely dispersed, but in the bilingual condition, the NEs cluster together. This suggests that phonologically-mediated transfer through Turkish is resulting in embeddings in which NEs are close to one another, relative to monolingual Uyghur embeddings.
5.4 Machine Translation Task
In addition to NER, we test the performance of our proposed approaches on the MT task to test generality of our conclusions. We use XNMT toolkit

(Neubig et al., 2018) to translate sentences from the low-resource language to English. We run similar transfer and monolingual experiments as done for NER. Due to space limitations, we use select subword combinations for the experiments, details of which can be found in Appendix. BLEU is used as the evaluation metric.
From Table 6, we observe that the combination of character-ngrams and lemma performs the best for Uyghur (+0.1) and the combination of character-ngrams, lemma and morph gives the best performance for Bengali (+1.7), over the word baseline, which demonstrates the importance of subword units for low-resource MT as well. One likely reason that the combination of characterngrams and lemmas consistently show the best performance is that, together, they capture lexical similarity, which is more important to translation than the syntactic information captured by morphological inï¬‚ection (â€œmorphâ€). However, experiments using CT-Joint and CT-FineTune (Table 5) do not follow the same trend as that of NER. We hypothesize that this is because the MT models were trained on a training set that did not have translation pairs from the high resource language. As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages.
6 Related Work
Word Embedding Models: Most algorithms for learning embeddings take inspiration from language modeling (Bengio et al., 2003), motivated by distributional hypothesis (Harris, 1954), and employ a shallow neural network to map the words into a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiï¬cant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings.
Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural

network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemesâ€™ meanings as part of the word representation to implicitly model the morphological knowledge.
Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciï¬c and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources.
7 Conclusion
In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations. CT-Joint and CTFineTune do not require morphological analyzers, but we have found that even a morphological analyzer built in 2-3 weeks can boost performance and is a worthwhile investment of resources. Preliminary evaluation on a separate task of MT reconï¬rms the utility of subword units and further research will reveal what these learned subword representations can contribute to other tasks.
Acknowledgement
This work is sponsored by Defense Advanced Research Projects Agency Information Innovation Oï¬ƒce (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the oï¬ƒcial policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.

References
Oliver Adams, Adam Makarucha, Graham Neubig, Steven Bird, and Trevor Cohn. 2017. Cross-lingual word embeddings for low-resource language modeling. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, volume 1, pages 937â€“947.
Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A Smith. 2016. Massively multilingual word embeddings. arXiv preprint arXiv:1602.01925.
Oded Avraham and Yoav Goldberg. 2017. The interplay of semantics and morphology in word embeddings. CoRR, abs/1704.01938.
Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137â€“1155.
Akash Bharadwaj, David Mortensen, Chris Dyer, and Jaime Carbonell. 2016. Phonologically aware neural model for named entity recognition in low resource transfer settings. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1462â€“1472.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606.
Jan Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In International Conference on Machine Learning, pages 1899â€“1907.
Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huan-Bo Luan. 2015. Joint learning of character and word embeddings. In IJCAI, pages 1236â€“1242. AAAI Press.
Ryan Cotterell and Hinrich SchÃ¼tze. 2015. Morphological word-embeddings. In Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1287â€“1292, Denver, Colorado. Association for Computational Linguistics, Association for Computational Linguistics.
Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, and Trevor Cohn. 2016. Learning crosslingual word embeddings without bilingual corpora. arXiv preprint arXiv:1606.09403.
Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual distributed representations without word alignments. In International Conference on Machine Learning, pages 748â€“756.
Vishal Goyal and Gupreet Singh Lehal. 2008. Hindi morphological analyzer and generator. In 2008 First

International Conference on Emerging Trends in Engineering and Technology, pages 1156â€“1159.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. Proceedings of ACL-08: HLT, pages 771â€“779.
Harald HammarstrÃ¶m, Sebastian Bank, Robert Forkel, and Martin Haspelmath. 2018. Glottolog 2.0. Max Planck Institute for the Science of Human History, Jena. Available online at http://glottolog.org, Accessed on 2018-05-21.
Zellig S Harris. 1954. Distributional structure. Word, 10(2-3):146â€“162.
Huiming Jin and Katharina Kann. 2017. Exploring cross-lingual transfer of morphological knowledge in sequence-to-sequence models. In Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 70â€“75.
Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and Eric Fosler-Lussier. 2017. Cross-lingual transfer learning for pos tagging without cross-lingual resources. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832â€“2838.
Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pages 2177â€“2185.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211â€“225.
Wang Ling, Chris Dyer, Alan W Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, Luis Marujo, and Tiago Luis. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1520â€“1530. Association for Computational Linguistics.
Thang Luong, Richard Socher, and Christopher Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104â€“113.
Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNsCRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064â€“1074. Association for Computational Linguistics.
Deepak Kumar Malladi and Prashanth Mannem. 2013. Statistical morphological analyzer for hindi. In International Joint Conference on Natural Language Processing, pages 1007â€“â€“1011, Nagoya, Japan.

Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017. Cheap translation for cross-lingual named entity recognition. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2536â€“2545.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Eï¬ƒcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeï¬€ Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111â€“3119.
David R. Mortensen, Siddharth Dalmia, and Patrick Littell. 2018. Epitran: Precision G2P for many languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris, France. European Language Resources Association (ELRA).
David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classiï¬cation. Linguisticae Investigationes. Publisher: John Benjamins Publishing Company.
Graham Neubig, Matthias Sperber, Xinyi Wang, Matthieu Felix, Austin Matthews, Sarguna Padmanabhan, Ye Qi, Devendra Singh Sachan, Philip Arthur, Pierre Godard, John Hewitt, Rachid Riad, and Liming Wang. 2018. XNMT: The extensible neural machine translation toolkit. In Conference of the Association for Machine Translation in the Americas (AMTA) Open Source Software Showcase, Boston.
Kemal Oï¬‚azer. 1994. Two-level description of Turkish morphology. Literary and Linguistic Computing, 9(2):137â€“148.
Jeï¬€rey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532â€“1543.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.
Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are pre-trained word embeddings useful for neural machine translation? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 529â€“535. Association for Computational Linguistics.
Sebastian Ruder. 2017. A survey of cross-lingual embedding models. arXiv preprint arXiv:1706.04902.

Qinlan Shen, Daniel Clothiaux, Emily Tagtow, Patrick Littell, and Chris Dyer. 2016. The role of context in neural morphological disambiguation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee.
Chuanqi Tan, Furu Wei, Nan Yang, Weifeng Lv, and Ming Zhou. 2017. S-net: From answer extraction to answer generation for machine reading comprehension. arXiv preprint arXiv:1706.04815.
Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting Liu, and Ming Zhou. 2016. Sentiment embeddings with applications to sentiment analysis. IEEE Trans. on Knowl. and Data Eng., 28(2):496â€“509.
Chen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016. Cross-lingual named entity recognition via wikiï¬cation. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 219â€“228.
Yulia Tsvetkov and Chris Dyer. 2016. Cross-lingual bridges with models of lexical borrowing. J. Artif. Int. Res., 55(1):63â€“93.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. arXiv preprint arXiv:1607.02789.
Jian Xu, Jiawei Liu, Liangang Zhang, Zhengyu Li, and Huanhuan Chen. 2016. Improve Chinese word embeddings by exploiting internal structure. In HLTNAACL.
Yang Xu and Jiawei Liu. 2017. Implicitly incorporating morphological information into word embedding. arXiv preprint arXiv:1701.02481.
Liang-Chih Yu, Jin Wang, K. Robert Lai, and Xuejie Zhang. 2018. Reï¬ning word embeddings using intensity scores for sentiment analysis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(3):671â€“681.
Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for lowresource neural machine translation. arXiv preprint arXiv:1604.02201.

Appendix
MT Experimental Setup
We conduct the two sets of experiments, similar to the NER experiments:
1. Experiments where word embeddings are transferred from Uyghur and Bengali, using Turkish and Hindi as the respective high resource languages. Both the proposed models, CT-Joint and CT-FineTune, are used for experimentation with select subword units.
2. Monolingual experiments on the two low resource languages: Uyghur and Bengali, with select subword combinations.
Data Processing
We use data, comprised of unlabeled corpora, training translation pairs (if provided), from the same sources as used for the NER experiments. The training data comprises of translation pairs between the source language and the target language, English. We create our own train-dev-test splits for the experiments as there are no oï¬ƒcial splits provided. The data splits can be seen in Table 7. The Uyghur corpus has 31 million tokens (extracted from a diï¬€erent set than the one used for NER) and the Turkish corpus about 40 million tokens. The Bengali corpus has 125 million tokens and we downsampled the original Hindi corpus to a comparable size.

Lang. Train Dev
Uyghur 99379 994 Bengali 101523 1989

Test
994 1988

Table 7: Sentences in train/dev/test set for MT.

Model Setup
We train the model using 512-dimensional word embeddings, pre-trained using strategies described in the main text. We run each MT system 3 times and report the median score, as a way to control for variance in training.

