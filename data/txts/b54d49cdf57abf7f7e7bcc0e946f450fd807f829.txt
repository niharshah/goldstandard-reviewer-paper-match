arXiv:1909.05477v2 [cs.LG] 30 Apr 2020

Published as a conference paper at ICLR 2020
MAXIMUM LIKELIHOOD CONSTRAINT INFERENCE FOR INVERSE REINFORCEMENT LEARNING
Dexter R.R. Scobee & S. Shankar Sastry Department of Electrical Engineering and Computer Sciences University of California, Berkeley {dscobee, sastry}@eecs.berkeley.edu
ABSTRACT
While most approaches to the problem of Inverse Reinforcement Learning (IRL) focus on estimating a reward function that best explains an expert agent’s policy or demonstrated behavior on a control task, it is often the case that such behavior is more succinctly represented by a simple reward combined with a set of hard constraints. In this setting, the agent is attempting to maximize cumulative rewards subject to these given constraints on their behavior. We reformulate the problem of IRL on Markov Decision Processes (MDPs) such that, given a nominal model of the environment and a nominal reward function, we seek to estimate state, action, and feature constraints in the environment that motivate an agent’s behavior. Our approach is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent’s demonstrations given our knowledge of an MDP. Using our method, we can infer which constraints can be added to the MDP to most increase the likelihood of observing these demonstrations. We present an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and we evaluate its efﬁcacy using both simulated behavior and recorded data of humans navigating around an obstacle.
1 INTRODUCTION
Advances in mechanical design and artiﬁcial intelligence continue to expand the horizons of robotic applications. In these new domains, it can be difﬁcult to design a speciﬁc robot behavior by hand. Even manually specifying a task for a reinforcement-learning-enabled agent is notoriously difﬁcult (Ho et al., 2015; Amodei et al., 2016). Inverse Reinforcement Learning (IRL) techniques can help alleviate this burden by automatically identifying the objectives driving certain behavior. Since ﬁrst being introduced as Inverse Optimal Control by Kalman (1964), much of the work on IRL has focused on learning environmental rewards to represent the task of interest (Ng & Russell, 2000; Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008). While these types of IRL algorithms have proven useful in a variety of situations (Abbeel et al., 2007; Vasquez et al., 2014; Ziebart, 2010; Scobee et al., 2018), their basis in assuming that reward functions fully represent task speciﬁcations makes them ill suited to problem domains with hard constraints or non-Markovian objectives.
Recent work has attempted to address these pitfalls by using demonstrations to learn a rich class of possible speciﬁcations that can represent a task (Vazquez-Chanlatte et al., 2018). Others have focused speciﬁcally on learning constraints, that is, behaviors that are expressly forbidden or infeasible (Pardowitz et al., 2005; Pe´rez-D’Arpino & Shah, 2017; Subramani et al., 2018; McPherson et al., 2018; Chou et al., 2018). Such constraints arise in safety-critical systems, where requirements such as an autonomous vehicle avoiding collisions with pedestrians are more naturally expressed as hard constraints than as soft reward penalties. It is towards the problem of inferring such constraints that we turn our attention.
In this work, we present a novel method for inferring constraints, drawing primarily from the Maximum Entropy approach to IRL described by Ziebart et al. (2008). We use this framework to reason about the likelihood of observing a set of demonstrations given a nominal task description, as well as about their likelihood if we imposed additional constraints on the task. This knowledge allows us to select a constraint, or set of constraints, which maximizes the demonstrations’ likelihood and
1

Published as a conference paper at ICLR 2020
best explains the differences between expected and demonstrated behavior. Our method improves on prior work by being able to simultaneously consider constraints on states, actions and features in a Markov Decision Process (MDP) to provide a principled ranking of all options according to their effect on demonstration likelihood.
2 RELATED WORK
2.1 INVERSE REINFORCEMENT LEARNING
A formulation of the IRL problem was ﬁrst proposed by Kalman (1964) as the Inverse problem of Optimal Control (IOC). Given a dynamical system and a control law, the author sought to identify which function(s) the control law was designed to optimize. This problem was brought into the domain of MDPs and Reinforcement Learning (RL) by Ng & Russell (2000), who proposed IRL as the task of, given an MDP and a policy (or trajectories sampled according to that policy), ﬁnd a reward function with respect to which that policy is optimal.
One of the chief difﬁculties in the problem of IRL is the fact that a policy can be optimal with respect to a potentially inﬁnite set of reward functions. The most trivial example of this is the fact that all policies are optimal with respect to a null reward function that always returns zero. Much of the work in IRL has been devoted to developing approaches that address this ambiguity by imposing additional structure to make the problem well-posed (Abbeel & Ng, 2004; Ratliff et al., 2006). Ziebart et al. (2008) approach the problem by employing the principle of maximum entropy (Jaynes, 1957), which allows the authors to develop an IRL algorithm that produces a single stochastic policy that matches feature counts without adding any additional constraints to the produced behavior. This so called Maximum Entropy IRL (MaxEnt) provides a framework for reasoning about demonstrations from experts who are noisily optimal. The induced probability distribution over trajectories forms the basis for our efforts in identifying the most likely behavior-modifying constraints.
2.2 BEYOND REWARD FUNCTIONS
While Markovian rewards do often provide a succinct and expressive way to specify the objectives of a task, they cannot capture all possible task speciﬁcations. Vazquez-Chanlatte et al. (2018) highlight the utility of non-Markovian Boolean speciﬁcations which can describe complex objectives (e.g. do this before that) and compose in an intuitive way (e.g. avoid obstacles and reach the goal). The authors of that work draw inspiration from the MaxEnt framework to develop their technique for using demonstrations to calculate the posterior probability that an agent is attempting to satisfy a Boolean speciﬁcation.
A subset of these types of speciﬁcations that is of particular interest to us is the speciﬁcation of constraints, which are states, actions, or features of the environment that must be avoided. Chou et al. (2018) explore how to infer trajectory feature constraints given a nominal model of the environment (lacking the full set of constraints) and a set of demonstrated trajectories. The core of their approach is to sample from the set of trajectories which have better performance than the demonstrated trajectories. They then infer that the set of possible constraints is the subset of the feature space that contains the higher-reward sampled trajectories, but not the demonstrated trajectories. Intuitively, they reason that if the demonstrator could have passed through those features to earn a higher reward, but did not, then there must have been a previously unknown constraint preventing that behavior. However, while their approach does allow for a cost function to rank elements from the set of possible constraints, the authors do not offer a mechanism for determining what cost function will best order these constraints.
Our approach to constraint inference from demonstrations addresses this open question by providing a principled ranking of the likelihood of constraints. We adapt the MaxEnt framework to allow us to reason about how adding a constraint will affect the likelihood of demonstrated behaviors, and we can then select the constraints which maximize this likelihood. We consider feature-space constraints as in Chou et al. (2018), and we explicitly augment the feature space with state- and action-speciﬁc features to directly compare the impacts of state-, action-, and feature-based constraints on demonstration likelihood.
2

Published as a conference paper at ICLR 2020

3 MAXIMUM LIKELIHOOD CONSTRAINT INFERENCE

3.1 PROBLEM FORMULATION

Following the formulation presented by Ziebart et al. (2008), we base our work in the set-
ting of a (ﬁnite-state) Markov Decision Process (MDP). We deﬁne an MDP M as a tuple
(S, {As}, {Ps,a} , D0, φ, R) where S is a ﬁnite set of discrete states; {As} is a set of the sets of actions available to be taken for each state s, such that As ⊆ A, where A is a ﬁnite set of discrete actions; {Ps,a} is a set of state transition probability distributions such that Ps,a(s ) = P (s |s, a) is the probability of transitioning to state s after taking action a from state s; D0 : S → [0, 1] is an initial state distribution; φ : S × A → Rk+ is a mapping to a k-dimensional space of non-negative features; and R : S × A → R is a reward function. A trajectory ξ through this MDP is a sequence of states st and actions at such that s0 ∼ D0 and state si+1 ∼ Psi,ai . Actions are chosen by an agent navigating the MDP according to a, potentially time-varying, policy π such that π(·|s, t) is a
probability distribution over actions in As. We denote a ﬁnite-time trajectory of length T + 1 by ξ = {s0:T , a0:T }.

At every time step t, a trajectory will accumulate features equal to φ(st, at). We use the notation φi(·, ·) to refer to the i-th element of the feature map, and we use the label φi to denote the i-th feature itself. We also introduce an augmented indicator feature mapping φ1 : S × A → {0, 1}nφ , where nφ = k + |S| + |A|. This augmented feature map uses binary variables to indicate the presence of a feature and expands the feature space by adding binary features to track occurrences of each state
and action, such that

φ1 (s, a) = 1 if φi(s, a) > 0 , φ1 (s, a) = 1 if s = si , φ1 (s, a) = 1 if a = ai .

φi

0 otherwise

si

0 otherwise ai

0 otherwise

(1)

Typically, agents are modeled as trying to maximize, or approximately maximize, the total reward

earned for a trajectory ξ, given by R(ξ) =

T t=0

γtR(st,

at),

where

γ

∈

(0,

1]

is

a

discount

factor.

Therefore, an agent’s policy π is closely tied to the form of the MDP’s reward function.

Conventional IRL focuses on inferring a reward function that explains an agent’s policy, revealed through the behavior observed in a set of demonstrated trajectories D. However, our method for constraint inference poses a different challenge: given an MDP M, including a reward function, and a set of demonstrations D, ﬁnd the most likely set of constraints C∗ that could modify M to explain
these demonstrations. We deﬁne our notion of constraints in the following section.

3.2 CONSTRAINTS FOR MDPS
Constraints are those behaviors that are not disallowed explicitly by the structure of the MDP, but which would be infeasible or prohibited for the underlying system being modeled by the MDP. This sort of discrepancy can occur when a generic or simpliﬁed MDP is designed without exact knowledge of speciﬁc constraints for the modeled system. For instance, for a generic MDP modeling the behavior of cars, we might want to include states for speeds up to 500km/h and actions for accelerations up to 12m/s2. However, for a speciﬁc car on a speciﬁc roadway, the set of states where the vehicle travels above 100km/h may be prohibited because of a speed limit, and the set of actions where the vehicle accelerates above 4m/s2 may be infeasible because of the physical limitations of the vehicle’s engine. Therefore, any MDP trajectory of this speciﬁc car system would not contain a state-action pair which violates these legal and physical limits. Figure 1 shows an example of constraints driving behavior.
We deﬁne a constraint set Ci ⊆ S × A as a set of state-action pairs that violate some speciﬁcation of the modeled system. We consider three general classes of constraints: state constraints, action constraints, and feature constraints. A state constraint set Csi = {(s, a) | s = si} includes all stateaction pairs such that the state component is si. An action constraint set Cai = {(s, a) | a = ai} includes all state-action pairs such that the action component is ai. A feature constraint set Cφi = {(s, a) | φi(s, a) > 0} includes all state-action pairs that produce a non-zero value for feature φi.
If we augment the set of features as described in (1), it is straightforward to see that state and action constraints become special cases of feature constraints, with constraint sets given by

3

Published as a conference paper at ICLR 2020

Ci

=

{(s,

a)

|

φ1(s,
i

a)

=

1}.

It

is

also

evident

that

we

can

obtain

compound

constraints,

respecting

two or more conditions, by taking the union of constraint sets Ci to obtain C = i Ci.

3.2.1 ADDING CONSTRAINTS TO AN MDP

We need to be able to reason about how adding a constraint to an MDP will inﬂuence the behavior of agents navigating that environment. If we impose a constraint on an MDP, then none of the state-action pairs in that constraint set may appear in a trajectory of the constrained MDP. To enforce this condition, we must restrict the actions available in each state so that it is not possible for an agent to produce one of the constrained state-action pairs. For a given constraint C, we can replace the set of available actions As in every state s with an alternative set ACs given by

ACs = As \ {a ∈ As | (s, a) ∈ C} .

(2) (a) Nominal MDP

Performing such substitutions for an MDP M will lead to a modiﬁed

MDP MC such that MC

=

(S

,

{A

C s

}

,

{

P

s,a

},

D0

,

φ,

R

).

The question then arises as to the how we should treat states with empty action sets ACs = ∅. Since an agent arriving in such an empty state would have no valid action to select, any trajectory visiting an
empty state must be deemed invalid. Indeed, such empty action sets
will be produced for any state si such that Csi ⊆ C.

For MDPs with deterministic transitions, agents know precisely
which state they will arrive in following a certain action. Therefore,
any agent respecting constraints will not take an action that leads to
an empty state, since doing so will lead to constraint violations. If we consider the set of empty states Sempty, then for the purposes of reasoning about an agent’s behavior, we can impose an additional constraint set Cempty = {(s, a) | ∃ sempty ∈ Sempty : Ps,a(sempty) = 1}. In this work, we will always implicitly add this constraint set, such that MC will be equivalent to M , C∪Cempty and we recursively add
these constraints until reaching a ﬁxed point.

For MDPs with stochastic transitions, the semantics of an empty state are less obvious and could lend themselves to multiple interpretations depending on the nature of the system being modeled. We offer a possible treatment in the appendix.

3.2.2 NOMINAL MDPS

(b) Constrained MDP
Figure 1: Illustration of trajectories likely to be produced by noisily optimal agents navigating an MDP. (a) Expected behavior on a generic, nominal MDP. (b) Demonstrated behavior from a speciﬁc, constrained MDP. Numbers represent state-based rewards, and the red-shaded tiles represent state constraints.

The nominal MDPs to which we will add constraints fall into two broad categories, which we denote as generic and baseline. The car MDP described at the beginning of Section 3.2 is an example of a generic nominal model: its state and action spaces are broad enough to encompass a wide range of car models, and we can use this nominal model to infer constraints that specialize the MDP to a speciﬁc car and task. For a generic nominal MDP, the reward function may also come from a generic, simpliﬁed task, such as “minimize time to the goal” or “minimize energy usage.”
A baseline nominal MDP is a snapshot of a system from a point in time where it was well characterized. With a baseline nominal MDP, the constraints that we infer will represent changes to the system with respect to this baseline. In this case, the nominal reward function can be learned using existing IRL techniques with demonstrated behavior from the baseline model. We take this approach in our human obstacle avoidance example in Section 4.2: we use demonstrations of humans walking through the empty space to learn a nominal reward, then we can detect the presence of a new obstacle in the space from subsequent demonstrations.

4

Published as a conference paper at ICLR 2020

3.3 DEMONSTRATION LIKELIHOOD MAXIMIZATION

Our goal is to ﬁnd the constraints C∗ which are most likely to have been added to a nominal MDP M, given a set of demonstrations D from an agent navigating the constrained MDP. Let us deﬁne PM to denote probabilities given that we are considering MDP M. Our problem then becomes to select the constraints that maximize PM(C | D). If we assume a uniform prior over possible constraints, then we know from Bayes’ Rule that PM(C | D) ∝ PM(D | C). Therefore, in order to ﬁnd the constraints that maximize PM(C | D), we can solve the equivalent problem of ﬁnding which constraints maximize the likelihood of the given demonstrations. In this section, we present our
approach to solving maximum likelihood constraint inference via solving demonstration likelihood
maximization.

Under the maximum entropy model presented by Ziebart et al. (2008), the probability of a certain ﬁnite-length trajectory ξ being executed by an agent traversing a deterministic MDP M is exponentially proportional to the reward earned by that trajectory.

PM(ξ) = 1 eβR(ξ)1M(ξ),

(3)

Z

where Z is the partition function, 1M(ξ) indicates if the trajectory is feasible for this MDP, and

β ∈ [0, ∞) is a parameter describing how closely an agent adheres to the task of optimizing the

reward function (as β → ∞, the agent becomes a perfect optimizer, and as β → 0, the agent’s actions

become perfectly random). In the sequel, we assume that a given reward function will appropriately

capture the role of β, so we omit β from our notation without loss of generality.

In the case of ﬁnite horizon planning, the partition function will be the sum of the exponentially weighted rewards for all feasible trajectories on MDP M of length no greater than the planning horizon. We denote this set of trajectories by ΞM. Because adding constraints C modiﬁes the set of feasible trajectories, we express this dependence as

Z(C) =

eR(ξ)1MC (ξ).

(4)

ξ∈ΞM

Assuming independence among demonstrated trajectories, the probability of observing a set D of N demonstrations is given by the product

1 PMC (D) =

eR(ξ)1MC (ξ).

(5)

Z (C )N

ξ∈D

Our goal is to maximize the demonstration probability given by (5). Because we take the reward

function and demonstrations as given, our only available decision variable in this maximization is the
constraint set C which alters the indicator 1MC and partition function Z(C).

C∗ = arg max PMC (D),

(6)

C ∈C

where C ⊆ 2S×A is the hypothesis space of possible constraints.

From the form of (5), it is clear that to solve (6), we must choose a constraint set that does not invalidate any demonstrated trajectory while simultaneously minimizing the value of Z(C). Consider the set of trajectories that would be made infeasible by augmenting the MDP with constraint C,
which we denote as Ξ−MC = {ξ ∈ ΞM | 1MC (ξ) = 0}. The value of Z(C) is minimized when we
maximize the sum of exponentiated rewards of these infeasible trajectories. Considering the form of
the trajectory probability given by (3), we can see that this sum is proportional to the total probability of observing a trajectory from Ξ−MC on the original MDP M

eR(ξ) ∝

PM(ξ) = PM(Ξ−MC ).

(7)

ξ∈Ξ− MC

ξ∈Ξ− MC

This insight leads us to the ﬁnal form of the optimization

C∗ = arg max PM(Ξ−MC )

C∈C

.

(8)

s.t. D ∩ Ξ−MC = ∅

5

Published as a conference paper at ICLR 2020

(a) Nominal MDP

(b) Add constraint C1

(c) Add constraint C2

Figure 2: Selecting constraints to maximize demonstration likelihood. Trajectories that are likely to be observed on a given MDP are shown as dashed, angular arrows, and a provided demonstration is shown as a solid, curved arrow. Adding C1 in (b) does little to align the expected trajectories with the demonstration. On the other hand, adding C2 in (c) makes the original expected trajectories infeasible and causes the new expected trajectories to agree with the demonstration, greatly increasing the likelihood of the demonstration on this constrained MDP.

In order to solve (8), we must reason about the probability distribution of trajectories on the original MDP M, then ﬁnd the constraint C such that Ξ−MC contains the most probability mass while not containing any demonstrated trajectories. Figure 2 provides a graphical representation of this reasoning. We highlight here that the fact that the chosen C must not conﬂict with any demonstration is an important condition: all provided demonstrations must perfectly respect a constraint in order for it to be learned, otherwise a less restrictive set of constraints may be learned instead. Future work will look to relax this requirement in order to learn about constraints that are generally respected by a set of demonstrations, without needing to ﬁrst isolate just the successful, constraint-respecting demonstrations.
While equation (8) is derived for deterministic MDPs, if we can assume, as proposed by Ziebart et al. (2008), that for a given stochastic MDP, the stochastic outcomes have little effect on an agent’s behavior and the partition function, then the solution to (8) will also approximate the optimal constraint selection for that MDP. However, in order to fully address the stochastic case, we would need to reformulate our approach based on maximum causal entropy (Ziebart, 2010). We save this extension for future work.
3.3.1 CONSTRAINT HYPOTHESIS SPACE
In order for the solutions to (8) to be meaningful, we must be careful with our choice of the constraint hypothesis space C. For instance, if we let C = 2S×A, then the optimal solution will always be to choose the most restrictive C to constrain all state-action pairs not observed in the demonstration set.
One approach to avoid this trivial solution is to use domain knowledge of the modeled system to restrict C to a library of plausible or common constraints. McPherson et al. (2018) construct such a library by using reachability theory to calculate a family of likely unsafe sets.
We could also potentially address this problem by adding a regularization term to the optimization that would penalize constraints based on some notion of “size,” which would encourage the selection of “smaller” constraints. The size of a constraint set could be deﬁned by the number of minimal constraints that it contains. These minimal constraint sets constrain a single state, action, or feature, and were introduced in Section 3.2 as Csi , Cai , and Cφi , respectively. While penalizing this deﬁnition of size would effectively discourage overﬁtting, considering every possible combination of minimal constraints causes the hypothesis space to grow exponentially in the number of states, actions, and features of the MDP, which may make directly solving this formulation intractable.
Another approach, which avoids this combinatorial explosion, is to use the minimal constraint sets themselves, not their combinations, as our hypothesis space, and to select from among these constraints in an iterative, greedy manner. By iteratively selecting individual minimal constraint sets, and choosing a proper stopping condition, it is possible to gradually grow the full estimated constraint
6

Published as a conference paper at ICLR 2020

set and avoid overﬁtting to the demonstrations. It is this method that we will utilize in this work. Section 3.4 details our approach for selecting the most likely minimal constraint, and Section 3.5 details our approach for iteratively growing the estimated constraint set.

3.4 PROBABILITY MASS FOR MINIMAL CONSTRAINTS

As detailed in Section 3.3, the most likely constraint set is the one whose eliminated trajectories Ξ−MC have the highest probability of being demonstrated on the original, unconstrained MDP. Therefore, to ﬁnd the most likely of the minimal constraints, we must ﬁnd the expected proportion of trajectories which will contain any state or action, or accrue any feature. By using our augmented indicator feature map from (1), we can reduce this problem to only examine feature accruals. Ziebart et al. (2008) present their forward-backward algorithm for calculating expected feature counts for an agent following a policy in the maximum entropy setting. This algorithm nearly sufﬁces for our purposes, but it computes the expectation of the total number of times a feature will be accrued (i.e. how often will this feature be observed per trajectory), rather than the expectation of the number of trajectories that will accrue that feature at any point in time. To address this problem, we present a

Algorithm 1 Feature Accrual History Calculation

Input: an MDP M, a policy π(a|s, t), a time horizon T

Output: expected feature accrual history Φ[1,T ]

/* Initialize state visitation and feature accrual history */ 1: for s ∈ S do 2: Ds,0 ← D0(s)

3: Φs,0 ← 0nφ×1 4: end for

/* Track feature accruals over the time horizon */

5: for t ∈ [0, T − 1] do

6: for s ∈ S do

7:

for a ∈ As do

8:

/* New feature accruals */

9:

/* “ ” denotes element-wise multiplication */

10: ∆Φs,t(a) ← φ1(s, a) Ds,t1nφ×1 − Φs,t

11:

end for

12: end for

13: for s ∈ S do

14:

Ds ,t+1 ←

Ds,tπ(a|s, t)P (s |s, a)

s∈S a∈As

15:

Φs ,t+1 ←

Φs,t + ∆Φs,t(a) π(a|s, t)P (s |s, a)
s∈S a∈As

16: end for

modiﬁed form of the “forward” pass 17: Φt+1 ← Φs,t+1

as Algorithm 1. Our algorithm tracks state visitations as well as feature accruals at each state, which allows us

s∈S
18: end for
19: Return Φ[1,T ]

to produce the same maximum en-

tropy distribution over trajectories as Ziebart et al. (2008) while not counting additional accruals for

trajectories that have already accrued a feature.

The input of Algorithm 1 includes the MDP itself, a time horizon, and a time-varying policy. This policy should capture the expected behavior of the demonstrator on the nominal MDP M, and it can be computed via the “backward” part of the algorithm from Ziebart et al. (2008). The output of
Algorithm 1, Φ[1,T ], is an nφ × T array such that the t-th column Φt is a vector whose i-th entry is the expected proportion of trajectories to have accrued the i-th feature by time t. In particular, the i-th element of ΦT is equal to PM(Ξ−MCi ), which allows us to now directly select the most likely constraint according to (8).

3.5 MAXIMUM-COVERAGE-BASED ITERATIVE CONSTRAINT INFERENCE
When using minimal constraint sets as the constraint hypothesis space, it is possible that the most likely constraint still does not provide a satisfactory explanation for the demonstrated behavior. In this case, it can be beneﬁcial to combine minimal constraints. If the task of solving (8) is framed as ﬁnding the combination of constraint sets that “covers” the most probability mass, then the problem becomes a direct analog for the classic maximum coverage problem. While this problem is known to be NP-hard, there exist a simple greedy algorithm with known suboptimality bounds (Hochbaum & Pathria, 1998).

7

Published as a conference paper at ICLR 2020

We present Algorithm 2 as our approach for adapting this greedy heuristic to solve the problem of

constraint inference. At each iteration, we grow our estimated constraint set by augmenting it with

the constraint set in our hypothesis space that covers the most currently uncovered probability mass.

By analogy to the maximum coverage problem (Hochbaum & Pathria, 1998), we derive the following

bound on the suboptimality of our approach.

Theorem 1. Let Cnc be the set of all constraints Cnc such that Cnc =

nc i=1

Ci

for

Ci

∈

C,

and

let

C∗nc be the solution to (8) using Cnc as the constraint hypothesis space. It follows, then, that at the

end of every iteration i of Algorithm 2,

−

i−1 i

−

P ΞMC∗ ≥ 1 − i

P Ξ C∗ .
Mi

This bound is directly analogous to the suboptimality bound for the Algorithm 2 Greedy Iterative Constraint Inference

greedy solution to the maximum cov- Input: MDP M, constraint hypothesis space C,

erage problem proven by Hochbaum

empirical probability distribution PD, threshold dDKL

& Pathria (1998). For space, the proof Output: estimated constraint set C∗

is included in the appendix.

1: C∗ ← ∅

Rather than selecting the number of 2: for i ∈ [1, |C| ] do

constraints nc to be used ahead of 3: Ci ← solution to (8) using MC∗ , C, and D

time, we check a stopping condition 4: ∆DKL = DKL PD || PMC∗ − DKL PD || PMC∗∪Ci

to decide if we should continue to add 5: if ∆DKL ≤ dDKL then

constraints. Because we are attempt- 6:

break

ing to maximize PMC (D), it might 7: end if

seem natural to use a probability- 8: C∗ ← C∗ ∪ Ci based stopping condition. However, 9: end for

choosing a stopping criterion based 10: Return C∗

on probability is problematic because

the probability of observing a set of demonstrations is dependent on the number of demonstrations

in the set, even if each individual demonstration has the same probability. We instead base our

stopping condition on KL divergence, which depends only on the distribution of trajectories in D and

not on the number of demonstrations. The quantity DKL PD || PMC∗ provides a measure of how well the distribution over trajectories induced by our inferred constraints, PMC∗ , agrees with the empirical probability distribution over trajectories observed in the demonstrations, PD. Using this KL

divergence in the stopping condition actually preserves a link to the probability of the demonstration

set, since the KL divergence will decrease monotonically as PMC (D) increases. The threshold

parameter dDKL is chosen to avoid overﬁtting to the demonstrations, combating the tendency to select

additional constraints that may only marginally better align our predictions with the demonstrations.

4 EXAMPLES
4.1 SYNTHETIC GRID WORLD
We consider the grid world MDP presented in Figure 3. The environment consists of a 9-by-9 grid of states, and the actions are to move up, down, left, right, or diagonally by one cell. The objective is to move from the starting state in the bottom-left corner (s0) to the goal state in the bottom-right corner (sG). Every state-action pair produces a distance feature, and the MDP reward is negative distance, which encourages short trajectories. There are additionally two more features, denoted green and blue, which are produced by taking actions from certain states, as shown in Figure 3.
The true MDP, from which agents generate trajectories, is shown in Figure 3a, including its constraints. The nominal, more generic MDP shown in Figure 3b is what we take as M for applying the iterative maximum likelihood constraint inference in Algorithm 2, with feature accruals estimated using Algorithm 1. While Figures 3c through 3e show the iteratively estimated constraints, which align with the true constraints, it is interesting to note that not all constraints present in the true MDP are identiﬁed. For instance, it is so unlikely that an agent would ever select the up-left diagonal action, that the fact that demonstrated trajectories did not contain that action is unsurprising and does not make that action an estimated constraint.

8

Published as a conference paper at ICLR 2020

(a) True MDP

(b) Nominal MDP

(c) C∗, nc = 1

(d) C∗, nc = 2

(e) C∗, nc = 6

Figure 3: Algorithm performance on a synthetic grid world MDP. Each subﬁgure represents the MDP by showing (clockwise from left) its states, actions, and features. Each element is shaded according to the proportion of trajectories that are expected to accrue the respective augmented feature, computed via Algorithm 1. Constraints are marked with a red “X,” and bright bounding boxes mark the green and blue feature-producing states. The result here are shown for a set of 100 demonstrations sampled according to the expectation for the True MDP (a). We begin with the nominal MDP shown in (b), and produce (c), (d), and (e) by applying Algorithm 2. Note that (c), (d), and (e) show the selections of feature, action, and state constraints, respectively.

Figure 4 shows how the performance of our approach varies based on the number of available demonstrations and the selection for the threshold dDKL . The false positive rate shown in Figure 4a is the proportion of selected constraints which are not constraints of the true system. We can observe two trends in this data that we would expect. First, lower values of dDKL lead to greater false positive rates since they allow Algorithm 2 to continue iterating and accept constraints that do less to align expectations and demonstrations. Second, having more demonstrations available provides more information and reduces the rate of false positives. Further, Figure 4b shows that more demonstrations also allows the behavior predicted by constraints to better align with the observations. It is interesting to note, however, that with fewer than 10 demonstrations and a very low dDKL , we may produce very low KL divergence, but at the cost of a high false positive rate. This phenomenon highlights the role of selecting dDKL to avoid over-ﬁtting. The threshold dDKL = 0.1 achieves a good balance of producing few false positives with sufﬁcient examples while also producing lower KL divergences, and we used this threshold to produce the results in Figures 3 and 5.
4.2 HUMAN OBSTACLE AVOIDANCE
In our second example, we analyze trajectories from humans as they navigate around an obstacle on the ﬂoor. We map these continuous trajectories into trajectories through a grid world where each cell represents a 1ft-by-1ft area

(a) False positives
(b) DKL Figure 4: Algorithm performance on the synthetic grid world. Each data point represents the mean result of 10 independent trajectory draws, and the margins show ±1 standard error.

9

Published as a conference paper at ICLR 2020

on the ground. The human agents are attempting to reach a ﬁxed goal state (sG) from a given initial state (s0), as shown in Figure 5. We performed MaxEnt IRL on human demonstrations of the task without the obstacle to obtain the nominal distance-based reward function. We restrict ourselves to estimating only state constraints, as we do not supply our algorithm with knowledge of any additional features in the environment and we assume that the humans’ motion is unrestrained.
Demonstrations were collected from 16 volunteers, and the results of performing constraint inference are shown in Figure 5. Our method is able to successfully predict the existence of a central obstacle. While we do not estimate every constrained state, the constraints that we do estimate make all of the obstacle states unlikely to be visited. In order to identify those states as additional constraints, we would have to decrease our dDKL threshold, which could also lead to more spurious constraint selections, such as the three shown in Figure 5.

5 CONCLUSION AND FUTURE WORK

We have presented our novel technique for learning con-

straints from demonstrations. We improve upon previous

work in constraint-learning IRL by providing a principled

framework for identifying the most likely constraint(s), and

we do so in a way that explicitly makes state, action, and

feature constraints all directly comparable to one another. We

believe that the numerical results presented in Section 4 are

promising and highlight the usefulness of our approach.

Figure 5: Human trajectories overlaid

on a grid world MDP. The shaded re-

Despite its beneﬁts, one drawback of our approach is that gion represents an obstacle in the hu-

the formulation is based on (3), which only exactly holds for man’s environment, and the red “X”s

deterministic MDPs. As mentioned in Section 3.3, we plan represent learned constraints.

to investigate the use of a maximum causal entropy approach

to address this issue and fully handle stochastic MDPs. Additionally, the methods presented here

require all demonstrations to contain no violations of the constraints we will estimate. We believe

that softening this requirement, which would allow reasoning about the likelihood of constraints that

are occasionally violated in the demonstration set, may be beneﬁcial in cases where trajectory data is

collected without explicit labels of success or failure. Finally, the structure of Algorithm 1, which

tracks the expected features accruals of trajectories over time, suggests that we may be able to reason

about non-Markovian constraints by using this historical information to our advantage.

Overall, we believe that our formulation of maximum likelihood constraint inference for IRL shows promising results and presents attractive avenues for further investigation.

ACKNOWLEDGMENTS
This work is supported by the National Science Foundation through grant CNS-1545126 (VeHICaL). We would like to thank Jaime Fisac, Andrea Bajcsy, Sylvia Herbert, and David Fridovich-Keil for their insightful discussions and for sharing their collected data.

REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1. ACM, 2004.
Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of reinforcement learning to aerobatic helicopter ﬂight. In Advances in neural information processing systems, pp. 1–8, 2007.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Mane´. Concrete problems in AI safety. arXiv, arXiv:1606.06565, 2016.

10

Published as a conference paper at ICLR 2020
Glen Chou, Dmitry Berenson, and Necmiye Ozay. Learning constraints from demonstrations. In Springer Proceedings in Advanced Robotics (SPAR) series on 2018 Workshop on the Algorithmic Foundations of Robotics (WAFR 2018). Springer, 2018.
Mark K Ho, Michael L Littman, Fiery Cushman, and Joseph L Austerweil. Teaching with rewards and punishments: Reinforcement or communication? In CogSci, 2015.
Dorit S Hochbaum and Anu Pathria. Analysis of the greedy approach in problems of maximum k-coverage. Naval Research Logistics (NRL), 45(6):615–627, 1998.
Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.
Rudolf Emil Kalman. When is a linear control system optimal? Journal of Basic Engineering, 86(1): 51–60, 1964.
David L. McPherson, Dexter R.R. Scobee, Joseph Menke, Allen Y Yang, and S. Shankar Sastry. Modeling supervisor safe sets for improving collaboration in human-robot teams. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 861–868, Oct 2018. doi: 10.1109/IROS.2018.8593865.
Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pp. 663–670. Morgan Kaufmann Publishers Inc., 2000.
Michael Pardowitz, Raoul Zollner, and Ru¨diger Dillmann. Learning sequential constraints of tasks from user demonstrations. In 5th IEEE-RAS International Conference on Humanoid Robots, 2005., pp. 424–429. IEEE, 2005.
Claudia Pe´rez-D’Arpino and Julie A Shah. C-learn: Learning geometric constraints from demonstrations for multi-step manipulation in shared autonomy. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 4058–4065. IEEE, 2017.
Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In Proceedings of the 23rd international conference on Machine learning, pp. 729–736. ACM, 2006.
Dexter R R Scobee, Vicenc Rubies Royo, Claire J Tomlin, and S Shankar Sastry. Haptic assistance via inverse reinforcement learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pp. 1510–1517. IEEE, 2018.
Guru Subramani, Michael Zinn, and Michael Gleicher. Inferring geometric constraints in human demonstrations. In Conference on Robot Learning, pp. 223–236, 2018.
Dizan Vasquez, Billy Okal, and Kai O Arras. Inverse reinforcement learning algorithms and features for robot navigation in crowds: an experimental comparison. In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 1341–1346. IEEE, 2014.
Marcell Vazquez-Chanlatte, Susmit Jha, Ashish Tiwari, Mark K Ho, and Sanjit Seshia. Learning task speciﬁcations from demonstrations. In Advances in Neural Information Processing Systems, pp. 5367–5377, 2018.
Brian D Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, Carnegie Mellon University, 2010.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.
11

Published as a conference paper at ICLR 2020

APPENDIX

A ADDING CONSTRAINTS TO STOCHASTIC MDPS

For MDPs with stochastic transitions, the semantics of an empty state are less obvious and could lend themselves to multiple interpretations depending on the nature of the system being modeled. In our context, we use constraints to describe how observed behaviors from demonstrations differ from possible behaviors allowed by the nominal MDP structure. We therefore assume that any demonstrations provided are, by the fact that they were selected to be provided, consistent with the system’s constraints, including avoiding empty states. This assumption implies that any stochastic state transitions that would have led to an empty state will not be observed in trajectories from the demonstration set. The omission of these transitions means that, for a given (s, a), if Ps,a(Sempty) = p, then a proportion p of these (s, a) pairs which occur as an agent navigates the environment will be excluded from demonstrations. Therefore, as we modify the MDP to reason about demonstrated behavior, we need updated transition probabilities which eliminate the probability mass of transitioning to empty states, an event which will never be observed in a demonstration. Such modiﬁed probabilities can be given as
PsC,a(s ) = 0 P s,a(s ) iofthser∈wiSseempty . (9)
1−P s,a(Sempty)

We must also capture the change to observed state-action pair frequencies by understanding that any observed policy πC will be related to an agent’s actual policy π according to

πC (a|s, t) = π(a|s, t)(1 − Ps,a(Sempty)) .

(10)

π(a |s, t)(1 − Ps,a(Sempty))

a ∈ACs

It is important to note that the modiﬁcations presented in (9) and (10) for non-deterministic MDPs are not meant to directly reﬂect the reality of the underlying system (we wouldn’t expect the actual transition dynamics to change, for instance), but to reﬂect the apparent behavior that we would expect to observe in the subset of trajectories that would be selected as demonstrations. We further note that applying these modiﬁcations to deterministic MDPs will result in the same expected behavior as augmenting the constraint set with Cempty.

B PROOF FOR THEOREM 1

Theorem 1. Let Cnc be the set of all constraints Cnc such that Cnc =

nc i=1

Ci

for

Ci

∈

C,

and

let

C∗nc be the solution to (8) using Cnc as the constraint hypothesis space. It follows, then, that at the

end of every iteration i of Algorithm 2,

−

i−1 i

−

P ΞMC∗ ≥ 1 − i

P Ξ C∗ .
Mi

Proof. The problem of ﬁnding C∗nc is analogous to solving the maximum coverage problem, discussed by Hochbaum & Pathria (1998), where the set of elements to be covered is the set of trajectories {ξ | ∃ C ∈ C : ξ ∈ Ξ−MC and D ∩ Ξ−MC = ∅} and the weight of each element ξ is PM(ξ). Because Algorithm 2 constructs C∗ iteratively by taking the union of the previous value of C∗ and the set
Ci ∈ C which solves (8), the value of C∗ at the end of the i-th iteration is analogous to the greedy solution of the maximum coverage problem with nc = i. Therefore, we can directly apply the suboptimality bound for the greedy solution proven in Hochbaum & Pathria (1998) to arrive at our
given bound on eliminated probability mass.

12

