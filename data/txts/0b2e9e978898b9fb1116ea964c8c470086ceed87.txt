IEEE TRANSACTIONS ON IMAGE PROCESSING

1

Bifurcated Backbone Strategy for RGB-D Salient Object Detection
Yingjie Zhai*, Deng-Ping Fan*, Jufeng Yang, Ali Borji, Ling Shao, Fellow, IEEE, Junwei Han, Senior Member, IEEE, and Liang Wang, Fellow, IEEE

arXiv:2007.02713v3 [cs.CV] 18 Aug 2021

Abstract‚ÄîMulti-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multimodal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGBD salient object detection to devise a novel cascaded reÔ¨Ånement network. In particular, Ô¨Årst, we propose to regroup the multilevel features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is simple, efÔ¨Åcient, and backbone-independent. Extensive experiments show that BBS-Net signiÔ¨Åcantly outperforms 18 SOTA models on 8 challenging datasets under 5 evaluation measures, demonstrating the superiority of our approach (‚àº4% improvement in S-measure vs. the top-ranked model: DMRA-iccv2019). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research.
Index Terms‚ÄîRGB-D salient object detection, bifurcated backbone strategy, multi-level features, cascaded reÔ¨Ånement.

RGB

Ours

DMRA

CPFP

TANet

Depth

GT

PCF

SE

LBE

RGB

Ours

DMRA

CPFP

TANet

Depth

GT

PCF

SE

LBE

Fig. 1. Saliency maps of state-of-the-art (SOTA) CNN-based methods (i.e., DMRA [19], CPFP [21], TANet [18], PCF [22] and Ours) and methods based on handcrafted features (i.e., SE [25] and LBE [26]). Our method generates higher-quality saliency maps and suppresses background distractors in challenging scenarios (top: complex background; bottom: depth with noise).

I. INTRODUCTION
T HE goal of salient object detection (SOD) is to Ô¨Ånd and segment the most visually prominent object(s) in an image [2], [3]. Over the last decade, SOD has attracted signiÔ¨Åcant attention due to its widespread applications in object recognition [4], content-based image retrieval [5], image segmentation [6], image editing [7], video analysis [8], [9], and visual tracking [10], [11]. Traditional SOD algorithms [12], [13] are typically based on handcrafted features and fall short in capturing high-level semantic information (see also [14], [15]). Recently, convolutional neural networks (CNNs) have been used for RGB SOD [16], [17], achieving better performance compared to the traditional methods.
However, the performance of RGB SOD models tends to drastically decrease in certain complex scenarios (e.g.,
*Equal contribution. Listing order is random. Yingjie Zhai, Deng-Ping Fan and Jufeng Yang are with College of Computer Science, Nankai University. Ali Borji is with Primer.AI, SF, USA. Ling Shao is with the Mohamed bin Zayed University of ArtiÔ¨Åcial Intelligence, Abu Dhabi, UAE, and also with the Inception Institute of ArtiÔ¨Åcial Intelligence, Abu Dhabi, UAE. Junwei Han is with School of Automation, Northwestern Polytechnical University, China. Liang Wang is with the National Laboratory of Pattern Recognition, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. A preliminary version of this work has appeared in ECCV 2020 [1]. Corresponding author: Jufeng Yang (yangjufeng@nankai.edu.cn).

cluttered backgrounds, multiple objects, varying illuminations, transparent objects, etc) [18]. One of the most important reasons behind these failure cases may be the lack of depth information, which is critical for saliency prediction. For example, an object with less texture but closer to the camera is usually salient than an object with more texture but farther away. Depth maps contain abundant spatial structure and layout information [19], providing geometrical cues for improving the performance of SOD. Besides, depth information can be easily obtained using popular devices, e.g., stereo cameras, Kinect and smartphones, which are becoming increasingly more ubiquitous. Therefore, various algorithms (e.g., [20], [21]) have been proposed to solve the SOD problem by combining RGB and depth information (i.e., RGB-D SOD).
To efÔ¨Åciently integrate RGB and depth cues for SOD, researchers have explored different but complementary multimodal and multi-level strategies [22]‚Äì[24] and have achieved encouraging results. However, existing RGB-D SOD methods still have to solve the following challenges:
(1) Effectively aggregating multi-level features. As discussed in [16], teacher features contain rich semantic macro information and can serve as strong guidance for locating salient objects, while student features provide afÔ¨Çuent micro details that are beneÔ¨Åcial for reÔ¨Åning object edges. Therefore,

IEEE TRANSACTIONS ON IMAGE PROCESSING

2

current RGB-D SOD methods use either a dedicated aggregation strategy [19], [21] or a progressive merging process [27], [28] to leverage multi-level features. However, because they directly fuse multi-level features without considering levelspeciÔ¨Åc characteristics, these operations suffer from the inherent problem of noisy low-level features [18], [29]. As a result, several methods are easily confused by the background (e.g., Ô¨Årst and second rows in Fig. 1).
(2) Excavating informative cues from the depth modality. Previous algorithms usually regard the depth map as a fourthchannel input [31], [32] of the original three-channel RGB image, or fuse RGB and depth features by simple summation [33], [34] and multiplication [35], [36]. However, these methods treat depth and RGB information from the same perspective and ignore the fact that RGB images capture color and texture, whereas depth maps capture the spatial relations among objects. Due to this modality difference, the above-mentioned simple combination methods are not very efÔ¨Åcient. Further, depth maps often have low quality, which introduces randomly distributed errors and redundancy into the network [37]. For example, the depth map in the last row of Fig. 1 is blurry and noisy. As a result, many methods (e.g., the top-ranked model DMRA [19]) fail to detect the full extent of the salient object.
To address the above issues, we propose a novel Bifurcated Backbone Strategy Network (BBS-Net) for RGB-D SOD. The proposed method exploits multi-level features in a cascaded reÔ¨Ånement way to suppress distractors in the lower layers. This strategy is based on the observation that teacher features provide discriminative semantic information without redundant details [16], [29], which may contribute signiÔ¨Åcantly to eliminating the lower-layer distractors. As shown in Fig. 2 (b), BBS-Net contains two cascaded decoder stages: (1) Crossmodal teacher features are integrated by the Ô¨Årst cascaded decoder CD1 to predict an initial saliency map S1. (2) Student features are reÔ¨Åned by an element-wise multiplication with the initial saliency map S1 and are then aggregated by another cascaded decoder CD2 to produce the Ô¨Ånal saliency map S2. To fully capture the informative cues in the depth map and improve the compatibility of RGB and depth features, we further introduce a depth-enhanced module (DEM). This module exploits the inter-channel and spatial relations of the depth features and discovers informative depth cues.
Additionally, to obtain reasonable performance in real-world scenarios, not only an efÔ¨Åcient model is needed but also a dataset with great generalization ability is required to train such model. There are several large-scale RGB-D datasets, e.g., NJU2K [38], NLPR [32], STERE [39], SIP [37] and DUT [19] with more than 1, 000 image pairs. However, researchers have often trained RGB-D models on the Ô¨Åxed training set (i.e., 1, 485 images from NJU2K and 700 images from NLPR). This limits the model‚Äôs generation ability in various scenarios. Further, they have not studied the generalization ability of different datasets and have not proposed powerful training sets. In this paper, one of our goals is to study this problem in detail.
Our main contributions are summarized as follows:
‚Ä¢ We propose a powerful Bifurcated Backbone Strategy

Network (BBS-Net) to deal with multiple complicated real-world scenarios in RGB-D SOD. To address the long-overlooked problem of noise in low-level features decreasing the performance of saliency models, we carefully explore the characteristics of multi-level features in a bifurcated backbone strategy (BBS), i.e., features are split into two groups, as shown in Fig. 2 (b). In this way, noise in student features can be eliminated effectively by the saliency map generated from teacher features. ‚Ä¢ We further introduce a depth-enhanced module (DEM) in BBS-Net to enhance the depth features before merging them with the RGB features. The DEM module concentrates on the most informative parts of depth maps by two sequential attention operations. We leverage the attention mechanism to excavate important cues from the depth features of multiple side-out layers. This module is simple but has proven effective for fusing RGB and depth modalities in a complementary way. ‚Ä¢ We conduct a comprehensive comparison with 18 SOTA methods using various metrics (e.g., max Fmeasure, MAE, S-measure, max E-measure, and PR curves). Experimental results show that BBS-Net outperforms all of these methods on eight public datasets, by a large margin. In terms of the predicted saliency maps, BBS-Net generates maps with sharper edges and fewer background distractors compared to existing models. ‚Ä¢ We conduct a number of cross-dataset experiments to evaluate the quality of current popular RGB-D datasets and introduce a training set with high generalization ability for fair comparison and future research. Current RGB-D methods train their networks using the Ô¨Åxed training-test splits of different datasets, without exploring the difÔ¨Åculties of those datasets. To the best of our knowledge, we are the Ô¨Årst to investigate this important but overlooked problem in the area of RGB-D SOD.
This work is based on our previous conference paper [1] and extends it signiÔ¨Åcantly in Ô¨Åve ways: 1) We further extend the approach by designing a depth adapter module, which makes the model contain around 50 percent parameters of the previous version but with similar performance. 2) We provide more details and experiments regarding our BBS-Net model, including motivation, feature visualizations, experimental settings, etc. 3) We investigate several previously unexplored issues, including cross-dataset generalization ability, post-processing methods, failure cases analysis, etc. 4) To further demonstrate our model performance, we conduct several comprehensive experiments over the recently released dataset, DUT [19]. 5) We perform in-depth analyses and draw several novel conclusions which are critical in developing more powerful models in the future. We are hopeful that our study will provide deep insights into the underlying design mechanisms of RGB-D SOD, and will spark novel ideas. The complete algorithm, benchmark results, and post-processing toolbox are publicly available at https://github.com/zyjwuyan/BBS-Net.
II. RELATED WORKS A. Salient Object Detection
Over the past several decades, SOD [40]‚Äì[42] has garnered signiÔ¨Åcant research interest due to its diverse applica-

IEEE TRANSACTIONS ON IMAGE PROCESSING

3

(a) In

Student feature Teacher feature
In CD1
FD S (b)

FD Full Decoder S1 CD1 Cascaded Decoder 1
CD2 Cascaded Decoder 2

CD2

S2

Convolutional Block Skip Connection Element-wise Multiplication

Fig. 2. (a) Existing multi-level feature aggregation methods for RGB-D SOD [18], [19], [21], [22], [27], [28], [30]. (b) In this paper, we adopt a bifurcated backbone strategy (BBS) to split the multi-level features into student and teacher features. The initial saliency map S1 is utilized to reÔ¨Åne the student features to effectively suppress distractors. Then, the reÔ¨Åned features are passed to another cascaded decoder to generate the Ô¨Ånal saliency map S2.

tions [43]‚Äì[45]. In early years, SOD methods were primarily based on intrinsic prior knowledge such as center-surround color contrast [46], global region contrast [12], background prior [47] and appearance similarity [48]. However, these methods heavily rely on heuristic saliency cues and low-level handcrafted features, thus lacking the guidance of high-level semantic information.
Recently, to solve this problem, deep learning based methods [49]‚Äì[53] have been explored, exceeding handcrafted feature-based methods in complex scenarios. These deep methods [54] usually leverage CNNs to extract multi-level multiscale features from RGB images and then aggregate them to predict the Ô¨Ånal saliency map. Such multi-level multiscale features [55], [56] can help the model better understand the contextual and semantic information to generate highquality saliency maps. Besides, since image-based SOD may be limited in some real-world applications such as video captioning [57], autonomous driving [58] and robotic interaction [59], SOD algorithms [8], [9] have also been explored for video analysis.
To further overcome the limits of deep models, researchers have also proposed to excavate edge information [60] to guide prediction. These methods use an auxiliary boundary loss to improve the training and representative ability of segmentation tasks [61]‚Äì[63]. With the auxiliary guidance from the edge information, deep models can predict maps with Ô¨Åner and sharper edges. In addition to edge guidance, another useful type of auxiliary information are depth maps, which capture the spatial distance information. These are the main focus of this paper.
B. RGB-D Salient Object Detection
‚Ä¢ Traditional Models. Previous algorithms for RGB-D SOD mainly rely on extracting handcrafted features [35], [36] from RGB and depth images. Contrast-based cues, including edge, color, texture and region, are largely utilized by these methods to compute the saliency of a local region. For example, Desingh et al. [64] adopted the region-based contrast to calculate contrast strengths for the segmented regions. Ciptadi et al. [65] used surface normals and color contrast to compute saliency. However, the local contrast methods are easily disturbed by high-frequency content [66], since they mainly rely on the boundaries of salient objects. Therefore, some algorithms, such as spatial prior [35], global

contrast [67], and background prior [68], proposed to compute saliency by combining both local and global information.
To combine saliency cues from RGB and depth modalities more effectively, researchers have explored multiple fusion strategies. Some methods [31], [32] process RGB and depth images together by regarding depth maps as fourth-channel inputs (early fusion). This operation is simple but does not achieve reliable results, since it disregards the differences between the RGB and depth modalities. Therefore, some algorithms [33], [36] extract the saliency information from the two modalities separately by Ô¨Årst leveraging two backbones to predict saliency maps and then fusing the saliency results (late fusion). Besides, to enable the RGB and depth modalities to share beneÔ¨Åts, other methods [26], [38] fuse RGB and depth features in a middle stage and then produce the corresponding saliency maps (middle fusion). Deep models also use the above three fusion strategies, and our method falls under the middle fusion category.
‚Ä¢ Deep Models. Early deep methods [66], [68] compute saliency conÔ¨Ådence scores by Ô¨Årst extracting handcrafted features, and then feeding them to CNNs. However, these algorithms need the low-level handcrafted features to be manually designed as input, and thus cannot be trained in an end-to-end manner. More recently, researchers have begun to extract deep RGB and depth features using CNNs in a bottomup fashion [70]. Unlike handcrafted features, deep features contain a lot of contextual and semantic information, and can thus better capture representations of the RGB and depth modalities. These methods have achieved encouraging results, which can be attributed to two important aspects of feature fusion. One is their extraction and fusion of multi-level and multi-scale features from different layers, while the other is the mechanism by which the two different modalities (RGB and depth) are combined.
Various architectures have been designed to effectively integrate the multi-scale features. For example, Liu et al. [27] obtained saliency map outputs from each side-out features by feeding a four-channel RGB-D image into a single backbone (single stream). Chen et al. [22] leveraged two independent networks to extract RGB and depth features respectively, and then combined them in a progressive merging way (double stream). Furthermore, to learn supplementary features, [18] designed a three-stream network consisting of two modalityspeciÔ¨Åc streams and a parallel cross-modal distillation stream to exploit complementary cross-modal information in the

IEEE TRANSACTIONS ON IMAGE PROCESSING

4

352√ó352√ó1 352√ó352√ó1 352√ó352√ó1

ùëì5ùëë Conv5
ùëì4ùëë Conv4 ùëì3ùëë Conv3
ùëì2ùëë Conv2

DEM

DEM

DEM

DEM

11√ó11√ó2048

ùëì5ùëüùëîùëè

ùëì5ùëêùëö

Conv5

GCM UP√ó2 BConv3 UP√ó2
BConv3

UP√ó4

22√ó22√ó1024

ùëì4ùëüùëîùëè

ùëì4ùëêùëö

Conv4

44√ó44√ó512

ùëìùëüùëîùëè

ùëì3ùëêùëö

3 Conv3

ùëì2ùëêùëö

GCM GCM

UP√ó2 BConv3
BConv3

UP√ó2

ùëì3ùëêùëö‚Ä≤

ùëì2ùëüùëîùëè Conv2

88√ó88√ó256

ùëì2ùëêùëö‚Ä≤ UP√ó4

(a) FCD1 Cascaded Decoder
44√ó44√ó32 C UP√ó2 BConv3
C
(b) T2

BConv3 Conv3√ó3

T1 PTM

UP√ó8
G

S1
ùëôùëêùëí ùëôùëêùëí

FCD2 88√ó88√ó32 Conv1√ó1 TransB Conv1√ó1 TransB Conv1√ó1

ùëì1ùëë Conv1

DEM

ùëì1rgb Conv1

88√ó88√ó64 ùëì1ùëêùëö

ùëì1ùëêùëö‚Ä≤

GCM Global Contextual Module DEM

Depth-Enhanced Module

S2
PTM: Progressively Transposed Module

RGB

Depth

C Concatenation

BConvN ConvN√óN+BN+ReLU Refinement Flow

352√ó352√ó1

352√ó352√ó3

Element-wise Summation Element-wise Multiplication

ConvN TransB

Convolutional Block Data Flow Conv+BN+ReLU+DeConv+BN+ReLU+Residual

Fig. 3. Architecture of our BBS-Net. Feature Extraction: ‚ÄòConv1‚Äô‚àº‚ÄòConv5‚Äô denote different layers from ResNet-50 [69]. Multi-level features (f1d ‚àº f5d) from the depth branch are enhanced by the DEM and are then fused with features (i.e., f1rgb ‚àº f5rgb) from the RGB branch. Stage 1: cross-modal teacher features (f3cm ‚àº f5cm) are Ô¨Årst aggregated by the cascaded decoder (a) to produce the initial saliency map S1. Stage 2: Then, student features (f1cm ‚àº f3cm) are reÔ¨Åned by the initial saliency map S1 and are integrated by another cascaded decoder to predict the Ô¨Ånal saliency map S2. See ¬ß III for details.

bottom-up feature extraction process (three streams). Depth maps are sometimes low-quality and may thus contain signiÔ¨Åcant noise or misleading information, which greatly decreases the performance of SOD models. To address this issue, Zhao et al. [21] proposed a contrast-enhanced network to improve the quality of depth maps using the contrast prior. Fan et al. [37] designed a depth depurator unit to evaluate the quality of depth maps and Ô¨Ålter out the low-quality ones automatically. Three recent works have explored uncertainty [71], depth prediction [72] and a joint learning strategy [73] for saliency detection and achieved reasonable performance. There were also some concurrent works published in recent top conferences (e.g., ECCV [74]‚Äì[76]). Discussing these works in detail is beyond the scope of this article. Please refer to the online benchmark (http://dpfan.net/d3netbenchmark/) and the latest survey [77] for more details.
III. PROPOSED METHOD
A. Overview
Current popular RGB-D SOD models directly integrate multi-level features using a single decoder (Fig. 2 (a)). In contrast, the network Ô¨Çow of the proposed BBS-Net (Fig. 3) explores a bifurcated backbone strategy. In ¬ß III-B, we Ô¨Årst detail the proposed bifurcated backbone strategy with the cascaded reÔ¨Ånement mechanism. Then, to fully excavate informative cues from the depth map, we introduce a new depth-enhanced module in ¬ß III-C. Additionally, we design a depth adapter module to further improve the efÔ¨Åciency of the model in ¬ß III-D.

B. Bifurcated Backbone Strategy (BBS)

Our cascaded reÔ¨Ånement mechanism leverages the rich semantic information in high-level cross-modal features to suppress background distractors. To support such a feat, we devise a bifurcated backbone strategy (BBS). It divides the multi-level cross-modal features into two groups, i.e., G1 = {Conv1, Conv2, Conv3} and G2 ={Conv3, Conv4, Conv5}, where Conv3 is the split point. The original multi-scale information is well preserved by each group.

‚Ä¢ Cascaded ReÔ¨Ånement Mechanism. To effectively leverage the characteristics of the features in the two groups‚Äô features, we train the network using a cascaded reÔ¨Ånement mechanism. This mechanism Ô¨Årst generates an initial saliency map with three cross-modal teacher features (i.e., G2) and then enhances the details of the initial saliency map S1 with three cross-modal student features (i.e., G1), which are reÔ¨Åned by the initial saliency map. This is based on the observation that high-level features contain rich semantic information that helps locate salient objects, while low-level features provide microlevel details that are beneÔ¨Åcial for reÔ¨Åning the boundaries. In other words, by exploring the characteristics of the multi-level features, this strategy can efÔ¨Åciently suppress noise in lowlevel cross-modal features, and can produce the Ô¨Ånal saliency map through a progressive reÔ¨Ånement.

SpeciÔ¨Åcally, we Ô¨Årst merge RGB and depth features pro-

cessed by the DEM to obtain the cross-modal features

{f

cm i

;

i

=

1, 2, ..., 5}.

In

stage

one,

the

three

cross-modality

teacher features (i.e., f3cm, f4cm, f5cm) are aggregated by the

IEEE TRANSACTIONS ON IMAGE PROCESSING

5

Ô¨Årst cascaded decoder, which is denoted as:

not of the same scale. represents the element-wise multipli-

S1 = T1 FCD1(f3cm, f4cm, f5cm) ,

cation, and Conv(¬∑) represents the standard 3√ó3 convolution (1) operation. Then, the updated features are integrated by a

where FCD1 is the Ô¨Årst cascaded decoder, S1 is the initial progressive concatenation strategy to produce the output:

slaaylieernscythamt atpra, nasnfodrmT1threepcrheasnennetsl ntwumo bseirmpfrloemco3n2votlout1io.nIanl S = T fkgcm ; Conv FUP fkg+cm1 ; Conv FUP (fkg+cm2 ) ,

stage two, we leverage the initial saliency map S1 to reÔ¨Åne

(6)

the three cross-modal student features, which is deÔ¨Åned as: where S is the predicted saliency map, [x; y] denotes the

ficm = ficm + ficm S1,

concatenation operation of x and y, and k ‚àà {1, 3}. In the Ô¨Årst (2) stage, T denotes two sequential convolutional layers (i.e., T1),

where ficm (i ‚àà {1, 2, 3}) represents the reÔ¨Åned features and denotes the element-wise multiplication. After that, the three
reÔ¨Åned student features are aggregated by another decoder followed by a progressively transposed module (PTM), which is formulated as:

S2 = T2 FCD2(f1cm , f2cm , f3cm ) ,

(3)

while, for the second stage, it represents the PTM module (i.e., T2). The scale of the output of the second decoder is 88√ó88, which is 1/4 of the ground-truth (352√ó352), so directly upsampling the output to the size of the ground-truth will lose some details. To address this issue, we propose a simple yet effective progressively transposed module (PTM, Fig. 3 (b)) to generate the Ô¨Ånal predicted map (S2) in a progressive

where FCD2 is the second cascaded decoder, S2 denotes the Ô¨Ånal saliency map, and T2 represents the PTM module.

upsampling way. It consists of two residual-based transposed blocks [79] and three sequential 1 √ó 1 convolutions. Each residual-based transposed block contains a 3 √ó 3 convolution

‚Ä¢ Cascaded Decoder. After computing the two groups of multi-level cross-modal features ({ficm, fic+m1, fic+m2}, i ‚àà {1, 3}), which are a fusion of the RGB and depth features from multiple layers, we need to efÔ¨Åciently leverage the multi-scale multi-level information in each group to carry out the cascaded reÔ¨Ånement. Therefore, we introduce a light-weight cascaded decoder [29] to integrate the two groups of multi-level crossmodal features. As shown in Fig. 3 (a), the cascaded decoder consists of three global context modules (GCM) and a simple feature aggregation strategy. The GCM is reÔ¨Åned from the RFB module [78]. SpeciÔ¨Åcally, it contains an additional branch to enlarge the receptive Ô¨Åeld and a residual connection [69] to preserve the information. The GCM module thus includes four parallel branches. For all of these branches, a 1√ó1 convolution

and a residual-based transposed convolution. Note that the proposed cascaded reÔ¨Ånement mechanism
is different from the recent reÔ¨Ånement strategies CRN [80], SRM [81], R3Net [82], and RFCN [17] in its usage of the initial map and multi-level features. The obvious difference and advantage of the proposed design is that our model only requires one round of saliency reÔ¨Ånement to produce a good saliency map, while CRN, SRM, R3Net, and RFCN all need more iterations, which increases both the training time and computational resources. Besides, the proposed cascaded mechanism is also different from CPD [29] in that it exploits both the details in student features and the semantic information in teacher features, while suppressing the noise in the student features at the same time.

is Ô¨Årst applied to reduce the channel size to 32. Then, for

the kth (k ‚àà {2, 3, 4}) branch, a convolution operation with a kernel size of 2k ‚àí 1 and dilation rate of 1 is applied. This is followed by another 3 √ó 3 convolution operation with the dilation rate of 2k ‚àí 1. We aim to excavate the global contextual information from the cross-modal features. Next, the outputs of the four branches are concatenated together and a 3√ó3 convolution operation is then applied to reduce the channel number to 32. Finally, the concatenated features form a residual connection with the input features. The GCM module operation in the two cascaded decoders is denoted by:

figcm = FGCM (fi).

(4)

C. Depth-Enhanced Module (DEM)
To effectively fuse the RGB and depth features, two main problems need to be solved: a) the compatibility of RGB and depth features needs to be improved due to the intrinsic modality difference, and b) the redundancy and noise in lowquality depth maps must be reduced. Inspired by [83], we design a depth-enhanced module (DEM) to address the issues by improving the compatibility of multi-modal features and excavating informative cues from the depth features.
SpeciÔ¨Åcally, let firgb, fid represent the feature maps of the ith (i ‚àà 1, 2, ..., 5) side-out layer from the RGB and depth

To further improve the representations of cross-modal features,
we leverage a pyramid multiplication and concatenation fea-
ture aggregation strategy to aggregate the cross-modal features ({figcm, fig+cm1 , fig+cm2 }, i ‚àà {1, 3}). As illustrated in Fig. 3 (a), Ô¨Årst, each reÔ¨Åned feature figcm is updated by multiplying it with all higher-level features:

branches, respectively. As shown in Fig. 3, each DEM is added before each side-out feature map from the depth branch to enhance the compatibility of the depth features. This sideout process improves the saliency representation of depth features and, at the same time, preserves the multi-level multiscale information. The fusion process of the two modalities is depicted as:

figcm = figcm Œ†kkm =ai+x 1Conv FUP (fkgcm) , (5)

ficm = firgb + FDEM (fid),

(7)

in which i ‚àà {1, 2, 3}, kmax = 3 or i ‚àà {3, 4, 5}, kmax = 5. where ficm denotes the cross-modal features of the ith layer. FUP represents the upsampling operation if the features are The DEM module contains a sequential channel attention op-

IEEE TRANSACTIONS ON IMAGE PROCESSING

6

RGB Depth

Ôºç
BConv3

BConv3

BConv3

Depth Output

Element-wise Summation

Element-wise Multiplication

BConv3 Conv3√ó3+BN+ReLU

Ôºç

Element-wise Subtraction

Fig. 4. Architecture of the depth adapter module (DAM).

eration and a spatial attention operation, which are formulated

as:

FDEM (fid) = Satt Catt(fid) ,

(8)

in which Catt(¬∑) and Satt(¬∑) represent the spatial and channel attention operations, respectively. More speciÔ¨Åcally, the channel attention is implemented as:

Catt(f ) = M Pmax(f ) ‚äó f,

(9)

where Pmax(¬∑) denotes the global max pooling operation for each feature map, M(¬∑) represents a multi-layer (twolayer) perceptron, f denotes the input feature map, and ‚äó is the multiplication by the dimension broadcast. The spatial attention is denoted as:

Satt(f ) = Conv Rmax(f ) f,

(10)

where Rmax(¬∑) is the global max pooling operation for each point in the feature map along the channel axis. The proposed depth enhanced module is different from previous RGB-D algorithms, which fuse the multi-level cross-modal features by direct concatenation [18], [22], [28], enhance the multilevel depth features by a simple convolutional layer [19] or improve the depth map by contrast prior [21]. To the best of our knowledge, we are the Ô¨Årst to introduce the attention mechanism to excavate informative cues from depth features in multiple side-out layers. Our experiments (see Tab. VI and Fig. 8) demonstrate the effectiveness of our approach in improving the compatibility of multi-modal features.
Besides, the spatial and channel attention mechanisms are different from the operation proposed in [83]. Based on the fact that SOD aims at Ô¨Ånding the most prominent objects in an image, we only leverage a single global max pooling [84] to excavate the most critical cues in depth features, which reduces the complexity of the module.

D. Improve the efÔ¨Åciency of BBS-Net.
Note that the above proposed BBS-Net leverages two backbones, without sharing weights, to extract RGB features and depth features. Such a design can make the model extract discriminative RGB features and depth features, respectively, but also introduces more parameters, leading to a suboptimal solution for lightweight applications. However, making the two branches share weights can cause a big degradation of the performance. It may be because the RGB image and the

depth image are two different modalities, i.e., RGB image contains color, structure, and semantic information while the depth image includes the spatial distance information. Thus a naive sharing-weight mechanism of the two-branch backbones cannot be suitable to extract the multi-modal features. To solve this problem, we design a depth adapter module (DAM) to consider the modality difference of the RGB image and depth image. The same backbone can be suitable to extract twomodality features without decreasing much performance.
The whole architecture of the DAM is shown in Fig. 4. Let Irgb and Idepth denote the input RGB and depth image pair, respectively. We Ô¨Årst calculate the modality difference Idif by,

Idif = Conv(Irgb ‚àí Idepth),

(11)

where Idepth is broadcast to the same dimension as Irgb. Such an operation can make the model understand the explicit difference between the depth image and the RGB image. Then the adapted depth output is computed by:

Idif = Conv Conv(Idepth) + Conv(Idepth) ‚àó Idif . (12)

In the efÔ¨Åcient version of BBS-Net, the backbones of the two branches share parameters. When calculating the depth features, the depth image is Ô¨Årst fed to the DAM module to obtain the adapted depth information and is then fed to the backbone to extract features. To further reduce model parameters, we also remove the last progressively transposed module (which makes negligible performance degradation) in the efÔ¨Åcient version of BBS-Net.

E. Implementation Details

‚Ä¢ Training Loss. Let H and W denote the height and
width of the input images. Given the input RGB image X ‚àà RH√óW√ó3 and its corresponding depth map D ‚àà RH√óW√ó1, our model predicts an initial saliency map S1 ‚àà [0, 1]H√óW√ó1 and a Ô¨Ånal saliency map S2 ‚àà [0, 1]H√óW√ó1. Let G ‚àà {0, 1}H√óW√ó1 denote the binary ground-truth saliency map.
We jointly optimize the two cascaded stages by deÔ¨Åning the
total loss:

L = Œ± ce(S1, G) + (1 ‚àí Œ±) ce(S2, G),

(13)

in which ce represents the binary cross entropy loss [21] and Œ± ‚àà [0, 1] controls the trade-off between the two parts of the losses. The ce is computed as:

ce(S, G) = G log S + (1 ‚àí G) log(1 ‚àí S), (14)

where S is the predicted saliency map.

‚Ä¢ Training and Test Protocol. We use PyTorch [85] to implement our model on a single 1080Ti GPU. Parameters of the backbone network (ResNet-50 [69]) are initialized from the model pre-trained on ImageNet [86]. Other parameters are initialized using the default PyTorch settings. We discard the last pooling and fully connected layers of ResNet-50 and leverage each middle output of the Ô¨Åve convolutional blocks as the side-out feature maps. The two branches do not share weights and the only difference between them is that the depth

IEEE TRANSACTIONS ON IMAGE PROCESSING

7

Fig. 5. PR Curves of the proposed model and 18 SOTA algorithms over six datasets. Dots on the curves represent the value of precision and recall at the maximum F-measure.

branch has the input channel number set to one. Note for the efÔ¨Åcient version BBS-Net , the two branches share weights, and the depth images are Ô¨Årst fed to the depth adapter module to reduce the modality difference. The Adam algorithm [87] is used to optimize our model, the betas are set to 0.9 and 0.99, and the weight decay is set to 0. We set the initial learning rate to 1e-4 and divide it by 10 every 60 epochs. The gradients are clipped into [‚àí0.5, 0.5] to make the training stable. The input RGB and depth images are resized to 352 √ó 352 for both the training and test phases. We augment all the training images using multiple strategies (i.e., random Ô¨Çipping, rotating, and border clipping). It takes about ten hours to train the model with a mini-batch size of 10 for 150 epochs. Our experiments show that the model is robust to the hyper-parameter Œ±. Thus, we set Œ± to 0.5 (i.e., same importance for the two losses). In the test phase, the predicted maps are upsampled to the same dimension of ground truth by the bilinear interpolation and are then normalized to [0,1].

TABLE I PERFORMANCE OF DIFFERENT MODELS ON THE DUT [19] DATASET. MODELS ARE TRAINED AND TESTED ON THE DUT USING THE PROPOSED
TRAINING AND TEST SETS SPLIT FROM [19].

# Handcrafted Deep-based

Dataset Method
MB [90] LHM [32] DESM [35] DCMC [91] CDCP [36] DMRA [19] A2dele [92]
SSF [93] BBS-Net (ours)

SŒ± ‚Üë
.607 .568 .659 .499 .687 .888 .886 .916 .920

DUT [19] max FŒ≤ ‚Üë max EŒæ ‚Üë

.577

.691

.659

.767

.668

.733

.406

.712

.633

.794

.883

.927

.892

.929

.924

.951

.927

.955

TABLE II MULTIPLE COMPARISONS OF BBS-NET AND BBS-NET . THE EFFICIENT VERSION BBS-NET HAS ONLY AROUND 50 PERCENT
PARAMETERS OF BBS-NET.

# BBS-Net BBS-Net

Parameters (M) 49.77 25.96

FLOPs (G) 31.40 25.26

fps 24.32 25.54

IV. EXPERIMENTS AND RESULTS
A. Experimental Settings
‚Ä¢ Datasets. We conduct our experiments on eight challenging RGB-D SOD benchmark datasets: NJU2K [38], NLPR [32], STERE [39], DES [35], LFSD [88], SSD [89], SIP [37] and DUT [19]. NJU2K [38] is the largest RGBD dataset containing 1, 985 image pairs. NLPR [32] consists of 1, 000 image pairs captured by a standard Microsoft Kinect with a resolution of 640√ó480. STERE [39] is the Ô¨Årst stereoscopic photo collection, containing 1, 000 images downloaded from the Internet. DES [35] is a small-scale RGB-D dataset that includes 135 indoor image pairs. LFSD [88] contains 60 image pairs from indoor scenes and 40 image pairs from outdoor scenes. SSD [89] includes 80 images picked from three stereo movies with both indoor and outdoor scenes. The collected images have a high resolution of 960 √ó 1, 080.

SIP [37] consists of 1, 000 image pairs captured by a smart phone with a resolution of 992 √ó 744, using a dual camera. DUT [19] includes 1200 images from multiple challenging scenes (e.g., transparent objects, multiple objects, complex backgrounds and low-intensity environments).
‚Ä¢ Training/Testing. We follow the same settings as [19], [22] for fair comparison. In particular, the training set contains 1, 485 samples from the NJU2K dataset and 700 samples from the NLPR dataset. The test set consists of the remaining images from NJU2K (500) and NLPR (300), and the whole of STERE (1, 000), DES, LFSD, SSD and SIP. As for the recent proposde DUT [19] dataset, following [19], we adopt the same training data of DUT, NJU2K, and NLPR to train the compared deep models (i.e., DMRA [19], A2dele [92], SSF [93], and our BBS-Net) and test the performance on the

IEEE TRANSACTIONS ON IMAGE PROCESSING

8

SIP SSD LFSD DES STERE NLPR NJU2K Data

TABLE III
QUANTITATIVE COMPARISON OF MODELS USING S-MEASURE (SŒ± ), MAX F-MEASURE (maxFŒ≤ ), MAX E-MEASURE (maxEŒæ ) AND MAE (M ) SCORES ON SEVEN PUBLIC DATASETS. ‚Üë (‚Üì) DENOTES THAT THE HIGHER (LOWER) THE SCORE, THE BETTER. DENOTES THE EFFICIENT VERSION OF BBS-Net.

Metric

Hand-crafted-features-Based Models

CNNs-Based Models

BBS-Net

LHM CDB DESM GP CDCP ACSD LBE DCMC MDSF SE DF AFNet CTMF MMCI PCF TANet CPFP DMRA Ours Ours

[32] [94] [35] [95] [36] [38] [26] [91] [96] [25] [66] [30] [70] [23] [22] [18] [21] [19]

SŒ± ‚Üë .514 .624 .665 .527 .669 .699 .695 .686 .748 .664 .763 .772 .849 .858 .877 .878 .879 .886 .916 .921 maxFŒ≤ ‚Üë .632 .648 .717 .647 .621 .711 .748 .715 .775 .748 .650 .775 .845 .852 .872 .874 .877 .886 .918 .920 maxEŒæ ‚Üë .724 .742 .791 .703 .741 .803 .803 .799 .838 .813 .696 .853 .913 .915 .924 .925 .926 .927 .948 .949
M ‚Üì .205 .203 .283 .211 .180 .202 .153 .172 .157 .169 .141 .100 .085 .079 .059 .060 .053 .051 .038 .035
SŒ± ‚Üë .630 .629 .572 .654 .727 .673 .762 .724 .805 .756 .802 .799 .860 .856 .874 .886 .888 .899 .925 .930 maxFŒ≤ ‚Üë .622 .618 .640 .611 .645 .607 .745 .648 .793 .713 .778 .771 .825 .815 .841 .863 .867 .879 .909 .918 maxEŒæ ‚Üë .766 .791 .805 .723 .820 .780 .855 .793 .885 .847 .880 .879 .929 .913 .925 .941 .932 .947 .959 .961
M ‚Üì .108 .114 .312 .146 .112 .179 .081 .117 .095 .091 .085 .058 .056 .059 .044 .041 .036 .031 .026 .023
SŒ± ‚Üë .562 .615 .642 .588 .713 .692 .660 .731 .728 .708 .757 .825 .848 .873 .875 .871 .879 .835 .905 .908 maxFŒ≤ ‚Üë .683 .717 .700 .671 .664 .669 .633 .740 .719 .755 .757 .823 .831 .863 .860 .861 .874 .847 .898 .903 maxEŒæ ‚Üë .771 .823 .811 .743 .786 .806 .787 .819 .809 .846 .847 .887 .912 .927 .925 .923 .925 .911 .940 .942
M ‚Üì .172 .166 .295 .182 .149 .200 .250 .148 .176 .143 .141 .075 .086 .068 .064 .060 .051 .066 .043 .041
SŒ± ‚Üë .578 .645 .622 .636 .709 .728 .703 .707 .741 .741 .752 .770 .863 .848 .842 .858 .872 .900 .930 .933 maxFŒ≤ ‚Üë .511 .723 .765 .597 .631 .756 .788 .666 .746 .741 .766 .728 .844 .822 .804 .827 .846 .888 .921 .927 maxEŒæ ‚Üë .653 .830 .868 .670 .811 .850 .890 .773 .851 .856 .870 .881 .932 .928 .893 .910 .923 .943 .965 .966
M ‚Üì .114 .100 .299 .168 .115 .169 .208 .111 .122 .090 .093 .068 .055 .065 .049 .046 .038 .030 .022 .021
SŒ± ‚Üë .553 .515 .716 .635 .712 .727 .729 .753 .694 .692 .783 .738 .788 .787 .786 .801 .828 .839 .859 .864 maxFŒ≤ ‚Üë .708 .677 .762 .783 .702 .763 .722 .817 .779 .786 .813 .744 .787 .771 .775 .796 .826 .852 .855 .858 maxEŒæ ‚Üë .763 .871 .811 .824 .780 .829 .797 .856 .819 .832 .857 .815 .857 .839 .827 .847 .863 .893 .896 .901
M ‚Üì .218 .225 .253 .190 .172 .195 .214 .155 .197 .174 .145 .133 .127 .132 .119 .111 .088 .083 .076 .072
SŒ± ‚Üë .566 .562 .602 .615 .603 .675 .621 .704 .673 .675 .747 .714 .776 .813 .841 .839 .807 .857 .858 .882 maxFŒ≤ ‚Üë .568 .592 .680 .740 .535 .682 .619 .711 .703 .710 .735 .687 .729 .781 .807 .810 .766 .844 .827 .859 maxEŒæ ‚Üë .717 .698 .769 .782 .700 .785 .736 .786 .779 .800 .828 .807 .865 .882 .894 .897 .852 .906 .894 .919
M ‚Üì .195 .196 .308 .180 .214 .203 .278 .169 .192 .165 .142 .118 .099 .082 .062 .063 .082 .058 .058 .044
SŒ± ‚Üë .511 .557 .616 .588 .595 .732 .727 .683 .717 .628 .653 .720 .716 .833 .842 .835 .850 .806 .876 .879 maxFŒ≤ ‚Üë .574 .620 .669 .687 .505 .763 .751 .618 .698 .661 .657 .712 .694 .818 .838 .830 .851 .821 .880 .883 maxEŒæ ‚Üë .716 .737 .770 .768 .721 .838 .853 .743 .798 .771 .759 .819 .829 .897 .901 .895 .903 .875 .919 .922
M ‚Üì .184 .192 .298 .173 .224 .172 .200 .186 .167 .164 .185 .118 .139 .086 .071 .075 .064 .085 .056 .055

test set of DUT. Please refer to Tab. I for more details.
‚Ä¢ Evaluation Metrics. We employ Ô¨Åve widely used metrics, including S-measure (SŒ±) [97], E-measure (EŒæ) [98], Fmeasure (FŒ≤) [99], mean absolute error (MAE), and precisionrecall (PR) curves to evaluate various methods. Evaluation code: http://dpfan.net/d3netbenchmark/.
B. Comparison with SOTAs
‚Ä¢ Contenders. We compare the proposed BBS-Net with ten algorithms based on handcrafted features [25], [26], [32], [35], [36], [38], [91], [94]‚Äì[96] and eight methods [18], [19], [21]‚Äì [23], [30], [66], [70] that use deep learning. We train and test these methods using their default settings. For the methods without released source codes, we compare with their reported results.
‚Ä¢ Quantitative Results. As shown in Tab. I, Tab. III, our method outperforms all algorithms based on handcrafted features as well as SOTA CNN-based methods by a large margin, in terms of all four evaluation metrics (i.e., S-measure (SŒ±), F-measure (FŒ≤), E-measure (EŒæ) and MAE (M )). Performance gains over the best compared algorithms (ICCV‚Äô19 DMRA [19] and CVPR‚Äô19 CPFP [21]) are (2.5% ‚àº 3.5%, 0.7% ‚àº 3.9%, 0.8% ‚àº 2.3%, 0.009 ‚àº 0.016) for the metrics (SŒ±, maxFŒ≤, maxEŒæ, M ) on seven challenging datasets. The PR curves of different methods on various datasets are shown in Fig. 5. It can be easily deduced from the PR curves that our method (i.e., solid red lines) outperforms all the SOTA algorithms.
In terms of speed, BBS-Net achieves 24.32 fps on a single GTX 1080Ti GPU (batch size of one), as shown in Tab. II,

which is suitable for real-time applications. In terms of parameters, BBS-Net contains only around 50 percent parameters of the BBS-Net (i.e., 25.96M vs. 49.77M), but its performance is similar to the BBS-Net and also superior to other compared methods (as shown in the last two columns in Tab. III). It means that BBS-Net can process more images in the same time (with a larger batch size) for real-world applications.
There are three popular backbone models used in deep RGB-D models (i.e., VGG-16 [100], VGG-19 [100] and ResNet-50 [69]). To further validate the effectiveness of the proposed method, we provide performance comparisons using different backbones in Tab. IV. We Ô¨Ånd that ResNet50 performs best among the three backbones, and VGG-19 and VGG-16 have similar performances. Besides, the proposed method exceeds the SOTA methods (e.g., TANet [18], CPFP [21], and DMRA [19]) with any of the backbones.
‚Ä¢ Visual Comparison. Fig. 6 provides examples of maps predicted by our method and several SOTA algorithms. Visualizations cover simple scenes (a) and various challenging scenarios, including small objects (b), multiple objects (c), complex backgrounds (d), and low contrast scenes (e).
First, the Ô¨Årst row of (a) shows an easy example. The Ô¨Çower in the foreground is evident in the original RGB image, but the depth map is of low quality and contains some misleading information. The SOTA algorithms, such as DMRA and CPFP, fail to predict the whole extent of the salient object due to the interference from the depth map. Our method can eliminate the side-effects of the depth map by utilizing the complementary depth information more effectively. Second, two examples of small objects are shown in (b). Despite the handle of the teapot in the Ô¨Årst row being tiny, our method can accurately detect it. Third, we show two examples with multiple objects in an

IEEE TRANSACTIONS ON IMAGE PROCESSING

9

RGB

Depth

GT

(a)
Simple Scene

Ours

DMRA

CPFP

TANet

PCF

MMCI

CTMF

AFNet

DF

(b)
Small Objects

(c)
Multiple Objects

(d)
Complex Background

(e)
Low Contrast
Scene

Fig. 6. Qualitative visual comparison of our model versus eight SOTA models.

TABLE IV PERFORMANCE COMPARISON USING DIFFERENT BACKBONE MODELS. WE EXPERIMENT WITH MULTIPLE POPULAR BACKBONE MODELS USED IN RGB-D
SOD, INCLUDING VGG-16 [100], VGG-19 [100] AND RESNET-50 [69].

Models
TANet (VGG-16) [18] CPFP (VGG-16) [21] Ours (VGG-16) DMRA (VGG-19) [19] Ours (VGG-19) D3Net (ResNet-50) [37] Ours (ResNet-50)

NJU2K [38] SŒ± ‚Üë M ‚Üì
.878 .060 .879 .053 .916 .039 .886 .051 .918 .037 .900 .041 .921 .035

NLPR [32] SŒ± ‚Üë M ‚Üì
.886 .041 .888 .036 .923 .026 .899 .031 .925 .025 .912 .030 .930 .023

STERE [39] SŒ± ‚Üë M ‚Üì
.871 .060 .879 .051 .896 .046 .835 .066 .901 .043 .899 .046 .908 .041

DES [35] SŒ± ‚Üë M ‚Üì
.858 .046 .872 .038 .908 .028 .900 .030 .915 .026 .898 .031 .933 .021

LFSD [88] SŒ± ‚Üë M ‚Üì
.801 .111 .828 .088 .845 .080 .839 .083 .852 .074 .825 .095 .864 .072

SSD [89] SŒ± ‚Üë M ‚Üì
.839 .063 .807 .082 .858 .055 .857 .058 .855 .056 .857 .058 .882 .044

SIP [37] SŒ± ‚Üë M ‚Üì
.835 .075 .850 .064 .874 .056 .806 .085 .878 .054 .860 .063 .879 .055

image in (c). Our method locates all salient objects in the image. It segments the objects more accurately and generates sharper edges compared to other algorithms. Even though the depth map in the Ô¨Årst row of (c) lacks clear information, our algorithm predicts the salient objects correctly. Fourth, (d) shows two examples with complex backgrounds. Here, our method produces reliable results, while other algorithms confuse the background as a salient object. Finally, (e) presents two examples in which the contrast between the object and the background is low. Many algorithms fail to detect and segment the entire extent of the salient object. Our method produces satisfactory results by suppressing background distractors and exploring the informative cues from the depth map.
C. Ablation Study
‚Ä¢ Analysis of Different Aggregation Strategies. To validate the effectiveness of our cascaded reÔ¨Ånement mechanism, we conduct several experiments to explore different aggregation strategies. Results are shown in Tab. V and Fig.

7. ‚ÄòLow3‚Äô means that we only integrate the low-level features (Conv1‚àº3) using the decoder without the reÔ¨Ånement from the initial map. Low-level features contain abundant details that are beneÔ¨Åcial for reÔ¨Åning the object edges, but at the same time introduce a lot of background distraction. Integrating only low-level features produces inadequate results and generates many distractors (e.g., the example in Fig. 7). ‚ÄòHigh3‚Äô only integrates the high-level features (Conv3‚àº5) to predict the saliency map. Compared with low-level features, high-level features contain more semantic information. As a result, they help locate the salient objects and preserve edge information. Thus, integrating high-level features leads to better results. ‚ÄòAll5‚Äô aggregates features from all Ô¨Åve levels (Conv1‚àº5) directly, using a single decoder for training and testing. It achieves comparable results with the ‚ÄôHigh3‚Äô but may include background noise introduced by the low-level features (see column ‚ÄòAll5‚Äô in Fig. 7). ‚ÄòBBS-NoRF‚Äô indicates that we directly remove the reÔ¨Ånement Ô¨Çow of our model. This leads to poor performance. ‚ÄòBBS-RH‚Äô is a reverse reÔ¨Ånement strategy to our cascaded reÔ¨Ånement mechanism, where teacher features

IEEE TRANSACTIONS ON IMAGE PROCESSING

10

TABLE V COMPARISON OF FEATURE AGGREGATION STRATEGIES. 1: ONLY AGGREGATING THE LOW-LEVEL FEATURES (Conv1‚àº3), 2: ONLY AGGREGATING THE
HIGH-LEVEL FEATURES (Conv3‚àº5), 3: DIRECTLY INTEGRATING ALL FIVE-LEVEL FEATURES (Conv1‚àº5) BY A SINGLE DECODER, 4: OUR MODEL WITHOUT THE REFINEMENT FLOW, 5: HIGH-LEVEL FEATURES (Conv3‚àº5) ARE FIRST REFINED BY THE INITIAL MAP AGGREGATED BY LOW-LEVEL FEATURES (Conv1‚àº3) AND ARE THEN INTEGRATED TO GENERATE THE FINAL SALIENCY MAP, AND 6: OUR CASCADED REFINEMENT MECHANISM.

#

Settings

NJU2K [38] SŒ± ‚Üë M ‚Üì

NLPR [32] SŒ± ‚Üë M ‚Üì

STERE [39] SŒ± ‚Üë M ‚Üì

DES [35] SŒ± ‚Üë M ‚Üì

LFSD [88] SŒ± ‚Üë M ‚Üì

SSD [89] SŒ± ‚Üë M ‚Üì

SIP [37] SŒ± ‚Üë M ‚Üì

1

Low 3 levels

.881 .051 .882 .038 .832 .070 .853 .044 .779 .110 .805 .080 .760 .108

2 High 3 levels .902 .042 .911 .029 .886 .048 .912 .026 .845 .080 .850 .058 .833 .073

3

All 5 levels

.905 .042 .915 .027 .891 .045 .901 .028 .845 .082 .848 .060 .839 .071

4

BBS-NoRF

.893 .050 .904 .035 .843 .072 .886 .039 .804 .105 .839 .069 .843 .076

5

BBS-RH

.913 .040 .922 .028 .881 .054 .919 .027 .833 .085 .872 .053 .866 .063

6 BBS-RL (ours) .921 .035 .930 .023 .908 .041 .933 .021 .864 .072 .882 .044 .879 .055

TABLE VI ABLATION ANALYSIS OF OUR BBS-Net. ‚ÄòBM‚Äô = BASE MODEL. ‚ÄòCA‚Äô = CHANNEL ATTENTIO. ‚ÄòSA‚Äô = SPATIAL ATTENTION. ‚ÄòPTM‚Äô = PROGRESSIVELY
TRANSPOSED MODULE.

# BM CASettinSgAs PTM SNŒ±JU‚Üë2K M[38]‚Üì SNŒ±L‚ÜëPR [M32]‚Üì SSŒ±TE‚ÜëRE M[39]‚Üì SŒ±DE‚ÜëS [3M5] ‚Üì SLŒ±F‚ÜëSD [M88]‚Üì SŒ±SS‚ÜëD [8M9] ‚Üì SŒ±S‚ÜëIP [3M7] ‚Üì

1

.908 .045 .918 .029 .882 .055 .917 .027 .842 .083 .862 .057 .864 .066

2

.913 .042 .922 .027 .896 .048 .923 .025 .840 .086 .855 .057 .868 .063

3

.912 .045 .918 .029 .891 .054 .914 .029 .855 .083 .872 .054 .869 .063

4

.919 .037 .928 .026 .900 .045 .924 .024 .861 .074 .873 .052 .869 .061

5

.921 .035 .930 .023 .908 .041 .933 .021 .864 .072 .882 .044 .879 .055

RGB GT Low3 High3 All5 BBS-RH BBS-RL

Fig. 7. Visual comparison of aggregation strategies. ‚ÄòLow3‚Äô only integrates low-level features (Conv1‚àº3), while ‚ÄòHigh3‚Äô aggregates high-level features (Conv3‚àº5) for predicting the saliency map. ‚ÄòAll5‚Äô combines all Ô¨Åvelevel features directly for prediction. ‚ÄòBBS-RH/BBS-RL‚Äô denotes that highlevel/low-level features are Ô¨Årst reÔ¨Åned by the initial map aggregated by the low-level/high-level features and are then integrated to predict the Ô¨Ånal map.

RGB GT

#1

#2

#3

#4

#5

Fig. 8. Analysis of gradually adding various modules. The Ô¨Årst two columns are the RGB and ground-truth images, respectively. ‚Äò#‚Äô denotes the corresponding row of Tab. VI.
(Conv3‚àº5) are Ô¨Årst reÔ¨Åned by the initial map aggregated by low-level features (Conv1‚àº3) and are then integrated to generate the Ô¨Ånal saliency map. It performs worse than the proposed mechanism (BBS-RL), because noise in lowlevel features cannot be effectively suppressed in this reverse reÔ¨Ånement strategy. Besides, compared to ‚ÄòAll5‚Äô, our method fully utilizes the features at different levels, and thus achieves signiÔ¨Åcant performance improvement (i.e., the last row in Tab. V) with fewer background distractors and sharper edges.
‚Ä¢ Impact of Different Modules. To validate the effectiveness of the different modules in the proposed BBSNet, we conduct various experiments, as shown in Tab. VI and Fig. 8. The base model (BM) is our BBS-Net without additional modules (i.e., CA, SA, and PTM). Note that the BM alone performs better than the SOTA methods over almost all datasets, as shown in Tab. III and Tab. VI. Adding the channel attention (CA) and spatial attention (SA) modules enhances the performance on most of the datasets (see the results shown in the second and third rows of Tab. VI). When we combine the two modules (the fourth row in Tab. VI), the performance is

TABLE VII EFFECTIVENESS ANALYSIS OF THE CASCADED DECODER IN TERMS OF
THE S-MEASURE (SŒ± ) ON SEVEN DATASETS.
Methods NJU2K NLPR STERE DES SSD LFSD SIP [38] [32] [39] [35] [89] [88] [37]
Element-wise sum .915 .925 .897 .925 .868 .856 .880 Cascaded decoder .921 .930 .908 .933 .882 .864 .879

TABLE VIII HYPER-PARAMETER Œ± ANALYSIS ON THE NJU2K DATASET. WE DO NOT REPORT THE RESULT FOR Œ± = 1, BECAUSE ITS LOSS OF
THE FINAL PREDICTED MAP IS 0.

Œ±

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

NJU2K (SŒ±) .918 .925 .923 .919 .924 .923 .920 .923 .922 .924 NJU2K (MAE) .037 .034 .034 .036 .033 .034 .035 .034 .035 .033

greatly improved on all datasets, compared to the BM. We can easily conclude from the ‚Äò#2‚Äô, ‚Äò#3‚Äô and ‚Äò#4‚Äô columns in Fig. 8 that the spatial attention and channel attention mechanisms in DEM allow the model to focus on the informative parts of the depth features, which results in better suppression of background clutter. Finally, we add a progressively transposed block before the second decoder to gradually upsample the feature map to the same resolution as the ground truth. The results in the Ô¨Åfth row of Tab. VI and the ‚Äô#5‚Äô column of Fig. 8 show that the ‚ÄòPTM‚Äô achieves impressive performance gains on all datasets and generates sharper edges with Ô¨Åner details.
To further analyze the effectiveness of the cascaded decoder, we experiment with changing it to an element-wise summation mechanism. That is to say, we Ô¨Årst change the features from different layers to the same dimension using 1 √ó 1 convolution and upsampling operation and then fuse them by elementwise summation. Experimental results in Tab. VII show that the cascaded decoder achieves comparable results on SIP, and outperforms the element-wise sum on the other six datasets, which demonstrates its effectiveness.
‚Ä¢ Hyper-parameter Analysis. We conduct an experiment to discuss the settings of Œ±. As shown in Tab. VIII, the performance (SŒ± and MAE) is about the same for different

IEEE TRANSACTIONS ON IMAGE PROCESSING

11

TABLE IX EFFECTIVENESS ANALYSIS OF THE DEPTH ADAPTER MODULE IN TERMS
OF THE S-MEASURE (SŒ± ) ON SEVEN DATASETS. REPRESENTS THE EFFICIENT VERSION OF BBS-Net, WHERE THE TWO BACKBONES SHARE
PARAMETERS.
Settings NJU2K NLPR STERE DES SSD LFSD SIP [38] [32] [39] [35] [89] [88] [37]
BBS-Net (w/o DAM) .905 .922 .899 .928 .856 .841 .849 BBS-Net (w/ DAM) .916 .925 .905 .930 .858 .859 .876
TABLE X S-MEASURE (SŒ± ) COMPARISON WITH SOTA RGB SOD METHODS. ‚ÄòW/O
DEPTH‚Äô AND ‚ÄòW/ DEPTH‚Äô REPRESENT TRAINING AND TESTING THE PROPOSED METHOD WITHOUT/WITH THE DEPTH INFORMATION (i.e., THE
INPUTS OF THE DEPTH BRANCH ARE OR ARE NOT SET TO ZEROS).
Methods NJU2K NLPR STERE DES LFSD SSD SIP [38] [32] [39] [35] [88] [89] [37]
PiCANet [101] .847 .834 .868 .854 .761 .832 PAGRN [50] .829 .844 .851 .858 .779 .793 R3Net [82] .837 .798 .855 .847 .797 .815 CPD [29] .894 .915 .902 .897 .815 .839 .859 PoolNet [16] .887 .900 .880 .873 .787 .773 .861
BBS-Net (w/o depth) .914 .925 .915 .912 .836 .855 .875 BBS-Net (w/ depth) .921 .930 .908 .933 .864 .882 .879

RGB Depth GT

(a)

(b)

(c) Ours

Fig. 9. Feature visualization. Here, (a), (b), and (c) are the average RGB feature, depth feature and cross-modal feature of the Conv3 layer. To visualize them, we average the feature maps along their channel axis to obtain the visualization map. ‚ÄòOurs‚Äô refers to the BBS-Net (w/ depth).

TABLE XI PERFORMANCE COMPARISON (MAE) OF DIFFERENT POST-PROCESSING STRATEGIES ON SEVEN DATASETS. THE LAST COLUMN IS THE TIME FOR THE POST-PROCESSING METHODS TO OPTIMIZE EACH IMAGE. SEE ¬ß V-B
FOR DETAILS.

Strategy

NJU2K NLPR STERE DES LFSD SSD SIP time [38] [32] [39] [35] [88] [89] [37] ms

BBS-Net

.035 .023 .041 .021 .072 .044 .055 -

BBS-Net+ADP .050 .024 .049 .018 .072 .053 .055 1.46

BBS-Net+Ostu .030 .020 .036 .018 .066 .039 .051 0.99

BBS-Net+CRF .030 .020 .035 .019 .065 .038 .051 450.8

RGB

GT

BBS-Net BBS-Net+ADP BBS-Net+Ostu BBS-Net+CRF

values of Œ±, thus we simply set it to 0.5 to balance the weight between the losses of the initial map and the Ô¨Ånal map.
‚Ä¢ Effectiveness Analysis of the Depth Adapter Module. To demonstrate the effectiveness of the proposed depth adapter module (DAM), we conduct an experiment in Tab. IX. As shown in the table, BBS-Net (w/ DAM) performs better than BBS-Net (w/o DAM) on seven datasets, especially on the dataset of NJU2K, LFSD, and SIP. The DAM can model the modality difference between the RGB image and depth image, reduces the gap between them. Thus the same backbone is more suitable to extract two different modality features.
V. DISCUSSION
A. Utility of Depth Information
To explore whether depth information can really contribute to the performance of SOD, we conduct two experiments, results of which are shown in Tab. X. On the one hand, we compare the proposed method with Ô¨Åve SOTA RGB SOD methods (i.e., PiCANet [101], PAGRN [50], R3Net [82], CPD [29], and PoolNet [16]) by neglecting the depth information. We train and test CPD and PoolNet using the same training and test sets as our model. For other methods, we use the published results from [19]. It is clear that the proposed methods (i.e., BBS-Net (w/ depth)) can signiÔ¨Åcantly exceed SOTA RGB SOD methods thanks to depth information. On the other hand, we train and test the proposed method without using the depth information by setting the inputs of the depth branch to zero (i.e., BBSNet (w/o depth)). Comparing the results of the last two rows in the table, we Ô¨Ånd that depth information effectively improves the performance of the proposed model (especially over the small datasets, i.e., DES, LFSD, and SSD).
The two experiments together demonstrate the beneÔ¨Åts of the depth information for SOD. Depth map serves as prior knowledge and provides spatial distance information and contour guidance to detect salient objects. For example, in Fig.

Fig. 10. Visual effects of different post-processing methods. We explore three methods, including the adaptive threshold cut (‚ÄòADP‚Äô in the paper), Ostu‚Äôs method and the popular algorithm of conditional random Ô¨Åelds (CRF).
9, depth feature (b) has high activation on the object border. Thus, cross-modal feature (c) has clearer borders compared with the original RGB feature (a).
B. Analysis of Post-processing Methods
According to [102]‚Äì[104], the predicted saliency maps can be further reÔ¨Åned by post-processing methods. This may be useful to sharpen the salient edges and suppress the background response. We conduct several experiments to study the effects of various post-processing methods, including the adaptive threshold cut (i.e., the threshold is deÔ¨Åned as the double of the mean value of the saliency map), Ostu‚Äôs method [105], and conditional random Ô¨Åeld (CRF) [106]. The performance comparisons of the post-processing methods in terms of MAE are shown in Tab. XI, while a visual comparison is provided in Fig. 10.
From the results, we draw the following conclusions. First, the three post-processing methods all make the salient edges sharper, as shown in the fourth to sixth columns in Fig. 10. Second, both Ostu and CRF help reduce the MAE effectively, as shown in Tab. XI. This is possibly because they can suppress the background noise. As shown in Fig. 10, Ostu and CRF can signiÔ¨Åcantly reduce the background noise, while the adaptive threshold operation further expands the background blur from the original results of BBS-Net. Further, in terms of overall results, CRF performs the best, while the adaptive threshold algorithm is the worst. Ostu performs worse than CRF, because it cannot always fully eliminate the background noise (e.g., the Ô¨Åfth and sixth columns in Fig. 10).
C. Failure Case Analysis
We illustrate six representative failure cases in Fig. 11. The failure examples are divided into four categories. In the

IEEE TRANSACTIONS ON IMAGE PROCESSING

12

Ours GT Depth RGB

(a)

(b)

(c)

(d)

(e)

(f)

of the whole dataset. Therefore, adding more difÔ¨Åcult exam-

ples to the training data could help mitigate the failure cases.

Finally, depth maps may sometimes introduce misleading

information, such as in column (d). Considering how to exploit

salient cues from the RGB image to suppress the noise in the

depth map could be a promising solution.

Fig. 11. Some representative failure cases of the model.
Ô¨Årst category, the model either misses the salient object or detects it imperfectly. For example, in column (a), our model fails to detect the salient object even when the depth map has clear boundaries. This is because the salient object has the same texture and content layout as the background in the RGB image. Thus, the model cannot Ô¨Ånd the salient object based only on the borders. In column (b), our method cannot fully segment the transparent salient objects, since the background has low contrast, and the depth map lacks useful information. The second situation is that the model identiÔ¨Åes the background as the salient part. For example, the lanterns in column (c) have a similar color to the background wallpaper, which confuses the model into thinking that the wallpaper is the salient object. Besides, the background of the RGB image in column (d) is complex and thus our model does not detect the complete salient objects. The third type of failure case is when an image contains several separate salient objects. In this case, our model may not detect them all. As shown in column (e), with Ô¨Åve salient objects in the RGB images, the model fails to detect the two objects that are far from the camera. This may be because the model tends to consider the objects that are closer to the camera more salient. The Ô¨Ånal case is when salient objects are occluded by non-salient ones. Note that in column (f), the car is occluded by two ropes in front of the camera. Here our model predicts the ropes as salient objects.
Most of these failure cases can be attributed to interference information from the background (e.g., color, contrast, and content). We propose some ideas that may be useful for solving these failure cases. The Ô¨Årst is to introduce some humandesigned prior knowledge, such as providing a boundary that can approximately distinguish the foreground from the background. Leveraging such prior knowledge, the model may better capture the characteristics of the background and salient objects. This strategy may contribute signiÔ¨Åcantly to solving the failure cases especially for columns (a) and (b). Besides, the depth map can also be seen as a type of prior knowledge for this task. Thus, some failure cases (i.e., (b), (c), and (e)) may be solved when a high-quality depth map is available. Second, we Ô¨Ånd that in the current RGB-D datasets, the image pairs for challenging scenarios (e.g., complex backgrounds, lowcontrast backgrounds, transparent objects, multiple objects, shielded objects, and small objects) constitute a small fraction

D. Cross-Dataset Generalization Analysis
For a deep model to obtain reasonable performance in real-world scenarios, it not only requires an efÔ¨Åcient design but must also be trained on a high-quality dataset with a great generalization power. A good dataset usually contains sufÔ¨Åcient images, with all types of variations that occur in reality, so that deep models trained on it can generalize well to the real world. In the area of RGB-D SOD, there are several large-scale datasets (i.e., NJU2K, NLPR, STERE, SIP, and DUT), with around 1, 000 training images.
‚Ä¢ Single Dataset Generalization Analysis. Here, we conduct cross-dataset generalization experiments on the abovementioned Ô¨Åve datasets to measure their generalization ability. To make fair comparisons among multiple datasets, we balance the datasets with equal number of training samples. SpeciÔ¨Åcally, we randomly choose 700 image pairs in each dataset for training, and the remaining images are used for testing. We then retrain the proposed model on a single training set, and test it on all four test sets. The results are summarized in Tab. XII. ‚ÄòSelf‚Äô represents the results of training and testing on the same dataset. ‚ÄòMean Others‚Äô indicates the average performance on all test sets except ‚Äòself‚Äô. ‚ÄòDrop‚Äô means the (percent) drop from ‚ÄòSelf‚Äô to ‚ÄòMean Others‚Äô. First, it can be seen from the table that NJU2K and DUT are the hardest datasets since their ‚ÄòMean Others‚Äô of column ‚ÄòNJU2K‚Äô and ‚ÄòDUT‚Äô are signiÔ¨Åcantly lower than the other three datasets. This may be because the two datasets include multiple challenging scenes (e.g., transparent objects, multiple objects, complex backgrounds, etc). Second, STERE has the best generalization ability, because the average drop of SŒ± and FŒ≤ is lowest among all Ô¨Åve datasets. Besides, SIP generalizes worst (i.e., the drop is the largest among all Ô¨Åve datasets), since it mainly focuses on a single person or multiple persons in the wild. We also notice that the score of the SIP column (‚ÄòMean Others‚Äô) is the highest. This is likely because the quality of the depth maps captured by the Huawei Mate10 is higher than that produced by traditional devices. Finally, none of the models trained with a single dataset perform best over all test sets. Thus, we further explore training on different combinations of datasets with the aim of building a dataset with a strong generalization ability for future research.
‚Ä¢ Dataset Combination for Generalization Improvement. According to the results in Tab. XII, the model trained on the SIP dataset does not generalize well to other datasets, so we discard it. We thus select four relatively large-scale datasets, i.e., NJU2K, NLPR, STERE, and DUT, to conduct our multi-dataset training experiments. As shown in Tab. XIII, we consider all possible training combinations of these four datasets and test the models on all available test sets. From

IEEE TRANSACTIONS ON IMAGE PROCESSING

13

TABLE XII PERFORMANCE COMPARISON WHEN TRAINING WITH DIFFERENT DATASETS. THE NUMBER IN PARENTHESES DENOTES THE NUMBER OF THE
CORRESPONDING TRAINING AND TEST IMAGES. SEE ¬ß V-D FOR DETAILS.

Test Train
NJU2K (700) NLPR (700) STERE (700)
SIP (700) DUT (700) Mean Others

NJU2K (1285) SŒ± ‚Üë FŒ≤ ‚Üë
.902 .894 .712 .689 .779 .741 .436 .325 .751 .777 .670 .633

NLPR (300) SŒ± ‚Üë FŒ≤ ‚Üë
.834 .795 .919 .903 .897 .868 .618 .528 .808 .761 .789 .738

STERE (300) SŒ± ‚Üë FŒ≤ ‚Üë
.864 .846 .876 .882 .915 .913 .534 .479 .736 .764 .753 .743

SIP (229) SŒ± ‚Üë FŒ≤ ‚Üë
.802 .782 .883 .881 .900 .900 .963 .972 .801 .802 .847 .841

DUT (500) SŒ± ‚Üë FŒ≤ ‚Üë
.741 .691 .795 .779 .724 .731 .423 .303 .887 .877 .671 .626

Self SŒ± ‚Üë FŒ≤ ‚Üë

.902 .894 .919 .903 .915 .913 .963 .972 .887 .877

-

-

Mean Others SŒ± ‚Üë FŒ≤ ‚Üë

.810 .779 .817 .808 .825 .810 .503 .409 .774 .776

-

-

Drop ‚Üì

SŒ±

FŒ≤

10.2% 11.2% 9.8% 47.8% 12.7%
-

12.9% 10.5% 11.3% 57.9% 11.5%
-

TABLE XIII PERFORMANCE COMPARISON WHEN TRAINING WITH DIFFERENT COMBINATIONS OF MULTIPLE DATASETS. ‚ÄòNJ‚Äô, ‚ÄòNL‚Äô, ‚ÄòST‚Äô, ‚ÄòSI‚Äô AND ‚ÄòDU‚Äô REPRESENT
NJU2K, NLPR, STERE, SIP AND DUT, RESPECTIVELY. THE NUMBER IN PARENTHESES DENOTES THE NUMBER OF CORRESPONDING TRAINING AND TEST IMAGES. THE NUMBER OF TRAINING IMAGES FOR EACH DATASET IS 700. THE TRAINING AND TEST SETS WILL BE AVAILABLE AT: HTTPS://DRIVE.GOOGLE.COM/DRIVE/FOLDERS/1UYGYG50-0Y7I21TWREVMCBPATQPFPSR0?USP=SHARING.

Test Train

NJ (1285)

NL (300)

ST (300)

DES (135) LFSD (80)

SSD (80)

SI (229)

DU (500)

SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì SŒ± ‚Üë FŒ≤ ‚Üë M ‚Üì

NJ+NL (1,400) .911 .905 .039 .926 .916 .025 .899 .898 .044 .934 .932 .019 .865 .862 .070 .861 .836 .054 .890 .893 .048 .799 .748 .095 NJ+ST (1,400) .913 .909 .038 .885 .859 .040 .916 .912 .035 .927 .910 .022 .853 .836 .078 .869 .848 .054 .885 .882 .052 .729 .719 .118 NJ+DU (1,400) .906 .893 .043 .852 .802 .050 .875 .854 .053 .884 .859 .037 .861 .854 .070 .862 .839 .053 .834 .825 .075 .905 .903 .041 NL+ST (1,400) .781 .748 .097 .930 .919 .024 .919 .920 .032 .942 .938 .019 .672 .645 .161 .774 .722 .090 .895 .894 .047 .836 .819 .070 NL+DU (1,400) .777 .771 .104 .923 .908 .023 .878 .882 .052 .940 .936 .019 .717 .728 .135 .801 .774 .091 .886 .888 .054 .905 .903 .040 ST+DU (1,400) .821 .794 .082 .893 .863 .036 .917 .914 .034 .940 .935 .020 .762 .734 .125 .777 .736 .092 .907 .910 .039 .913 .914 .037 NJ+NL+ST (2,100) .913 .910 .038 .923 .904 .027 .922 .924 .033 .943 .939 .018 .865 .858 .072 .853 .818 .056 .902 .905 .043 .816 .780 .088 NJ+NL+DU (2,100) .911 .905 .041 .924 .909 .027 .902 .901 .043 .942 .939 .018 .865 .856 .067 .866 .838 .051 .894 .897 .048 .916 .915 .036 NJ+ST+DU (2,100) .910 .903 .041 .890 .867 .039 .923 .923 .031 .932 .918 .021 .859 .851 .073 .863 .838 .055 .896 .899 .046 .917 .916 .035 NL+ST+DU (2,100) .825 .808 .079 .924 .911 .026 .919 .920 .033 .946 .944 .017 .751 .732 .125 .797 .758 .082 .901 .905 .043 .916 .911 .036

NJ+NL+ST+DU (2,800) .912 .905 .039 .932 .917 .024 .921 .920 .033 .946 .942 .018 .864 .856 .070 .858 .829 .054 .903 .905 .042 .917 .913 .037

the results in the table, we draw the following conclusions. First, more training examples do not necessarily lead to better performance on some test sets. For example, although ‚ÄòNJ+NL+ST‚Äô, ‚ÄòNJ+NL+DU‚Äô and ‚ÄòNJ+NL+ST+DU‚Äô contain external training sets, unlike ‚ÄòNJ+NL‚Äô, they perform similarly with ‚ÄòNJ+NL‚Äô on the test set of ‚ÄòNL‚Äô. Second, including the NJU2K dataset is important for the model to generalize well to small datasets (i.e., LFSD, SSD). The model trained using the combinations without NJU2K (i.e., ‚ÄòNL+ST‚Äô ‚ÄòNL+DU‚Äô, ‚ÄòST+DU‚Äô and ‚ÄòNL+ST+DU‚Äô) all obtain low F-measure values (less than 0.8) on the LFSD and SSD test sets. In contrast, including ‚ÄòNJ‚Äô in the training sets increases the F-measures on the LFSD and SSD datasets by over 0.05. Finally, including more examples in the training sets can improve the stability of the model, as it allows diverse scenarios to be taken into consideration. Thus, the model trained on ‚ÄòNJ+NL+ST+DU‚Äô, which has the most examples, obtains the best, or are very close to the best, performance. Due to the limited size of current RGB-D datasets, it is hard for a model trained using a single dataset to perform well under various scenarios. Thus, we recommend training a model using a combination of datasets with diverse examples to avoid model over-Ô¨Åtting issues. To promote the development of RGB-D SOD, we hope more challenging RGB-D datasets with diverse examples and high-quality depth maps can be proposed in the future.
VI. CONCLUSION
In this paper, we present a Bifurcated Backbone Strategy Network (BBS-Net) for the RGB-D SOD. To effectively suppress the intrinsic distractors in low-level cross-modal features, we propose to leverage the characteristics of multi-level crossmodal features in a cascaded reÔ¨Ånement way: low-level features are reÔ¨Åned by the initial saliency map that is produced by the high-level cross-modal features. Besides, we introduce a

depth-enhanced module to excavate the informative cues from the depth features in the channel and spatial views, in order to improve the cross-modal compatibility when merging RGB and depth features. Experiments on eight challenging datasets demonstrate that BBS-Net outperforms 18 SOTA models, by a large margin, under multiple evaluation metrics. Finally, we conduct a comprehensive analysis of the existing RGBD datasets and introduce a powerful training set with a strong generalization ability for future research.
REFERENCES
[1] D.-P. Fan, Y. Zhai, A. Borji, J. Yang, and L. Shao, ‚ÄúBBS-Net: RGBD Salient Object Detection with a Bifurcated Backbone Strategy Network,‚Äù in ECCV, 2020, pp. 275‚Äì292.
[2] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, ‚ÄúSalient object detection: A benchmark,‚Äù IEEE TIP, vol. 24, no. 12, pp. 5706‚Äì5722, 2015.
[3] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, and R. Yang, ‚ÄúSalient object detection in the deep learning era: An in-depth survey,‚Äù IEEE TPAMI, 2021.
[4] M.-M. Cheng, Y. Liu, W. Lin, Z. Zhang, P. L. Rosin, and P. H. S. Torr, ‚ÄúBING: binarized normed gradients for objectness estimation at 300fps,‚Äù CVM, vol. 5, no. 1, pp. 3‚Äì20, 2019.
[5] M.-M. Cheng, Q. Hou, S. Zhang, and P. L.Rosin, ‚ÄúIntelligent visual media processing: When graphics meets vision,‚Äù JCST, vol. 32, no. 1, pp. 110‚Äì121, 2017.
[6] W. Wang, J. Shen, R. Yang, and F. Porikli, ‚ÄúSaliency-aware video object segmentation,‚Äù IEEE TPAMI, vol. 40, no. 1, pp. 20‚Äì33, 2017.
[7] M.-M. Cheng, F.-L. Zhang, N. J. Mitra, X. Huang, and S.-M. Hu, ‚ÄúRepÔ¨Ånder: Finding approximately repeated scene elements for image editing,‚Äù TOG, vol. 29, no. 4, pp. 83:1‚Äì83:8, 2010.
[8] D.-P. Fan, W. Wang, M.-M. Cheng, and J. Shen, ‚ÄúShifting more attention to video salient object detection,‚Äù in CVPR, 2019, pp. 8554‚Äì 8564.
[9] P. Yan, G. Li, Y. Xie, Z. Li, C. Wang, T. Chen, and L. Lin, ‚ÄúSemisupervised video salient object detection using pseudo-labels,‚Äù in ICCV, 2019, pp. 7284‚Äì7293.
[10] A. Borji, S. Frintrop, D. N.Sihite, and L. Itti, ‚ÄúAdaptive object tracking by learning background context,‚Äù in CVPRW, 2012, pp. 23‚Äì30.
[11] S. Hong, T. You, S. Kwak, and B. Han, ‚ÄúOnline tracking by learning discriminative saliency map with convolutional neural network,‚Äù in ICML, 2015, pp. 597‚Äì606.

IEEE TRANSACTIONS ON IMAGE PROCESSING

14

[12] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S.-M. Hu, ‚ÄúGlobal contrast based salient region detection,‚Äù IEEE TPAMI, vol. 37, no. 3, pp. 569‚Äì582, 2015.
[13] D. Zhang, D. Meng, and J. Han, ‚ÄúCo-saliency detection via a selfpaced multiple-instance learning framework,‚Äù IEEE TPAMI, vol. 39, no. 5, pp. 865‚Äì878, 2016.
[14] A. Borji and L. Itti, ‚ÄúState-of-the-art in visual attention modeling,‚Äù IEEE TPAMI, vol. 35, no. 1, pp. 185‚Äì207, 2012.
[15] A. Borji, ‚ÄúSaliency prediction in the deep learning era: Successes and limitations,‚Äù IEEE TPAMI, pp. 679‚Äì700, 2019.
[16] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang, ‚ÄúA simple pooling-based design for real-time salient object detection,‚Äù in CVPR, 2019, pp. 3917‚Äì3926.
[17] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan, ‚ÄúSalient object detection with recurrent fully convolutional networks,‚Äù IEEE TPAMI, vol. 41, no. 7, pp. 1734‚Äì1746, 2018.
[18] H. Chen and Y. Li, ‚ÄúThree-stream attention-aware network for RGBD salient object detection,‚Äù IEEE TIP, vol. 28, no. 6, pp. 2825‚Äì2835, 2019.
[19] Y. Piao, W. Ji, J. Li, M. Zhang, and H. Lu, ‚ÄúDepth-induced multi-scale recurrent attention network for saliency detection,‚Äù in ICCV, 2019, pp. 7254‚Äì7263.
[20] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, ‚ÄúSaliency detection on light Ô¨Åeld,‚Äù IEEE TPAMI, vol. 39, no. 8, pp. 1605‚Äì1616, 2016.
[21] J.-X. Zhao, Y. Cao, D.-P. Fan, M.-M. Cheng, X.-Y. Li, and L. Zhang, ‚ÄúContrast prior and Ô¨Çuid pyramid integration for RGBD salient object detection,‚Äù in CVPR, 2019, pp. 3927‚Äì3936.
[22] H. Chen and Y. Li, ‚ÄúProgressively complementarity-aware fusion network for RGB-D salient object detection,‚Äù in CVPR, 2018, pp. 3051‚Äì3060.
[23] H. Chen, Y. Li, and D. Su, ‚ÄúMulti-modal fusion network with multiscale multi-path and cross-modal interactions for RGB-D salient object detection,‚Äù IEEE TCYBERNETICS, vol. 86, pp. 376‚Äì385, 2019.
[24] G. Li, Z. Liu, L. Ye, Y. Wang, and H. Ling, ‚ÄúCross-Modal Weighting Network for RGB-D Salient Object Detection,‚Äù in ECCV, 2020, pp. 665‚Äì681.
[25] J. Guo, T. Ren, and J. Bei, ‚ÄúSalient object detection for RGB-D image via saliency evolution,‚Äù in IEEE ICME, 2016, pp. 1‚Äì6.
[26] D. Feng, N. Barnes, S. You, and C. McCarthy, ‚ÄúLocal background enclosure for RGB-D salient object detection,‚Äù in CVPR, 2016, pp. 2343‚Äì2350.
[27] Z. Liu, S. Shi, Q. Duan, W. Zhang, and P. Zhao, ‚ÄúSalient object detection for RGB-D image by single stream recurrent convolution neural network,‚Äù Neurocomputing, vol. 363, pp. 46‚Äì57, 2019.
[28] C. Zhu, X. Cai, K. Huang, T. H. Li, and G. Li, ‚ÄúPdnet: Prior-model guided depth-enhanced network for salient object detection,‚Äù in IEEE ICME, 2019, pp. 199‚Äì204.
[29] Z. Wu, L. Su, and Q. Huang, ‚ÄúCascaded partial decoder for fast and accurate salient object detection,‚Äù in CVPR, 2019, pp. 3907‚Äì3916.
[30] N. Wang and X. Gong, ‚ÄúAdaptive fusion for RGB-D salient object detection,‚Äù IEEE Access, vol. 7, pp. 55 277‚Äì55 284, 2019.
[31] R. Cong, J. Lei, H. Fu, Q. Huang, X. Cao, and N. Ling, ‚ÄúHSCS: Hierarchical sparsity based co-saliency detection for RGBD images,‚Äù IEEE TMM, vol. 21, no. 7, pp. 1660‚Äì1671, 2019.
[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, ‚ÄúRGBD salient object detection: a benchmark and algorithms,‚Äù in ECCV, 2014, pp. 92‚Äì109.
[33] X. Fan, Z. Liu, and G. Sun, ‚ÄúSalient region detection for stereoscopic images,‚Äù in DSP, 2014, pp. 454‚Äì458.
[34] Y. Fang, J. Wang, M. Narwaria, P. Le Callet, and W. Lin, ‚ÄúSaliency detection for stereoscopic images,‚Äù IEEE TIP, vol. 23, no. 6, pp. 2625‚Äì 2636, 2014.
[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, ‚ÄúDepth enhanced saliency detection method,‚Äù in ICIMCS, 2014, pp. 23‚Äì27.
[36] C. Zhu, G. Li, W. Wang, and R. Wang, ‚ÄúAn innovative salient object detection using center-dark channel prior,‚Äù in CVPRW, 2017, pp. 1509‚Äì 1515.
[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, ‚ÄúRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,‚Äù IEEE TNNLS, pp. 2075‚Äì2089, 2020.
[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, ‚ÄúDepth saliency based on anisotropic center-surround difference,‚Äù in ICIP, 2014, pp. 1115‚Äì1119.
[39] Y. Niu, Y. Geng, X. Li, and F. Liu, ‚ÄúLeveraging stereopsis for saliency analysis,‚Äù in CVPR, 2012, pp. 454‚Äì461.
[40] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum, ‚ÄúLearning to detect a salient object,‚Äù IEEE TPAMI, vol. 33, no. 2, pp. 353‚Äì367, 2010.

[41] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, ‚ÄúFrequency-tuned salient region detection,‚Äù in CVPR, 2009, pp. 1597‚Äì1604.
[42] D.-P. Fan, M.-M. Cheng, J.-J. Liu, S.-H. Gao, Q. Hou, and A. Borji, ‚ÄúSalient objects in clutter: Bringing salient object detection to the foreground,‚Äù in ECCV, 2018, pp. 186‚Äì202.
[43] G. Li and Y. Yu, ‚ÄúDeep contrast learning for salient object detection,‚Äù in CVPR, 2016, pp. 478‚Äì487.
[44] P. Zhang, D. Wang, H. Lu, H. Wang, and B. Yin, ‚ÄúLearning uncertain convolutional features for accurate saliency detection,‚Äù in ICCV, 2017, pp. 212‚Äì221.
[45] X. Zhao, Y. Pang, L. Zhang, H. Lu, and L. Zhang, ‚ÄúSuppress and balance: A simple gated network for salient object detection,‚Äù in ECCV, 2020, pp. 35‚Äì51.
[46] L. Itti, C. Koch, and E. Niebur, ‚ÄúA model of saliency-based visual attention for rapid scene analysis,‚Äù IEEE TPAMI, vol. 20, no. 11, pp. 1254‚Äì1259, 1998.
[47] G. Li and Y. Yu, ‚ÄúVisual saliency based on multiscale deep features,‚Äù in CVPR, 2015, pp. 5455‚Äì5463.
[48] M.-M. Cheng, J. Warrell, W.-Y. Lin, S. Zheng, V. Vineet, and N. Crook, ‚ÄúEfÔ¨Åcient salient region detection with soft image abstraction,‚Äù in ICCV, 2013, pp. 1529‚Äì1536.
[49] S. Chen, X. Tan, B. Wang, H. Lu, X. Hu, and Y. Fu, ‚ÄúReverse attentionbased residual network for salient object detection,‚Äù IEEE TIP, vol. 29, pp. 3763‚Äì3776, 2020.
[50] X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang, ‚ÄúProgressive attention guided recurrent network for salient object detection,‚Äù in CVPR, 2018, pp. 714‚Äì722.
[51] J. Su, J. Li, Y. Zhang, C. Xia, and Y. Tian, ‚ÄúSelectivity or invariance: Boundary-aware salient object detection,‚Äù in ICCV, 2019, pp. 3799‚Äì 3808.
[52] L. Zhang, J. Wu, T. Wang, A. Borji, G. Wei, and H. Lu, ‚ÄúA multistage reÔ¨Ånement network for salient object detection,‚Äù IEEE TIP, vol. 29, pp. 3534‚Äì3545, 2020.
[53] H. Li, G. Chen, G. Li, and Y. Yu, ‚ÄúMotion guided attention for video salient object detection,‚Äù in ICCV, 2019, pp. 7274‚Äì7283.
[54] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan, ‚ÄúAmulet: Aggregating multi-level convolutional features for salient object detection,‚Äù in ICCV, 2017, pp. 202‚Äì211.
[55] B. Wang, Q. Chen, M. Zhou, Z. Zhang, X. Jin, and K. Gai, ‚ÄúProgressive feature polishing network for salient object detection,‚Äù in AAAI, 2020, pp. 12 128‚Äì12 135.
[56] J. Wei, S. Wang, and Q. Huang, ‚ÄúF3net: Fusion, feedback and focus for salient object detection,‚Äù in AAAI, 2020, pp. 123 221‚Äì12 328.
[57] Y. Pan, T. Yao, H. Li, and T. Mei, ‚ÄúVideo captioning with transferred semantic attributes,‚Äù in CVPR, 2017, pp. 984‚Äì992.
[58] Z. Zhang, S. Fidler, and R. Urtasun, ‚ÄúInstance-level segmentation for autonomous driving with deep densely connected mrfs,‚Äù in CVPR, 2016, pp. 669‚Äì677.
[59] N. Xu, B. L. Price, S. Cohen, J. Yang, and T. S. Huang, ‚ÄúDeep interactive object selection,‚Äù in CVPR, 2016, pp. 373‚Äì381.
[60] S. Xie and Z. Tu, ‚ÄúHolistically-nested edge detection,‚Äù IJCV, vol. 125, no. 1-3, pp. 3‚Äì18, 2017.
[61] Y. Zhuge, G. Yang, P. Zhang, and H. Lu, ‚ÄúBoundary-guided feature aggregation network for salient object detection,‚Äù IEEE SPL, vol. 25, no. 12, pp. 1800‚Äì1804, 2018.
[62] J.-X. Zhao, J.-J. Liu, D.-P. Fan, Y. Cao, J. Yang, and M.-M. Cheng, ‚ÄúEGNet: Edge guidance network for salient object detection,‚Äù in ICCV, 2019, pp. 8779‚Äì8788.
[63] Z. Wu, L. Su, and Q. Huang, ‚ÄúStacked cross reÔ¨Ånement network for edge-aware salient object detection,‚Äù in ICCV, 2019, pp. 7264‚Äì7273.
[64] K. Desingh, K. Krishna, D. Rajanand, and C. Jawahar, ‚ÄúDepth really matters: Improving visual salient region detection with depth,‚Äù in BMVC, 2013, pp. 1‚Äì11.
[65] A. Ciptadi, T. Hermans, and J. M. Rehg, ‚ÄúAn in depth view of saliency,‚Äù in BMVC, 2013, pp. 1‚Äì11.
[66] L. Qu, S. He, J. Zhang, J. Tian, Y. Tang, and Q. Yang, ‚ÄúRGBD salient object detection via deep fusion,‚Äù IEEE TIP, vol. 26, no. 5, pp. 2274‚Äì 2285, 2017.
[67] R. Cong, J. Lei, H. Fu, J. Hou, Q. Huang, and S. Kwong, ‚ÄúGoing from RGB to RGBD saliency: A depth-guided transformation model,‚Äù IEEE TCYBERNETICS, pp. 1‚Äì13, 2019.
[68] R. Shigematsu, D. Feng, S. You, and N. Barnes, ‚ÄúLearning RGB-D salient object detection using background enclosure, depth contrast, and top-down features,‚Äù in CVPRW, 2017, pp. 2749‚Äì2757.
[69] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in CVPR, 2016, pp. 770‚Äì778.

IEEE TRANSACTIONS ON IMAGE PROCESSING

15

[70] J. Han, H. Chen, N. Liu, C. Yan, and X. Li, ‚ÄúCNNs-Based RGB-D saliency detection via cross-view transfer and multiview fusion,‚Äù IEEE TCYBERNETICS, vol. 48, no. 11, pp. 3171‚Äì3183, 2018.
[71] J. Zhang, D.-P. Fan, Y. Dai, S. Anwar, F. Sadat Saleh, T. Zhang, and N. Barnes, ‚ÄúUC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders,‚Äù in CVPR, 2020, pp. 8582‚Äì 8591.
[72] Y. Wang, Y. Li, J. H. Elder, H. Lu, and R. Wu, ‚ÄúSynergistic saliency and depth prediction for RGB-D saliency detection,‚Äù in ACCV, 2021.
[73] K. F. Fu, D.-P. Fan, G.-P. Ji, and Q. Zhao, ‚ÄúJL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection,‚Äù in CVPR, 2020, pp. 3052‚Äì3062.
[74] A. Luo, X. Li, F. Yang, Z. Jiao, H. Cheng, and S. Lyu, ‚ÄúCascade Graph Neural Networks for RGB-D Salient Object Detection,‚Äù in ECCV, 2020, pp. 346‚Äì364.
[75] X. Zhao, L. Zhang, Y. Pang, H. Lu, and L. Zhang, ‚ÄúA Single Stream Network for Robust and Real-time RGB-D Salient Object Detection,‚Äù in ECCV, 2020, pp. 646‚Äì662.
[76] C. Li, R. Cong, Y. Piao, Q. Xu, and C. C. Loy, ‚ÄúRGB-D Salient Object Detection with Cross-Modality Modulation and Selection,‚Äù in ECCV, 2020, pp. 225‚Äì241.
[77] T. Zhou, D.-P. Fan, M.-M. Cheng, J. Shen, and L. Shao, ‚ÄúRGB-D Salient Object Detection: A Survey,‚Äù IEEE TMM, 2021.
[78] S. Liu, D. Huang, and Y. Wang, ‚ÄúReceptive Ô¨Åeld block net for accurate and fast object detection,‚Äù in ECCV, 2018, pp. 404‚Äì419.
[79] X. Hu, K. Yang, L. Fei, and K. Wang, ‚ÄúACNet: Attention Based Network to Exploit Complementary Features for RGBD Semantic Segmentation,‚Äù in ICIP, 2019, pp. 1440‚Äì1444.
[80] Q. Chen and V. Koltun, ‚ÄúPhotographic image synthesis with cascaded reÔ¨Ånement networks,‚Äù in CVPR, 2017, pp. 1511‚Äì1520.
[81] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu, ‚ÄúA stagewise reÔ¨Ånement model for detecting salient objects in images,‚Äù in ICCV, 2017, pp. 4039‚Äì4048.
[82] Z. Deng, X. Hu, L. Zhu, X. Xu, J. Qin, G. Han, and P.-A. Heng, ‚ÄúR3Net: Recurrent residual reÔ¨Ånement network for saliency detection,‚Äù in IJCAI, 2018, pp. 684‚Äì690.
[83] S. Woo, J. Park, J.-Y. Lee, and I. So Kweon, ‚ÄúCBAM: Convolutional block attention module,‚Äù in ECCV, 2018, pp. 3‚Äì19.
[84] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, ‚ÄúIs object localization for free? - weakly-supervised learning with convolutional neural networks,‚Äù in CVPR, 2015, pp. 685‚Äì694.
[85] B. Steiner, Z. DeVito, S. Chintala, S. Gross, A. Paszke, F. Massa, A. Lerer, G. Chanan, Z. Lin, E. Yang et al., ‚ÄúPyTorch: An imperative style, high-performance deep learning library,‚Äù in NIPS, 2019, pp. 8024‚Äì8035.
[86] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation with deep convolutional neural networks,‚Äù in NIPS, 2012, pp. 1106‚Äì 1114.
[87] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù in ICLR, 2015.
[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, ‚ÄúSaliency detection on light Ô¨Åeld,‚Äù in CVPR, 2014, pp. 2806‚Äì2813.
[89] C. Zhu and G. Li, ‚ÄúA three-pathway psychobiological framework of salient object detection using stereoscopic technology,‚Äù in CVPRW, 2017, pp. 3008‚Äì3014.
[90] C. Zhu, G. Li, X. Guo, W. Wang, and R. Wang, ‚ÄúA multilayer backpropagation saliency detection algorithm based on depth mining,‚Äù in CAIP, 2017, pp. 14‚Äì23.
[91] R. Cong, J. Lei, C. Zhang, Q. Huang, X. Cao, and C. Hou, ‚ÄúSaliency detection for stereoscopic images based on depth conÔ¨Ådence analysis and multiple cues fusion,‚Äù IEEE SPL, vol. 23, no. 6, pp. 819‚Äì823, 2016.
[92] Y. Piao, Z. Rong, M. Zhang, W. Ren, and H. Lu, ‚ÄúA2dele: Adaptive and Attentive Depth Distiller for EfÔ¨Åcient RGB-D Salient Object Detection,‚Äù in CVPR, 2020, pp. 9060‚Äì9069.
[93] M. Zhang, W. Ren, Y. Piao, Z. Rong, and H. Lu, ‚ÄúSelect, Supplement and Focus for RGB-D Saliency Detection,‚Äù in CVPR, 2020, pp. 3472‚Äì 3481.
[94] F. Liang, L. Duan, W. Ma, Y. Qiao, Z. Cai, and L. Qing, ‚ÄúStereoscopic saliency model using contrast and depth-guided-background prior,‚Äù Neurocomputing, vol. 275, pp. 2227‚Äì2238, 2018.
[95] J. Ren, X. Gong, L. Yu, W. Zhou, and M. Ying Yang, ‚ÄúExploiting global priors for RGB-D saliency detection,‚Äù in CVPRW, 2015, pp. 25‚Äì32.
[96] H. Song, Z. Liu, H. Du, G. Sun, O. Le Meur, and T. Ren, ‚ÄúDepth-aware salient object detection and segmentation via multiscale discriminative

[97] [98] [99] [100] [101] [102] [103] [104] [105] [106]

saliency fusion and bootstrap learning,‚Äù IEEE TIP, vol. 26, no. 9, pp. 4204‚Äì4216, 2017. D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, and A. Borji, ‚ÄúStructuremeasure: A new way to evaluate foreground maps,‚Äù in ICCV, 2017, pp. 4548‚Äì4557. D.-P. Fan, C. Gong, Y. Cao, B. Ren, M.-M. Cheng, and A. Borji, ‚ÄúEnhanced-alignment measure for binary foreground map evaluation,‚Äù in IJCAI, 2018, pp. 698‚Äì‚Äì704. R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, ‚ÄúFrequency-tuned salient region detection,‚Äù in CVPR, 2009, pp. 1597‚Äì1604. K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556, 2014. N. Liu, J. Han, and M.-H. Yang, ‚ÄúPiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection,‚Äù in CVPR, 2018, pp. 3089‚Äì3098. J. Yang and M.-H. Yang, ‚ÄúTop-down visual saliency via joint crf and dictionary learning,‚Äù IEEE TPAMI, vol. 39, no. 3, pp. 576‚Äì588, 2016. W. Wang, S. Zhao, J. Shen, S. C. Hoi, and A. Borji, ‚ÄúSalient object detection with pyramid attention and salient edges,‚Äù in CVPR, 2019, pp. 1448‚Äì1457. Y. Zeng, Y. Zhuge, H. Lu, and L. Zhang, ‚ÄúJoint learning of saliency detection and weakly supervised semantic segmentation,‚Äù in ICCV, 2019, pp. 7223‚Äì7233. N. Otsu, ‚ÄúA threshold selection method from gray-level histograms,‚Äù IEEE SMC, vol. 9, no. 1, pp. 62‚Äì66, 1979. P. Kra¬®henbu¬®hl and V. Koltun, ‚ÄúEfÔ¨Åcient inference in fully connected crfs with gaussian edge potentials,‚Äù in NIPS, 2011, pp. 109‚Äì117.

