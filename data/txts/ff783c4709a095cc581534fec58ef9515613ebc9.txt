Published as a conference paper at ICLR 2021
REMEMBERING FOR THE RIGHT REASONS: EXPLANATIONS REDUCE CATASTROPHIC FORGETTING
Sayna Ebrahimi1, Suzanne Petryk1, Akash Gokul1, William Gan1, Joseph E. Gonzalez1, Marcus Rohrbach2, Trevor Darrell1 1UC Berkeley, 2 Facebook AI Research {sayna,spetryk,akashgokul,wjgan,jegonzal,trevordarrell}@berkeley.edu mrf@fb.com
ABSTRACT
The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the evidence for previously made decisions. As a Ô¨Årst step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has ‚Äúthe right reasons‚Äù for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularizationbased approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at https://github.com/ SaynaEbrahimi/Remembering-for-the-Right-Reasons.
1 INTRODUCTION
Humans are capable of continuously learning novel tasks by leveraging their lifetime knowledge and expanding them when they encounter a new experience. They can remember the majority of their prior knowledge despite the never-ending nature of their learning process by simply keeping a running tally of the observations distributed over time or presented in summary form. The Ô¨Åeld of continual learning or lifelong learning (Thrun & Mitchell, 1995; Silver et al., 2013) aims at maintaining previous performance and avoiding so-called catastrophic forgetting of previous experience (McCloskey & Cohen, 1989; McClelland et al., 1995) when learning new skills. The goal is to develop algorithms that continually update or add parameters to accommodate an online stream of data over time.
An active line of research in continual learning explores the effectiveness of using small memory budgets to store data points from the training set (Castro et al., 2018; Rajasegaran et al., 2020; RebufÔ¨Å et al., 2017; Wu et al., 2019), gradients (Lopez-Paz et al., 2017), or storing an online generative model that can fake them later (Shin et al., 2017). Memory has been also exploited in the form of accommodating space for architecture growth and storage to fully recover the old performance when needed (Ebrahimi et al., 2020b; Rusu et al., 2016). Some methods store an old snapshot of the model to distill the features (Li & Hoiem, 2016) or attention maps (Dhar et al., 2019) between the teacher and student models.
1

Published as a conference paper at ICLR 2021

‚Ñ≥rep y pred
Ltask( y, y pred)

‚Ñ≥rep+‚Ñ≥RRR y pred
s pred Ltask( y, y pred) + LRRR ( ≈ù, s pred)

Figure 1: An illustration of applying RRR paradigm. (Left) In a typical experience replay scenario, samples from prior tasks are kept in a memory buffer Mrep and revisited during training. (Right) In our proposed idea (RRR), in addition to Mrep, we also store model explanations (saliency maps) as MRRR for those samples and encourage the model to remember the original reasoning for the
prediction. Note that the saliency maps are small masks resulting in a negligible memory overhead
(see Section 4.1).

The internal reasoning process of deep models is often treated as a black box and remains hidden from the user. However, recent work in explainable artiÔ¨Åcial intelligence (XAI) has developed methods to create human-interpretable explanations for model decisions (Simonyan et al., 2013; Zhang et al., 2018; Petsiuk et al., 2018; Zhou et al., 2016; Selvaraju et al., 2017). We posit that the catastrophic forgetting phenomenon is due in part to not being able to rely on the same reasoning as was used for a previously seen observation. Therefore, we hypothesize that forgetting can be mitigated when the model is encouraged to remember the evidence for previously made decisions. In other words, a model which can remember its Ô¨Ånal decision and can reconstruct the same prior reasoning. Based on this approach, we develop a novel strategy to exploit explainable models for improving performance.
Among the various explainability techniques proposed in XAI, saliency methods have emerged as a popular tool to identify the support of a model prediction in terms of relevant features in the input. These methods produce saliency maps, deÔ¨Åned as regions of visual evidence upon which a network makes a decision. Our goal is to investigate whether augmenting experience replay with explanation replay reduces forgetting and how enforcing to remember the explanations will affect the explanations themselves. Figure 1 illustrates our proposed method.
In this work, we propose RRR, a training strategy guided by model explanations generated by any white-box differentiable explanation method; RRR adds an explanation loss to continual learning. White-box methods generate an explanation by using some internal state of the model, such as gradients, enabling their use in end-to-end training. We evaluate our approach using various popular explanation methods including vanilla backpropagation (Zeiler & Fergus, 2014), backpropagation with smoothing gradients (Smoothgrad) (Smilkov et al., 2017), Guided Backpropagation (Springenberg et al., 2014), and Gradient Class Activation Mapping (Grad-CAM) (Selvaraju et al., 2017) and compare their performance versus their computational feasibility. We integrate RRR into several state of the art class incremental learning (CIL) methods, including iTAML (Rajasegaran et al., 2020), EEIl (Castro et al., 2018), BiC (Wu et al., 2019), TOPIC (Tao et al., 2020), iCaRL (RebufÔ¨Å et al., 2017), EWC (Kirkpatrick et al., 2017), and LwF (Li & Hoiem, 2016). Note that RRR does not require task IDs at test time. We qualitatively and quantitatively analyze model explanations in the form of saliency maps and demonstrate that RRR remembers its earlier decisions in a sequence of tasks due to the requirement to focus on the the right evidence. We empirically show the effect of RRR in standard and few-shot class incremental learning (CIL) scenarios on popular benchmark datasets including CIFAR100, ImageNet100, and Caltech-UCSD Birds 200 using different network architectures where RRR improves overall accuracy and forgetting over experience replay and other memory-based method.
Our contribution is threefold: we Ô¨Årst propose our novel, simple, yet effective memory constraint, which we call Remembering for the Right Reasons (RRR), and show that it reduces catastrophic forgetting by encouraging the model to look at the same explanations it initially found for its decisions. Second, we show how RRR can be readily combined with memory-based and regularization-based
2

Published as a conference paper at ICLR 2021

CL methods to improve performance. Third, we demonstrate how guiding a continual learner to remember its explanations can improve the quality of the explanations themselves; i.e., the model looks at the right region in an image when making correct decisions while it focuses its maximum attention on the background when it misclassiÔ¨Åes an object.

2 BACKGROUND: WHITE-BOX EXPLANABILITY TECHNIQUES

Here we brieÔ¨Çy review the explainability methods we have evaluated our approach with. The core idea behind RRR is to guide explanations or saliency maps during training to preserve their values. Hence, only gradient-based saliency techniques can be used which are differentiable and hence trainable during training for the mainstream task as opposed to black-box saliency methods which can be used only after training to determine important parts of an image.

Vanilla Backpropagation (Zeiler & Fergus, 2014): The simplest way to understand and visualize which pixels are most salient in an image is to look at the gradients. This is typically done by making a forward pass through the model and taking the gradient of the given output class with respect to the input. Those pixel-wise derivative values can be rendered as a normalized heatmap representing the amount of change in the output probability of a class caused by perturbing that pixel. To store a saliency map for each RGB image of size 3 √ó W √ó H, we need an equivalent memory size of storing W √ó H pixel values.

Backpropagation with SmoothGrad: Smilkov et al. (2017) showed that the saliency maps obtained using raw gradients are visually noisy and using them as a proxy for feature importance is sub-optimal. They proposed a simple technique for denoising the gradients that adds pixel-wise Gaussian noise to n copies of the image, and simply averages the resulting gradients. SmoothGrad requires the same amount of memory to store the saliency maps while it takes n times longer to repeat generating each saliency map. We found n = 40 to be large enough to make a noticeable change in the saliencies in our experiments.

Gradient-weighted Class Activation Mapping (Grad-CAM) (Selvaraju et al., 2017): is a whitebox explainability technique which uses gradients to determine the inÔ¨Çuence of speciÔ¨Åc feature map activations on a given prediction. Because later layers in a convolutional neural network are known to encode higher-level semantics, taking the gradient of a model output with respect to the activations of these feature maps discovers which high-level semantics are important for the prediction. We refer to this layer as the target layer in our analysis. For example, when using Grad-CAM to visualize explanations for image classiÔ¨Åcation, taking the gradient of the correct class prediction with respect to the last convolutional layer highlights class-discriminative regions in the image (such as the wings of a bird when identifying bird species).

Consider the pre-softmax score yc for class c in an image classiÔ¨Åcation output. In general, any
differentiable activation can be used. Consider also a single convolutional layer with K feature maps, with a single feature map noted as Ak ‚àà Ru√óv. Grad-CAM takes the derivative of yc with respect to each feature map Ak. It then performs global average pooling over the height and width
dimensions for each of these feature map gradients, getting a vector of length K. Each element in this vector is used as a weight Œ±kc , indicating the importance of feature map k for the prediction yc. Next, these importance weights are used in computing a linear combination of the feature maps.
Followed by a ReLU (Jarrett et al., 2009) to zero-out any activations with a negative inÔ¨Çuence on the prediction of class c, the Ô¨Ånal Grad-CAM output (s) is as below with Akij deÔ¨Åned at location (i, j) in feature map Ak.

c

1 u v ‚àÇyc

c

K ck

Œ±k = uv

‚àÇAk

sGrad-CAM = ReLU

Œ±k A

(1)

i=1 j=1

ij

k=1

Unlike the common saliency map techniques of Guided BackProp (Springenberg et al., 2014), Guided GradCAM (Selvaraju et al., 2016), Integrated Gradients (Sundararajan et al., 2017b), Gradient Input (Shrikumar et al., 2016), Backpropagation with SmoothGrad (Smilkov et al., 2017) etc., vanilla Backpropagation and Grad-CAM pass important ‚Äúsanity checks‚Äù regarding their sensitivity to data and model parameters (Adebayo et al., 2018). We will compare using vanilla Backpropagation, Backpropagation with SmoothGrad, and Grad-CAM in RRR in Section 4. We will refer to the function that computes the output s of these saliency method as X AI.

3

Published as a conference paper at ICLR 2021

Algorithm 1 Remembering for the Right Reasons (RRR) for Continual Learning

1: function TRAIN (fŒ∏, Dtr, Dts)
2: T : # of tasks, n: # of samples in task 3: R ‚Üê 0 ‚àà RT √óT 4: Mrep ‚Üê {}
5: MRRR ‚Üê {}

function UPDATE MEM(fŒ∏k, Dktr, Mrep, MRRR) (xi, k, yi) ‚àº Dktr Mrep ‚Üê Mrep ‚à™ {(xi, k, yi)}
sÀÜ ‚Üê X AI(fŒ∏k(xi, k)) MRRR ‚Üê MRRR ‚à™ {sÀÜ}

6: for k = 1 to T do

7:

for i = 1 to n do

return Mrep, MRRR end function

8:

Compute cross entropy on task (Ltask)

9:

Compute LRRR using Eq. 2

10:

Œ∏ ‚Üê Œ∏ ‚àí Œ±‚àáŒ∏(Ltask+LRRR)

function EVAL(fŒ∏k, D{ts1¬∑¬∑¬∑k}) for i = 1 to k do

11: end for

Rk,i = Accuracy(fŒ∏k(x, i), y|‚àÄ(x, y) ‚àà Dits)

12:

Mrep, MRRR ‚Üê UPDATE MEM(fŒ∏k, Dktr, Mrep, end for

MRRR)

return R

13:

Rk,{1¬∑¬∑¬∑k} ‚Üê EVAL (fŒ∏k, D{ts1¬∑¬∑¬∑k})

end function

14: end for

15: return fŒ∏, R

16: end function

3 REMEMBERING FOR THE RIGHT REASONS (RRR)
Memory-based methods in continual learning have achieved high performance on vision benchmarks using a small amount of memory, i.e. storing a few samples from the training data into the replay buffer to directly train with them when learning new tasks. This simple method, known as experience replay, has been explored and shown to be effective (RebufÔ¨Å et al., 2017; Wu et al., 2019; Castro et al., 2018; Rajasegaran et al., 2020; Ebrahimi et al., 2020b; Hayes et al., 2019; Riemer et al., 2018). In this work we aim to go one step further and investigate the role of explanations in continual learning, particularly on mitigating forgetting and change of model explanations.
We consider the problem of learning a sequence of T data distributions Dtr = {D1tr, ¬∑ ¬∑ ¬∑ , DTtr}, where Dktr = {(xki , yik)ni=k1} is the data distribution for task k with n sample tuples of input (xk ‚äÇ X ) and set of output labels (yk ‚äÇ Y). The goal is to sequentially learn the model fŒ∏ : X √ó T ‚Üí Y for each task that can map each input x to its target output, y, while maintaining its performance on all prior tasks. We aim to achieve this by using memory to enhance better knowledge transfer as well as better avoidance of catastrophic forgetting. We assume two limited memory pools Mrep for raw samples and MRRR for model explanations. In particular, Mrep = {(xji , yij)m i=1 ‚àº Djtr=1¬∑¬∑¬∑k‚àí1} stores m samples in total from all prior tasks to k. Similarly MRRR stores the saliency maps generated based on fŒ∏k by one of the explanation methods (X AI) discussed in Section 2 for images in Mrep where fŒ∏k is fŒ∏ being trained for task k. We use a single-head architecture where the task ID integer t is not required at test time.
Upon Ô¨Ånishing the kth task, we randomly select m/(k‚àí1) samples per task from its training data and update our replay buffer memory Mrep. RRR uses model explanations on memory samples to perform continual learning such that the model preserves its reasoning for previously seen observations. We explore several explanation techniques to compute saliency maps using fŒ∏k for the stored samples in the replay buffer to populate the xai buffer memory Mxai. The stored saliency maps will serve as reference explanations during the learning of future tasks to prevent model parameters from being altered resulting in different reasoning for the same samples. We implement RRR using an L1 loss on the error in saliency maps generated after training a new task with respect to their stored reference evidence.

LRRR(fŒ∏, Mrep, MRRR) = E((x,y),sÀÜ)‚àº(Mrep,MRRR)||X AI(fŒ∏k(x)) ‚àí sÀÜ||1

(2)

where X AI(¬∑) denotes the explanation method used to compute saliency maps using the model
trained on the last seen example from task k, and sÀÜ are the reference saliency maps generated by X AI(fŒ∏k) upon learning each task prior to T and stored in to the memory. We show below that combining RRR into the objective function of state-of-the-art memory and regularization-based methods
results in signiÔ¨Åcant performance improvements. The full algorithm for RRR is given in Alg. 1.

4

Published as a conference paper at ICLR 2021

Accuracy (%) Accuracy (%)

80 10-way 5-shot on CUB200 in 11 Tasks

ResNet18-ER+RRR (Grad-CAM)

70

ResNet18-ER ResNet18-ER+RRR (SmoothGrad)

ResNet18-ER+RRR (Backprop)

60

ResNet18-Finetune AlexNet-ER+RRR (Grad-CAM)

AlexNet-ER

50

AlexNet-Finetune

SqueezeNet-ER+RRR (Grad-CAM)

40

SqueezeNet-ER SqueezeNet-Finetune

30

20

10 100 110 120 130 140 150 160 170 180 190 200
Number of classes

80 10-way 5-shot on CUB200 in 11 Tasks

TOPIC+RRR

iCaRL

70

TOPIC EEIL+RRR

ER+RRR ER

EEIL

Finetune

60

iCaRL+RRR

Joint

50

40

30

20

10 100 110 120 130 140 150 160 170 180 190 200
Number of classes

Figure 2: Few-shot CIL learning of CUB200 in 11 tasks where each point shows the classiÔ¨Åcation accuracy on all seen classes so far. (Left) Shows ER with and without LRRR using different backbone architectures and saliency map techniques. (Right) Performance of the state-of-the-art existing approaches with and without LRRR on CUB200 including TOPIC (Tao et al., 2020), EEIL (Castro et al., 2018), iCaRL (RebufÔ¨Å et al., 2017). Joint training serves as the upper bound. Results for baselines are obtained using their original implementation. All results are averaged over 3 runs and
mean and standard deviation values are given in the appendix. Best viewed in color.

4 EXPERIMENTS
In this section, we apply RRR in two distinct learning regimes: standard and few-shot class incremental learning. These are the most challenging CL scenarios, in which task descriptions are not available at test time. We Ô¨Årst explore the effect of backbone architecture and the saliency map technique on RRR performance. We then report our obtained results integrating LRRR into existing memory-based and regularization-based methods.
4.1 FEW-SHOT CIL PERFORMANCE
We Ô¨Årst explore CIL of low-data regimes where preventing overÔ¨Åtting to few-shot new classes is another challenge to overcome in addition to avoiding catastrophic forgetting of old classes. We use C classes and K training samples per class as the C-way K-shot few-shot class incrementally learning setting where we have a set of b base classes to learn as the Ô¨Årst task while the remaining classes are learned with only a few randomly selected samples. In order to provide a direct comparison to the state-of-the-art work of Tao et al. (2020) we precisely followed their setup and and used the same Caltech-UCSD Birds dataset (Wah et al., 2011), divided into 11 disjoint tasks and a 10-way 5-shot setting, where the Ô¨Årst task contains b = 100 base classes resulting in 3000 samples for training and 2834 images for testing. The remaining 100 classes are divided into 10 tasks where 5 samples per class are randomly selected as the training set, while the test set is kept intact containing near 300 images per task. The images in CUB200 are resized to 256 √ó 256 and then randomly cropped to 224 √ó 224 for training. We store 4 images per class from base classes in the Ô¨Årst task and 1 sample per each few-shot class in the remaining 10 tasks (Tao et al., 2020). We used the RAdam (Liu et al., 2019) optimizer with a learning rate of 0.001 which was reduced by 0.2 at epochs 20, 40, and 60 and trained for a total of 70 epochs with a batch size of 128 for the Ô¨Årst and 10 for the remaining tasks.
Figure 2 (left) shows results for ER with and without LRRR using different backbone architectures and saliency map techniques. Among the tested saliency map methods, Grad-CAM on ResNet18 outperforms Vanilla Backpropagation and SmoothGrad by 2-3% while SmoothGrad and vanilla Backpropagation achieve similar CL performance. To compute the memory overhead of storing the output for a saliency method, if we assume the memory required to store an image is M , vanilla Backpropagation and SmoothGrad generate a pixel-wise saliency map that occupies M/3 of memory. However, in Grad-CAM the saliency map size is equal to the feature map of the target layer in the architecture. In our study with Grad-CAM we chose our target layer to be the last convolution layer before the fully-connected layers. For instance using ResNet18 for colored 224 √ó 224 images results in the Grad-CAM output of 7 √ó 7 occupying 196B. Table 2 shows the target layer name and
5

Published as a conference paper at ICLR 2021

Accuracy (%) Accuracy (%) Accuracy (%)

100

CIFAR100 in 10 Tasks

80

60

40

20

iTAML+RRR
0 iTAML
BiC+RRR BiC

10

20

30

EEIL+RRR EEIL ER+RRR

ER LwF+RRR LwF

40 Numb50er of c6l0asses 70 80

(a)

EWC+RRR EWC Joint

90

100

100

CIFAR100 in 20 Tasks

80

60

40

20

iTAML+RRR

EEIL+RRR

ER

EWC+RRR

0 iTAML

EEIL

LwF+RRR

EWC

BiC+RRR

ER+RRR

LwF

Joint

BiC

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95100

Number of classes

(b)

100 80 60 40 20 0
10

ImageNet100 in 10 Tasks

iTAML+RRR iTAML BiC+RRR BiC
20 30

EEIL+RRR

ER+RRR

EEIL

ER

iCaRL+RRR

LwF+RRR

iCaRL

40 50 60 70 80

Number of classes

(c)

LwF Finetune Joint
90 100

Figure 3: Effect of RRR on existing methods for CIL on CIFAR100 in (a) 10 and (b) 20 tasks and (c) ImageNet100 in 10 tasks. Each point shows the classiÔ¨Åcation accuracy on all seen classes so far. Results for iTAML, BiC, and EEIL are produced with their original implementation while EWC and LwF are re-implemented by us. All results are averaged over 3 runs and mean and standard deviation values are given in the appendix. Best viewed in color.

saliency map size for other network architectures used in this work (AlexNet and SqueezeNet1 1) as well.
Figure 2 (right) shows the effect of adding LRRR on existing recent state-of-the-art methods such as TOPIC (Tao et al., 2020), EEIL (Castro et al., 2018), and iCaRL (RebufÔ¨Å et al., 2017). Tao et al. (2020) used a neural gas network (Martinetz et al., 1991; Fritzke et al., 1995) which can learn and preserve the topology of the feature manifold formed by different classes and we have followed their experimental protocol for our CUB200 experiment by using identical samples drawn in each task which are used across all the baselines for fair comparison. Adding LRRR improves the performance of all the baselines; TOPIC becomes nearly on-par with joint training which serves as the upper bound and does not adhere to continual learning. The gap between ER and iCaRL is also reduced when ER uses LRRR.
4.2 STANDARD CIL PERFORMANCE
In order to provide a direct comparison to the recent work of Rajasegaran et al. (2020) we perform our standard CIL experiment on CIFAR100 (Krizhevsky & Hinton, 2009) and ImageNet100 where we assume a memory budget of 2000 samples which are identical across all the baselines. Following Rajasegaran et al. (2020) we divide CIFAR100 to 10 and 20 disjoint tasks with 10 and 5 classes at a time. Figures 3a and 3b show the classiÔ¨Åcation accuracy upon learning each task on all seen classes. We see a consistent average improvement of 2 ‚àí 4% when LRRR is added as an additional constraint to preserve the model explanations across all methods, from the most naive memory-based method, experience replay (ER), to more sophisticated approaches which store a set of old class exemplars along with meta-learning (iTAML), correct bias for new classes (BiC), and Ô¨Åne tune on the exemplar set (EEIL). We also applied the RRR constraint on regularization-based methods such as EWC and LwF with no memory used as a replay buffer. The accuracy for both improves despite not beneÔ¨Åting from revisiting the raw data. However, they fall behind all memory-based methods with or without LRRR. The Ô¨Ånal accuracy on the entire sequence for joint training (multi-task learning) with RAdam optimizer (Liu et al., 2019) is 80.03% which serves as an upper bound as it has access to data from all tasks at all time.
Figure 3c shows our results on learning ImageNet100 in 10 tasks. The effect of adding LRRR to baselines on the ImageNet100 experiment is more signiÔ¨Åcant (3‚àí6%) compared to CIFAR100. This is mainly due to the larger size and better quality of images in ImageNet100, resulting in generating larger Grad-CAM saliency maps. These experiments clearly reveal the effectiveness of LRRR on model explanations in a continual learning problem at nearly zero cost of memory overhead when a memory buffer is already created and applied as a catastrophic forgetting avoidance strategy. This makes Grad-CAM the ideal approach to generate saliency maps when applying the RRR training strategy, as it achieves the highest accuracy while utilizing the least storage space to store saliencies. Note that we adopt Grad-CAM to generate saliency maps in the remaining experiments in this paper. We also keep using only ResNet18 for a fair comparison with the literature.
6

Published as a conference paper at ICLR 2021

Table 1: PG experiment results on few-shot CIL CUB200 measuring (a) PG-ACC (%) and PG-BWT (%) and (b) precision and recall averaged over all tasks. P ri,i and Rei,i evaluate the pointing game on each task ti directly after the model has been trained on ti. P rT,i and ReT,i are obtained by the evaluation for task ti using the model trained for all T tasks.

(a) PG localization accuracy and backward transfer

Methods
ER ER+RRR TOPIC TOPIC+RRR

PG-ACC (%)
54.0 58.5 72.7 74.2

PG-BWT (%)
-17.4 -15.6 -0.9 -2.1

(b) Precision and recall using PG experiment

Methods
ER ER+RRR TOPIC TOPIC+RRR

Precision

P ri,i P rT,i

80.0 82.1 91.0 92.8

68.9 70.3 88.4 89.1

Recall

Rei,i ReT,i

64.1 64.2 98.1 99.6

65.1 66.8 97.4 99.2

5 ANALYSIS OF MODEL EXPLANATIONS

In this section we want to answer the question ‚ÄúHow often does the model remember its decision for the right reason upon learning a sequence of tasks?‚Äù. In particular, we want to evaluate how often the model is ‚Äúpointing‚Äù at the right evidence for its predictions, instead of focusing its maximum attention on the background or other objects in the image. We use the Pointing Game experiment (PG) (Zhang et al., 2018) for this evaluation, which was introduced to measure the discriminativeness of a visualization method for target object localization. Here, we use ground truth segmentation annotation labels provided with the CUB-200 dataset to deÔ¨Åne the true object region.

First, we look into hits and misses deÔ¨Åned by the PG experiment. When the location of the maximum

in a predicted saliency map falls inside the object, it is considered as a hit and otherwise it is a

miss. Figure 5 shows an example from CUB200 where the segmentation annotation is used to

determine whether the peak of the predicted saliency map (marked with red cross) falls inside the

object region. This example is regarded as hit as the red cross is inside the segmentation mask

for the bird. PG localization accuracy is deÔ¨Åned as the number of hits over the total number of

predictions. We would like to measure both the overall PG performance of a continual learner as

well as how much learning new tasks causes it to forget its ability to hit the target object. For

these metrics, inspired by (Lopez-Paz et al., 2017), we deÔ¨Åne PG-ACC = T1

T i=1

RT ,i

as

the

average PG localization accuracy computed over all prior tasks after training for each new task and

PG-BWT = T ‚àí1 1

T ‚àí1 i=1

RT ,i

‚àí

Ri,i

(backward

transfer)

which

indicates

how

much

learning

new

tasks has inÔ¨Çuenced the PG localization accuracy on previous tasks where Rn,i is the on task i after

learning the nth task. Results for ER and TOPIC with and without LRRR on CUB200 are shown in

Table 1a. It shows how constraining different models to remember their initial evidence can lead to

better localization of the bird across learning new tasks.

However, PG performance does not capture all of our desired properties for a continual learner. Ideally, we not only want a model to predict the object correctly if it is looking at the right evidence, but also we want it to not predict an object if it is not able to Ô¨Ånd the right evidence for it. To measure how close our baselines can get to this ideal model when they are combined with LRRR, we measure the precision as tp/(tp+fp), and recall as tp/(tp+fn). We evaluate these metrics once immediately after learning each task, denoted as P ri,i and Rei,i, respectively, and again at the end of the learning process of Ô¨Ånal task T denoted as P rT,i and ReT,i, where the Ô¨Årst subscript refers to the model ID and the second subscript is the test dataset ID on which the model is evaluated. The higher the precision for a model is, the less often it has made the right decision without looking at the right evidence. On the other hand, the higher the recall, the less often it makes a wrong decision despite looking at the correct evidence. We show our evaluation on these metrics in Table 1b for ER and TOPIC with and without LRRR on CUB200 where LRRR increases both precision and recall across all methods, demonstrating that our approach continually makes better predictions because it Ô¨Ånds the right evidence for its decisions.

In our Ô¨Ånal analysis, we would like to visualize the evolution of saliency maps across learning a sequence of tasks. Figure 4 illustrates the evolution of saliency maps for an image from the test-set of the second task, which both ER without LRRR (top row) and with LRRR (bottom row) have seen during training for the second task. We only visualize the generated saliencies after Ô¨Ånishing tasks #2, #5, #7, #9, and #11 for simplicity. We indicate the correctness of the prediction made by each model with ‚Äòcorrect‚Äô or ‚Äòincorrect‚Äô written on top of their corresponding saliency map. Our goal is

7

Published as a conference paper at ICLR 2021

ER ùêÑùêë + ùêëùêëùêë

After Task 2 Correct

After Task 5 Correct

After Task 7 Incorrect

After Task 9 Incorrect

After Task 11 Incorrect

Correct

Incorrect

Correct

Correct

Correct

Figure 4: An illustration of the progression of saliencies on an image from the test set of the second task, evaluated after the model is trained on tasks #2, #5, #7, #9, and #11 on CUB200. Failure case for ER w.o. LRRR (top row), where saliency drifts from the original and the prediction becomes incorrect. ER+RRR (bottom row) retains close to the original saliency as the model trains on more tasks, with the exception of Task #5 which it is able to correct later on. Its performance is retained as well, for saliencies that are close to the original.
to visualize if adding the loss term LRRR prevents the drifting of explanations. Given the same input image, the ER without LRRR model makes an incorrect prediction after being continually trained for 11 tasks while never recovering from its mistake. On the other hand, when it is combined with LRRR. it is able to recover from an early mistake after task 5. Considering the saliency map obtained after Ô¨Ånishing task one as a reference evidence, we can see that ER‚Äôs evidence drifts further from the reference. On the bottom row, the region of focus of ER+RRR remains nearly identical to its initial evidence, apart from one incorrect prediction. As applying LRRR corrects its saliency back to the original, this prediction is corrected as well. This supports the conclusion that retaining the original saliency is important for retaining the original accuracy.
6 RELATED WORK
Continual learning: Past work in CL has generally made use of either memory, model structure, or regularization to prevent catastrophic forgetting. Memory-based methods store some form of past experience into a replay buffer. However, the deÔ¨Ånition of ‚Äúexperience‚Äù varies between methods. Rehearsal-based methods use episodic memories as raw samples (Robins, 1995; RebufÔ¨Å et al., 2017; Riemer et al., 2018) or their gradients (Lopez-Paz et al., 2017; Chaudhry et al., 2019) for the model to revisit. Incremental ClassiÔ¨Åer and Representation Learning (iCaRL) (RebufÔ¨Å et al., 2017), is a class-incremental learner that uses a nearest-exemplar algorithm for classiÔ¨Åcation and prevents catastrophic forgetting by using an episodic memory. iTAML (Rajasegaran et al., 2020) is a task-agnostic meta-learning algorithm that uses a momentum based strategy for meta-update and in addition to the object classiÔ¨Åcation task, it predicts task labels during inference. An end-to-end incremental learning framework (EEIL) (Castro et al., 2018) also uses an exemplar set along with data augmentation and balanced Ô¨Åne-tuning to alleviate the imbalance between the old and new classes. Bias Correction Method (BiC) (Wu et al., 2019) is another class-incremental learning algorithm for large datasets in which a linear model is used to correct bias towards new classes using a fully connected layer. In contrast, pseudo-rehearsal methods generate the replay samples using a generative model such as an autoencoder (Kemker & Kanan, 2017) or a GAN (Kamra et al., 2017; Shin et al., 2017). Regularization-based methods deÔ¨Åne different metrics to measure importance and limit the changes on parameters accordingly (Kirkpatrick et al., 2017; Zenke et al., 2017; Ebrahimi et al., 2020a; Serra et al., 2018; Li & Hoiem, 2016; Dhar et al., 2019) but in general these methods have limited capacity. Structure-based methods control which portions of a model are responsible for speciÔ¨Åc tasks such that the model increases its capacity in a controlled fashion as more tasks are added. Inference for different tasks can be restricted to various neurons (Fernando et al., 2017; Yoon et al., 2018), columns (Rusu et al., 2016), task-speciÔ¨Åc modules (Ebrahimi et al., 2020b), or parameters selected by a mask (Mallya & Lazebnik, 2018; Serra et al., 2018). In RRR we explored the addition of explanations to replay buffer and showed that saliency-based explanations offer performance upgrade as well as improvement in explanations across all memory-based and regularization-based baselines we tried.
8

Published as a conference paper at ICLR 2021
Visual explanation approaches or saliency methods attempt to produce a posterior explanation or a pseudo-probability map for the detected signals from the target object in the input image. These approaches can be broadly divided into three categories including activation, gradient, and perturbation based methods. Activation-based methods (Zhou et al., 2016; Selvaraju et al., 2017; Chattopadhay et al., 2018) use a weighted linear combination of feature maps whereas gradient-based methods (Baehrens et al., 2010; Sundararajan et al., 2017a; Springenberg et al., 2014; Shrikumar et al., 2017; Zhang et al., 2018) use the derivative of outputs w.r.t the input image to compute pixel-wise importance scores to generate attention maps. Methods in these categories are only applicable to differentiable models, including convolutional neural networks (CNNs). In contrast, perturbation-based methods are model-agnostic and produce saliency maps by observing the change in prediction when the input is perturbed (Petsiuk et al., 2018; Ribeiro et al., 2016; Ross et al., 2017; Zhou et al., 2014; Seo et al., 2018). While these methods attempt to identify if models are right for the wrong reason, Ross et al. (2017) took a step further and applied penalties to correct the explanations provided in supervised/unsupervised fashion during training. Selvaraju et al. (2019) used human explanations in the form of question and answering to bring model explanations closer to human answers.
7 CONCLUSION
In this paper, we proposed the use of model explanations with continual learning algorithms to enhance better knowledge transfer as well as better recall of the previous tasks. The intuition behind our method is that encouraging a model to remember its evidence will increase the generalisability and rationality of recalled predictions and help retrieving the relevant aspects of each task. We advocate for the use of explainable AI as a tool to improve model performance, rather than as an artifact or interpretation of the model itself. We demonstrate that models which incorporate a ‚Äúremember for the right reasons‚Äù constraint as part of a continual learning process can both be interpretable and more accurate. We empirically demonstrated the effectiveness of our approach in a variety of settings and provided an analysis of improved performance and explainability.
REFERENCES
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pp. 9505‚Äì9515, 2018.
David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and KlausRobert MAÀú zÀáller. How to explain individual classiÔ¨Åcation decisions. Journal of Machine Learning Research, 11(Jun):1803‚Äì1831, 2010.
Francisco M Castro, Manuel J Mar¬¥ƒ±n-Jime¬¥nez, Nicola¬¥s Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European conference on computer vision (ECCV), pp. 233‚Äì248, 2018.
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 839‚Äì847. IEEE, 2018.
Arslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. EfÔ¨Åcient lifelong learning with A-GEM. In International Conference on Learning Representations, 2019.
Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning without memorizing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5138‚Äì5146, 2019.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. In International Conference on Learning Representations, 2020a.
Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach. Adversarial continual learning. In European Conference on Computer Vision (ECCV), 2020b.
9

Published as a conference paper at ICLR 2021
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.
Bernd Fritzke et al. A growing neural gas network learns topologies. Advances in neural information processing systems, 7:625‚Äì632, 1995.
Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efÔ¨Åcient experience replay for streaming learning. In 2019 International Conference on Robotics and Automation (ICRA), pp. 9769‚Äì9776. IEEE, 2019.
Kevin Jarrett, Koray Kavukcuoglu, Marc‚ÄôAurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In 2009 IEEE 12th international conference on computer vision, pp. 2146‚Äì2153. IEEE, 2009.
Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual learning. arXiv preprint arXiv:1710.10368, 2017.
Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. arXiv preprint arXiv:1711.10563, 2017.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.
Zhizhong Li and Derek Hoiem. Learning without forgetting. In European Conference on Computer Vision, pp. 614‚Äì629. Springer, 2016.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019.
David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467‚Äì6476, 2017.
Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Thomas Martinetz, Klaus Schulten, et al. A‚Äù neural-gas‚Äù network learns topologies. 1991.
James L McClelland, Bruce L McNaughton, and Randall C O‚Äôreilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109‚Äì165. Elsevier, 1989.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.
Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. In Proceedings of the British Machine Vision Conference (BMVC), 2018.
Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Mubarak Shah. itaml : An incremental task-agnostic meta-learning approach. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
Sylvestre-Alvise RebufÔ¨Å, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiÔ¨Åer and representation learning. In CVPR, 2017.
10

Published as a conference paper at ICLR 2021
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ‚Äù why should i trust you?‚Äù explaining the predictions of any classiÔ¨Åer. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135‚Äì1144, 2016.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018.
Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2): 123‚Äì146, 1995.
Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. Right for the right reasons: Training differentiable models by constraining their explanations. arXiv preprint arXiv:1703.03717, 2017.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. Grad-cam: Why did you say that? arXiv preprint arXiv:1611.07450, 2016.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618‚Äì626, 2017.
Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. Taking a hint: Leveraging explanations to make vision and language models more grounded. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2591‚Äì2600, 2019.
Dasom Seo, Kanghan Oh, and Il-Seok Oh. Regional multi-scale approach for visually pleasing explanations of deep neural networks. arXiv preprint arXiv:1807.11720, 2018.
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4548‚Äì4557. PMLR, 2018.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990‚Äì2999, 2017.
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: Learning important features through propagating activation differences. arXiv preprint arXiv:1605.01713, 2016.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3145‚Äì3153. JMLR. org, 2017.
Daniel L Silver, Qiang Yang, and Lianghao Li. Lifelong machine learning systems: Beyond learning algorithms. In 2013 AAAI spring symposium series, 2013.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classiÔ¨Åcation models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vie¬¥gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
11

Published as a conference paper at ICLR 2021
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319‚Äì 3328. JMLR. org, 2017a.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365, 2017b.
Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, and Yihong Gong. Fewshot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12183‚Äì12192, 2020.
Sebastian Thrun and Tom M Mitchell. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25‚Äì46, 1995.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 374‚Äì382, 2019.
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. In International Conference on Learning Representations, 2018.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pp. 818‚Äì833. Springer, 2014.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3987‚Äì3995. PMLR, 2017.
Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126 (10):1084‚Äì1102, 2018.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. arXiv preprint arXiv:1412.6856, 2014.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2921‚Äì2929, 2016.
12

Published as a conference paper at ICLR 2021

A APPENDIX

A.1 GRAD-CAM TARGET LAYERS
Table 2 shows the target layer names used in Grad-CAM for different network architectures according to their standard PyTorch (Paszke et al., 2017) implementations. Saliency map size is equal to the activation map of the target layers.

Table 2: Target layer names and activation maps size for saliencies generated by different network architectures in Grad-CAM.

SqueezeNet1 1 AlexNet ResNet18

Target layer name in PyTorch torchvision package
features.0.12.expand3x3 features.0.10 features.7.1.conv2

Saliency map size
13 √ó 13 13 √ó 13 7√ó7

B POINTING GAME VISUALIZATION
Figure 5 shows an example from CUB200 in the Pointing Game. We used the segmentation annotation to verify whether the peak of the predicted saliency map (marked with red cross) falls inside the object region. It is regarded as hit as the red cross is inside the segmentation mask for the bird.

Figure 5: An example of PG evaluation as hit for an image in CUB200. Left: image saliency map overlaid on the image. Right: the segmentation label where the red cross marks the peak saliency.

C TABULAR RESULTS

In this section, we have tabulated results shown in Figure 2 and Figure 3 with means and standard deviations averaged over 3 runs.

Table 3: ClassiÔ¨Åcation accuracy of few-shot CIL learning of CUB200 at the end of 11 tasks for ER
with and without LRRR using different backbone architectures and saliency map techniques. Results are averaged over 3 runs. Figure 2 (left) in the main paper is generated using numbers in this Table.

RN18-RRR-GCam RN18-ER RN18-RRR-Smooth RN18-RRR-BP RN18-Finetune
Alex-RRR-GCam Alex-ER Alex-Finetune
SQ-RRR-GCam SQ-ER SQ-Finetune

1
67.8 ¬± 0.8 67.8 ¬± 0.8 67.8 ¬± 0.8 67.8 ¬± 0.8 67.8 ¬± 0.8
56.7 ¬± 0.7 56.7 ¬± 0.7 56.7 ¬± 0.7
46.8 ¬± 0.5 46.8 ¬± 0.5 46.8 ¬± 0.5

2
53.5 ¬± 0.7 49.7 ¬± 0.9 50.9 ¬± 0.6 50.8 ¬± 0.8 44.8 ¬± 0.6
46.6 ¬± 0.5 44.6 ¬± 0.7 42.8 ¬± 0.8
36.2 ¬± 0.4 33.2 ¬± 0.5 32.0 ¬± 0.7

3
45.6 ¬± 0.6 41.7 ¬± 0.8 43.5 ¬± 0.9 43.9 ¬± 0.6 32.2 ¬± 0.5
43.9 ¬± 0.7 41.3 ¬± 0.7 39.6 ¬± 0.8
30.1 ¬± 0.6 27.1 ¬± 0.6 25.2 ¬± 0.7

4
39.6 ¬± 0.7 35.8 ¬± 0.7 37.0 ¬± 0.8 36.6 ¬± 0.4 25.8 ¬± 0.7
41.3 ¬± 0.7 38.7 ¬± 0.7 36.9 ¬± 0.8
28.3 ¬± 0.4 25.3 ¬± 0.6 23.9 ¬± 0.7

5
35.3 ¬± 0.9 31.4 ¬± 0.9 33.0 ¬± 0.7 32.7 ¬± 0.6 25.6 ¬± 0.7
33.7 ¬± 0.5 31.1 ¬± 0.7 29.5 ¬± 0.7
25.1 ¬± 0.5 22.1 ¬± 0.5 20.2 ¬± 0.8

6
32.3 ¬± 1.1 28.5 ¬± 0.8 29.5 ¬± 0.6 28.9 ¬± 0.6 25.2 ¬± 0.7
27.4 ¬± 0.7 24.5 ¬± 0.7 23.3 ¬± 0.6
23.4 ¬± 0.5 20.5 ¬± 0.5 19.4 ¬± 0.4

7
29.4 ¬± 0.9 25.5 ¬± 0.8 26.8 ¬± 0.8 27.2 ¬± 0.5 20.8 ¬± 0.6
25.3 ¬± 0.7 22.6 ¬± 0.7 21.4 ¬± 0.8
19.3 ¬± 0.6 16.3 ¬± 0.4 14.9 ¬± 0.4

8
25.9 ¬± 0.8 22.1 ¬± 0.8 23.9 ¬± 0.8 23.8 ¬± 0.6 16.8 ¬± 0.7
22.0 ¬± 0.5 19.6 ¬± 0.6 17.9 ¬± 0.7
19.0 ¬± 0.6 16.0 ¬± 0.6 14.4 ¬± 0.5

9
25.7 ¬± 0.6 21.8 ¬± 0.8 23.9 ¬± 0.8 23.8 ¬± 0.6 18.8 ¬± 0.5
21.5 ¬± 0.6 19.1 ¬± 0.8 18.0 ¬± 0.7
18.5 ¬± 0.5 15.5 ¬± 0.6 13.8 ¬± 0.4

10
26.3 ¬± 0.7 22.5 ¬± 1.1 23.4 ¬± 0.8 24.0 ¬± 0.4 18.3 ¬± 0.4
21.4 ¬± 0.6 18.7 ¬± 0.8 17.0 ¬± 0.5
18.4 ¬± 0.5 15.4 ¬± 0.6 14.2 ¬± 0.5

11
23.6 ¬± 0.7 19.8 ¬± 0.9 21.5 ¬± 0.5 21.5 ¬± 0.6 17.1 ¬± 0.6
21.2 ¬± 0.6 19.1 ¬± 0.8 16.9 ¬± 0.4
18.2 ¬± 0.6 15.2 ¬± 0.7 13.7 ¬± 0.6

13

Published as a conference paper at ICLR 2021

Table 4: Performance of the state-of-the-art existing approaches with and without LRRR on CUB200 including TOPIC (Tao et al., 2020), EEIL (Castro et al., 2018), iCaRL (RebufÔ¨Å et al., 2017). Results for baselines are obtained using their original implementation. Results are averaged over 3 runs. Figure 2 (right) in the main paper is generated using numbers in this Table.

EEIL EEIL+RRR iCaRL iCaRL+RRR TOPIC TOPIC+RRR FT ER RRR
JT

1
68.6 ¬± 0.4 68.6 ¬± 0.4 68.6 ¬± 0.4 68.6 ¬± 0.4 68.6 ¬± 0.4 68.6 ¬± 0.4 68.6 ¬± 0.4 67.8 ¬± 0.8 67.8 ¬± 0.8
68.6 ¬± 0.4

2
53.6 ¬± 0.4 56.6 ¬± 0.5 52.6 ¬± 0.7 55.6 ¬± 1.2 62.4 ¬± 0.8 62.5 ¬± 0.9 44.8 ¬± 0.5 49.7 ¬± 0.9 53.5 ¬± 0.7
62.4 ¬± 0.4

3
47.9 ¬± 0.3 50.9 ¬± 0.6 48.6 ¬± 1.2 53.6 ¬± 0.7 54.8 ¬± 0.4 56.8 ¬± 0.4 32.2 ¬± 0.8 41.7 ¬± 0.8 45.6 ¬± 0.6
57.2 ¬± 0.4

4
44.2 ¬± 0.8 48.3 ¬± 0.5 44.1 ¬± 0.5 47.1 ¬± 0.8 49.9 ¬± 1.2 51.5 ¬± 0.5 25.8 ¬± 0.4 35.8 ¬± 0.7 39.6 ¬± 0.7
52.8 ¬± 0.5

5
36.3 ¬± 0.9 39.7 ¬± 1.2 36.6 ¬± 0.3 39.6 ¬± 0.5 45.2 ¬± 0.6 48.2 ¬± 0.4 25.6 ¬± 1.1 31.4 ¬± 0.9 35.3 ¬± 0.9
49.5 ¬± 0.9

6
27.4 ¬± 1.2 31.4 ¬± 0.7 29.5 ¬± 0.9 32.5 ¬± 0.8 41.4 ¬± 0.3 44.4 ¬± 0.4 25.2 ¬± 0.7 28.5 ¬± 0.8 32.3 ¬± 1.1
46.1 ¬± 0.5

7
25.9 ¬± 0.7 28.3 ¬± 1.2 27.8 ¬± 0.4 31.8 ¬± 0.4 38.3 ¬± 0.8 42.3 ¬± 0.7 20.8 ¬± 1.1 25.5 ¬± 0.8 29.4 ¬± 0.9
42.8 ¬± 1.1

8
24.7 ¬± 0.5 28.0 ¬± 0.6 26.2 ¬± 0.5 29.2 ¬± 0.6 35.3 ¬± 0.6 38.3 ¬± 0.6 16.7 ¬± 0.4 22.1 ¬± 0.8 25.9 ¬± 0.8
40.1 ¬± 0.8

9
23.9 ¬± 0.7 26.5 ¬± 0.6 24.0 ¬± 0.6 27.0 ¬± 0.8 32.2 ¬± 0.3 35.2 ¬± 0.9 18.8 ¬± 1.1 21.8 ¬± 0.6 25.7 ¬± 0.6
38.7 ¬± 0.7

10
24.1 ¬± 0.7 27.4 ¬± 0.6 23.8 ¬± 0.6 27.8 ¬± 0.6 28.3 ¬± 0.6 32.3 ¬± 0.9 18.2 ¬± 0.3 22.5 ¬± 1.1 26.3 ¬± 0.7
37.1 ¬± 0.5

11
22.1 ¬± 0.5 25.2 ¬± 0.9 21.1 ¬± 0.7 24.1 ¬± 0.3 26.2 ¬± 1.2 29.2 ¬± 0.5 17.1 ¬± 0.8 19.8 ¬± 0.9 23.6 ¬± 0.7
35.6 ¬± 0.9

Table 5: Performance of the state-of-the-art existing approaches with and without LRRR on CIFAR100 in 10 tasks. Results for iTAML (Rajasegaran et al., 2020), BiC (Wu et al., 2019), and EEIL (Castro et al., 2018) are produced with their original implementation while EWC (Kirkpatrick et al., 2017) and LwF (Li & Hoiem, 2016) are re-implemented by us. Results are averaged over 3 runs. Figure 3a in the main paper is generated using numbers in this Table.

iTAML+RRR iTAML BiC BiC+RRR EEIL EEIL+RRR LwF LwF+RRR EWC EWC+RRR ER RRR

1
89.2 ¬± 0.5 89.2 ¬± 0.5 90.3 ¬± 0.7 90.3 ¬± 0.7 80.0 ¬± 0.7 80.0 ¬± 0.7 86.1 ¬± 1.2 86.1 ¬± 1.2 86.1 ¬± 1.2 86.1 ¬± 1.2 86.1 ¬± 1.2 86.1 ¬± 1.2

2
92.3 ¬± 0.7 88.9 ¬± 0.5 82.1 ¬± 0.7 84.9 ¬± 1.1 80.5 ¬± 1.2 83.5 ¬± 0.3 69.0 ¬± 0.7 72.4 ¬± 0.8 52.6 ¬± 0.4 56.0 ¬± 0.4 74.5 ¬± 0.9 78.5 ¬± 0.9

3
89.5 ¬± 1.2 87.0 ¬± 1.1 75.1 ¬± 0.4 76.4 ¬± 0.6 75.5 ¬± 0.9 78.7 ¬± 1.2 55.0 ¬± 0.3 57.0 ¬± 1.1 48.6 ¬± 0.4 53.9 ¬± 1.2 65.2 ¬± 0.8 69.2 ¬± 1.1

4
87.5 ¬± 1.2 85.7 ¬± 1.1 69.8 ¬± 1.2 69.3 ¬± 0.3 71.5 ¬± 0.4 74.0 ¬± 1.2 45.8 ¬± 0.3 48.3 ¬± 0.3 38.5 ¬± 0.5 44.4 ¬± 0.9 62.5 ¬± 0.8 63.5 ¬± 1.2

5
84.1 ¬± 0.8 84.1 ¬± 1.1 65.3 ¬± 0.8 65.1 ¬± 0.9 68.0 ¬± 1.2 71.7 ¬± 0.3 40.4 ¬± 0.5 43.2 ¬± 0.8 31.1 ¬± 0.9 35.1 ¬± 0.5 56.7 ¬± 0.7 58.7 ¬± 0.8

6
83.5 ¬± 0.9 81.8 ¬± 0.3 61.3 ¬± 0.9 63.3 ¬± 0.4 62.0 ¬± 0.9 65.1 ¬± 0.4 36.7 ¬± 0.9 39.3 ¬± 0.5 26.5 ¬± 0.3 28.6 ¬± 0.6 50.5 ¬± 0.3 53.5 ¬± 1.1

7
83.9 ¬± 0.7 80.0 ¬± 0.6 57.4 ¬± 0.7 59.7 ¬± 1.1 59.0 ¬± 0.7 61.2 ¬± 0.5 30.8 ¬± 0.7 34.1 ¬± 0.6 21.7 ¬± 0.6 25.1 ¬± 1.1 47.6 ¬± 0.4 49.6 ¬± 0.7

8
81.2 ¬± 0.3 79.0 ¬± 0.3 54.9 ¬± 0.5 55.4 ¬± 0.8 55.1 ¬± 1.2 57.6 ¬± 0.5 28.6 ¬± 0.5 32.1 ¬± 1.1 20.0 ¬± 0.7 23.1 ¬± 0.5 43.4 ¬± 0.3 44.4 ¬± 0.3

9
79.6 ¬± 0.9 78.6 ¬± 0.8 53.2 ¬± 0.9 55.8 ¬± 0.7 51.4 ¬± 0.8 54.1 ¬± 0.4 26.1 ¬± 0.7 29.8 ¬± 0.7 18.9 ¬± 0.5 18.8 ¬± 0.9 41.6 ¬± 0.9 42.6 ¬± 1.2

10
79.7 ¬± 0.5 77.8 ¬± 0.6 50.3 ¬± 0.7 52.1 ¬± 0.5 48.7 ¬± 0.4 51.7 ¬± 0.3 24.2 ¬± 0.7 27.1 ¬± 0.6 16.6 ¬± 0.9 19.0 ¬± 1.2 38.1 ¬± 1.1 39.1 ¬± 1.1

Table 7: Performance of the state-of-the-art existing approaches with and without LRRR on ImageNet100 in 10 tasks. Results for iTAML (Rajasegaran et al., 2020), BiC (Wu et al., 2019), and EEIL (Castro et al., 2018) are produced with their original implementation while EWC (Kirkpatrick et al., 2017) and LwF (Li & Hoiem, 2016) are re-implemented by us. Results are averaged over 3 runs. Figure 3c in the main paper is generated using numbers in this Table.

iTAML iTAML+RRR EEIL EEIL+RRR BiC BiC+RRR iCaRL iCaRL+RRR LwF LwF+RRR FT ER RRR

1
99.4 ¬± 0.8 99.4 ¬± 0.8 99.5 ¬± 0.4 99.5 ¬± 0.4 98.3 ¬± 0.7 98.3 ¬± 0.7 99.3 ¬± 0.4 99.3 ¬± 0.4 99.3 ¬± 0.5 99.3 ¬± 0.5 99.3 ¬± 0.5 99.3 ¬± 0.5 99.3 ¬± 0.5

2
96.4 ¬± 0.9 97.3 ¬± 0.5 98.8 ¬± 1.1 98.1 ¬± 0.7 94.9 ¬± 0.8 98.9 ¬± 0.3 97.2 ¬± 0.9 97.9 ¬± 1.2 95.2 ¬± 0.9 97.1 ¬± 1.2 49.4 ¬± 0.3 95.2 ¬± 0.8 96.5 ¬± 0.3

3
94.4 ¬± 0.9 96.6 ¬± 0.7 95.9 ¬± 0.9 97.4 ¬± 1.1 93.5 ¬± 0.7 96.5 ¬± 0.6 93.5 ¬± 0.9 94.1 ¬± 0.7 85.9 ¬± 0.9 89.3 ¬± 0.6 32.6 ¬± 0.3 88.1 ¬± 0.8 93.4 ¬± 0.8

4
93.0 ¬± 0.3 96.3 ¬± 1.1 93.0 ¬± 0.4 96.7 ¬± 0.4 90.9 ¬± 1.2 93.9 ¬± 0.4 91.0 ¬± 0.3 92.8 ¬± 0.7 73.9 ¬± 1.1 79.1 ¬± 0.5 24.7 ¬± 0.6 78.1 ¬± 0.9 84.8 ¬± 0.7

5
92.4 ¬± 1.2 95.3 ¬± 0.5 88.3 ¬± 1.1 93.3 ¬± 0.5 89.0 ¬± 1.2 92.0 ¬± 0.7 87.5 ¬± 1.2 91.7 ¬± 0.9 63.7 ¬± 0.8 69.1 ¬± 1.1 20.0 ¬± 1.2 72.5 ¬± 0.6 78.7 ¬± 0.4

6
90.6 ¬± 0.3 93.1 ¬± 0.5 86.7 ¬± 1.2 89.4 ¬± 1.1 87.3 ¬± 0.6 89.3 ¬± 1.1 82.1 ¬± 1.2 85.7 ¬± 1.1 54.8 ¬± 0.8 59.4 ¬± 1.1 16.7 ¬± 0.3 69.1 ¬± 0.8 74.7 ¬± 0.4

7
89.9 ¬± 0.4 92.1 ¬± 0.6 83.0 ¬± 1.2 86.5 ¬± 0.3 85.2 ¬± 0.7 87.2 ¬± 0.8 77.1 ¬± 0.4 82.1 ¬± 0.6 50.1 ¬± 0.6 57.2 ¬± 0.7 13.9 ¬± 0.3 67.1 ¬± 0.6 73.1 ¬± 0.5

8
90.3 ¬± 0.8 92.1 ¬± 0.6 81.1 ¬± 0.5 86.1 ¬± 1.1 83.2 ¬± 0.4 87.2 ¬± 1.1 72.8 ¬± 0.6 74.4 ¬± 0.9 44.5 ¬± 0.9 48.2 ¬± 1.1 12.3 ¬± 0.7 61.8 ¬± 0.6 68.4 ¬± 0.8

9
90.3 ¬± 1.1 92.9 ¬± 0.9 78.2 ¬± 0.7 81.8 ¬± 0.4 82.5 ¬± 0.9 85.5 ¬± 0.9 67.1 ¬± 0.8 72.2 ¬± 0.8 40.7 ¬± 0.5 45.1 ¬± 0.6 11.1 ¬± 0.6 55.1 ¬± 0.3 60.2 ¬± 0.3

10
89.8 ¬± 0.4 91.9 ¬± 0.4 75.4 ¬± 0.4 77.0 ¬± 0.3 81.1 ¬± 1.1 84.1 ¬± 0.6 63.5 ¬± 1.1 68.1 ¬± 0.9 36.7 ¬± 0.3 41.5 ¬± 0.5 9.9 ¬± 0.7 50.1 ¬± 1.1 55.1 ¬± 0.7

14

Published as a conference paper at ICLR 2021

Table 6: Performance of the state-of-the-art existing approaches with and without LRRR on CIFAR100 in 20 tasks. Results for iTAML (Rajasegaran et al., 2020), BiC (Wu et al., 2019), and EEIL (Castro et al., 2018) are produced with their original implementation while EWC (Kirkpatrick et al., 2017) and LwF (Li & Hoiem, 2016) are re-implemented by us. Results are averaged over 3 runs. Figure 3b in the main paper is generated using numbers in this Table.

(a) Tasks 1-10

iTAML iTAML+RRR BiC BiC+RRR EEIL EEIL+RRR LwF LwF+RRR EWC EWC+RRR ER RRR

1
84.7 ¬± 0.6 84.7 ¬± 0.6 95.7 ¬± 0.6 95.7 ¬± 0.6 81.9 ¬± 0.5 81.9 ¬± 0.5 85.1 ¬± 0.7 85.1 ¬± 0.7 85.1 ¬± 0.7 85.1 ¬± 0.7 85.1 ¬± 0.7 85.1 ¬± 0.7

2
85.7 ¬± 0.4 89.9 ¬± 0.5 90.3 ¬± 0.9 93.3 ¬± 0.6 86.3 ¬± 0.3 88.4 ¬± 0.8 68.8 ¬± 0.9 71.6 ¬± 0.6 61.3 ¬± 0.5 68.9 ¬± 0.5 83.1 ¬± 0.9 85.1 ¬± 0.9

3
86.5 ¬± 0.3 89.2 ¬± 0.9 80.9 ¬± 0.8 84.7 ¬± 1.1 84.9 ¬± 0.4 87.6 ¬± 0.7 58.6 ¬± 1.1 61.8 ¬± 0.7 47.4 ¬± 0.8 52.2 ¬± 0.9 81.8 ¬± 0.7 83.8 ¬± 0.4

4
86.5 ¬± 0.8 89.2 ¬± 0.6 75.8 ¬± 0.8 77.5 ¬± 0.9 80.7 ¬± 0.3 82.6 ¬± 1.2 50.5 ¬± 1.2 54.2 ¬± 0.5 36.2 ¬± 0.3 39.9 ¬± 0.9 74.9 ¬± 0.3 77.9 ¬± 0.4

5
86.3 ¬± 1.2 89.0 ¬± 1.1 73.5 ¬± 0.6 73.4 ¬± 0.6 77.7 ¬± 0.6 78.5 ¬± 0.6 43.5 ¬± 0.9 46.2 ¬± 0.9 31.3 ¬± 0.6 35.2 ¬± 0.3 70.4 ¬± 0.3 72.4 ¬± 1.2

6
85.7 ¬± 0.8 87.2 ¬± 0.6 71.5 ¬± 1.2 74.8 ¬± 0.6 74.9 ¬± 0.3 76.9 ¬± 0.4 37.5 ¬± 0.6 40.7 ¬± 0.7 27.9 ¬± 0.5 30.0 ¬± 0.3 61.5 ¬± 1.2 64.5 ¬± 0.7

7
84.9 ¬± 1.1 88.0 ¬± 0.4 67.8 ¬± 0.4 69.6 ¬± 0.7 70.9 ¬± 0.7 71.2 ¬± 0.7 33.7 ¬± 0.9 36.7 ¬± 1.2 23.7 ¬± 1.1 24.3 ¬± 0.8 60.8 ¬± 1.1 62.8 ¬± 0.7

8
82.6 ¬± 0.3 85.6 ¬± 1.1 65.4 ¬± 0.8 67.4 ¬± 0.3 67.4 ¬± 0.7 67.3 ¬± 0.4 30.4 ¬± 0.9 34.4 ¬± 0.4 22.5 ¬± 0.4 24.0 ¬± 0.6 57.0 ¬± 0.7 59.0 ¬± 0.3

9
80.8 ¬± 0.7 86.6 ¬± 0.3 62.7 ¬± 1.2 65.7 ¬± 0.5 64.9 ¬± 0.5 67.0 ¬± 1.2 26.8 ¬± 1.1 29.8 ¬± 0.7 20.8 ¬± 0.8 23.7 ¬± 0.4 54.3 ¬± 0.4 57.3 ¬± 0.8

10
82.4 ¬± 0.3 85.4 ¬± 0.3 61.9 ¬± 1.2 64.9 ¬± 0.6 62.4 ¬± 0.3 64.5 ¬± 0.3 24.9 ¬± 0.7 27.2 ¬± 1.2 18.9 ¬± 0.7 21.0 ¬± 1.1 48.2 ¬± 0.6 51.2 ¬± 1.1

(b) Tasks 11-20

iTAML iTAML+RRR BiC BiC+RRR EEIL EEIL+RRR LwF LwF+RRR EWC EWC+RRR ER RRR

11
80.0 ¬± 1.1 85.5 ¬± 0.5 59.2 ¬± 0.4 62.2 ¬± 0.5 60.9 ¬± 0.6 63.7 ¬± 0.6 23.9 ¬± 0.7 27.7 ¬± 0.7 17.2 ¬± 1.1 20.7 ¬± 0.3 45.8 ¬± 0.6 48.8 ¬± 0.3

12
80.6 ¬± 0.5 85.2 ¬± 0.8 57.0 ¬± 0.6 59.1 ¬± 0.7 59.5 ¬± 0.6 62.9 ¬± 0.4 21.4 ¬± 0.7 26.9 ¬± 0.9 16.0 ¬± 0.5 19.5 ¬± 0.4 42.7 ¬± 0.7 46.7 ¬± 0.9

13
74.3 ¬± 0.8 79.7 ¬± 0.6 56.1 ¬± 1.2 58.2 ¬± 0.5 57.8 ¬± 0.6 59.7 ¬± 0.4 20.0 ¬± 0.7 25.7 ¬± 0.7 15.0 ¬± 0.8 18.4 ¬± 0.7 41.6 ¬± 0.6 43.6 ¬± 1.1

14
70.7 ¬± 0.6 74.3 ¬± 0.4 55.7 ¬± 0.6 57.8 ¬± 0.5 55.1 ¬± 0.3 57.0 ¬± 0.3 19.1 ¬± 0.9 24.5 ¬± 1.2 14.5 ¬± 0.8 17.3 ¬± 0.5 41.2 ¬± 0.6 44.2 ¬± 0.7

15
71.3 ¬± 1.1 74.0 ¬± 0.9 53.8 ¬± 0.5 54.4 ¬± 1.2 53.9 ¬± 0.5 55.6 ¬± 0.8 18.7 ¬± 0.8 23.6 ¬± 0.6 13.4 ¬± 1.1 16.2 ¬± 0.4 36.5 ¬± 0.4 39.5 ¬± 0.3

16
68.3 ¬± 0.5 73.4 ¬± 1.1 52.4 ¬± 1.2 56.6 ¬± 0.9 51.7 ¬± 0.3 53.5 ¬± 0.4 17.1 ¬± 0.8 22.6 ¬± 0.7 12.4 ¬± 0.4 15.8 ¬± 0.5 36.5 ¬± 0.6 38.5 ¬± 0.9

17
70.3 ¬± 0.8 74.8 ¬± 0.9 49.7 ¬± 0.6 53.9 ¬± 0.7 50.1 ¬± 0.8 53.5 ¬± 0.3 15.6 ¬± 0.8 19.5 ¬± 0.3 12.3 ¬± 0.4 15.0 ¬± 0.7 33.8 ¬± 0.4 35.8 ¬± 0.3

18
68.3 ¬± 0.6 74.4 ¬± 0.4 49.2 ¬± 1.2 52.4 ¬± 1.1 49.4 ¬± 0.5 52.7 ¬± 0.4 14.7 ¬± 0.8 18.6 ¬± 0.5 11.5 ¬± 0.8 16.6 ¬± 0.9 32.4 ¬± 1.2 33.4 ¬± 0.3

19
69.5 ¬± 0.3 73.9 ¬± 0.5 47.7 ¬± 1.1 49.5 ¬± 0.8 47.4 ¬± 0.6 49.1 ¬± 0.3 14.0 ¬± 0.8 19.7 ¬± 0.8 11.2 ¬± 0.8 14.3 ¬± 0.4 31.4 ¬± 0.7 32.4 ¬± 0.3

20
66.0 ¬± 0.6 71.8 ¬± 0.9 46.7 ¬± 1.2 49.4 ¬± 0.9 46.9 ¬± 0.9 47.8 ¬± 0.4 13.7 ¬± 1.1 18.4 ¬± 1.2 9.44 ¬± 0.5 13.2 ¬± 0.3 30.2 ¬± 0.5 31.2 ¬± 0.3

15

