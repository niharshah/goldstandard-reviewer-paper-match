Overcoming a Theoretical Limitation of Self-Attention
David Chiang and Peter Cholak University of Notre Dame
{dchiang,cholak}@nd.edu

arXiv:2202.12172v1 [cs.LG] 24 Feb 2022

Abstract
Although transformers are remarkably eï¬€ective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformerâ€™s classiï¬cation decisions become less and less conï¬dent (that is, with crossentropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahnâ€™s lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we ï¬nd that they can fail to generalize to longer strings; we oï¬€er a simple remedy to this problem that also improves length generalization in machine translation.
1 Introduction
Although transformers (Vaswani et al., 2017) are remarkably eï¬€ective for many tasks, there are some surprisingly easy-looking formal languages that they struggle with. Hahn (2020) tries to explain some of these by showing (his Lemma 5) that changing a single input symbol only changes the output of a transformer encoder by ğ‘‚ (1/ğ‘›), where ğ‘› is the input string length. Thus, for a language where acceptance depends on a single input symbol, a transformer might accept or reject strings with perfect accuracy, but for large ğ‘›, it must do so with low conï¬dence, giving accepted strings a probability just above Â½ and rejected strings a probability just below Â½. More precisely, as ğ‘› increases, the cross-entropy approaches its worst possible value of 1 bit per string.

Here, we examine this limitation using two simple regular languages:
PARITY = {ğ‘¤ âˆˆ Î£âˆ— | ğ‘¤ has an odd number of 1s} FIRST = {ğ‘¤ âˆˆ Î£âˆ— | ğ‘¤1 = 1}
where (here and throughout the paper) Î£ = {0, 1}. Hahnâ€™s lemma applies to PARITY because the network must attend to all the symbols of the string, and a change in any one of them changes the correct answer. We have chosen FIRST as one of the simplest examples of a language that the lemma applies to. It only requires attention on the ï¬rst symbol, but the lemma still applies because a change in this symbol changes the correct answer.
Although the lemma might be interpreted as limiting the ability of transformers to recognize these languages, we show three ways that this limitation can be overcome.
First, we show by explicit constructions that transformers do in fact exist that can recognize both languages with perfect accuracy for arbitrary lengths. We have implemented these constructions and veriï¬ed them experimentally (Â§3).
As predicted by Hahnâ€™s lemma, our constructed transformers have cross-entropy that approaches 1 bit (that is, just barely better than random guessing) as input length increases. But we show that by adding layer normalization, the cross-entropy can be made arbitrarily close to zero, independent of string length (Â§4).
In practice, we ï¬nd, like Bhattamishra et al. (2020a), that transformers cannot learn PARITY. Perhaps more surprisingly, when learning FIRST, transformers can have diï¬ƒculty generalizing from shorter strings to longer strings. Although this is not a logical consequence of Hahnâ€™s lemma, it is a consequence of the behavior that Hahnâ€™s lemma predicts. Fortunately, this problem can be ï¬xed with a simple modiï¬cation, multiplying attention logits by log ğ‘›. This modiï¬cation also improves length generalization in machine translation (Â§5).

2 Background
2.1 Notation If ğœ™ is a true-or-false statement, we write

1 if ğœ™ is true I[ğœ™] =
0 otherwise.

For any ğ‘š, ğ‘› > 0, we write 0ğ‘šÃ—ğ‘› for the ğ‘š Ã— ğ‘› zero matrix and Iğ‘›Ã—ğ‘› for the ğ‘› Ã— ğ‘› identity matrix.
2.2 Transformers
Following Hahn (2020), we consider transformer encoders with a sigmoid output layer on a single position. Diï¬€erently from Hahn (2020), but in line with common practice (Devlin et al., 2019), we prepend a token CLS (for â€œclassiï¬cationâ€) and use the encoder output at this tokenâ€™s position for classifying the string.
We use the original deï¬nition of transformers (Vaswani et al., 2017), except for positional encodings.
2.2.1 Input layer The input to the network is a string ğ‘¤ âˆˆ Î£âˆ—. Let ğ‘› = |ğ‘¤| + 1, let ğ‘¤0 = CLS, and let ğ‘¤ğ‘– be the ğ‘–-th symbol of ğ‘¤.
The input layer has a word embedding and positional encodings,

WE :

Î£

â†’

ğ‘‘
R

PE :

N

â†’

ğ‘‘
R

which are used to compute input vectors for ğ‘– = 0, . . . ğ‘›:
a0,ğ‘– = WE(ğ‘¤ğ‘–) + PE(ğ‘–).

The word embeddings are typically learned, while the positional encodings vary somewhat. Originally (Vaswani et al., 2017), they were ï¬xed and deï¬ned in terms of sine and cosine waves, but they can also be learned (Gehring et al., 2017), in which case they are deï¬ned only up to some maximum position. Here, we allow ourselves to deï¬ne PE as an arbitrary function on all positions. It would seem that to remain in the spirit of the original paper, PE should be easy to compute, independent of ğ‘¤, and parallelizable over positions.

2.2.2 Encoder layers
The body of the encoder is a stack of ğ¿ layers, each of which has a self-attention sublayer followed by a position-wise feedforward sublayer. For â„“ = 1, . . . , ğ¿, layer â„“ is deï¬ned as follows, where â„ = 1, . . . , ğ», and ğ‘– = 0, . . . , ğ‘›:

qâ„“,â„,ğ‘– = WQ,â„“,â„aâ„“âˆ’1,ğ‘–

Kâ„“,â„ = WK,â„“,â„aâ„“âˆ’1,0 Â· Â· Â· WK,â„“,â„aâ„“âˆ’1,ğ‘›

Vâ„“,â„ = WV,â„“,â„aâ„“âˆ’1,0 Â· Â· Â· WV,â„“,â„aâ„“âˆ’1,ğ‘›

ğ»

âˆ‘ï¸

câ„“,ğ‘– = LN

Att(qâ„“,â„,ğ‘–, Kâ„“,â„, Vâ„“,â„) + aâ„“âˆ’1,ğ‘–

â„=1

hâ„“,ğ‘– = max 0, WF,â„“,1câ„“,ğ‘– + bF,â„“,1

aâ„“,ğ‘– = LN WF,â„“,2hâ„“,ğ‘– + bF,â„“,2 + câ„“,ğ‘–

where boldface lowercase letters stand for vectors in Rğ‘‘ and boldface uppercase letters stand for matrices in Rğ‘‘Ã—ğ‘‘. The learned parameters of the model are the Wâ€™s and bâ€™s. The function Att is
scaled dot-product attention, deï¬ned as

Att :

ğ‘‘
R

Ã—

R ( ğ‘›+1) Ã—ğ‘‘

Ã—

R ( ğ‘›+1) Ã—ğ‘‘

â†’

ğ‘‘
R

Kq Att(q, K, V) = V softmax âˆš
ğ‘‘

where the result of the softmax, sometimes written as ğ›¼, is a vector of attention weights. The function LN is layer normalization, whose deï¬nition we defer to Â§4.

2.2.3 Output layer Finally, the network linearly projects the encoding of CLS to a scalar and applies a sigmoid function:

ğ‘¦ = ğœ (Wğ¿+1ağ¿,0 + bğ¿+1)

where Wğ¿+1 âˆˆ R1Ã—ğ‘‘ and bğ¿+1 âˆˆ R1Ã—1. The network accepts ğ‘¤ iï¬€ the output probability is greater than 12 .
3 Exact Solutions

The ï¬rst way to overcome the limitation suggested by Hahnâ€™s lemma is to show by explicit construction that our two languages can in fact be recognized with perfect accuracy by transformers.

3.1 FFNN for PARITY
Rumelhart et al. (1986) showed that for any ğ‘›, there is a feedforward neural network (FFNN) that computes PARITY for strings of length exactly ğ‘›. They

also showed that a randomly initialized FFNN can learn to do this automatically.
Since our construction is partially based on theirs, it may be helpful to review their construction in detail. Let ğ‘¤ be the input string, |ğ‘¤| = ğ‘›, and ğ‘˜ be the number of 1s in ğ‘¤. The input is a vector x such that xğ‘– = I[ğ‘¤ğ‘– = 1]. The ï¬rst layer computes ğ‘˜ and compares it against 1, 2, . . . , ğ‘›:

ï£®1 1 Â· Â· Â· 1ï£¹

ï£¯

ï£º

ï£¯1 1 Â· Â· Â· 1ï£º

W1

=

ï£¯ .

.

.

ï£º .

ï£¯ï£¯ .. .. . . .. ï£ºï£º

ï£¯

ï£º

ï£¯1 1 Â· Â· Â· 1ï£º

ï£°

ï£»

so that

ï£® âˆ’0.5 ï£¹

ï£¯

ï£º

ï£¯ âˆ’1.5 ï£º

b1 = ï£¯ . ï£º

ï£¯.ï£º

ï£¯.ï£º

ï£¯

ï£º

ï£¯âˆ’ğ‘› + 0.5ï£º

ï£°

ï£»

ï£®I[ğ‘˜ â‰¥ 1]ï£¹

ï£¯

ï£º

ï£¯I[ğ‘˜ â‰¥ 2]ï£º

h1 = ğ» (W1x + b1) = ï£¯ . ï£º

ï£¯.ï£º

ï£¯.ï£º

ï£¯

ï£º

ï£¯I[ğ‘˜ â‰¥ ğ‘›]ï£º

ï£°

ï£»

where ğ» is the step function (ğ» (ğ‘¥) = I[ğ‘¥ > 0]),

applied elementwise.

The second layer adds up the odd elements and

subtracts the even elements:

W2 = 1 âˆ’1 Â· Â· Â· (âˆ’1)ğ‘›+1 b2 = âˆ’0.5

ğ‘¦ = ğ» (W2h1 + b2)

which is 1 if ğ‘˜ is odd and 0 is ğ‘˜ is even.

3.2 Transformer for PARITY

Proposition 1. There is a transformer encoder with sigmoid output layer that recognizes (in the above sense) the language PARITY for strings of arbitrary length.

Initially, we will construct a transformer encoder without layer normalization (that is, LN(x) = x); then we will show how to add layer normalization (Â§4). Let ğ‘˜ be the number of occurrences of 1 in ğ‘¤. All vectors computed by the network have ğ‘‘ = 9 dimensions; if we show fewer dimensions, assume the remaining dimensions to be zero.
The word and position embeddings are:

ï£®1ï£¹
ï£¯ï£¯0ï£ºï£º ï£¯ï£º WE(0) = ï£¯0ï£º ï£¯ï£º ï£¯0ï£º ï£¯ï£º ï£¯0ï£º ï£°ï£»
ï£®0ï£¹
ï£¯ï£¯0ï£ºï£º ï£¯ï£º WE(CLS) = ï£¯1ï£º ï£¯ï£º ï£¯0ï£º ï£¯ï£º ï£¯0ï£º ï£°ï£»

ï£®0ï£¹
ï£¯ï£¯1ï£ºï£º
ï£¯ï£º WE(1) = ï£¯0ï£º
ï£¯ï£º ï£¯0ï£º ï£¯ï£º ï£¯0ï£º ï£°ï£»

ï£®0ï£¹

ï£¯ ï£¯

0

ï£º ï£º

ï£¯ï£º PE(ğ‘–) = ï£¯ 0 ï£º .

ï£¯ï£º
ğ‘–

ï£¯ï£º

ï£¯ğ‘›ï£º

ï£¯cos ğ‘–ğœ‹ï£º ï£°ï£»

Since we are numbering positions starting from 0, dimension 4 ranges from 0 to ğ‘›âˆ’1 , and dimension 5
ğ‘›
is +1 for even positions and âˆ’1 for odd positions. We argue that dimension 5, being a cosine wave,
is a fairly standard choice, although its period (2) is shorter than the shortest period in standard sinusoidal encodings (2ğœ‹). Dimension 4 is admittedly not standard; however, we argue that it is a reasonable encoding, and extremely easy to compute.
Thus, the encoding of word ğ‘¤ğ‘– is:

ï£® I[ğ‘¤ğ‘– = 0] ï£¹

ï£¯ ï£¯

I[ğ‘¤ğ‘– = 1]

ï£º ï£º

a0,ğ‘– = ï£¯ï£¯I[ğ‘¤ğ‘– = CLS]ï£ºï£º .

ï£¯

ï£º

ğ‘–

ï£¯

ï£º

ï£¯ğ‘›ï£º

ï£¯ cos ğ‘–ğœ‹ ï£º

ï£°

ï£»

The network has ğ¿ = 2 layers and ğ» = 2 heads.
The ï¬rst self-attention layer has one head which
ï¬nds ğ‘˜, the number of 1s. More precisely, be-
cause attention always averages, it must compute the â€œaverageâ€ number of 1s, that is, ğ‘˜ , and stores
ğ‘›
it in dimension 6. It also stores 1 in dimension 7,
ğ‘›
which we will need later.

WQ,1,1 = 0

WK,1,1 = 0

ï£®

05Ã—5

ï£¹

WV,1,1

=

ï£¯ ï£¯0

1

0

0

ï£º 0ï£º

ï£¯

ï£º

ï£¯0 0 1 0 0ï£º

ï£°

ï£»

The second head doesnâ€™t do anything (WV,1,2 = 0; the queries and keys can be anything). After the residual connection, we have:

ï£® I[ğ‘¤ğ‘– = 0] ï£¹

ï£¯ ï£¯

I[ğ‘¤ğ‘– = 1]

ï£º ï£º

ï£¯ï£¯I[ğ‘¤ğ‘– = CLS]ï£ºï£º

c1,ğ‘–

=

ï£¯ ï£¯

ğ‘–

ï£º ï£º.

ğ‘›

ï£¯

ï£º

ï£¯ cos ğ‘–ğœ‹ ï£º

ï£¯

ï£º

ğ‘˜

ï£¯

ï£º

ï£¯ğ‘›ï£º ï£¯1ï£º

ï£°ğ‘›ï£»

In the construction of Rumelhart et al. (1986), the next step is to compute I[ğ‘– â‰¤ ğ‘˜] for each ğ‘–, using step activation functions. There are two differences in our construction. First, we have ReLU activation functions, not step activation functions. Second, because attention must sum to one, if ğ‘› is odd then the even and odd positions will get different attention weights, so the trick of subtracting even positions from odd positions will not work. Instead, we want to compute I[ğ‘– = ğ‘˜] (Fig. 1).

1

0 ğ‘˜âˆ’2 ğ‘˜âˆ’1 ğ‘˜ ğ‘˜+1 ğ‘˜+2
Figure 1: Piecewise linear function equivalent on the integers to I[ğ‘– = ğ‘˜].

The ï¬rst FFNN has two layers. The ï¬rst is:

ï£®0 0 0 âˆ’1 0 1 âˆ’1ï£¹

WF,1,1

=

ï£¯ ï£¯0

0

0

âˆ’1

0

1

ï£º 0ï£º

ï£¯

ï£º

ï£¯0 0 0 âˆ’1 0 1 1 ï£º

ï£°

ï£»

ï£®0ï£¹

bF,1,1

=

ï£¯ï£º ï£¯0ï£º

.

ï£¯ï£º ï£¯0ï£º

ï£°ï£»

This gives:

1 ï£®max(0, ğ‘˜ âˆ’ ğ‘– âˆ’ 1)ï£¹

h1,ğ‘– =

ï£¯ ï£¯

max(0, ğ‘˜ âˆ’ ğ‘–)

ï£º ï£º.

ğ‘›

ï£¯ ï£¯max(0,

ğ‘˜

âˆ’

ğ‘–

+

ï£º 1) ï£º

ï£°

ï£»

The second layer linearly combines these three values to get I[ğ‘– = ğ‘˜] as desired.

WF,1,2 = 07Ã—3 1 âˆ’2 1

bF,1,2 = 0.

After the residual connection, we have:

ï£® I[ğ‘¤ğ‘– = 0] ï£¹

ï£¯

ï£º

ï£¯ I[ğ‘¤ğ‘– = 1] ï£º

ï£¯

ï£º

ï£¯I[ğ‘¤ğ‘– = CLS]ï£º

ï£¯

ï£º

ï£¯

ğ‘–

ï£º

a1,ğ‘– = ï£¯ï£¯ cosğ‘›ğ‘–ğœ‹ ï£ºï£º .

ï£¯

ï£º

ï£¯

ğ‘˜

ï£º

ï£¯ğ‘›ï£º ï£¯1ï£º

ï£¯ğ‘›ï£º

ï£¯ I[ğ‘–=ğ‘˜] ï£º

ï£¯

ï£º

ğ‘›

ï£°

ï£»

The second self-attention layer tests whether po-

sition ğ‘˜ is even or odd. It does this using two heads,

one which attends more strongly to the odd posi-

tions, and one which attends more strongly to the

even positions; both average dimension 8: âˆš
WQ,2,1 = 0 0 ğ‘ ğ‘‘ 0 0 0 0 0

WK,2,1 = 0 0 0 0 âˆ’1 0 0 0

WV,2,1 =

08Ã—8

00000001

âˆš

WQ,2,2 = 0 0 ğ‘ ğ‘‘ 0 0 0 0 0

WK,2,2 = 0 0 0 0 1 0 0 0

WV,2,2 =

08Ã—8

0 0 0 0 0 0 0 âˆ’1

where ğ‘ > 0 can be any constant. The second FFNN doesnâ€™t do anything (WF,2,1 = bF,2,1 = WF,2,2 = bF,2,2 = 0). The vector at CLS (posi-
tion 0) is then

ï£®0ï£¹ ï£¯ï£º ï£¯0ï£º ï£¯ï£º ï£¯1ï£º ï£¯ï£º ï£¯0ï£º ï£¯ï£º a2,0 = ï£¯ 1 ï£º ï£¯ï£º ï£¯ğ‘˜ï£º ï£¯ğ‘›ï£º ï£¯1ï£º ï£¯ğ‘›ï£º ï£¯ I[ğ‘˜=0] ï£º ï£¯ï£º
ğ‘›
ï£¯ï£º ï£¯ğ‘ ï£º ï£°ï£»
where ğ‘  has a somewhat complicated value. If ğ‘› is
even, it turns out to be
ğ‘  = (âˆ’1)ğ‘˜+1 2 tanh ğ‘ ğ‘›2

which is positive if ğ‘˜ is odd and negative if ğ‘˜ is even. As predicted by Hahn, it is in ğ‘‚ (1/ğ‘›). If ğ‘› is odd, the expression for ğ‘  is more complicated (see Appendix A), but it is still positive iï¬€ ğ‘˜ is odd, and it is still in ğ‘‚ (1/ğ‘›).
Finally, the output layer is a sigmoid layer that just looks at dimension 9:

W3 = 0 0 0 0 0 0 0 0 1 ğ‘¦= 1 . 1 + exp(âˆ’ğ‘ )

b3 = 0

So the output is greater than 12 iï¬€ ğ‘˜ is odd.
3.3 Transformer for FIRST
Next, we construct a transformer for FIRST. In line with the common practice of learning per-position word embeddings (Gehring et al., 2017), we use position embeddings that test whether a word is at position 1:

ï£® I[ğ‘¤ğ‘– = 0] ï£¹

a0,ğ‘– = ï£¯ï£¯ï£¯ I[ğ‘¤ğ‘– = 1] ï£ºï£ºï£º . ï£¯I[ğ‘¤ğ‘– = CLS]ï£º

ï£¯

ï£º

ï£¯ I[ğ‘– = 1] ï£º

ï£°

ï£»

The ï¬rst self-attention layer does nothing (WV,1,1 = 0), so after the residual connection, c1,ğ‘– = a0,ğ‘–.
The ï¬rst FFNN computes a new component (5)

that tests whether ğ‘– = 1 and ğ‘¤1 = 1:

WF,1,1 = âˆ’1 0 âˆ’1 1

ï£®0ï£¹

ï£¯ï£¯0ï£ºï£º

WF,1,2

=

ï£¯ï£º ï£¯0ï£º

ï£¯ï£º ï£¯0ï£º

ï£¯ï£º

ï£¯1ï£º ï£°ï£»

ï£® I[ğ‘¤ğ‘– = 0] ï£¹

ï£¯ ï£¯

I[ğ‘¤ğ‘– = 1]

ï£º ï£º

a1,ğ‘–

=

ï£¯ ï£¯

I[ğ‘¤ğ‘– = CLS]

ï£º ï£º.

ï£¯

ï£º

ï£¯ I[ğ‘– = 1] ï£º

ï£¯

ï£º

ï£¯I[ğ‘¤ğ‘– = 1 âˆ§ ğ‘– = 1]ï£º

ï£°

ï£»

bF,1,1 = 0 bF,1,2 = 0

(We have chosen WF,1,1 in a slightly unusual way to avoid using the bias term bF,1,1, in anticipation of Â§4 when we will add layer normalization.)
The second self-attention layer has a single head, which makes CLS focus on position 1.
âˆš WQ,2,1 = 0 0 ğ‘ ğ‘‘ 0 0
WK,2,1 = 0 0 0 1 0
WV,2,1 = 0 0 005Ã—5âˆ’ 1 1
2

where ğ‘ > 0 is a constant. The second FFNN doesnâ€™t do anything (WF,2,1 = bF,2,1 = WF,2,2 = bF,2,2 = 0). So at CLS (position 0),

ï£®0ï£¹

ï£¯ï£¯0ï£ºï£º

ï£¯ï£º

a2,0

=

ï£¯1ï£º ï£¯ï£º

ï£¯0ï£º

ï£¯ï£º ï£¯0ï£º

ï£¯ï£º

ï£¯ğ‘ ï£º

ï£°ï£» ğ‘  = exp eğ‘x+p ğ‘›ğ‘ âˆ’ 1 I[ğ‘¤1 = 1] âˆ’ 12 . (1)

The ï¬nal output layer just selects component 6:

W3 = 0 0 0 0 0 1

b3 = 0.

So the output probability, ğ‘¦ = ğœ(ğ‘ ), is greater than

1 2

iï¬€

ğ‘¤1

=

1.

However,

it

will

get

closer

to

1 2

as

ğ‘›

increases.

3.4 Experiments
We implemented both of the above constructions using modiï¬ed versions of PyTorchâ€™s built-in implementation of transformers (Paszke et al., 2019).
The code for this and other experiments in this paper are available at https://github.com/ndnlp/parity.

cross-entropy (bits)

PARITY

1

no layer norm

0.5
0 0

layer norm ğœ– = 10âˆ’5 layer norm ğœ– = 0
20 40 60 80 100

FIRST

cross-entropy (bits)

1
0.5
0 0

no layer norm layer norm ğœ– = 10âˆ’5
layer norm ğœ– = 0 200 400 600 800 1,000
string length ğ‘›

Figure 2: Cross-entropy of exact solutions for PARITY and FIRST computed over 1000 random strings of length ğ‘›. Without layer norm, the cross-entropy quickly approaches its upper bound of one. With layer norm and ğœ– > 0, the cross-entropy is better but still grows with ğ‘›. With ğœ– = 0, cross-entropy is independent of ğ‘›.

These constructions achieve perfect accuracy for strings with lengths sampled from [1, 1000].
However, in Fig. 2, the red curves (â€œno layer normâ€) show that, as strings grow longer, the crossentropy approaches its worst possible value of 1 bit per string. We discuss this problem next.
4 Layer Normalization
The second way to mitigate or eliminate the limitation of Hahnâ€™s lemma is layer normalization (Ba et al., 2016), which is deï¬ned, for any vector x, as
LN(x; ğ›¾, ğ›½) = x âˆ’ mean(x) â—¦ ğ›¾ + ğ›½ âˆšï¸ var(x) + ğœ–
where the functions mean and var compute the mean and variance, respectively, of the elements of x, and â—¦ is the elementwise (Hadamard) product. We ï¬x ğ›½ = 0 and ğ›¾ = 1, so that the result has approximately zero mean and unit variance. The constant ğœ– was not present in the original deï¬nition (Ba et al., 2016) but is added in all implementations that we are aware of, for numerical stability.
The original transformer deï¬nition performs layer normalization immediately after every residual connection. In this section, we modify our
It is also common to place layer normalization before residual connections (Wang et al., 2019; Nguyen and Salazar, 2019), but we follow the original transformer deï¬nition here.

two constructions above to use layer normalization. This modiï¬cation has two steps.

4.1 Removing centering
The ï¬rst is to nullify the centering eï¬€ect of layer normalization by making the network compute each value ğ‘¥ as well as its negation âˆ’ğ‘¥. The new word encodings are deï¬ned in terms of those in the original construction:

aÂ¯0,ğ‘– = âˆ’aa00,ğ‘–,ğ‘– .

Likewise for the self-attention parameters:

WÂ¯ Q,â„“,â„ = WQ,â„“,â„ 0 WÂ¯ K,â„“,â„ = WK,â„“,â„ 0 WÂ¯ V,â„“,â„ = âˆ’WWVV,â„“,â„“,â„,â„ 00 .

Likewise for the position-wise FFNN parameters:

WÂ¯ F,â„“,1 = WÂ¯ F,â„“,2 =

WF,â„“,1 0
WF,â„“ ,2 âˆ’WF,â„“ ,2

bÂ¯ F,â„“,1 = bF,â„“,1 bÂ¯ F,â„“,2 = âˆ’bbFF,â„“,â„“,2,2 .

Then each layer of activations is

cÂ¯â„“,ğ‘– = LN

câ„“ ,ğ‘– âˆ’câ„“ ,ğ‘–

â„“,ğ‘–

aâ„“ ,ğ‘–

aÂ¯ = LN âˆ’aâ„“,ğ‘– .

The argument to LN always has zero mean, so that layer normalization does not add or subtract anything. It does scale the activations, but in the case of the two transformers constructed above, any activation layer can be scaled by any positive number without changing the ï¬nal decisions (see Appendix B).

4.2 Reducing cross-entropy
Furthermore, in any transformer, we can use layer normalization to shrink the cross-entropy as small as we like, contrary to Hahnâ€™s Lemma 5. In Hahnâ€™s formulation, position-wise functions like layer normalization can be subsumed into his ğ‘“ act, but the lemma assumes that ğ‘“ act is Lipschitz-continuous, and layer normalization with ğœ– = 0 is not.
Proposition 2. For any transformer ğ‘‡ with layer normalization (ğœ– = 0) that recognizes a language L, and for any ğœ‚ > 0, there is a transformer with layer normalization that recognizes L with crossentropy at most ğœ‚.

Proof. Let ğ‘‘ be the number of dimensions in the original vectors of activations, and let ğ¿ be the number of layers. Then we add a new layer whose self-attention doesnâ€™t do anything (WV,ğ¿+1,â„ = 0) and whose FFNN is deï¬ned in terms of the original output layer:

WÂ¯ F,ğ¿+1,1 = Iğ‘‘ âˆ’Iğ‘‘

bÂ¯ F,ğ¿+1,1 = 0ğ‘‘ 0ğ‘‘

WÂ¯ F,ğ¿+1,2 = âˆ’Iğ‘‘ Iğ‘‘

ï£® bğ¿+1 ï£¹

bÂ¯ F,ğ¿+1,2 = ï£¯ï£¯âˆ’bğ¿+1ï£ºï£º .

ï£¯ ï£¯

0ğ‘‘âˆ’2

ï£º ï£º

ï£°

ï£»

ï£® Wğ¿+1

+

ï£¯ ï£¯

âˆ’Wğ¿+1

ï£¯ï£¯0 ( ğ‘‘ âˆ’2) Ã—ğ‘‘

ï£°

âˆ’Wğ¿+1 ï£¹

Wğ¿+1

ï£º ï£º

0 ( ğ‘‘âˆ’2) Ã—ğ‘‘ ï£ºï£º

ï£»

This causes the residual connection to zero out all dimensions except two, so that if ğ‘  was the original output logit, the output of this new layer (before layer normalization) is

ï£®ğ‘ ï£¹

aÂ¯ ğ¿+1,ğ‘– = LN

ï£¯ ï£¯

âˆ’ğ‘ 

ï£º ï£º

.

ï£¯ï£¯0ğ‘‘âˆ’2ï£ºï£º

ï£°ï£»

Now, if ğœ– = 0, layer normalization scales this vector to have unit variance exactly, so it becomes

âˆšï¸ ï£®Â± ğ‘‘/2ï£¹

aÂ¯ ğ¿+1,ğ‘–

=

ï£¯ âˆšï¸ ï£º ï£¯âˆ“ ğ‘‘/2ï£º

.

ï£¯ ï£¯

0ğ‘‘âˆ’2

ï£º ï£º

ï£°

ï£»

The new output layer simply selects the ï¬rst dimension, scaling it by ğ‘:

WÂ¯ ğ¿+2 = ğ‘ 0 0ğ‘‘âˆ’2

bÂ¯ ğ¿+2 = 0.

Finally, set ğ‘ = âˆ’ âˆš1 log(exp ğœ‚ âˆ’ 1). If the
ğ‘‘/2
input string is in L, then the cross-entropy is âˆšï¸
log ğœ(ğ‘ ğ‘‘/2) = ğœ‚. Similarly, if the input string
is not in L, then the cross-entropy is log(1 âˆ’ âˆšï¸
ğœ(âˆ’ğ‘ ğ‘‘/2)) = ğœ‚.

However, in practice, ğœ– is always set to a nonzero value, which makes layer normalization Lipschitzcontinuous, so Hahnâ€™s Lemma 5 still applies.

4.3 Experiments
We tested our exact solutions, modiï¬ed as described above to use layer normalization. Figure 2 shows that layer normalization with ğœ– > 0 improves the cross-entropy, but it still grows with ğ‘› and approaches 1. With ğœ– = 0, the cross-entropy is independent of ğ‘› and, as argued above (Proposition 2), can be made as low as desired.

cross-entropy (bits)

4

2

0

0.6

0.8

1

1.2

1.4

1

accuracy

0.5

0

0.6

0.8

1

1.2

1.4

parameter value

Figure 3: The cross-entropy and accuracy of our solu-
tion to PARITY are both extremely sensitive to the parameter WÂ¯ V6,,21,1, which is responsible for computing ğ‘›ğ‘˜ . The correct parameter value is 1.

5 Learnability
In this section, we turn to the question of learnability, which will lead to a third way of overcoming the limitation suggested by Hahnâ€™s lemma.
5.1 Experiments: standard transformers
We tried training transformers on both PARITY and FIRST. Each transformer had the same number of layers and heads and the same ï¬xed positional encodings as the corresponding exact solution. We used ğ‘‘model = 16 for word encodings, self-attention, and FFNN outputs, and ğ‘‘FFNN = 64 for FFNN hidden layers. We used layer normalization (ğœ– = 10âˆ’5) after residual connections. We used PyTorchâ€™s default initialization and trained using Adam (Kingma and Ba, 2015) with learning rate 3 Ã— 10âˆ’4 (Karpathy, 2016). We did not use dropout, as it did not seem to help.
We found, like Bhattamishra et al. (2020a), that a transformer with the above settings was unable to learn PARITY. We tried many other settings as well, to no avail. To give an idea of why our constructed solution, in particular, is diï¬ƒcult to ï¬nd, Fig. 3 shows the cross-entropy and accuracy of the model if we start with our solution (with layer normalization, ğœ– = 0) and vary the parameter WÂ¯ V6,,21,1, which is responsible for computing ğ‘˜ . At 1, it
ğ‘›
has a cross-entropy of 0 and accuracy of 1, which are both optimal, but the cross-entropy oscillates so rapidly that even a small perturbation of this parameter would make it diï¬ƒcult to recover the solution by gradient descent.

FIRST is much easier to learn, but the bad news is that the learned transformers do not generalize well to longer sentences. Figure 4 (left column) shows that when a transformer is trained from scratch on shorter strings (ğ‘› = 10, 30, 100, 300) and tested on longer strings (ğ‘› = 1000), the accuracy is not perfect. Indeed, for training ğ‘› = 10, the accuracy is hardly better than random guessing.

5.2 Flawed transformer for FIRST

In our solution above (Â§3.3), the second selfattention layer attended mostly to the ï¬rst position, but not totally. It relied on the fact that in the second self-attention layer, the values of the non-ï¬rst positions (V2ğ‘–,,41 and V2ğ‘–,,51 for ğ‘– â‰  1) are exactly zero and therefore do not contribute to the output.
In practice, because word embeddings are randomly initialized in all dimensions, and are added to every layer via residual connections, itâ€™s unlikely for any activation to be exactly zero. This explains why our exact solution cannot be learned.
But, as a further thought experiment about what the model might be learning instead, consider the following transformer, which uses only a single layer (ğ¿ = 1) and does not zero out the values of the non-ï¬rst positions. As we will see, it performs worse than the transformer of Â§3.3 for long strings.
âˆš WQ,1,1 = 0 0 ğ‘ ğ‘‘ 0
WK,1,1 = 0 0 0 1
WV,1,1 = âˆ’ 1 104Ã—4âˆ’ 1 0 .
22 2
The FFNN doesnâ€™t do anything (WF,1,1 = bF,1,1 = WF,1,2 = bF,1,2 = 0), and the ï¬nal output layer just selects component 5. So if ğ‘˜ is the total number of 1s, the ï¬nal logit at CLS (position 0) would be

exp ğ‘ âˆ’ 1

1

=

I[ğ‘¤1 = 1] âˆ’

ğ‘ 

exp ğ‘ + ğ‘› âˆ’ 1

2

+1

ğ‘› ğ‘˜âˆ’ .

exp ğ‘ + ğ‘› âˆ’ 1 2

If ğ‘ > log(ğ‘› âˆ’ 1), then this is positive iï¬€ ğ‘¤1 = 1. But if ğ‘ â‰¤ log(ğ‘› âˆ’ 1), the new second term can be big enough to make the model output an incorrect answer. This suggests that if we train a transformer on strings with length up to ğ‘, then the learned parameters will be large enough to classify strings of length up to ğ‘ correctly, but may misclassify strings longer than ğ‘.
This explanation is corroborated by the bottomleft graph in Fig. 4, which shows the attention

weight on the ï¬rst position of the test string (summed over layers, averaged over strings) as a function of training epoch (starting from random initial parameters). The training strings have varying length (ğ‘›) and the test strings have ï¬xed length (1000). We might hope that the attention weight would converge to the same value independent of ğ‘›. But the lower ğ‘› is, the more the attention weight is diluted, making it easier for the value in position 1 to be outweighed by values in other positions.

5.3 Log-length scaled attention

Fortunately, this problem is easy to ï¬x by scaling the logits of each attention layer by log ğ‘›, that is, redeï¬ning attention as

log ğ‘› Att(q, K, V) = V softmax âˆš Kq. (2)
ğ‘‘

Then taking the model in Â§5.2 with ğ‘ = 1 gives

ğ‘›âˆ’1

1

1

=

I[ğ‘¤1 = 1] âˆ’ +

ğ‘› ğ‘˜âˆ’

ğ‘  2ğ‘› âˆ’ 1

2 2ğ‘› âˆ’ 1 2

which is positive iï¬€ ğ‘¤1 = 1. Moreover, scaling is another way to make the cross-entropy low:

Proposition 3. For any ğœ‚ > 0 there is a transformer with attention deï¬ned as in Eq. (2), and with or without layer normalization, that recognizes FIRST with cross-entropy at most ğœ‚.

Proof. Without layer normalization, we can take the model in Â§3.3, set ğ‘ = 1 and log-scale the attention weights, which changes ğ‘  from Eq. (1) to

ğ‘› =

1 I[ğ‘¤1 = 1] âˆ’

ğ‘ 

2ğ‘› âˆ’ 1

2

1 < |ğ‘ | â‰¤ 1.

4

2

With layer normalization (and ğœ– > 0), we can

apply the modiï¬cation of Â§4 to nullify the center-

ing eï¬€ect of layer normalization. Then since the
variance of a2,0 is 16 (1 + ğ‘ 2), the layer-normalized ï¬nal logit is

1

âˆ’ 12

ğ‘ Â¯ = ğ‘  (1 + ğ‘ 2) + ğœ–

6

and since |ğ‘ | > 14 , |ğ‘ Â¯| > 1 4

5

âˆ’ 12

+ğœ– .

24

In either case, since the ï¬nal logit has a lower bound not dependent on ğ‘›, the output layer weights can be scaled as in the proof of Proposition 2 to make the cross-entropy at most ğœ‚.

train tokens test tokens
baseline scaled

train all test all
3M+3M 32k+34k
32.6 32.5

train short test long
1M+1M 24k+25k
11.4 12.4

Table 1: When training and testing on data with the same length distribution, scaling attention logits has no signiï¬cant eï¬€ect on BLEU, but when training on short sentences (â‰¤ 20 tokens) and testing on long sentences (> 20 tokens), scaling helps signiï¬cantly (ğ‘ < 0.01).

5.4 Experiments: scaled attention
Figure 4 (right column) shows the training of transformers with scaling of attention logits by log ğ‘›. For all training lengths ğ‘›, the model is able to learn with perfect test cross-entropy and accuracy.
We see a similar eï¬€ect on low-resource Englishto-Vietnamese machine translation (Table 1), using Witwicky, an open-source implementation of transformers. We use all default settings; in particular, residual connections come after layer normalization (ğœ– = 10âˆ’5). We measure translation accuracy using BLEU (Papineni et al., 2002) and use bootstrap resampling with 1000 samples for signiï¬cance testing.
When train and test length distributions are the same, scaling attention logits has no signiï¬cant effect. But if we train only on sentences with median length or shorter (â‰¤ 20 tokens) and test only on sentences longer than median length (> 20 tokens), scaling attention logits by log ğ‘› improves BLEU by +1, which is statistically signiï¬cant (ğ‘ < 0.01).
6 Related Work
Using very diï¬€erent assumptions on the form of transformers and inputs, a number of recent theoretical studies of transformers show that they can solve much more diï¬ƒcult problems than the ones studied here. Transformer encoders can be shown to be universal approximators by ï¬xing the string length and using a number of layers exponential in the length (Yun et al., 2020). Transformer encoderâ€“decoders, where the decoder can run for an unbounded number of steps, have been shown to be Turing-complete (Bhattamishra et al., 2020b; PÃ©rez et al., 2021).
https://github.com/tnq177/witwicky

Baseline 1

Scaled attention logits 1

accuracy

0.5

0.5

total attention weight cross-entropy (bits)

0

0

1.5

1.5

1

1

0.5

0.5

0

0

1

1

0.5

0.5

0

0

1

10

100

1000

1

10

100

1000

epoch (log-scale)

epoch (log-scale)

ğ‘› = 10

ğ‘› = 30

ğ‘› = 100

ğ‘› = 300

Figure 4: Training a transformer on FIRST. Each epoch has 100 training strings of varying length (see legend)
and 100 test strings of length 1000. All curves are averaged over 20 runs. Left: Standard transformer with layer normalization (ğœ– = 10âˆ’5). Right: Same, with attention logits scaled by log ğ‘›.

RASP (Weiss et al., 2021) is a simple programming language whose programs can be compiled into transformers. While PARITY can easily be written in RASP, this does not imply in itself the existence of transformers that can recognize PARITY, for two reasons. First, RASPâ€™s aggregate operation (which corresponds to attention) always attends uniformly to a subset of positions, unlike softmax attention. Second, RASPâ€™s elementwise operations are embedded directly in the output transformer; they are not translated into FFNNs.
Bhattamishra et al. (2020a) carry out theoretical and experimental studies of transformers for various formal languages. The theoretical results are for a diï¬€erent variant of transformers than ours (transformer encoders with self-attention masked so that each position attends only to previous positions), and focus on such transformersâ€™ ability to maintain counters that are constrained to be nonnegative. Their experimental results suggest that transformers have diï¬ƒculty learning some regular languages, including PARITY.

7 Conclusion
Weâ€™ve seen that the questions of (a) whether a neural network can recognize a language, (b) whether it can achieve low cross-entropy on a language, and (c) whether it can learn to recognize a language are three separate questions, because we have given examples of (a) without (b) and (b) without (c).
Namely, our explicit construction for PARITY shows that a neural network can recognize a language with perfect accuracy (a) but poor crossentropy (b). Adding layer normalization (ğœ– = 0) enables it to achieve low cross-entropy (b), but still does not learn well (c). We observe that because the answer to (b) can hinge on small details of the model, (b) is not probably not very useful as a way of measuring expressivity.
However, we did ï¬nd that the limited inï¬‚uence of a single input symbol, implied by Hahnâ€™s lemma, has a serious practical implication for learnability (c). Namely, transformers can fail to generalize from shorter training strings to longer testing strings. Our proposed ï¬x, scaling attention logits by log ğ‘›, is easy to implement and very eï¬€ective on a real machine translation task.

Acknowledgements
We would like to thank Toan Nguyen for assistance with his machine translation code, and Gail Weiss for catching some mistakes.
This paper is based upon work supported in part by the Oï¬ƒce of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract #FA865017-C-9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the oï¬ƒcial policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoï¬€rey E. Hinton. 2016. Layer normalization. arXiv:1607.06450.
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020a. On the ability and limitations of Transformers to recognize formal languages. In Proc. EMNLP, pages 7096â€“7116.
Satwik Bhattamishra, Arkil Patel, and Navin Goyal. 2020b. On the computational power of Transformers and its implications in sequence modeling. In Proc. CoNLL, pages 455â€“475.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL HLT, pages 4171â€“4186.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In Proc. ICML, pages 1243â€“1252.
Michael Hahn. 2020. Theoretical limitations of selfattention in neural sequence models. Trans. ACL, 8:156â€“171.
Andrej Karpathy. 2016. 3e-4 is the best learning rate for Adam, hands down. Twitter.
Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A method for stochastic optimization. In Proc. ICLR.
William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A. Smith. 2021. Eï¬€ects of parameter norm growth during transformer training: Inductive bias from gradient descent. In Proc. EMNLP, pages 1766â€“1781.
Toan Q. Nguyen and Julian Salazar. 2019. Transformers without tears: Improving the normalization of

self-attention. In Proc. International Workshop on Spoken Language Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311â€“318.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library. In Proc. NeurIPS.
Jorge PÃ©rez, Pablo BarcelÃ³, and Javier Marinkovic. 2021. Attention is Turing-complete. Journal of Machine Learning Research, 22(75):1â€“35.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning Internal Representations by Error Propagation, pages 318â€“362. MIT Press, Cambridge, MA, USA.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS, pages 5998â€“6008.
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep Transformer models for machine translation. In Proc. ACL, pages 1810â€“1822.
Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021. Thinking like Transformers. In Proc. ICML.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. 2020. Are Transformers universal approximators of sequence-to-sequence functions? In Proc. ICLR.

A Correctness of PARITY Construction

In Â§3.2, we constructed a transformer that recog-

nizes PARITY; here we ï¬ll in details of calculating
ğ‘  = a92,0. If ğ‘› is even, the ï¬rst head computes âˆš
q2,1,0 = ğ‘ ğ‘‘

K2ğ‘–,,11,0 = âˆ’ cos ğ‘–ğœ‹ = (âˆ’1)ğ‘–+1

2,1,0
ğ›¼

=

exp(âˆ’1)ğ‘–+1ğ‘

ğ‘–

ğ‘› 2

(exp

ğ‘

+

exp

âˆ’ğ‘)

V2,1,0

=

I[ğ‘–

=

ğ‘˜] .

ğ‘–,9

ğ‘›

Similarly, the second head computes âˆš
q2,2,0 = ğ‘ ğ‘‘

K2ğ‘–,,12,0 = cos ğ‘–ğœ‹ = (âˆ’1)ğ‘–

2,2,0
ğ›¼

=

exp (âˆ’1) ğ‘– ğ‘

ğ‘–

ğ‘› 2

(exp

ğ‘

+

exp

âˆ’ğ‘)

V2,2,0 = âˆ’ I[ğ‘– = ğ‘˜] .

ğ‘–,9

ğ‘›

Then

ğ‘ 

=

a2,0

=

1 2,1,0 ğ›¼

âˆ’

1 2,2,0 ğ›¼

9 ğ‘›ğ‘˜

ğ‘›ğ‘˜

exp(âˆ’1)ğ‘˜+1ğ‘ âˆ’ exp(âˆ’1)ğ‘˜ ğ‘ =
ğ‘›22 (exp ğ‘ + exp âˆ’ğ‘) = (âˆ’1)ğ‘˜+1 exp ğ‘ âˆ’ exp âˆ’ğ‘
ğ‘›22 (exp ğ‘ + exp âˆ’ğ‘)

= (âˆ’1)ğ‘˜+1 2 tanh ğ‘ ğ‘›2

is negative if ğ‘˜ is even and positive if ğ‘˜ is odd. If ğ‘› is odd, calculating ğ‘  is more complicated
because there are unequal numbers of more- and less-attended positions. The attention weights are

2,1,0
ğ›¼

=

exp(âˆ’1)ğ‘–+1ğ‘

ğ‘– ğ‘›âˆ’21 exp ğ‘ + ğ‘›2+1 exp âˆ’ğ‘

ğ‘1

2,2,0
ğ›¼

=

exp (âˆ’1) ğ‘– ğ‘

ğ‘– ğ‘›2+1 exp ğ‘ + ğ‘›âˆ’21 exp âˆ’ğ‘

ğ‘2
ğ‘  = (exp(âˆ’1)ğ‘˜+1ğ‘)ğ‘2 âˆ’ (exp(âˆ’1)ğ‘˜ ğ‘)ğ‘1 . ğ‘›ğ‘1ğ‘2

If ğ‘˜ is even,

ğ‘›âˆ’1 exp âˆ’2ğ‘ âˆ’ ğ‘›âˆ’1 exp 2ğ‘

ğ‘ = 2

2

ğ‘›ğ‘1ğ‘2

= âˆ’ (ğ‘› âˆ’ 1) sinh 2ğ‘ < 0

ğ‘›ğ‘1ğ‘2

whereas if ğ‘˜ is odd,

ğ‘›+1 exp 2ğ‘ âˆ’ ğ‘›+1 exp âˆ’2ğ‘

ğ‘ = 2

2

ğ‘›ğ‘1ğ‘2

(ğ‘› + 1) cosh 2ğ‘

=

> 0.

ğ‘›ğ‘1ğ‘2

B Scale-Invariance of PARITY and FIRST

Constructions

In Â§4.1, we claimed that the scaling eï¬€ect of layer normalization has no eï¬€ect on the decisions of our constructions for PARITY and FIRST. This is related to the property of approximate homogeneity studied by Merrill et al. (2021).
In general, we rely on the fact that the FFNNs we use all have no bias terms (bF,â„“,1 and bF,â„“,2), so the FFNNs are 1-homogenous (scaling the input scales the output by the same amount). For the selfattentions, our WQ,â„“,â„ all have a constant factor ğ‘ built into them, so any scaling of the input can be absorted into this constant.
For PARITY, suppose that layer normalization scales câ„“ by ğ¶â„“ and aâ„“ by ğ´â„“.
cÂ¯1,ğ‘– = ğ¶1 âˆ’cc11,ğ‘–,ğ‘–

Because the ï¬rst FFNN has no bias term, aÂ¯1,ğ‘– = ğ´1ğ¶1 âˆ’aa11,ğ‘–,ğ‘–

In the second self-attention layer, the attention logits and the values are scaled by ğ´1ğ¶1. Weâ€™re only interested in what happens to ğ‘  = c29,0. If ğ‘› is even, ğ‘  becomes:
ğ‘ Â¯ = (âˆ’1)ğ‘˜+1 2ğ¶2 ğ´1ğ¶1 tanh ğ´1ğ¶1ğ‘ . ğ‘›2
Since the second FFNN is the identity function, its layer normalization has no eï¬€ect (ğ´2 = 1). So the ï¬nal logit is ğ‘ Â¯, which is still negative if ğ‘˜ is even and positive if ğ‘˜ is odd. Similarly if ğ‘› is odd.
For FIRST, again suppose that layer normalization scales câ„“ by ğ¶â„“ and aâ„“ by ğ´â„“. As before,
aÂ¯1,ğ‘– = ğ´1ğ¶1 âˆ’aa11,ğ‘–,ğ‘–

In the second self-attention layer, the attention log-

its and the values are scaled by ğ´1ğ¶1. Weâ€™re only interested in what happens to ğ‘  = c26,0:

ğ‘ Â¯ = exp ğ´1ğ¶1ğ‘ ğ¶2 ğ´1ğ¶1 I[ğ‘¤1 = 1] âˆ’ 1

exp ğ´1ğ¶1ğ‘ + ğ‘› âˆ’ 1

2

Since the second FFNN is the identity function,

ğ´2 = 1. So the ï¬nal logit is ğ‘ Â¯, which is still positive if ğ‘¤1 = 1 and negative otherwise.

