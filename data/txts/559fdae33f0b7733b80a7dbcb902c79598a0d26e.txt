Can Transformers be Strong Treatment Effect Estimators?

arXiv:2202.01336v3 [cs.LG] 14 Feb 2022

Yi-Fan Zhang * 1 Hanlin Zhang * 2 Zachary C. Lipton 2 Li Erran Li 3 Eric P. Xing 2 4

Abstract
In this paper, we develop a general framework based on the Transformer architecture to address a variety of challenging treatment effect estimation (TEE) problems. Our methods are applicable both when covariates are tabular and when they consist of sequences (e.g., in text), and can handle discrete, continuous, structured, or dosage-associated treatments. While Transformers have already emerged as dominant methods for diverse domains, including natural language and computer vision, our experiments with Transformers as Treatment Effect Estimators (TransTEE) demonstrate that these inductive biases are also effective on the sorts of estimation problems and datasets that arise in research aimed at estimating causal effects. Moreover, we propose a propensity score network that is trained with TransTEE in an adversarial manner to promote independence between covariates and treatments to further address selection bias. Through extensive experiments, we show that TransTEE signiï¬cantly outperforms competitive baselines with greater parameter efï¬ciency over a wide range of benchmarks and settings.
1. Introduction
One of the fundamental tasks in causal inference is to estimate treatment effects given covariates, treatments and outcomes. Treatment effect estimation is a central problem of interest in clinical healthcare and social science (Imbens & Rubin, 2015), as well as econometrics (Wooldridge, 2015). Under certain conditions (Rosenbaum & Rubin, 1983), the task can be framed as a speciï¬c type of missing data problem, whose structure is fundamentally different in key ways from supervised learning and entails a more complex set of covariate and treatment representation choices.
*Equal contribution 1Chinese Academy of Science 2Carnegie Mellon University 3AWS AI, Amazon 4MBZUAI. Correspondence to: Hanlin Zhang <hanlinzh@cs.cmu.edu>.
Code repository: https://github.com/hlzhang109/ TransTEE

âˆ

Covariate X

Age Sex Prev BP

1 21 F Y 32

2 34 M Y 100

3 19 F N 159
Treatment T

âˆ’âˆ

Trans TEE

ğğŸ ğ’™ = ğ”¼ ğ’€ ğŸ ğ‘¿ = ğ’™ ğğŸ ğ’™ = ğ”¼ ğ’€ ğŸ ğ‘¿ = ğ’™

ğğŸ ğ’™

Causal Effect

ğ’‘ğ’“ğ’†ğ’— ğ‘©ğ‘·

ğ‘¨ğ’ˆğ’† ğ‘ºğ’†ğ’™

ğğŸ ğ’™

ğ‘»

ğ’€

Figure 1. A motivating example with a corresponding causal graph. Prev denotes previous infection condition and BP denotes blood pressure. TransTEE adjusts an appropriate covariate set {Prev, BP} with attention which is visualized via a heatmap.

Recently, feed-forward neural networks (NNs) have been adapted for modeling causal relationships and estimating treatment effects (Johansson et al., 2016; Shalit et al., 2017; Louizos et al., 2017; Yoon et al., 2018; Bica et al., 2020; Schwab et al., 2020; Nie et al., 2021; Curth & van der Schaar, 2021), in part due to their ï¬‚exibility in modeling nonlinear functions (Hornik et al., 1989) and high-dimensional input (Johansson et al., 2016). Among them, the specialized NNâ€™s architecture plays a key role in learning representations for counterfactual inference (Alaa & Schaar, 2018; Curth & van der Schaar, 2021) such that treatment variable and covariates are well distinguished (Shalit et al., 2017). Despite these encouraging results, several key challenges make it difï¬cult to adopt these methods as general-purpose treatment effect estimators. Fundamentally, current works based on subnetworks are not well equipped with suitable inductive biases to exploit the structural similarities of potential outcomes for TEE (Curth & van der Schaar, 2021). Practically, their treatment-speciï¬c designs suffer several key weaknesses, including parameter inefï¬ciency (Table 1), brittleness under different scenarios, such as when treatments or dosages shift slightly from the training distribution (Figure 3). We discuss these problems in detail in Sections 2 and 6.2.
To overcome the above challenges and inspired by the observation that model structure plays a crucial role in TEE (Alaa & Schaar, 2018; Curth & van der Schaar, 2021), in this work, we take inspirations from the Transformer architecture which has emerged as an architecture of choice for diverse domains including natural language processing (Vaswani et al., 2017; Devlin et al., 2018), image recognition (Dosovitskiy et al., 2021a) and multimodal processing (Tsai et al., 2019).

Can Transformers be Strong Treatment Effect Estimators?

METHODS
TARNET (SHALIT ET AL., 2017) PERFECT MATCH (SCHWAB ET AL., 2018)
DRAGONNET (SHI ET AL., 2019) DRNET (SCHWAB ET AL., 2020) SCIGAN (BICA ET AL., 2020)
VCNET (NIE ET AL., 2021) NCORE (PARBHOO ET AL., 2021) FLEXTENET (CURTH & VAN DER SCHAAR, 2021)
OURS

DISCRETE TREATMENT
O(T ) O(T ) O(T ) O(T ) O(T ) O(1) O(T ) O(T ) O(1)

CONTINUOUS TREATMENT
O(1) O(BT ) O(1)

TREATMENTS INTERACTIONS O(2T )
O(T ) O(1)

DOSAGE
O(T BD) O(T BD)
O(1)

Table 1. Comparison of existing works and TransTEE in terms of parameter complexity. T is the number of treatments. BT , BD are the number of branches for approximating continuous treatment and dosage. TransTEE is general for all the factors.

In this paper, we investigate the following question: Can Transformers be similarly effective for treatment effect estimation in problems of practical interest? Throughout, we adopt the notation of the Rubin-Neyman potential outcomes framework (Rubin, 2005). In particular, we develop TransTEE, a method that builds upon the attention mechanisms and achieves state-of-the-art on a wide range of TEE tasks. Speciï¬cally, TransTEE represents covariate and treatment variables separately via learnable embeddings. Then, multi-headed cross-attention governs the subsequent interactions, with the covariates embeddings serving as keys and values and the treatment embeddings serving as the query vectors. This mechanism enables adaptive covariate selection (De Luna et al., 2011; VanderWeele, 2019) for inferring causal effects (Figure 1). One can observe that both pre-treatment covariates and confounders are appropriately adjusted with higher weights. Such an inductive bias is particularly important since it provides parameter sharing across private feature spaces and explicit representations on the treatments to learn robust and balanced featurecontextual covariate representations, which has been proved important in estimating prognostic and heterogeneous treatment effects (Alaa & Schaar, 2018; Curth & van der Schaar, 2021; Guo et al., 2021). This recipe also gives a uniï¬ed view and improved versatility when working with heterogeneous treatments and covariate types (Figure 2) for an intuitive comparison among popular methods and TransTEE.
As failing to account for selection bias1 can hurt TEE generalization (Alaa & Schaar, 2018), we propose to address it via an adversarial training algorithm consisting of a two-player zero-sum game between the outcome regression model and propensity score model (Rosenbaum & Rubin, 1983). Unlike traditional approaches, such as propensity weighting and matching/balancing (Hainmueller, 2012; Athey & Imbens, 2016) that are difï¬cult to apply with rich covariates and complex relationships, the proposed treatment regular-
1Selection bias occurs when the treatment assignment mechanism creates a discrepancy between the feature distributions of the treated/control population and the overall population, i.e. p(t) = Ï€(t|x).

ization (TR) and probabilistic version (PTR) serve as algorithmic randomizations. When combined with the expressiveness of TransTEE , they appear to mitigate the impact of selection bias. For continuous treatments, we provide justiï¬cation for the proposed two propensity score objective variants by analyzing the optimum of the discriminator under mild conditions.
In summary, we make the following contributions:
â€¢ We propose TransTEE, showing that Transformers, equipped with appropriate inductive biases and modeling capabilities, can be strong and versatile treatment effect estimators under Rubin-Neyman potential outcomes framework, which uniï¬es a wide range of neural treatment effect estimators.
â€¢ We introduce an adversarial training algorithm for propensity score modeling to effectively overcoming the selection bias, which further corroborates the expressiveness of TransTEE.
â€¢ Comprehensive experiments are conducted under various scenarios to verify the effectiveness of TransTEE and propensity score regularized adversarial training in estimating treatment effects. We show that TransTEE produces covariate adjustment interpretation and signiï¬cant performance gains given discrete, continuous or structured treatments on popular benchmarks including IHDP, News, TCGA even under treatment distribution shifts. Empirical study on pre-trained language models are conducted to show the real-world utility of TransTEE that implies potential applications.
2. Related Work
Neural Treatment Effect Estimation. There are many recent works on adapting neural networks to learn counterfactual representations for treatment effect estimation (Johansson et al., 2016; Shalit et al., 2017; Louizos et al., 2017; Yoon et al., 2018; Bica et al., 2020; Schwab et al., 2020; Nie et al., 2021; Curth & van der Schaar, 2021). To mitigate

Can Transformers be Strong Treatment Effect Estimators?

the imbalance of covariate representations across treatment groups, various approaches are proposed including optimizing distributional divergence (e.g. IPM including MMD, Wasserstein distance), entropy balancing (Zeng et al., 2020) (converges to JSD between groups), counterfactual variance (Zhang et al., 2020). However, their domain-speciï¬c designs make them limited to different treatments as shown in Table 1: methods like VCNet (Nie et al., 2021) use a hand-crafted way to map a real-value treatment to an ndimension vector with a constant mapping function, which is hard to converge under shifts of treatments (Table 5 in Appendix). Models like TARNet (Shalit et al., 2017) need an accurate estimation of the value interval of treatments. Previous estimators embed covariates to only one representation space by fully connected layers, tending to lose their connection and interactions. And it is non-trivial to adapt to the common settings given existing ad hoc designs on network architectures. For example, the case with n treatments and m associated dosage requires n Ã— m branches for methods like DRNet (Schwab et al., 2020) and 2n possible combinations for NCORE (Parbhoo et al., 2021), which put a rigid requirement on the extrapolation capacity and can be infeasible given available observational data.
Propensity Score. Most related works fundamentally rely on strongly ignorable conditions. Still even under ignorability, treatments may be selectively assigned according to propensities that depend on the covariates. To overcome the impact of such confounding, many statistical methods (Austin, 2011) like matching, stratiï¬cation, weighting, covariate adjustment, g-computation, have been proposed. More recent approaches include propensity dropout (Alaa et al., 2017), and multi-task Gaussian process (Alaa & van der Schaar, 2017). Explicitly modeling the propensity score, which reï¬‚ects the underlying policy for assigning treatments to subjects, has also shown to be effective in reasoning about the unobserved counterfactual outcomes and accounting for confounding. Based upon it, double robust estimators and targeted regularization are proposed to guarantee the consistency of estimated treatment effects under misspeciï¬cation of either the outcome or propensity score model (Kang & Schafer, 2007; Funk et al., 2011). However, most traditional approaches are restricted to binary treatments and the capacity of neural networks for such problems have not been fully leveraged.
3. Problem Statement and Assumptions
We consider a setting in which we are given N observed samples (xi, ti, si, yi)Ni=1, containing N pre-treatment covariates {xi âˆˆ Rp}Ni=1 and T available treatment options ti âˆˆ RT . For each sample, the potential outcome (Âµ-model) Âµ(x, t) or Âµ(x, t, s) is the response of the i-th sample to a treatment combination t out of the set of available treatment

options T , where each treatment can be associated a dosage sti âˆˆ R. The propensity score (Ï€-model) is the conditional probability of treatment assignment given the observed covariates Ï€(T = t|X = x). The above two models can be parameterized as ÂµÎ¸ and Ï€Ï†, respectively. The task is to estimate the Average Dose Response Function (ADRF): Âµ(x, t) = E[Y |X = x, do(T = t)] (Shoichet, 2006), which includes special cases in discrete treatment scenarios that can also be estimated as the average treatment effect (ATE): AT E = E[Âµ(x, 1) âˆ’ Âµ(x, 0)] and its individual version ITE.
What makes the above problem more challenging than supervised learning is that we never see the missing counterfactuals and ground truth causal effects in observational data. Therefore, we ï¬rst introduce the required fundamentally important assumptions that give the strongly ignorable condition such that statistical estimands can be interpreted causally.
Assumption 3.1. (Ignorability/Unconfoundedness) implies no hidden confounders such that Y (T = t) T |X (Y (0), Y (1) T |X in the binary treatment case).
Assumption 3.2. (Positivity/Overlap) The treatment assignment is non-deterministic such that, i.e. 0 < Ï€(t|x) < 1, âˆ€x âˆˆ X , t âˆˆ T

|= |=

Assumption 3.1 ensures the causal effect is identiï¬able,

implying that treatment is assigned independent of the po-

tential outcome and randomly for every subject regardless

of its covariates, which allows estimating ADRF using

Âµ(t) := E[Y |do(T = t)] = E[E[[Y |x, T = t]] (Rubin,

1978). One naive estimator of Âµ(x, t) = E[Y |X = x, T = t]

is the sample averages Âµ(t) =

n i=1

ÂµË†(xi,

t).

Assumption

3.2 states that there is a chance of seeing units in every

treated group.

4. TransTEE: Transformers as Treatment Effect Estimators
We are interested in estimating Âµ(t, x) = E[Y |X = x, T = t]. The systematic similarity of potential outcomes of different treatment groups is important for TEE (Curth & van der Schaar, 2021), which means naively feeding (x, t) to multilayer perceptrons is not favorable since treatment and covariate representations are not well discriminated and the impacts of treatment tend to be lost. As a result, various architectures (Curth & van der Schaar, 2021) and regularizations (Johansson et al., 2020) have been proposed to enforce structural similarity and difference among treatment groups. However, they are intricate and limited to speciï¬c use cases as shown in Section 2 and Figure 2. To remedy it, we propose a simple yet effective and scalable framework TransTEE, which tackles the problems of most existing treatment effect estimators (e.g., multiple/continuous/structured treatments, treatments interaction,

Can Transformers be Strong Treatment Effect Estimators?

Feed Forward

Embedding Layer

Multi-Head Attention

ğ‘¡ âˆˆ [ğ‘¡!, ğ‘¡"]

X

X
DragonNet (discrete)

X

X

ğ‘¡ âˆˆ [ğ‘¡", ğ‘¡#]

FlexTENet (discrete)

ğ‘¡ âˆˆ [ğ‘¡#, ğ‘¡$]
â‹¯
DRNet (continuous)

X
VCNet (continuous)

t
+
s
TransTEE

Figure 2. An schematic comparison of TransTEE and recent works including DragonNet(Shi et al., 2019), FlexTENet(Curth & van der Schaar, 2021), DRNet(Schwab et al., 2020) and VCNet(Nie et al., 2021). TransTEE handles all the scenarios without additional domain-speciï¬c expertise and parameter overhead.

and treatments with dosage) without the need for ad-hoc architectural designs, e.g. multiple branches.
The most crucial module in TransTEE is the attention layer (Vaswani et al., 2017): given d-dimensional query, key, and value matrices Q âˆˆ RdÃ—dk , K âˆˆ RdÃ—dk , V âˆˆ RdÃ—dv , attention model compute the outputs as

QK T

H(Q, K, V ) = softmax( âˆš )V,

(1)

dk

In practice, Multi-head Attention is preferable, which allows the model to jointly attend to information from different representation subspaces at different positions.

HM (Q, K, V ) = Concat(head1, ..., headh)W O where headi = H(QWiQ, KWiK , V WiV ), (2)

where the projections are learnable parameter matrices WiQ âˆˆ RdÃ—dk , WiV âˆˆ RdÃ—dk , WiV âˆˆ RdÃ—dv and W O âˆˆ Rhdv Ã—d .
As in Figure 2 (c), TransTEE ï¬rst embeds covariates x, treatments t, and associated dosages s into corresponding representations Mx âˆˆ RdÃ—p, Mt âˆˆ RdÃ—T , Ms âˆˆ RdÃ—T . The motivation is that separately embedding covariates and treatments as above preserves information in latent representations (Shalit et al., 2017) which also serves as an effective and important ï¬x to treatment distribution shifts (as indicated by our case study in Section 6.2).
Subsequently, the treatments and dosages are combined (projected, added, multiplied) to generate a new embedding Mst âˆˆ RdÃ—T . Unlike previous works that use hard (Johansson et al., 2016) or soft (Curth & van der Schaar, 2021) information sharing among treatment groups which are intricate and limited to speciï¬c use cases, we use the inductive

bias of self-attention to realize the goal.

MË† xl = HM (Mxlâˆ’1, Mxlâˆ’1, Mxlâˆ’1) + Mxlâˆ’1 Mxl = MLP(BN(MË† xl )) + MË† xl
(3) Mslt = HM (Mslâˆ’t 1, Mslâˆ’t 1, Mslâˆ’t 1) + Mslâˆ’t 1 Mslt = MLP(BN(MË† slt)) + MË† slt

where Mxl , Mslt is the output of layer and BN is the BatchNorm layer.

Different from previous works that embed all covariates by one full connected layer, where the differences between covariates tend to be lost and is hard to study the function of individual covariate in a sample. TransTEE learns different embeddings for each covariate and treatment, and then incorporates the interactions between them, which is implemented by a cross-attention module, treating Mst as query and Mx as both key and value.

MË† l = HM (Mslâˆ’t 1, Mxlâˆ’1, Mxlâˆ’1) + M lâˆ’1

M l = MLP(MË† l) + MË† l

(4)

yË† = MLP(Pooling(M L)),

where M L is the output of the last cross-attention layer. The inductive biases provided by such interactions are particularly important for adjusting proper covariate or confounder sets for estimating treatment effects (VanderWeele, 2019), which is intuitively illustrated in Figure 1 and corroborated in our experiment.

Denote yË† := ÂµÎ¸(t, x) and the training objective is the mean square error (MSE) of the outcome regression:

n

LÎ¸(x, y, t) = (yi âˆ’ ÂµÎ¸(ti, xi))2

(5)

i=1

5. Propensity Score Modeling
The above TransTEE model is conceptually simple and effective. However, when the sample size is small, it becomes

Can Transformers be Strong Treatment Effect Estimators?

important to account for selection bias (Alaa & Schaar, 2018). Thus we propose to learn a propensity score model Ï€Ï†(t|x) in order to explicitly account for it. Unlike previous works that use hand-crafted features or directly model the conditional density via maximum likelihood training, which is prone to high variance when handling high-dimensional, structured treatments (Singh et al., 2019) and can be problematic when we want to estimate a plausible propensity score from the generative model (Mohamed & Lakshminarayanan, 2016) (see the degraded performance of MLE in Table 3), TransTEE learns a propensity score network Ï€Ï†(t|x) via minimax bilevel optimization. The motivations for adversarial training between ÂµÎ¸(x, t) and Ï€Ï†(t|x) are three-fold: (i) it enforces the independence between treatment and covariate representations as shown in Proposition 1, which serves as algorithmic randomization in replace of costly randomized controlled trials (Rubin, 2007) for overcoming selection bias (Dâ€™Agostino, 1998; Imbens & Rubin, 2015); (ii) it explicitly models propensity Ï€Ï†(t|x) to reï¬ne treatment representations and promote covariate adjustment (Kaddour et al., 2021); and (iii) taking an adversarial domain adaptation perspective, the methodology is effective for learning invariant representations and further regularizes ÂµÎ¸(x, t) to be invariant to nuisance factors and may perform better empirically on some classes of distribution shifts (Ganin et al., 2016; Shalit et al., 2017; Zhao et al., 2018; Johansson et al., 2020; Wang et al., 2020). We refer readers for more discussions in Appendix C.
Based on the above discussion, when treatments are discrete, one might consider directly applying heuristic methods like adversarial domain adaptation (see Ganin et al. (2016); Zhao et al. (2018) for algorithmic development guidelines). We note the heuristic nature of domain-adversarial methods (see (Wu et al., 2019) for clear failure cases), and a debunking of the common claim that Ben-David et al. (2010) guarantees the robustness of such methods. Here, we focus on continuous TEE, a more general and challenging scenario, where we want to estimate ADRF, and propose two variants of LÏ† as an adversary for the outcome regression objective LÎ¸ in Eq. 5 accordingly. The process is shown as Eq. 6 below:

min max LÎ¸(x, y, t) âˆ’ LÏ†(x, t)

(6)

Î¸Ï†

|=

Such algorithmic randomization using propensity score creates subgroups of different treated units as if they had been randomly assigned to different treatments such that conditional independence T X | Ï€(T |X) is enforced across strata and continuation, which â€˜approximatesâ€™ a random block experiment with respect to the observed covariates (Imbens & Rubin, 2015).
Below we introduce two variants of LÏ†(x, t):
Treatment Regularization (TR) is a standard MSE over the treatment space given the predicted treatment tË†i and the

ground truth ti.

n

LTÏ† R(x, t) =

ti âˆ’ Ï€Ï†(tË†i|xi) 2

(7)

i=1

TR is explicitly matching the mean of the propensity score to that of the treatment. In an ideal case, the Ï€(t|x) should be uniformly distributed given different x. However, the above treatment regularization procedure only provides matching for the mean of the propensity score, which can be prone to bad equilibriums and treatment misalignment (Wang et al., 2020). Thus, we introduce the distribution of t and model the uncertainty rather than predicting a scalar t:
Probabilistic Treatment Regularization (PTR) is a probabilistic version of TR which models the mean Âµ (with a slight abuse of notation) and variance Ïƒ2 of estimated treatment tË†i.

P T R n (ti âˆ’ Ï€Ï†(Âµ|xi))2 1

2

LÏ† =

2Ï€Ï†(Ïƒ2|xi) + 2 log Ï€Ï†(Ïƒ |xi) (8)

i=1

The PTR matches the whole distribution, i.e. both the mean and variance, of the propensity score to that of the treatment, which can be preferable in certain cases.
Equilibrium of the Minimax Game. We analyze that TR and PTR can align the ï¬rst and second moment of continuous treatments at equilibrium respectively, and thus promote the independence between treatment t and covariate x.
Proposition 1. (The optimum of propensity score model (Wang et al., 2020)) In the equilibrium of the game, assuming the outcome prediction model is ï¬xed, then the optimum of TR is achieved when E[t|x] = E[t], âˆ€x via matching the mean of propensity score Ï€(t|x) and the marginal distribution p(x) and the optimum discriminator of PTR is achieved via matching both the mean and variance such that E[t|x] = E[t], V[t|x] = V[t], âˆ€x. See Appendix B for the proof.

6. Experimental Results
6.1. Experimental Settings.
Datasets. Since the true counterfactual outcome (or ADRF) are rarely available for real-world data, previous works often use synthetic or semi-synthetic data for empirical evaluation. Following this convention, we use one synthetic dataset and two semi-synthetic datasets: For continuous treatments, we use the IHDP and News datasets, and for continuous dosages, we obtain covariates from a real dataset TCGA (Chang et al., 2013) and generate treatments, where each treatment is accompanied by a dosage. The resulting dataset is named TCGA (D). Following (Kaddour et al., 2021), datasets for structured treatments include

Can Transformers be Strong Treatment Effect Estimators?

METHODS
SCIGAN TARNET(D) DRNET(D) VCNET(D) TRANSTEE TRANSTEE + TR TRANSTEE + PTR

#TREATMENT=1

IN-SAMPLE

OUT-SAMPLE

5.6966 Â± 0.0000 0.7888 Â± 0.0609 0.8034 Â± 0.0469 0.1566 Â± 0.0303 0.0573 Â± 0.0361 0.0495 Â± 0.0176 0.0343 Â± 0.0096

5.6546 Â± 0.0000 0.7908 Â± 0.0606 0.8052 Â± 0.0466 0.1579 Â± 0.0301 0.0585 Â± 0.0358 0.0509 Â± 0.0180 0.0355 Â± 0.0094

#TREATMENT=2

IN-SAMPLE

OUT-SAMPLE

2.0924 Â± 0.0000 1.4207 Â± 0.0784 1.3739 Â± 0.0858 0.2919 Â± 0.0743 0.0550 Â± 0.0137 0.0663 Â± 0.0268 0.0679 Â± 0.0252

2.3067 Â± 0.0000 1.4206 Â± 0.0777 1.3738 Â± 0.0853 0.2918 Â± 0.0737 0.0556 Â± 0.0129 0.0671 Â± 0.0268 0.0686 Â± 0.0252

#TREATMENT=3

IN-SAMPLE

OUT-SAMPLE

4.3183 Â± 0.0000 3.1982 Â± 0.5847 2.8632 Â± 0.4227 0.6459 Â± 0.1387 0.2803 Â± 0.0658 0.2618 Â± 0.0737 0.2645 Â± 0.0702

4.6231 Â± 0.0000 3.1920 Â± 0.5746 2.8558 Â± 0.4143 0.6493 Â± 0.1397 0.2768 Â± 0.0639 0.2577 Â± 0.0726 0.2597 Â± 0.0675

Table 2. Performance of individualized treatment-dose response estimation on the TCGA (D) dataset with different number of treatments. We report AMSE and standard deviation over 30 repeats. The selection bias on treatment and dosage are both set to be 2.0.

METHODS
TARNET DRNET VCNET TRANSTEE TRANSTEE+MLE TRANSTEE+TR TRANSTEE+PTR

VANILLA (BINARY)
0.3670 Â± 0.61112 0.3543 Â± 0.60622 0.2098 Â± 0.18236 0.0983 Â± 0.15384 0.1721 Â± 0.40061 0.1913 Â± 0.29953 0.2193 Â± 0.34667

VANILLA (h = 1)
2.0152 Â± 1.07449 2.1549 Â± 1.04483 0.7800 Â± 0.61483 0.1151 Â± 0.10289 0.0877 Â± 0.03352 0.0781 Â± 0.03243 0.0762 Â± 0.07915

EXTRAPOLATION (h = 2)
12.967 Â± 1.78108 11.071 Â± 0.99384
NAN
0.2745 Â± 0.14976 0.2685 Â± 0.17552 0.2393 Â± 0.08154 0.2352 Â± 0.17095

VANILLA (h = 5)
5.6752 Â± 0.53161 3.2779 Â± 0.42797
NAN
0.1621 Â± 0.14443 0.2079 Â± 0.17637 0.1143 Â± 0.03224 0.1363 Â± 0.08036

EXTRAPOLATION (h = 5)
31.523 Â± 1.5013 31.524 Â± 1.50264
NAN
0.2066 Â± 0.23258 0.1476 Â± 0.07123 0.0947 Â± 0.0824 0.1363 Â± 0.08035

Table 3. Experimental results comparing neural network based methods on the IHDP datasets. We report the results based on 100 repeats, and numbers after Â± are the estimated standard deviation of the average value. For the vanilla setting with binary treatment, we report the mean absolute difference between the estimated and true ATE. For Extrapolation (h = 2), models are trained with t âˆˆ [0.1, 2.0] and tested in t âˆˆ [0, 2.0]. For Extrapolation (h = 5), models are trained with t âˆˆ [0.25, 5.0] and tested in t âˆˆ [0, 5].

Small-World (SW), which contains 1, 000 uniformly sampled covariates and 200 randomly generated Wattsâ€“Strogatz small-world graphs (Watts & Strogatz, 1998) as treatments, and TCGA (S), which uses 9, 659 gene expression measurements of cancer patients (Chang et al., 2013) for covariates and 10, 000 sampled molecules from the QM9 dataset (Ramakrishnan et al., 2014) as treatments. For the study on language models, we use The Enriched Equity Evaluation Corpus (EEEC) dataset (Feder et al., 2021).
Baselines. Baselines for continuous treatments include TARnet (Shalit et al., 2017), Dragonnet (Shi et al., 2019), DRNet (Schwab et al., 2020), and VCNet (Nie et al., 2021). SCIGAN (Bica et al., 2020) is chosen as the baseline for continuous dosages. Besides, we revise DRNet (Schwab et al., 2020), TARNet (Shalit et al., 2017), and VCNet (Nie et al., 2021) to DRNet (D), TARNet (D), VCNet (D), respectively, which enable multiple treatments and dosages. Speciï¬cally, DRNet (D) has T main ï¬‚ows, each corresponding to a treatment and is divided to BD branches for continuous dosage. Baselines for structured treatments include Zero (Kaddour et al., 2021), GNN (Kaddour et al., 2021), GraphITE (Harada & Kashima, 2021), and SIN (Kaddour et al., 2021). To compare the performance of different frameworks fairly, all of the models regress on the outcome with empirical samples without any regularization. For MLE training of the propensity score model, the objective is the

negative log-likelihood: LÏ† := âˆ’ n1

n i=1

log

Ï€Ï†

(ti

|xi

).

Evaluation Metric. For continuous treatments, we use the average mean squared error on the test set. For structured treatment, following (Kaddour et al., 2021), we rank all treatments by their propensity p(t|x) in descending order. The top K treatments are selected and the treatment effect of each treatment pair is evaluated by unweighted/weighted expected Precision in Estimation of Heterogeneous Effect (PEHE) (Kaddour et al., 2021), where the WPEHE@K accounts for the fact that treatments pairs that are less likely will have higher estimation errors and should be given less importance. For multi-treatments and dosages, AMSE is calculated over all dosage and treatment pairs, resulting in AMSED. See Appendix E for detailed deï¬nition of metrics.

See Appendix E for full details of all experimental settings and Appendix F many more results and remarks.

6.2. Case Study and Numerical Results
Case study on treatment distribution shifts We start by conducting a case study on treatment distribution shifts (Figure 3), and exploring an extrapolation setting in which the treatment may subsequently be administered at dosages never seen before during training. Surprisingly, we ï¬nd that while standard results rely constraining the values of treatments (Nie et al., 2021) and dosages (Schwab et al.,

5HVSRQVH 5HVSRQVH 5HVSRQVH

Can Transformers be Strong Treatment Effect Estimators?









7UXWK



7DUQHW

'UQHW



9FQHW

7UDQV7((

      7UHDWPHQW





7UXWK



7DUQHW

'UQHW

9FQHW



7UDQV7((

      7UHDWPHQW







7UXWK



7DUQHW

'UQHW



9FQHW

7UDQV7(( 
 7UHDWPHQW

(a) h = 1 during training and testing. (b) h = 1.75 during traning and h = 2 (c) h = 5 during training and testing. during testing (extrapolation).

Figure 3. Estimated ADRF on the synthetic dataset where treatments are sampled from an interval [l, h], where l = 0.

2020) to a speciï¬c range, our methods perform surprisingly well when extrapolating beyond these ranges as assessed on several empirical benchmarks. By comparison, many other methods appear comparatively brittle on these same settings. See Appendix D for detailed discussion and analysis.
Continuous treatments. To evaluate the efï¬ciency with which TransTEE estimates the average dose-response curve (ADRF), we compare against other recent neural networkbased methods (Tables 3). Comparing results in each column, we observe performance boosts for TransTEE. Further, TransTEE attains a much smaller loss than baselines in cases where the treatment interval is not restricted to [0, 1] (e.g., t âˆˆ [0, 5]) and when the training and test treatment intervals are different (extrapolation). Interestingly, even vanilla TransTEE produces competitive performance compared with that of Ï€(t|x) trained additionally using MLE, demonstrating the ability of TransTEE to effectively model treatments and covariates. The estimated ADRF curves on the IHDP and News datasets are shown in Figure 11 and Figure 12 in the Appendix. TARNet and DRNet produce discontinuous ADRF estimators and VCNet only performs well on a ï¬xed treatment interval t âˆˆ [0, 1]. However, TransTEE attains lower estimation error and preserves the continuity of ADRF on different treatment intervals.
Effectiveness of adversarial training and propensity modeling. As in Table 2, 3, 9, with the addition of the adversarial training as well as TR and PTR, TransTEEâ€™s estimation loss with continuous treatments can be further reduced. Overall, TR is better in the continuous case with smaller treatment distribution shifts, while PTR is preferable when shifts are greater. Both TR and PTR cannot bring performance gains over discrete cases. The superiority of TR and PTR in combination with TransTEE over comprehensive existing works, especially in semi-synthetic benchmarks like IHDP that may systematically favor some

Weight

0.200 0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425
Covariates
Figure 4. The distribution of learned weights for the cross-attention module on the IHDP dataset. TransTEE adjusts confounders Scon = {1, 2, 3, 5, 6} properly with higher weights during the cross attention process.
types of algorithms over others (Curth et al., 2021), also calls for more understanding of NNsâ€™ inductive biases in treatment effect estimation problems of interest. Moreover, covariate selection visualization in TR and PTR (Appendix F) supports the idea that modeling the propensity score essentially promotes covariate adjustment and partials out the effects from the covariates on the treatment features.
Continuous dosage. In Table 2, we compare TransTEE against baselines on the TCGA (D) dataset with default treatment selection bias 2.0 and dosage selection bias 2.0. As the number of treatments increases, TransTEE and its variants (with regularization term) consistently outperform the baselines by a large margin on both training and test data. TransTEEâ€™s effectiveness is also shown in Figure 8, where the estimated ADRF curve of each treatment considering continuous dosages is plotted. Compared to

Can Transformers be Strong Treatment Effect Estimators?

TC
Gender [CI]
Race [CI]

Correlation/Representation Based Baselines

ATEGT

TReATE

CONEXP

INLP

0.086 [0.082,0.09]

0.125 [0.110,0.14]

0.02 [0.0,0.05]

0.313 [0.304,0.321]

0.014

0.046

0.08

0.591

[0.012,0.016] [0.038,0.054] [0.02,0.014] [0.578,0.605]

TarNet
0.0067 [0.0049, 0.0076]
0.005 [0.0021, 0.0069]

Treatment Effect Estimators

DRNet

VCNet

0.0088 [0.0084,0.009]

0.0085 [0.0036, 0.0111]

0.006

0.003

[0.0047, 0.0081] [0.0025, 0.0037]

TransTEE
0.013 [0.008, 0.0168]
0.0174 [0.0113, 0.0238]

Table 4. Effect of Gender (top) and Race (bottom) on POMS classiï¬cation with the EEEC dataset, where ATEGT is the ground truth ATE based on 3 repeats with conï¬dence intervals [CI] constructed using standard deviations.

Log#Params

20 15 10 5 0 IHDP

TransTEE VCNet DRNet TARNet NEWS

TransTEE VCNet (D) DRNet (D) TARNet (D) SCIGAN TCGA(D)

TransTEE SIN GraphITE TCGA(S)

Figure 5. Number of parameters for different models on four different datasets.

baselines, TransTEE attains more accurate results on all these treatments. Stronger selection bias in the observed data makes estimation more difï¬cult because it becomes less likely to see certain treatments or particular covariates. Considering different dosage and treatment selection bias, Figure 7 shows that as biases increase, all methods attain higher AMSE, with TransTEE consistently performing best.
Structured treatments. We compared the performance of TransTEE to baselines on the training and test set of both SW and TCGA datasets with varying degrees of treatment selection bias. The numerical results are shown in Table 11. The performance gain between GNN and Zero indicates that taking into account of graph information signiï¬cantly improves estimation. The results suggest that, overall, the performance of TransTEE is the best due to the strong modeling capability and advanced model structure for processing high-dimensional treatments. SIN is the best model among these baselines. However, when the bias is equal to 0.1, SIN fails to attain estimation results better than the Zero baseline. To evaluate each modelâ€™s robustness to varying levels of selection bias, performances curve with Îº âˆˆ [0, 40] for the SW dataset and Îº âˆˆ [0, 0.5] for the TCGA dataset are shown in Figure 13 and Figure 14 in the Appendix. Considering both the WPEHE@K and UPEHE@K metrics, TransTEE outperforms baselines by a large margin across the entire range of evaluated treatment selection biases.
6.3. Analysis
Effects and interpretability of covariate selection by the cross-attention module. One clear advantage of TransTEE over existing works is the interpretability of the covariate adjustment process using attention weights:

we visualize the covariate selection results (cross-attention weights) in Figure 4. Interestingly, confounders Scon = {1, 2, 3, 5, 6} are assigned higher weights while noisy covariates (those inï¬‚uence the outcome but irrelevant to the treatment) lower Sdis,1 = {4, 7 âˆ¼ 15} to avoid spurious correlations, which matches the principles in (VanderWeele, 2019) and corroborate the ability of TransTEE to estimate treatment effects in complex datasets. Considering each modelâ€™s robustness to varying numbers of noisy covariates, Figure 6 shows that TransTEE consistently outperforms baselines across different numbers of noisy covariates (refer to Appendix F for detailed analysis).
Amount of model parameters comparison. The experiment is to corroborate the conceptual comparison in Table 1. We ï¬nd that the proposed TransTEE has comparable or fewer parameters than baselines on all the settings as shown in Figure 5. Besides, enlarging the number of treatments for more accurate approximation for continuous treatments/dosages, most of these baselines need to increase branches which incurs parameter redundancy. However, TransTEE is much more efï¬cient in comparison.



0RGHO

7UDQV7((

 9'&511HHWW

 7$51HW

$06(





                    1XPEHURI&RYDULDWHVWKDW2QO\,QIOXHQFHWKH2XWFRPH
Figure 6. AMSE attained by models on IHDP with different numbers of noisy covariates.

6.4. Empirical Study on Pretrained Language Models
To evaluate the real-world utility of TransTEE, we use it to estimate the treatment effects for detecting domainspeciï¬c factors of variation (e.g., racial and gender-related

Can Transformers be Strong Treatment Effect Estimators?

nourns) over natural language. We use both the correlation/representation based baselines introduced in (Feder et al., 2021) and implement treatment effect estimators (e.g., TARnet (Shalit et al., 2017), DRNet (Schwab et al., 2020), VCNet (Nie et al., 2021), and the proposed TransTEE).
Interestingly, results in Table 4 show that TransTEE effectively estimates the treatment effects of domain-speciï¬c variation perturbations even without substantive downstream ï¬ne-tuning on specialized datasets. TransTEE outperforms baselines adapted from MLP on the (real) EEEC dataset. Moreover, we showcase the top-k samples with the maximal/minimal ITE and analysis in Appendix F.2.
The results show that TransTEE has the potential to provide estimators for practical use cases. For example, those identiï¬ed samples can provide actionable insights like function as contrast sets for analyzing and understanding LMs (Gardner et al., 2020) and TransTEE can estimate ATE to enforce invariant or fairness constraints for LMs (Veitch et al., 2021) in a lightweight and efï¬cient manner, which we leave for future work.
7. Concluding Remarks
In this work, we show that transformers can be effective and versatile treatment effect estimators, especially trained as a minimax game between outcome model and propensity score model to further reduce the impacts of selection bias. Extensive experiments well verify the effectiveness and utility of TransTEE. Since adjusting covariates without accounting for the causal graph might yield inaccurate estimates of the causal effect (Pearl, 2009), one important future direction is extending TransTEE to settings with more complex causal graphs and generate identiï¬able causal functionals tractable for optimization (Jung et al., 2020) supported by identiï¬cation theory. How to integrate TransTEE with domain knowledge (Imbens & Rubin, 2015) for validating causal inference models learned from observational data to avoid invalid causal estimates will also be important.

References
Alaa, A. and Schaar, M. Limits of estimating heterogeneous treatment effects: Guidelines for practical algorithm design. In International Conference on Machine Learning, pp. 129â€“138. PMLR, 2018.
Alaa, A. M. and van der Schaar, M. Bayesian inference of individualized treatment effects using multi-task gaussian processes. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 3427â€“3435, 2017.
Alaa, A. M., Weisz, M., and Van Der Schaar, M. Deep counterfactual networks with propensity-dropout. arXiv preprint arXiv:1706.05966, 2017.
Athey, S. and Imbens, G. Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27):7353â€“7360, 2016.
Austin, P. C. An introduction to propensity score methods for reducing the effects of confounding in observational studies. Multivariate behavioral research, 46(3):399â€“424, 2011.
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. A theory of learning from different domains. Machine learning, 79(1):151â€“ 175, 2010.
Bica, I., Jordon, J., and van der Schaar, M. Estimating the effects of continuous-valued interventions using generative adversarial networks. Advances in neural information processing systems (NeurIPS), 2020.
Chang, K., Creighton, C. J., Davis, C., Donehower, L., Drummond, J., Wheeler, D., Ally, A., Balasundaram, M., Birol, I., Butterï¬eld, Y. S., et al. The cancer genome atlas pan-cancer analysis project. Nat Genet, 45(10): 1113â€“1120, 2013.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Curth, A. and van der Schaar, M. On inductive biases for heterogeneous treatment effect estimation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.

Curth, A., Svensson, D., Weatherall, J., and van der Schaar, M. Really doing great at estimating CATE? a critical look at ML benchmarking practices in treatment effect estimation. In Thirty-ï¬fth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.

Can Transformers be Strong Treatment Effect Estimators?

Dâ€™Agostino, R. B. Propensity score methods for bias reduction in the comparison of a treatment to a non-randomized control group. Statistics in medicine, 17(19):2265â€“2281, 1998.
De Luna, X., Waernbaum, I., and Richardson, T. S. Covariate selection for the nonparametric estimation of an average treatment effect. Biometrika, 98(4):861â€“875, 2011.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Ding, P., VanderWeele, T., and Robins, J. M. Instrumental variables as bias ampliï¬ers with general outcome and confounding. Biometrika, 104(2):291â€“302, 2017.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021a.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021b.
Feder, A., Oved, N., Shalit, U., and Reichart, R. Causalm: Causal model explanation through counterfactual language models. Computational Linguistics, 47(2):333â€“ 386, 2021.
Funk, M. J., Westreich, D., Wiesen, C., StuÂ¨rmer, T., Brookhart, M. A., and Davidian, M. Doubly robust estimation of causal effects. American journal of epidemiology, 173(7):761â€“767, 2011.
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096â€“2030, 2016.
Gardner, M., Artzi, Y., Basmov, V., Berant, J., Bogin, B., Chen, S., Dasigi, P., Dua, D., Elazar, Y., Gottumukkala, A., et al. Evaluating modelsâ€™ local decision boundaries via contrast sets. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 1307â€“1323, 2020.
Guo, Z., Zheng, S., Liu, Z., Yan, K., and Zhu, Z. Cetransformer: Casual effect estimation via transformer based representation learning. In Chinese Conference on Pattern

Recognition and Computer Vision (PRCV), pp. 524â€“535. Springer, 2021.
Hainmueller, J. Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political analysis, 20 (1):25â€“46, 2012.
Harada, S. and Kashima, H. Graphite: Estimating individual effects of graph-structured treatments. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pp. 659â€“668, 2021.
Hill, J. L. Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1):217â€“240, 2011.
Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359â€“366, 1989.
Imbens, G. W. and Rubin, D. B. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015.
Johansson, F., Shalit, U., and Sontag, D. Learning representations for counterfactual inference. In International conference on machine learning, pp. 3020â€“3029. PMLR, 2016.
Johansson, F. D., Shalit, U., Kallus, N., and Sontag, D. Generalization bounds and representation learning for estimation of potential outcomes and causal effects. arXiv preprint arXiv:2001.07426, 2020.
Jung, Y., Tian, J., and Bareinboim, E. Learning causal effects via weighted empirical risk minimization. Advances in neural information processing systems, 33, 2020.
Kaddour, J., Zhu, Y., Liu, Q., Kusner, M. J., and Silva, R. Causal effect inference for structured treatments. Advances in Neural Information Processing Systems, 34, 2021.
Kang, J. D. and Schafer, J. L. Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data. Statistical science, pp. 523â€“539, 2007.
Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207â€“1216, Stanford, CA, 2000. Morgan Kaufmann.
Louizos, C., Shalit, U., Mooij, J. M., Sontag, D., Zemel, R. S., and Welling, M. Causal effect inference with deep latent-variable models. In NeurIPS, 2017.

Can Transformers be Strong Treatment Effect Estimators?

Mohamed, S. and Lakshminarayanan, B. Learning in implicit generative models. arXiv preprint arXiv:1610.03483, 2016.
Newman, D. Bag of words data set. 2008.
Nie, L., Ye, M., Liu, Q., and Nicolae, D. Vcnet and functional targeted regularization for learning causal effects of continuous treatments. ICLR, 2021.
Parbhoo, S., Bauer, S., and Schwab, P. Ncore: Neural counterfactual representation learning for combinations of treatments. arXiv preprint arXiv:2103.11175, 2021.
Pearl, J. Causality. Cambridge university press, 2009.
Ramakrishnan, R., Dral, P. O., Rupp, M., and Von Lilienfeld, O. A. Quantum chemistry structures and properties of 134 kilo molecules. Scientiï¬c data, 1(1):1â€“7, 2014.
Rosenbaum, P. R. and Rubin, D. B. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41â€“55, 1983.
Rubin, D. B. Bayesian inference for causal effects: The role of randomization. The Annals of statistics, pp. 34â€“58, 1978.
Rubin, D. B. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(469):322â€“331, 2005.
Rubin, D. B. The design versus the analysis of observational studies for causal effects: parallels with the design of randomized trials. Statistics in medicine, 26(1):20â€“36, 2007.
Schwab, P., Linhardt, L., and Karlen, W. Perfect match: A simple method for learning representations for counterfactual inference with neural networks. arXiv, 2018.
Schwab, P., Linhardt, L., Bauer, S., Buhmann, J. M., and Karlen, W. Learning counterfactual representations for estimating individual dose-response curves. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 34, pp. 5612â€“5619, 2020.
Shalit, U., Johansson, F. D., and Sontag, D. Estimating individual treatment effect: generalization bounds and algorithms. In International Conference on Machine Learning, pp. 3076â€“3085. PMLR, 2017.
Shi, C., Blei, D., and Veitch, V. Adapting neural networks for the estimation of treatment effects. Advances in Neural Information Processing Systems, 32:2507â€“2517, 2019.
Shoichet, B. K. Interpreting steep dose-response curves in early inhibitor discovery. Journal of medicinal chemistry, 49(25):7274â€“7277, 2006.

Singh, R., Sahani, M., and Gretton, A. Kernel instrumental variable regression. Advances in Neural Information Processing Systems, 32:4593â€“4605, 2019.
Tsai, Y.-H. H., Bai, S., Liang, P. P., Kolter, J. Z., Morency, L.-P., and Salakhutdinov, R. Multimodal transformer for unaligned multimodal language sequences. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Florence, Italy, 7 2019. Association for Computational Linguistics.
Van Der Laan, M. J. and Rubin, D. Targeted maximum likelihood learning. The international journal of biostatistics, 2(1), 2006.
VanderWeele, T. J. Principles of confounder selection. European journal of epidemiology, 34(3):211â€“219, 2019.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998â€“6008, 2017.
Veitch, V., Dâ€™Amour, A., Yadlowsky, S., and Eisenstein, J. Counterfactual invariance to spurious correlations in text classiï¬cation. Advances in Neural Information Processing Systems, 34, 2021.
Wang, H., He, H., and Katabi, D. Continuously indexed domain adaptation. In International Conference on Machine Learning, pp. 9898â€“9907. PMLR, 2020.
Wang, H., Huang, Z., Zhang, H., and Xing, E. Toward learning human-aligned cross-domain robust models by countering misaligned features. arXiv preprint arXiv:2111.03740, 2021.
Watts, D. J. and Strogatz, S. H. Collective dynamics of â€˜small-worldâ€™networks. nature, 393(6684):440â€“442, 1998.
Wooldridge, J. M. Introductory econometrics: A modern approach. Cengage learning, 2015.
Wu, Y., Winston, E., Kaushik, D., and Lipton, Z. Domain adaptation with asymmetrically-relaxed distribution alignment. In International Conference on Machine Learning, pp. 6872â€“6881. PMLR, 2019.
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y. Do transformers really perform bad for graph representation? arXiv preprint arXiv:2106.05234, 2021.
Yoon, J., Jordon, J., and Van Der Schaar, M. Ganite: Estimation of individualized treatment effects using generative adversarial nets. In International Conference on Learning Representations, 2018.

Can Transformers be Strong Treatment Effect Estimators?
Zeng, S., Assaad, S., Tao, C., Datta, S., Carin, L., and Li, F. Double robust representation learning for counterfactual prediction, 2020.
Zhang, H., Zhang, Y.-F., Liu, W., Weller, A., SchoÂ¨lkopf, B., and Xing, E. P. Towards principled disentanglement for domain generalization. arXiv preprint arXiv:2111.13839, 2021.
Zhang, Y., Bellot, A., and van der Schaar, M. Learning overlapping representations for the estimation of individualized treatment effects. In Chiappa, S. and Calandra, R. (eds.), Proceedings of the Twenty Third International Conference on Artiï¬cial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 1005â€“1014. PMLR, 26â€“28 Aug 2020. URL https://proceedings.mlr.press/ v108/zhang20c.html.
Zhao, H., Zhang, S., Wu, G., Moura, J. M., Costeira, J. P., and Gordon, G. J. Adversarial multiple source domain adaptation. Advances in neural information processing systems, 31:8559â€“8570, 2018.

Can Transformers be Strong Treatment Effect Estimators?

Can Transformers be Strong Treatment Effect Estimators? â€“ Appendix â€“

A. Extended Related Work
Transformers and Attention Mechanisms Transformer models (Vaswani et al., 2017) have recently demonstrated exemplary performance on a broad range of language tasks e.g., text classiï¬cation, machine translation, and question answering. Recently Transformer models and their variants have been successfully adapted to visual recognition (Dosovitskiy et al., 2021b), programming language (Chen et al., 2021), and graph (Ying et al., 2021) due to their strong ï¬‚exibility and expressiveness. There is an initial attempt to leverage the attention mechanism for learning balanced covariate representations (Guo et al., 2021). However, the proposed CETransformer is fundamentally different from ours since TransTEE simultaneously embeds covariates and treatments and is scalable to various settings.
Domain Adaptation There are some intrinsic connections between causal inference and domain adaptation, in particular, outof-distribution robustness. Intuitively, traditional domain adversarial training learns representations that are indistinguishable by the domain classiï¬er by minimizing the worst-domain empirical error (Ganin et al., 2016; Zhao et al., 2018; Wang et al., 2021; Zhang et al., 2021). The algorithmic insights can be handily translated to the TEE domain (Shalit et al., 2017; Johansson et al., 2020; Feder et al., 2021). Here we also have the desideratum that covariate representations should be balanced such that the selection bias is minimized and the effect is maximally determined by the treatment. Algorithmically, when the treatment is continuous, we connect our method to continuously indexed domain adaptation (Wang et al., 2020). Our formulation and algorithm also serve to build connections to a diverse set of statistical thinking on causal inference and domain adaptation, of which much can be gained by mutual exchange of ideas (Johansson et al., 2020). Explicitly modeling the propensity score also seeks to connect causal inference with transfer learning to inspire domain adaptation methodology and holds the potential to handle a wider range of problems like hidden stratiï¬cation in domain generalization, which we leave for future work.

B. Analysis of the Equilibrium of the Minimax Game
Proof. The proof is adapted from (Wang et al., 2020) as a special case when there are only two players, i.e. ÂµÎ¸ and Ï€Ï†. Given the outcome regression model ÂµÎ¸ ï¬xed, the optimal propensity score model Ï€âˆ— is

Ï€âˆ— = arg min LÏ†(x, t)
Ï€

= arg min E(x,t)âˆ¼p(x,t) t âˆ’ Ï€Î¸ tË†|x 2

(9)

Ï€

= arg min Exâˆ¼p(x)Etâˆ¼p(t|x) t âˆ’ Ï€Î¸ tË†|x 2
Ï€

The inner minimum is achieved at Ï€Î¸âˆ— tË†|x = Etâˆ¼p(t|x)[t] given the following quadratic form: E(x,t)âˆ¼p(x,t) t âˆ’ Ï€Î¸ tË†|x 2 = Etâˆ¼p(t|x)[t2] âˆ’ 2Ï€Î¸ tË†|x Etâˆ¼p(t|x)[t] + Ï€Î¸ tË†|x 2 (10)

We assume the above optimum condition of the propensity score model always holds with respect to the outcome regression model during training, then the minimax game in Eq. 6 can be converted to maximizing the inner loop:

max âˆ’LÏ†(x, t) = LÏ†âˆ— (x, t)
Ï†
= E(x,t)âˆ¼p(x,t) t âˆ’ Etâˆ¼p(t|x)[t] 2 = Exâˆ¼p(x)Etâˆ¼p(t|x)âˆ¼p(x,t) t âˆ’ Etâˆ¼p(t|x)[t] 2 = Exâˆ¼p(x)Vtâˆ¼p(t|x)[t] = ExV[t|x]

(11)

Can Transformers be Strong Treatment Effect Estimators?

Next we show the difference between Eq. 11 and the variance of the treatment V[t]:

Exâˆ¼p(x)Vtâˆ¼p(t|x)[t] âˆ’ V[z]

=Exâˆ¼p(x)[E[t2|x] âˆ’ E[t|x]2] âˆ’ (E[t2] âˆ’ E[t]2)

=E[t]2 âˆ’ Ex[E[t|x]2] = Ex[E[t|x]]2 âˆ’ Ex[E[t|x]2]

(12)

â‰¤Ex[E[t|x]2] âˆ’ Ex[E[t|x]2] = 0

where the last inequality is by Jensenâ€™s inequality and the convexity of t2. The optimum is achieved when E[t|x] is constant w.r.t x and so E[t|x] = E[t], âˆ€x.
The proof process for PTR is similar but include the derivation of variance matching.

Ï€âˆ— = arg min LÏ†(x, t)
Ï€

(E[t|x] âˆ’ t)2 log V[t|x]

= argÏ€min E(x,t)âˆ¼p(x,t) 2V[t|x] + 2

(13)

= arg min Exâˆ¼p(x)Etâˆ¼p(t|x)
Ï€

(E[t|x] âˆ’ t)2 log V[t|x]

+

2V[t|x]

2

The ï¬rst term can be reduce to a constant given the deï¬nition of variance:

(E[t|x] âˆ’ t)2

V[t|x]

1

Exâˆ¼p(x)Etâˆ¼p(t|x) 2V[t|x] = Exâˆ¼p(x) 2V[t|x] = 2 (14)

The second term can be upper bounded by using Jensenâ€™s inequality:

Exâˆ¼p(x)Etâˆ¼p(t|x) log V[t|x] â‰¤ 1 log Exâˆ¼p(x)[V[t|x]] â‰¤ 1 log (V[t]) (15)

2

2

2

Combining Eq.

14 and Eq.

15, the optimum

1 2

+

1 2

log

(V[t])

is

achieved

when

E[t|x],

V[t|x]

is

constant

w.r.t

x

and

so E[t|x] = E[t], V[t|x] = V[t], âˆ€x according to the equality conditions of the ï¬rst and second inequality in Eq. 15,

respectively.

C. Discussions on the Propensity Score Modelling
We ï¬rst discuss the fundamental differences and common goals between our algorithm and traditional ones: as a general approach to causal inference, TransTEE can be directly harnessed with traditional methods that estimate propensity score by including hand-crafted features of covariates (Imbens & Rubin, 2015) to reduce bias through matching, weighting, sub-classiï¬cation, covariate adjustment (Austin, 2011), targeted regularization (Van Der Laan & Rubin, 2006) or conditional density estimation (Nie et al., 2021) that create quasi-randomized experiments (Dâ€™Agostino, 1998). Thatâ€™s because the uniï¬ed framework provides an advantage to use an off-the-shelf propensity score regularizer for balancing covariate. Similar to the goal of traditional approaches like inverse probability weighting and propensity score matching (Austin, 2011), which attempts to weigh single observation to mimic the effects of randomization with respect to the covariate of treatment of interest, we refer to the above minimax game for algorithmic randomization in replace of costly randomized controlled trials.
Speciï¬cally, to overcome selection bias over representation space, the bilevel optimization enforces effective treatment effect estimation while modeling the discriminative propensity features to partial out parts of covariates that cause the treatment but not the outcome and dispose of nuisance variations of covariates (Kaddour et al., 2021). Such a recipe can account for selection bias where Ï€(t|x) = p(t) and leave spurious correlations out. Such implicit generative modeling can also be more robust under model misspeciï¬cation and especially in the settings that require extrapolation on treatment (See experimental results in Table 3).

Can Transformers be Strong Treatment Effect Estimators?

D. Analysis of the Failure Cases over Treatment Distribution Shifts
As shown in Figure 3 (a,c), with the shifts of the treatment interval, the estimation performance of DRNet and TARNet decline. VCNet achieves âˆ estimation loss when h = 5 because its hand-craft projection matrix can only process values near [0, 1]. Another problem brought by this assumption is the extrapolation dilemma, which can be seen in Figure 3(b). When training on t âˆˆ [0, 1.75], these discrete approximation methods cannot transfer to new distribution t âˆˆ (1.75, 2.0]. These unseen treatments are rounded down to the nearest neighbors t in T and be seemed the same as t . We conduct ablation about the treatment embedding as in Table 5 in Appendix. Such a simple ï¬x (VCNet+Embeddings) removes the demand on a ï¬xed interval constraint to treatments and attains superior performance on both interpolation and extrapolation settings. The result clearly shows the pitfalls of hand-crafted feature mapping for TEE. We highlight that it is neglected by most existing works (Schwab et al., 2020; Nie et al., 2021; Shi et al., 2019; Guo et al., 2021). Extrapolation is still a challenging open problem. We can see that no existing work does well when training and test treatment intervals have big gaps. However, the empirical evidence validates the improved effectiveness of TransTEE that uses learnable embeddings to map continuous treatments to hidden representations.
Below we show the assumption that the value of treatments or dosages are in a ï¬xed interval [l, h] is sub-optimal and thus these methods get poor extrapolation results. For simplicity, we only consider a data sample has only one continuous treatment t and the result is similar for continuous dosage.
Proposition 2. Given a data sample (x, t, y), where x âˆˆ Rd, t âˆˆ [l, h], y âˆˆ R. Assume Âµ is a L-Lipschitz function over (x, t) âˆˆ Rd+1, namely |Âµ(u) âˆ’ Âµ(v)| â‰¤ L u âˆ’ v . Partitioning [l, h] uniformly into Î´ sub-interval, and then get T = l + hâˆ’Î´ l âˆ— 0, l + hâˆ’Î´ l âˆ— 1, ..., l + hâˆ’Î´ l âˆ— Î´ . Previous studies most rounding down a treatment t to its nearest value in T (either l + htâˆ’Î´ l hâˆ’Î´ l or l + htâˆ’Î´ l hâˆ’Î´ l ) and use |T | branches to approximate the entire continuum [l, h]. The approximation error can be bounded by

tÎ´ h âˆ’ l

tÎ´ h âˆ’ l

max Âµ x,

âˆ’ Âµ(x, t), Âµ x,

âˆ’ Âµ(x, t)

hâˆ’l Î´

hâˆ’l Î´

â‰¤ max L

tÎ´ h âˆ’ l âˆ’t ,L

tÎ´ h âˆ’ l âˆ’t

(16)

hâˆ’l Î´

hâˆ’l Î´

hâˆ’l â‰¤L
Î´

The bound is affected by both the number of branches Î´ and treatment interval [l, h]. However, as far as we know, most previous works ignore the impacts of the treatment interval [l, h] and adopt a simple but much stronger assumption that treatments are all in the interval [0, 1] (Nie et al., 2021) or a ï¬xed interval (Schwab et al., 2020). These observations well manifest the motivation of our general framework for TEE without the need for treatment-speciï¬c architectural designs.

METHODS
TARNET (SHALIT ET AL., 2017) DRNET (SCHWAB ET AL., 2020)
VCNET(NIE ET AL., 2021) VCNET+EMBEDDINGS

VANILLA
0.045 Â± 0.0009 0.042 Â± 0.0009 0.018 Â± 0.0010 0.013 Â± 0.00465

VANILLA (h = 5)
0.3864 Â± 0.04335 0.3871 Â± 0.03851
NAN
0.0167 Â± 0.01150

EXTRAPOLATION (h = 2)
0.0984 Â± 0.02315 0.0885 Â± 0.00094 0.0669 Â± 0.05227 0.0118 Â± 0.00482

EXTRAPOLATION (h = 5)
0.3647 Â± 0.03626 0.3647 Â± 0.03625
NAN
0.0178 Â± 0.00887

Table 5. Experimental results comparing NN-based methods on simulated datasets. Numbers reported are AMSE of testing data based on 100 repeats, and numbers after Â± are the estimated standard deviation of the average value. For Extrapolation (h = 2), models are trained with t âˆˆ [0, 1.75] and tested in t âˆˆ [0, 2]. For Extrapolation (h = 5), models are trained with t âˆˆ [0, 4] and tested in t âˆˆ [0, 5]

E. Additional Experimental Setups
E.1. Detail Evaluation Metrics.

1N AMSE =

fË†(x , t) âˆ’ f (x , t) Ï€(t)dt

(17)

TN

T

i

i

i=1

Can Transformers be Strong Treatment Effect Estimators?

1N 1

UPEHE@K = N

C2

2
fË†(xi, t, t ) âˆ’ f (xn, t, t )

i=1 K t,t

(18)

1N 1

WPEHE@K = N

C2

2
fË†(xi, t, t ) âˆ’ f (xi, t, t ) p(t|x)p(t |x) ,

i=1 K t,t

1 NT AMSE =

fË†(x , t, s) âˆ’ f (x , t, s) Ï€(s)dt

(19)

D NT

i D

n

i=1 t=1

E.2. Network Structure and Parameter Setting Table. 6 and Table. 7 show the detail of TransTEE architecture and hyper-parameters.

Module Embedding Layer
Output Size
Self-Attention
Output Size
Cross-Attention
Output Size Projection Layer
Output Size

Covariates

Treatment

[Linear]

[Linear]

Bsz Ã— p Ã— #Emb

bsz Ã— 1 Ã— # Emb

ï£® Multi-head Att ï£¹

ï£® Multi-head Att ï£¹

ï£¯ ï£¯

BatchNorm

ï£º ï£º Ã— #Layers

ï£¯ ï£¯

BatchNorm

ï£º ï£º Ã— #Layers

ï£° Linear ï£»

ï£° Linear ï£»

BatchNorm

BatchNorm

Bsz Ã— p Ã— #Emb

Bsz Ã— 1 Ã— #Emb

ï£® Multi-head Att ï£¹

ï£¯ ï£¯

BatchNorm

ï£º ï£º Ã— #Layers

ï£° Linear ï£»

BatchNorm

Bsz Ã— 1 Ã— #Emb

[Linear]

Bsz Ã— 1

Table 6. Architecture details of TransTEE, where p is the number of covariates.

Dataset
Simu IHDP News SW TCGA

Bsz
500 128 256 500 1000

# Emb
10 10 10 16 48

# Layers
1 1 1 1 3

# Heads
2 2 2 2 4

Lr
0.01 0.0005 0.01 0.01 0.01

Lr. S
Cos Cos Cos None None

Table 7. Hyper-parameters on different datasets. Bsz indicates the batch size, # Emb indicates the embedding dimension, Lr. S indicates the scheduler of the learning rate (Cos is the cosine annealing Learning rate).

E.3. Simulation details.

Synthetic Dataset (Nie et al., 2021). The synthetic dataset contains 500 training points and 200 testing points. Data is generated as follows: xj âˆ¼ Unif[0, 1], where xj is the j-th dimension of x âˆˆ R6, and

tËœ|x = 10 sin (max(x1, x2, x3)) + max(x3, x4, x5)3 +sin(0.5x ) (1 + exp(x âˆ’ 0.5x ))+x2+2 sin(x )+2x âˆ’6.5+N (0, 0.25)

1 + (x1 + x5)2

3

4

3

3

4

5

y|x, t = cos(2Ï€(t âˆ’ 0.5)) where t = (1 + exp(âˆ’tËœ))âˆ’1.

2 4 max(x1, x6)3 t + 1 + 2x2
3

+ N (0, 0.25)

Can Transformers be Strong Treatment Effect Estimators?

for treatment in [0, h], we revised it to t = (1 + exp âˆ’tËœ)âˆ’1 âˆ— h,
IHDP (Hill, 2011) is a semi-synthetic dataset containing 25 covariates, 747 observations and binary treatments. For treatments in [0, 1], we follow VCNet (Nie et al., 2021) and generate treatments and responses by:

tËœ|x = 2x1 + 2 max(x3, x5, x6) + 2 tanh 5 1 + x2 0.2 + min(x3, x5, x6)

(x iâˆˆSdis,2 i âˆ’ c2) âˆ’ 4 + N (0, 0.25)
|Sdis,2|

sin(3Ï€t) y|x, t =
1.2 âˆ’ t

tanh 5

(x iâˆˆSdis,1 i âˆ’ c1) |Sdis,1|

+ exp(0.2(x1 âˆ’ x6)) 0.5 + 5 min(x2, x3, x5)

+ N (0, 0.25),

where t = (1 + exp(âˆ’tËœ))âˆ’1, Scon = {1, 2, 3, 5, 6} is the index set of continuous features, Sdis,1 =

{4, 7, 8, 9, 10, 11, 12, 13, 14, 15}, Sdis,2 = {16, 17, 18, 19, 20, 21, 22, 23, 24, 25} and Sdis,1 Sdis,2 = [25] âˆ’ Scon. Here

c1 = E

iâˆˆSdis,1 xi |Sdis,1 |

,c2 = E

iâˆˆSdis,2 xi . To allow comparison on various treatment intervals t âˆˆ [0, h], treatments and
|Sdis,2 |

responses are generated by:

t = (1 + exp(âˆ’tËœ))âˆ’1 âˆ— h

sin(3Ï€t/h) y|x, t =
1.2 âˆ’ t/h

tanh 5

(x iâˆˆSdis,1 i âˆ’ c1) |Sdis,1|

+ exp(0.2(x1 âˆ’ x6)) 0.5 + 5 min(x2, x3, x5)

+ N (0, 0.25),

where the orange part is the only different compared to the generalization of vanilla IHDP dataset (h = 1). Note that Sdis,1 only impacts outcome that serves to be noisy covariates; Sdis,2 contains pre-treatment covariates that only impact treatments, which also serves to be instrumental variables. This allows us to observe the improvement using TransTEE when noisy covariates exist. Following (Hill, 2011) covariates are standardized with mean 0 and standard deviation 1.

News. The News dataset consists of 3000 randomly sampled news items from the NY Times corpus (Newman, 2008) and

was originally introduced as a benchmark in the binary treatment setting. We generate the treatment and outcome in a similar

way as (Nie et al., 2021) but with a dynamic range or treatment intervals [0, h]. We ï¬rst generate v1, v2, v3 âˆ¼ N (0, 1) and

then set vi = vi/ vi 2; i âˆˆ {1, 2, 3}. Given x, we generate t from Beta

2,

v3 x 2v x

âˆ— h.And we generate the outcome by

2

y |x, t = exp v2 x âˆ’ 0.3 v3 x
y|x, t = 2(max(âˆ’2, min(2, y )) + 20v1 x) âˆ— 4(t âˆ’ 0.5)2 + sin Ï€2 t

+ N (0, 0.5)

TCGA (D) (Bica et al., 2020) We obtain covariates x from a real dataset The Cancer Genomic Atlas (TCGA) and consider

3

treatments,

where

each

treatment

is

accompanied

by

one

dosage

and

a

set

of

parameters,

v

t 1

,

v2t

,

v3t

.

For

each

run,

we

randomly sample a vector, uti âˆ¼ N (0, 1) and then set vit = uti/ uti where Â· is Euclidean norm. The shape of the

response curve for each treatment, ft(x, s) is given in Table E.3. We add âˆ¼ N (0, 0.2) noise to the outcomes. Interventions

are assigned by sampling a dosage, dt, for each treatment from a beta distribution, dt|x âˆ¼ Beta(Î±, Î²t). Î± â‰¥ 1 controls

the

dosage

selection

bias

(Î±

=

1

gives

the

uniform

distribution).

Î²t

=

Î±âˆ’1 sâˆ—

+

2

âˆ’

Î±,

where

sâˆ—t

is

the

optimal

dosage2

for

t

treatment t. We then assign a treatment according to tf |x âˆ¼ Categorical(Softmax(Îºf (x, st))) where increasing Îº increases

selection bias, and Îº = 0 leads to random assignments. The factual intervention is given by (tf , stf ). Unless otherwise

speciï¬ed, we set Îº = 2 and Î± = 2.

For structural treatments, we ï¬rst deï¬ne the Baseline effect (Bica et al., 2020). For each run of the experiment, we randomly sample a vector u0 âˆ¼ Unif[0, 1], and set v0 = u0/ uo , where Â· is the Euclidean norm. The baseline effect is deï¬ned as

Âµ0(x) = v0 x

Small-World (Kaddour et al., 2021). 20-dimensional multivariate covariates are uniformly sampled according to xi âˆ¼ Unif[âˆ’1, 1]. There are 1, 000 units in in-sample dataset, and 500 in the out-sample one. Graph interventions For each graph
2For symmetry, if sâˆ—t = 0, we sample sâˆ—t from 1âˆ’Beta(Î±, Î²t) where Î²t is set as though sâˆ—t = 1.

Can Transformers be Strong Treatment Effect Estimators?

$06(
$06(

6&,*$1



7DUQHW'

'UQHW'

9FQHW'



7UDQV7((

















'RVDJH6HOHFWLRQ%LDV

(a) Performance with different dosage selection bias.



6&,*$1

7DUQHW'



'UQHW'

9FQHW'

 7UDQV7((

















7UHDWPHQW6HOHFWLRQ%LDV

(b) Performance with different treatment selection bias.

Figure 7. Performance of ï¬ve methods on TCGA (D) dataset with varying bias levels.

Treatment 1 2 3

Dose-Response
f1(x, s) = C (v11) x + 12(v31) xs âˆ’ 12(v31) xs2 f2(x, s) = C (v12) x + sin Ï€( vv3222 xx s)
f3(x, s) = C (v13) x + 12s(s âˆ’ b)2, where b = 0.75 ((vv3233)) xx

Optimal dosage
sâˆ— = (v21) x
1 2(v31) x
sâˆ— = (v32) x
2 2(v22) x
3b if b â‰¥ 0.75 else 1

Table 8. Dose response curves used to generate semi-synthetic outcomes for patient features x. In the experiments, we set C = 10. v1t , v2t , v3t are the parameters associated with each treatment t.

intervention, a number of nodes between 10 and 120 are uniformly sampled, the number of neighbors for each node is between 3 and 8, and the probability of rewiring each edge is between 0.1 and 1. Wattsâ€“Strogatz small-world graphs are repeatedly generated until a connected one is get. Each vertex has one feature, i.e. its degree centrality. A graphâ€™s node connectivity is denoted as Î½(G) and its average shortest path length as (G). Similar for the baseline effect, two randomly sampled vectors vÎ½, v are generated. Then, given an assigned graph treatment G and a covariate vector x, the outcome is generated by
y = 100Âµ0(x) + 0.2Î½(G)2 Â· vÎ½ x + (G) Â· Î½ x + , âˆ¼ N (0, 1)
TCGA (S) (Kaddour et al., 2021) We use 9, 659 gene expression measurements of cancer patients for covariates. The in-sample and datasets consist of 5, 000 units and the out-sample one of 4, 659 units, respectively. Each unit is a covariate vector x âˆˆ R4000 and these units are split randomly into in- and out-sample datasets in each run randomly. For each covariate vector x, its 8-dimensional PCA components xPCA âˆˆ R8 is computed. Graph interventions We randomly sample 10, 000 molecules from the Quantum Machine 9 (QM9) dataset (Ramakrishnan et al., 2014) (with 133k molecules in total) in each run. We create a relational graph, where each node corresponds to an atom and consists of 78 atom features. We label each edge corresponding to the chemical bond types, e.g., single, double, triple, and aromatic bonds. We collect 8 molecule properties mu, alpha, homo, lumo, gap, r2, zpve, u0 in a vector z âˆˆ R8, which is denoted as the the assigned molecule treatment. Finally, we generate outcomes by
y = 10Âµ0(x) + 0.01z xPCA + , âˆ¼ N (0, 1)
Enriched Equity Evaluation Corpus (EEEC) (Feder et al., 2021) consists of 33, 738 English sentences and the label of each sentence is the mood state it conveys. The task is also known as Proï¬le of Mood States (POMS). Each of the

Can Transformers be Strong Treatment Effect Estimators?

5HVSRQVH 5HVSRQVH 5HVSRQVH

7UXWK



'UQHW'



9FQHW'

6&,*$1



7UDQV7((











      7UHDWPHQW

(a) Estimated ADRF for treatment 1.











7UXWK

'UQHW'



9FQHW'

6&,*$1



7UDQV7((

      7UHDWPHQW

(b) Estimated ADRF for treatment 2.









7UXWK

'UQHW'

9FQHW'



6&,*$1

7UDQV7((

      7UHDWPHQW

(c) Estimated ADRF for treatment 3.

Figure 8. Estimated ADRF on testing set from a typical run of DRNet (D), TARNet (D), VCNet (D), and SCIGAN. All of these methods are well optimized. TransTEE can well estimate the dosage-response curve for all treatments.

sentences in the dataset is comprised using one of 42 templates, with placeholders for a personâ€™s name and the emotion, e.g., â€<Person> made me feel <emotional state word>.â€. A list of common names that are tagged as male or female, and as African-American or European will be used to ï¬ll the placeholder (<Person>). One of four possible mood states: Anger, Sadness, Fear and Joy is used to ï¬ll the emotion placeholder. Hence, EEEC has two counterfactual examples, which are Gender and Race. For the Gender case, it changes the name and the Gender pronouns in the example and switches them, such that for the original example: â€It was totally unexpected, but Roger made me feel pessimistic.â€ it will have the counterfactual example: â€It was totally unexpected, but Amanda made me feel pessimistic.â€ For the Race concept, it creates counterfactuals such that for the original example Josh made me feel uneasiness for the ï¬rst time ever in my life., the counterfactual example is: â€Darnell made me feel uneasiness for the ï¬rst time ever in my life.â€. For each counterfactual example, the personâ€™s name is taken at random from the pre-existing list corresponding to its type.

$06( $06(

               

                

(a) Ablation study of PTR

(b) Ablation study of TR.

Figure 9. Ablation study of the balanced weight for treatment regularization on the IHDP dataset.

Can Transformers be Strong Treatment Effect Estimators?
F. Additional Experimental Results
F.1. Additional Numerical Results and Ablation Studies

:HLJKW :HLJKW :HLJKW

0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425
&RYDULDWHV
(a) TransTEE.

       
                        
&RYDULDWHV
(b) TransTEE+TR.

       
                        
&RYDULDWHV
(c) TransTEE+PTR

Figure 10. The distribution of learned weights for the cross-attention module on the IHDP dataset of different models.

5HVSRQVH 5HVSRQVH 5HVSRQVH 5HVSRQVH

7UXWK



7DUQHW

'UQHW

9FQHW



7UDQV7((



7UXWK

7DUQHW



'UQHW



9FQHW 7UDQV7((





7UXWK

7DUQHW



'UQHW

9FQHW



7UDQV7((









      
7UHDWPHQW


       7UHDWPHQW


  7UHDWPHQW



7UXWK



7DUQHW

'UQHW



9FQHW



7UDQV7((







  7UHDWPHQW

(a) h = 1 during training and (b) h = 2, l = 0.1 during tran-(c) h = 5 during training and (d) h = 5, l = 0.25 during train-

testing.

ing and h = 2, l = 0 during test- testing.

ing and h = 5, l = 0 during test-

ing (extrapolation).

ing (extrapolation).

Figure 11. Estimated ADRF on testing set from a typical run of TarNet (Shalit et al., 2017), DRNet (Schwab et al., 2020), VCNet (Nie et al., 2021) and ours on IHDP dataset. All of these methods are well optimized. (a) TARNet and DRNet do not take the continuity of ADRF into account and produce discontinuous ADRF estimators. VCNet produces continuous ADRF estimators through a hand-crafted mapping matrix. The proposed TransTEE embed treatments into continuous embeddings by neural network and attains superior results. (b,d) When training with 0.1 â‰¤ t â‰¤ 2.0 and 0.25 â‰¤ t â‰¤ 5.0. TARNet and DRNet cannot extrapolate to distributions with 0 < t â‰¤ 2.0 and 0 â‰¤ t â‰¤ 5.0. (c) The hand-crafted mapping matrix of VCNet can only be used in the scenario where t < 2. Otherwise, VCNet cannot converge and incur an inï¬nite loss. At the same time, as h be enhanced, TARNet and DRNet with the same number of branches perform worse. The proposed TransTEE need not know h in advance and can extrapolate well.
Choice of the balancing weight for treatment regularization. To understand the effect of propensity score modeling, we conduct an ablation study on the balancing weights of both TR and PTR. Figure 9 presents the results of the experiments on the IHDP dataset. The main observation is that both TR and PTR with proper strength consistently improve estimation compared to TransTEE without regularization. The best performers are achieved when Î» is 0.5 for both two methods, which shows that the best balancing parameter (0.5 on our experiments.) for these two regularization terms should be searched carefully. Besides, training both the treatment predictor and the feature encoder simultaneously in a zero-sum game is difï¬cult and sometimes unstable (shown in Figure 9 right)
Analysis of covariate adjustment learned by cross-attention module. Compared to previous methods that only adapt MLP to learn covariate representations, TransTEE controls both pre-treatment variables and confounders in a proper and explainable manner. TransTEE injects each covariate to one embedding independently and then let treatments select proper covariates for prediction by a cross-attention module.

Can Transformers be Strong Treatment Effect Estimators?



7UXWK

7DUQHW

'UQHW



9FQHW

7UDQV7((





7UXWK

7DUQHW

'UQHW



9FQHW

7UDQV7((





7UXWK

7DUQHW

'UQHW



9FQHW

7UDQV7((





7UXWK

7DUQHW

'UQHW



9FQHW

7UDQV7((



5HVSRQVH 5HVSRQVH 5HVSRQVH 5HVSRQVH



































7UHDWPHQW

        
7UHDWPHQW













7UHDWPHQW















7UHDWPHQW

(a) h = 1 during training and (b) h = 1.9, l = 0 during tran-(c) h = 5 during training and (d) h = 4, l = 0 during training

testing.

ing and h = 2, l = 0 during test- testing.

and h = 5, l = 0 during testing

ing (extrapolation).

(extrapolation).

Figure 12. Estimated ADRF on testing set from a typical run of TarNet (Shalit et al., 2017), DRNet (Schwab et al., 2020), VCNet (Nie et al., 2021) and ours on News dataset. All of these methods are well optimized. Suppose t âˆˆ [l, h]. (a) TARNet and DRNet do not take the continuity of ADRF into account and produce discontinuous ADRF estimators. VCNet produces continuous ADRF estimators through a hand-crafted mapping matrix. The proposed TransTEE embed treatments into continuous embeddings by neural network and attains superior results. (b,d) When training with 0 â‰¤ t â‰¤ 1.9 and 0 â‰¤ t â‰¤ 4.0. TARNet and DRNet cannot extrapolate to distributions with 0 < t â‰¤ 2.0 and 0 â‰¤ t â‰¤ 5.0. (c) The hand-crafted mapping matrix of VCNet can only be used in the scenario where t < 2. Otherwise, VCNet cannot converge and incur an inï¬nite loss. At the same time, as h be enhanced, TARNet and DRNet with the same number of branches perform worse. The proposed TransTEE need not know h in advance and can extrapolate well.

METHODS
TARNET DRNET VCNET TRANSTEE TRANSTEE+TR TRANSTEE+PTR

VANILLA
0.082 Â± 0.019 0.083 Â± 0.032 0.013 Â± 0.005 0.010 Â± 0.004 0.011 Â± 0.003 0.011 Â± 0.004

VANILLA (h = 5)
0.956 Â± 0.041 0.956 Â± 0.041
NAN
0.017 Â± 0.008 0.016 Â± 0.008 0.014 Â± 0.007

EXTRAPOLATION (h = 2)
0.716 Â± 0.038 0.703 Â± 0.038
NAN
0.024 Â± 0.017 0.019 Â± 0.008 0.022 Â± 0.008

EXTRAPOLATION (h = 5)
0.847 Â± 0.053 0.834 Â± 0.053
NAN
0.029 Â± 0.019 0.028 Â± 0.002 0.029 Â± 0.016

Table 9. Experimental results comparing neural network based methods on the News datasets. Numbers reported are based on 20 repeats, and numbers after Â± are the estimated standard deviation of the average value. For Extrapolation (h = 2), models are trained with t âˆˆ [0, 1.9] and tested in t âˆˆ [0, 2]. For For Extrapolation (h = 5), models are trained with t âˆˆ [0, 4.5] and tested in t âˆˆ [0, 5]

The attention mechanism is a powerful representation tool (Vaswani et al., 2017) to explain how certain decisions are made and we visualize the selection results (cross-attention weights) in Figure 4. As described in Section E.3, the IHDP dataset has 25 covariates, which is divided to 3 groups: Scon = {1, 2, 3, 5, 6}, Sdis,1 = {4, 7, 8, 9, 10, 11, 12, 13, 14, 15}, and Sdis,2 = {16, 17, 18, 19, 20, 21, 22, 23, 24, 25}. Scon inï¬‚uences both t and y, Sdis,1 inï¬‚uences only y, and Sdis,1 inï¬‚uences only t. Covariates in Sdis,1 are named noisy covariates, because they have no correlation with the treatment. Their causal relationships are illustrated in Figure 15. We show in Section 6.3 that vanilla TransTEE already has the ability the adjust confounders for effectively inferring causal effects.

wcon w1 w2

TransTEE 0.27 0.37 0.36

+TR

0.59 0.20 0.21

+PTR 0.32 0.33 0.35

Table 10. Attention weights for Scon, Sdis,1, and Sdis,2 respectively.

We further conduct 10 repetitions for TransTEE and its TR and PTR counterparts as reported in Figure 10, which visualizes the cross-attention weights of them Denote wcon, w1, w2 as the summation of weights assigned to Scon, Sdis,1, Sdis,2 respectively and Table 10 shows the results. We can see that, incorporated with both TR and PTR regularization, TransTEE assigns more weights to confounding covariates (Scon) and less weight on noisy covariates, which veriï¬es the effectiveness of the proposed regularization terms and justiï¬es the improved numerical performance of TR and PTR. Moreover, TR is better than PTR since it also reduces w2 by a larger margin. This observation gives a suggestion that we should systematically probe TR and PTR besides comparing

Can Transformers be Strong Treatment Effect Estimators?

their numerical performance in settings where controlling instrumental variables will incur biases in TEE (VanderWeele, 2019) like when unconfoundedness assumption is violated (Ding et al., 2017).

Robustness to noisy covariates. We manipulate Sdis,1, Sdis,2 to generate datasets with differ-

ent noisy covariates, e.g., when the number of covariates that only inï¬‚uence the outcome is 6,

Sdis,1 = {4}, and Sdis,2 = {7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, ğ‘ºğ’…ğ’Šğ’”,ğŸ ğ‘ºğ’„ğ’ğ’ ğ‘ºğ’…ğ’Šğ’”,ğŸ

when the number of covariates that inï¬‚uence the outcome is 24, Sdis,1 =

{4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, }, and Sdis,2 = {25}. Figure

Figure 6 shows that, as the number of covariates that only inï¬‚uence the outcome increases, both TARNet ğ‘»

ğ’€

and DRNet become better estimators, however, VCNet performs worse and even inferior to TARNet and DRNet when the number is large than 16. In contrast, the estimation error incurred by the proposed TransTEE is always low and superior to baselines by a large margin.

Figure 15. Causal graph of the IHDP dataset.

Comparison of MLE or adversarial propensity score modeling on the propensity score. Seeing

results in Table 3, additionally combine TransTEE with maximum likelihood training of Ï€(t|x) does

provide some performance gains. However, an adversarially trained Ï€-model can be signiï¬cantly better, especially for

extrapolation settings. The results well manifest the effectiveness of TR and PTR on reducing selection bias and improving

estimation performance. In fact, approaches like TMLE are not robust if the initial estimator is poor (Shi et al., 2019).

F.2. Showcase of sentences and their counterfactual counterparts with the maximal/minimal ATEs.
Table 12 showcases the top-10 samples with the maximal/ minimal ATEs. Interestingly, we can see most sentences with a large ATE have similar patterns, that is â€œ< clause >, but/and < Person > made me feel < Adj >â€. Besides, most sentences with a large ATE have a small length, which is 11 words on average. By contrast, sentences with small ATEs follow other patterns and are longer, which is 17.6 on average. Consider the effect of Race, Table 13 showcases the top-10 samples. Similarly, there are also some dominant patterns that have pretty high or low ATEs and the average length of sentences with high ATEs is smaller than sentences with low ATEs (12 vs 14.7). Besides, the position of perturbation words (the name from a speciï¬c race) for sentences with the maximal/minimal ATEs is totally different, which is at the beginning for the former and at the middle for the latter. Namely, TransTEE helps us mitigate spurious correlations that exist in model prediction, e.g., length of sentences, the position of perturbation words, certain sentence patterns and is useful in mitigating undesirable bias ingrained in the data. Besides, a well-optimized TransTEE is able to estimate the effect of every sentence and is of great beneï¬t for model interpretation and analysis especially under high inference latency.

Can Transformers be Strong Treatment Effect Estimators?

Method
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE
Zero GNN GraphITE SIN TransTEE

SW In-sample Out-sample

41.72 Â± 0.00 17.38 Â± 0.01 17.37 Â± 0.01 15.79 Â± 1.72 14.74 Â± 0.09

49.69 Â± 0.00 24.53 Â± 0.01 24.56 Â± 0.02 28.78 Â± 4.54 21.78 Â± 1.07

40.75 Â± 0.00 18.26 Â± 0.00 18.27 Â± 0.01 18.15 Â± 1.97 15.30 Â± 1.12

43.76 Â± 0.00 20.91 Â± 0.01 20.95 Â± 0.02 23.62 Â± 3.93 18.73 Â± 2.09

45.74 Â± 0.00 22.09 Â± 0.01 22.12 Â± 0.00 22.14 Â± 2.30 18.99 Â± 0.83

44.95 Â± 0.00 23.01 Â± 0.01 23.03 Â± 0.02 23.70 Â± 3.67 19.65 Â± 1.97

49.19 Â± 0.00 24.18 Â± 0.01 24.22 Â± 0.01 25.48 Â± 3.02 20.16 Â± 0.42

45.96 Â± 0.00 24.20 Â± 0.01 24.22 Â± 0.03 25.44 Â± 3.50 21.08 Â± 1.78

49.95 Â± 0.00 25.13 Â± 0.00 25.17 Â± 0.02 27.07 Â± 2.98 21.32 Â± 0.79

50.10 Â± 0.00 26.93 Â± 0.01 26.94 Â± 0.02 28.11 Â± 3.51 22.99 Â± 1.43

55.40 Â± 0.00 29.30 Â± 0.03 29.34 Â± 0.01 31.07 Â± 3.07 24.71 Â± 0.41

58.42 Â± 0.00 32.15 Â± 0.03 32.16 Â± 0.01 34.17 Â± 3.41 25.84 Â± 0.73

57.99 Â± 0.00 31.41 Â± 0.03 31.45 Â± 0.01 33.58 Â± 3.37 26.48 Â± 0.27

66.78 Â± 0.00 37.57 Â± 0.05 37.58 Â± 0.00 40.83 Â± 3.64 32.40 Â± 0.85

62.52 Â± 0.00 34.13 Â± 0.04 34.17 Â± 0.02 36.79 Â± 3.35 28.84 Â± 0.23

64.61 Â± 0.00 36.48 Â± 0.04 36.49 Â± 0.01 40.99 Â± 5.14 31.40 Â± 0.71

62.65 Â± 0.00 34.26 Â± 0.04 34.30 Â± 0.02 37.08 Â± 3.35 28.89 Â± 0.19

65.59 Â± 0.00 37.65 Â± 0.04 37.66 Â± 0.00 41.79 Â± 5.21 32.25 Â± 0.69

TCGA (Bias=0.1)

TCGA (Bias=0.3)

In-sample Out-sample In-sample Out-sample

13.93 Â± 0.00 10.90 Â± 7.71 15.04 Â± 0.20 46.47 Â± 2.19 9.07 Â± 2.15
13.93 Â± 0.00 10.75 Â± 7.60 14.88 Â± 0.19 45.29 Â± 2.33 9.07 Â± 2.02
14.14 Â± 0.00 10.87 Â± 7.69 15.05 Â± 0.18 44.72 Â± 2.35 9.09 Â± 1.97
14.31 Â± 0.00 10.99 Â± 7.77 15.24 Â± 0.19 44.55 Â± 2.35 9.17 Â± 1.96
14.47 Â± 0.00 11.11 Â± 7.86 15.40 Â± 0.19 44.48 Â± 2.35 9.23 Â± 1.95
14.53 Â± 0.00 11.16 Â± 7.89 15.47 Â± 0.19 44.45 Â± 2.37 9.27 Â± 1.94
14.61 Â± 0.00 11.22 Â± 7.93 15.55 Â± 0.19 44.48 Â± 2.38 9.31 Â± 1.94
14.66 Â± 0.00 11.26 Â± 7.96 15.60 Â± 0.19 44.47 Â± 2.39 9.34 Â± 1.94
14.69 Â± 0.00 11.28 Â± 7.98 15.64 Â± 0.19 44.49 Â± 2.40 9.36 Â± 1.93

WPEHE@2 13.13 Â± 0.00 10.91 Â± 7.71 14.96 Â± 0.30 54.41 Â± 7.81 9.33 Â± 2.13
WPEHE@3 13.61 Â± 0.00 10.91 Â± 7.72 15.12 Â± 0.29 53.72 Â± 8.09 9.58 Â± 2.04
WPEHE@4 13.75 Â± 0.00 10.88 Â± 7.69 15.14 Â± 0.28 53.12 Â± 8.09 9.66 Â± 2.01
WPEHE@5 13.95 Â± 0.00 10.97 Â± 7.76 15.29 Â± 0.28 52.78 Â± 8.04 9.72 Â± 2.00
WPEHE@6 14.04 Â± 0.00 11.02 Â± 7.79 15.37 Â± 0.28 52.54 Â± 7.99 9.77 Â± 1.99
WPEHE@7 14.09 Â± 0.00 11.06 Â± 7.82 15.42 Â± 0.28 52.40 Â± 7.98 9.81 Â± 1.99
WPEHE@8 14.14 Â± 0.00 11.09 Â± 7.85 15.47 Â± 0.28 52.34 Â± 7.97 9.85 Â± 1.99
WPEHE@9 14.20 Â± 0.00 11.13 Â± 7.87 15.53 Â± 0.28 52.31 Â± 7.97 9.88 Â± 2.00 WPEHE@10 14.23 Â± 0.00 11.16 Â± 7.89 15.56 Â± 0.28 52.28 Â± 7.96 9.90 Â± 2.00

13.93 Â± 0.00 13.58 Â± 0.18 13.49 Â± 0.23 7.93 Â± 0.79 7.54 Â± 3.60
13.93 Â± 0.00 13.63 Â± 0.18 13.49 Â± 0.22 7.94 Â± 0.75 7.58 Â± 3.62
14.14 Â± 0.00 13.87 Â± 0.18 13.64 Â± 0.20 7.99 Â± 0.73 7.67 Â± 3.70
14.31 Â± 0.00 13.98 Â± 0.17 13.68 Â± 0.17 8.10 Â± 0.75 7.76 Â± 3.75
14.47 Â± 0.00 14.07 Â± 0.22 13.74 Â± 0.12 8.22 Â± 0.75 7.80 Â± 3.83
14.53 Â± 0.00 14.12 Â± 0.21 13.97 Â± 0.08 8.28 Â± 0.74 7.82 Â± 3.84
14.60 Â± 0.00 14.19 Â± 0.25 14.30 Â± 0.04 8.33 Â± 0.74 7.88 Â± 3.84
14.61 Â± 0.00 14.21 Â± 0.24 14.35 Â± 0.04 8.36 Â± 0.74 7.90 Â± 3.85
14.69 Â± 0.00 14.29 Â± 0.22 14.38 Â± 0.04 8.39 Â± 0.74 7.94 Â± 3.87

13.13 Â± 0.00 13.22 Â± 0.18 13.70 Â± 0.52 11.04 Â± 1.52 8.37 Â± 3.64
13.61 Â± 0.00 13.58 Â± 0.19 14.19 Â± 0.43 11.53 Â± 1.59 8.65 Â± 3.75
13.75 Â± 0.00 13.71 Â± 0.19 14.30 Â± 0.35 11.66 Â± 1.59 8.71 Â± 3.78
13.95 Â± 0.00 13.92 Â± 0.18 14.37 Â± 0.37 11.76 Â± 1.59 8.80 Â± 3.82
14.04 Â± 0.00 14.11 Â± 0.18 14.58 Â± 0.38 11.82 Â± 1.58 8.84 Â± 3.89
14.09 Â± 0.00 14.14 Â± 0.18 14.69 Â± 0.40 11.85 Â± 1.58 8.89 Â± 3.89
14.12 Â± 0.00 14.20 Â± 0.18 14.85 Â± 0.43 11.87 Â± 1.57 8.90 Â± 3.90
14.14 Â± 0.00 14.22 Â± 0.17 14.90 Â± 0.43 11.90 Â± 1.57 8.94 Â± 3.91
14.23 Â± 0.00 14.32 Â± 0.18 14.93 Â± 0.43 11.92 Â± 1.58 8.95 Â± 3.92

TCGA (Bias=0.5) In-sample Out-sample

13.93 Â± 0.00 12.86 Â± 0.38 12.41 Â± 0.02 10.31 Â± 0.93 9.52 Â± 3.59

13.61 Â± 0.00 14.62 Â± 0.91 14.38 Â± 0.30 14.09 Â± 2.14 10.10 Â± 3.79

13.61 Â± 0.00 12.92 Â± 0.33 12.56 Â± 0.01 10.89 Â± 1.07 9.64 Â± 3.56

14.14 Â± 0.00 15.29 Â± 1.04 15.18 Â± 0.31 14.27 Â± 1.92 10.59 Â± 3.88

13.75 Â± 0.00 13.13 Â± 0.34 12.77 Â± 0.02 11.38 Â± 1.04 9.78 Â± 3.63

14.31 Â± 0.00 15.47 Â± 1.05 15.38 Â± 0.30 14.37 Â± 1.83 10.74 Â± 3.91

13.95 Â± 0.00 13.31 Â± 0.37 12.95 Â± 0.03 11.75 Â± 1.22 9.91 Â± 3.66

14.47 Â± 0.00 15.67 Â± 1.05 15.59 Â± 0.30 14.59 Â± 1.84 10.89 Â± 3.94

14.04 Â± 0.00 13.45 Â± 0.38 13.09 Â± 0.04 11.97 Â± 1.19 10.01 Â± 3.70

14.53 Â± 0.00 15.76 Â± 1.04 15.68 Â± 0.29 14.74 Â± 1.86 10.96 Â± 3.95

14.53 Â± 0.00 13.51 Â± 0.38 13.16 Â± 0.04 12.11 Â± 1.18 10.06 Â± 3.71

14.09 Â± 0.00 15.81 Â± 1.03 15.74 Â± 0.29 14.83 Â± 1.87 11.01 Â± 3.95

14.61 Â± 0.00 13.58 Â± 0.38 13.23 Â± 0.04 12.22 Â± 1.17 10.10 Â± 3.72

14.14 Â± 0.00 15.87 Â± 1.02 15.78 Â± 0.28 14.91 Â± 1.89 11.04 Â± 3.96

14.66 Â± 0.00 13.63 Â± 0.38 13.28 Â± 0.04 12.40 Â± 1.23 10.14 Â± 3.73

14.20 Â± 0.00 15.92 Â± 1.01 15.83 Â± 0.28 15.08 Â± 1.80 11.08 Â± 3.97

14.69 Â± 0.00 13.66 Â± 0.38 13.31 Â± 0.04 12.49 Â± 1.22 10.16 Â± 3.74

14.23 Â± 0.00 15.96 Â± 1.01 15.87 Â± 0.27 15.13 Â± 1.81 11.10 Â± 3.98

Table 11. Error of CATE estimation for all methods, measured by WPEHE@2-10. Results are averaged over 5 trials, Â± denotes std error. In-Sample means results in the training set and Out-sample means results in the test set. (The baseline results are reproduced using the ofï¬cial code of (Kaddour et al., 2021) in a consistent experimental environment, which can be slightly different than the results reported in (Kaddour et al., 2021))

Can Transformers be Strong Treatment Effect Estimators?

WPEHE@8 WPEHE@5 WPEHE@2 WPEHE@9 WPEHE@6 WPEHE@3 WPEHE@10 WPEHE@7 WPEHE@4

WPEHE@8 WPEHE@5 WPEHE@2 WPEHE@9 WPEHE@6 WPEHE@3 WPEHE@10 WPEHE@7 WPEHE@4

SIN

GNN

GraphITE

Zero

TransTEE

SIN

GNN

GraphITE

Zero

TransTEE

50

50

25

25

50

50

25

25

50 25
0
SIN 40 20

20 40 0 20 40 0 20

(a) SW In-Sample.

GNN

GraphITE

Zero

40 TransTEE

50 25
0
SIN 50 25

20 40 0 20 40 0 20

(b) SW Out-Sample

GNN

GraphITE

Zero

40 TransTEE

WPEHE@8 WPEHE@5 WPEHE@2 WPEHE@9 WPEHE@6 WPEHE@3 WPEHE@10 WPEHE@7 WPEHE@4

40

50

20

25

40 20
0.0 0.2 0.4

0.0 0.2 0.4

0.0 0.2 0.4

(c) TCGA In-Sample

50 25
0.0 0.2 0.4

0.0 0.2 0.4

0.0 0.2 0.4

(d) TCGA Out-Sample

Figure 13. WPEHE@K over increasing bias strength Îº and varying K âˆˆ {2, ..., 10} on the SW and the TCGA dataset.

WPEHE@8 WPEHE@5 WPEHE@2 WPEHE@9 WPEHE@6 WPEHE@3 WPEHE@10 WPEHE@7 WPEHE@4

Can Transformers be Strong Treatment Effect Estimators?

UPEHE@8 UPEHE@5 UPEHE@2 UPEHE@9 UPEHE@6 UPEHE@3 UPEHE@10 UPEHE@7 UPEHE@4

UPEHE@8 UPEHE@5 UPEHE@2 UPEHE@9 UPEHE@6 UPEHE@3 UPEHE@10 UPEHE@7 UPEHE@4

SIN

GNN

GraphITE

Zero

TransTEE

SIN

GNN

GraphITE

Zero

TransTEE

50

50

25

25

50

50

25

25

50 25
0 20 40 0 20 40 0 20 40

SIN
75 50 25

(a) SW In-Sample.

GNN

GraphITE

Zero

TransTEE

75 50 25

75 50 25
0.0 0.2 0.4

0.0 0.2 0.4

0.0 0.2 0.4

(c) TCGA In-Sample

UPEHE@8 UPEHE@5 UPEHE@2 UPEHE@9 UPEHE@6 UPEHE@3 UPEHE@10 UPEHE@7 UPEHE@4

50 25
0
SIN 50 25

20 40 0 20 40 0 20

(b) SW Out-Sample

GNN

GraphITE

Zero

40 TransTEE

50 25

50 25
0.0 0.2 0.4

0.0 0.2 0.4

0.0 0.2 0.4

(d) TCGA Out-Sample

Figure 14. UPEHE@K over increasing bias strength Îº and varying K âˆˆ {2, ..., 10} on the SW and the TCGA dataset.

UPEHE@8 UPEHE@5 UPEHE@2 UPEHE@9 UPEHE@6 UPEHE@3 UPEHE@10 UPEHE@7 UPEHE@4

Can Transformers be Strong Treatment Effect Estimators?

Factual Counterfactual
Factual Counterfactual

Index 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Index
1
2
3
4 5 6
7
8 9 10
1
2
3
4 5 6
7
8 9 10

Sentences with The Maximal ATEs

Sentence

It was totally unexpected, but Roger made me feel pessimistic. We went to the restaurant, and Alphonse made me feel frustration. It was totally unexpected, but Amanda made me feel pessimistic. We went to the university, and my husband made me feel angst. It is far from over, but so far i made Jasmine feel frustration. We were told that Torrance found himself in a consternation situation. We went to the university, and my son made me feel revulsion. To our amazement, the conversation with my aunt was dejected. To our amazement, the conversation with my aunt was dejected. We went to the supermarket, and Roger made me feel uneasiness.

It was totally unexpected, but Amanda made me feel pessimistic. We went to the school, and Latisha made me feel frustration. It was totally unexpected, but Roger made me feel pessimistic. We went to the market, and my daughter made me feel angst. It is far from over, but so far i made Jamel feel frustration. We were told that Tia found herself in a consternation situation. We went to the hairdresser, and my sister made me feel revulsion. To our amazement, the conversation with my uncle was dejected. To our amazement, the conversation with my uncle was dejected. We went to the university, and Amanda made me feel uneasiness.
Sentences with The Minimal ATEs

Sentence

To our amazement, the conversation with Jack was irritating, no added information is given in this part. To our surprise, my husband found himself in a vexing situation, this is only here to confuse the classiï¬er. The conversation with Amanda was irritating, we could from simply looking, this is only here to confuse the classiï¬er. this is only here to confuse the classiï¬er, The situation makes Torrance feel irate, but it does not matter now. this is random noise, I made Alphonse feel irate, time and time again. We were told that Roger found himself in a irritating situation, no added information is given in this part. Amanda made me feel irate whenever I came near, no added information is given in this part. While unsurprising, the conversation with my uncle was outrageous, this is only here to confuse the classiï¬er. It is a mystery to me, but it seems i made Darnell feel irate. The conversation with Melanie was irritating, you could feel it in the air, no added information is given in this part.

To our amazement, the conversation with Kristin was irritating, no added information is given in this part. To our surprise, this girl found herself in a vexing situation, this is only here to confuse the classiï¬er. The conversation with Frank was irritating, we could from simply looking, this is only here to confuse the classiï¬er. this is only here to confuse the classiï¬er, The situation makes Shaniqua feel but it does not matter now. this is random noise, I made Nichelle feel irate, time and time again. We were told that Melanie found herself in a irritating situation, no added information is given in this part. Justin made me feel irate whenever I came near, no added information is given in this part. While unsurprising, the conversation with my mother was outrageous, this is only here to confuse the classiï¬er. It is a mystery to me, but it seems i made Lakisha feel irate. The conversation with Ryan was irritating, you could feel it in the air, no added information is given in this part.

irate,

ATE 0.6393 0.578 0.5109 0.4538 0.4366 0.4203 0.399 0.3952 0.3952 0.3752 0.6393 0.578 0.5109 0.4538 0.4366 0.4203 0.399 0.3952 0.3952 0.3752
ATE
0
0
0
0 0 0
0
0 0 0
0
0
0
0 0 0
0
0 0 0

Table 12. Top-10 samples with the maximal and minimal ATE for the effect of Gender. Perturbation words in factual sentences and counterfactual sentences are colored by Orange and Magenta respecttively.

Can Transformers be Strong Treatment Effect Estimators?

Factual Counterfactual
Factual Counterfactual

Index
1
2 3 4 5 6 7 8 9 10
1
2 3 4 5 6 7 8 9 10
Index
1 2 3 4 5 6 7 8
9
10
1 2 3 4 5 6 7 8
9
10

Sentences with The Maximal ATEs
Sentence
sometimes noise helps, not here, The conversation with Shereen was cry, we could from simply looking. Darnell made me feel uneasiness for the ï¬rst time ever in my life. Alonzo feels pity as he paces along to the shop. Adam feels despair as he paces along to the school. Ebony made me feel unease for the ï¬rst time ever in my life. Nancy made me feel dismay for the ï¬rst time ever in my life. Lamar made me feel revulsion for the ï¬rst time ever in my life. Alonzo made me feel revulsion for the ï¬rst time ever in my life. While we were walking to the market, Josh told us all about the recent pessimistic events. Alonzo made me feel unease for the ï¬rst time ever in my life.
sometimes noise helps, not here, The conversation with Katie was cry, we could from simply looking. Josh made me feel uneasiness for the ï¬rst time ever in my life. Josh feels pity as he paces along to the shop. Terrence feels despair as he paces along to the hairdresser. Ellen made me feel unease for the ï¬rst time ever in my life. Latisha made me feel dismay for the ï¬rst time ever in my life. Jack revulsione me feel revulsion for the ï¬rst time ever in my life. Frank made me feel revulsion for the ï¬rst time ever in my life. While we were walking to the college, Torrance told us all about the recent pessimistic events. Roger made me feel unease for the ï¬rst time ever in my life.
Sentences with The Minimal ATEs
Sentence
We went to the bookstore, and Alonzo made me feel fearful, really, there is no information here. nothing here is relevant, I made Jack feel angry, time and time again. do not look here, it will just confuse you, Jamel feels fearful at the start. We went to the bookstore, and Justin made me feel irritated. As he approaches the restaurant, Justin feels irritated. Now that it is all over, Andrew feels irritated. do not look here, it will just confuse you, Ebony feels fearful at the start. do not look here, it will just confuse you, Lakisha feels fearful at the start. There is still a long way to go, but the situation makes Lakisha feel irritated, this is only here to confuse the classiï¬er. I have no idea how or why, but i made Alan feel irritated.
We went to the market, and Roger made me feel fearful, really, there is no information here. nothing here is relevant, I made Jamel feel angry, time and time again. do not look here, it will just confuse you, Harry feels fearful at the start. We went to the church, and Lamar made me feel irritated. As he approaches the shop, Malik feels irritated. Now that it is all over, Torrance feels irritated. do not look here, it will just confuse you, Amanda feels fearful at the start. do not look here, it will just confuse you, Amanda feels fearful at the start. There is still a long way to go, but the situation makes Katie feel irritated, this is only here to confuse the classiï¬er. I have no idea how or why, but i made Darnell feel irritated.

ATE
0.9976
0.6853 0.6563 0.6066 0.592 0.548 0.5074 0.4911 0.4886 0.4877
0.9976
0.6853 0.6563 0.6066 0.592 0.548 0.5074 0.4911 0.4886 0.4877
ATE
0 0 0 0 0 0 0 0
0
0
0 0 0 0 0 0 0 0
0
0

Table 13. Top-10 samples with the maximal and minimal ATE for the effect of Race. Perturbation words in factual sentences and counterfactual sentences are colored by Orange and Magenta respectively.

