Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

arXiv:1811.01704v4 [cs.LG] 16 Apr 2020

ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural Networks

Ahmed T. Elthakeb Prannoy Pilligundla Fatemehsadat Mireshghallah Amir Yazdanbakhsh‚àó

Hadi Esmaeilzadeh

Alternative Computing Technologies (ACT) Lab

University of California San Diego

‚àóGoogle Brain

{a1yousse, ppilligu, fmireshg}@eng.ucsd.edu ayazdan@google.com hadi@eng.ucsd.edu

Abstract
Deep Neural Networks (DNNs) typically require massive amount of computation resource in inference tasks for computer vision applications. Quantization can significantly reduce DNN computation and storage by decreasing the bitwidth of network encodings. Recent research affirms that carefully selecting the quantization levels for each layer can preserve the accuracy while pushing the bitwidth below eight bits. However, without arduous manual effort, this deep quantization can lead to significant accuracy loss, leaving it in a position of questionable utility. As such, deep quantization opens a large hyper-parameter space (bitwidth of the layers), the exploration of which is a major challenge. We propose a systematic approach to tackle this problem, by automating the process of discovering the quantization levels through an end-to-end deep reinforcement learning framework (RELEQ). We adapt policy optimization methods to the problem of quantization, and focus on finding the best design decisions in choosing the state and action spaces, network architecture and training framework, as well as the tuning of various hyperparamters. We show how RELEQ can balance speed and quality, and provide an asymmetric general solution for quantization of a large variety of deep networks (AlexNet, CIFAR-10, LeNet, MobileNet-V1, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy (‚â§ 0.3% loss) while minimizing the computation and storage cost. With these DNNs, RELEQ enables conventional hardware to achieve 2.2√ó speedup over 8-bit execution. Similarly, a custom DNN accelerator achieves 2.0√ó speedup and energy reduction compared to 8-bit runs. These encouraging results mark RELEQ as the initial step towards automating the deep quantization of neural networks.
1. Introduction
Deep Neural Networks (DNNs) have made waves across a variety of domains, from image recognition [26] and synthesis, object detection [39, 42], natural language processing [7], medical imaging, self-driving cars, video surveillance, and personal assistance [18, 27, 20, 28]. DNN compute efficiency

has become a major constraint in unlocking further applications and capabilities, as these models require rather massive amounts of computation even for a single inquiry. One approach to reduce the intensity of the DNN computation is to reduce the complexity of each operation. To this end, quantization of neural networks provides a path forward as it reduces the bitwidth of the operations as well as the data footprint [22, 41, 23]. Albeit alluring, quantization can lead to significant accuracy loss if not employed with diligence. Years of research and development has yielded current levels of accuracy, which is the driving force behind the wide applicability of DNNs nowadays. To prudently preserve this valuable feature of DNNs, accuracy, while benefiting from quantization the following two fundamental problems need to be addressed. (1) learning techniques need to be developed that can train or tune quantized neural networks given a level of quantization for each layer. (2) Algorithms need to be designed that can discover the appropriate level of quantization for each layer while considering the accuracy. This paper takes on the second challenge as there are inspiring efforts that have developed techniques for quantized training [49, 50, 34].
This paper builds on the algorithmic insight that the bitwidth of operations in DNNs can be reduced below eight bits without compromising their classification accuracy. However, this possibility is manually laborious [32, 33, 45] as to preserve accuracy, the bitwidth varies across individual layers and different DNNs [49, 50, 29, 34]. Each layer has a different role and unique properties in terms of weight distribution. Thus, intuitively, different layers display different sensitivity towards quantization. Over-quantizing a more sensitive layer can result in stringent restrictions on subsequent layers to compensate and maintain accuracy. Nonetheless, considering layer-wise quantization opens a rather exponentially large hyper-parameter space, specially when quantization below eight bits is considered. For example, ResNet-20 exposes a hyper-parameter space of size 8l = 820 > 1018, where l = 20 is the number of layers and 8 is the possible quantization levels. This exponentially large hyper-parameter space grows with the number of the layers making it impractical to exhaustively assess and determine the quantization level for each layer.

1

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

State of Relative Accuracy

Recoverable Accuracy Loss
3 Reward Shaping
Pareto 1 Frontier

2
4 Desired Region
Possible Solutions (Layer-wise bitwidth
assignments)

Fewer Bits State of Quantization

More Bits

Figure 1. Optimum Automatic Quantization of a Neural Network

To that end, this paper sets out to automate effectively navigating this hyper-parameter space using Reinforcement Learning (RL). We develop an end-to-end framework, dubbed RELEQ, that exploits the sample efficiency of the Proximal Policy Optimization [40] to explore the quantization hyperparameter space. The RL agent starts from a full-precision previously trained model and learns the sensitivity of final classification accuracy with respect to the quantization level of each layer, determining its bitwidth while keeping classification accuracy virtually intact. Observing that the quantization bitwidth for a given layer affects the accuracy of subsequent layers, our framework implements an LSTM-based RL framework which enables selecting quantization levels with the context of previous layers‚Äô bitwidths. Rigorous evaluations with a variety of networks (AlexNet, CIFAR, LeNet, SVHN, VGG-11, ResNet-20, and MobileNet) shows that RELEQ can effectively find heterogenous deep quantization levels that virtually preserve the accuracy (‚â§0.3% loss) while minimizing the computation and storage cost. The results (Table 2) show that there is a high variance in quantization levels across the layers of these networks. For instance, RELEQ finds quantization levels that average to 6.43 bits for MobileNet, and to 2.81 bits for ResNet-20. With the seven benchmark DNNs, RELEQ enables conventional hardware [5] to achieve 2.2√ó speedup over 8-bit execution. Similarly, a custom DNN accelerator [24] achieves 2.0√ó speedup and 2.0√ó energy reduction compared to 8-bit runs. These results suggest that RELEQ takes an effective first step towards automating the deep quantization of neural networks.
2. RL for Deep Quantization of DNNs 2.1. Need for Heterogeneity
Deep neural networks, by construction, and the underlying training algorithms cast different properties on different layers as they learn different levels of features representations. First, it is widely known that neural networks are heavily overparameterized [2]; thus, different layers exhibit different levels of redundancy. Second, for a given initialization and upon training, each layer exhibits a distribution of weights (typically bell-shaped) each of which has a different dynamic

range leading to different degrees of robustness to quantization error, hence, different/heterogenous precision requirements. Third, our experiments (see Figure 6), where the design space of deep quantization is enumerated for small and moderate size networks, empirically shows that indeed Pareto optimal frontier is mostly composed of heterogenous bitwidths assignment. Furthermore, recent works empirically studied the layer-wise functional structure of overparameterized deep models and provided evidence for the heterogeneous characteristic of layers. Recent experimental work [48] also shows that layers can be categorized as either ‚Äúambient‚Äù or ‚Äúcritical‚Äù towards post-training re-initialization and re-randomization. Another work [15] showed that a heterogeneously quantized versions of modern networks with the right mix of different bitwidths can match the accuracy of homogeneous versions with lower effective bitwidth on average. All aforementioned points poses a requirement for methods to efficiently discover heterogenous bitwidths assignment for neural networks. However, exploiting this possibility is manually laborious [32, 33, 45] as to preserve accuracy, the bitwidth varies across individual layers and different DNNs [49, 50, 29, 34]. Next subsection describes how to formulate this problem as a multi-objective optimization solved through Reinforcement Learning.
2.2. Multi-Objective Optimization
Figure 1 shows a sketch of the multi-objective optimization problem of layer-wise quantization of a neural network showing the underlying search space and the different design components. Given a particular network architecture, different patterns of layer-wise quantization bitwidths form a network specific design space (possible solutions). Pareto frontier
(¬Ç) defines the optimum patterns of layer-wise quantization
bitwidths (Pareto optimal solutions). However, not all Pareto optimal solutions are equally of interest as the accuracy objective has a priority over the quantization objective. In the context of neural network quantization, preferred/desired region on the Pareto frontier for a given neural network is motivated by how recoverable the accuracy is for a given state of quantization (i.e., a particular pattern of layer-wise bitwidths) of the network. Practically, the amount of recoverable accuracy loss (upon quantization) is determined by many factors: (1) the quantized training algorithm; (2) the amount of finetuning epochs; (3) the particular set of used hyperparameters; (4) the amenability of the network to recover accuracy, which is determined by the network architecture and how much overparameterization
it exhibits. As such, a constraint (¬É) is resulted below which
even solutions on Pareto frontier are not interesting as they yield either unrecoverable or unaffordable accuracy loss depending on the application in hand. We formulate this as a Reinforcement Learning (RL) problem where, by tuning a parametric reward
function (¬Ñ), the RL agent can be guided towards solutions around the desirable region (¬Ö) that strike a particular balance
between the ‚ÄúState of Relative Accuracy‚Äù (y-axis) and the ‚ÄúState

2

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

of Quantization‚Äù (x-axis). As such, RELEQ is an automated method for efficient exploration of large hyper-parameter space (heterogeneous bitwidths assignments) that is orthogonal to the underlying quantized training technique, quantization method, network architecture, and the particular hardware platform. Changing one or more of these components could yield different search spaces, hence, different results.
2.3. Method Overview
RELEQ trains a reinforcement learning agent that determines the level of deep quantization (below 8 bits) for each layer of the network. RELEQ agent explores the search space of the quantization levels (bit width), layer by layer. To account for the interplay between the layers with respect to quantization and accuracy, the state space designed for RELEQ comprises of both static information about the layers and dynamic information regarding the network state during the RL process (Section 2.4). In order to consider the effects of previous layers‚Äô quantization levels, the agent steps sequentially through the layers and chooses a bitwidth from a predefined set, e.g., {2,3,4,5,6,7,8}, one layer at a time (Section 2.5). The agent, consequently, receives a reward signal that is proportional to its accuracy after quantization and its benefits in term of computation and memory cost. The underlying optimization problem is multi-objective (higher accuracy, lower compute, and reduced memory); however, preserving the accuracy is the primary concern. To this end, we shape the reward asymmetrically to incentivize accuracy over the benefits (Section 2.6). With this formulation of the RL problem, RELEQ employs the stateof-the-art Proximal Policy Optimization (PPO) [40] to train its policy and value networks. This section details the components and the research path we have examined to design them.
2.4. State Space Embedding to Consider Interplay between Layers
Table 1. Layer and network parameters for state embedding.
Layer Specific Network Specific
Layer index Static Layer Dimensions N/A
Weight Statistics (standard deviation)

Dynamic

Quantization Level (Bitwidth)

State of Quantization
State of Accuracy

Interplay between layers. The final accuracy of a DNN is the result of interplay between its layers and reducing the bitwidth of one layer can impact how much another layer can be quantized. Moreover, the sensitivity of accuracy varies across

layers. We design the state space and the actions to consider these sensitivities and interplay by including the knowledge about the bitwidth of previous layers, the index of the layer-under-quantization, layer size, and statistics (e.g., standard deviation) about the distribution of the weights. However, this information is incomplete without knowing the accuracy of the network given a set of quantization levels and state of quantization for the entire network. Table 1 shows the parameters used to embed the state space of RELEQ agent, which are categorized across two different axes. (1) ‚ÄúLayer-Specific‚Äù parameters which are unique to the layer vs. ‚ÄúNetwork-Specific‚Äù parameters that characterize the entire network as the agent steps forward during training process. (2) ‚ÄúStatic‚Äù parameters that do not change during the training process vs. ‚ÄúDynamic‚Äù parameters that change during training depending on the actions taken by the agent while it explores the search space.

State of quantization and relative accuracy. The ‚ÄúNetwork-

Specific‚Äù parameters reflect some indication of the state of

quantization and relative accuracy. State of Quantization is a

metric to evaluate the benefit of quantization for the network

and it is calculated using the compute cost and memory cost

of each layer. For a neural network with L layers, we define

compute cost of layer l as the number of Multiply-Accumulate (MAcc) operations (nMl Acc), where (l = 0,...,L). Additionally, since RELEQ only quantizes weights, we define memory cost of layer l as the number of weights (nwl ) scaled by the ratio of Memory Access Energy (EMemoryAccess) to MAcc Computation Energy (EMAcc), which is estimated to be around 120√ó [16]. It

is intuitive to consider the that sum of memory and compute

costs linearly scales with the number of bits for each layer (nbl its). The nbmiatsx term is the maximum bitwidth among the predefined set of bitwidths that‚Äôs available for the RL agent to

pick from. Lastly, the State of Quantization (StateQuantization)

is the sum over all layers (L) that accounts for the total memory

and compute cost of the entire network.

‚àëLl=0[(nwl

√ó

EMemoryAccess E

+nMl Acc)√ónbl its]

StateQuantization = L ‚àë

MAcc
[nw √ó EMemoryAccess +nMAcc]√ónbits

l=0 l

EMAcc

l

max

Besides the potential benefits, captured by StateQuantization, RELEQ considers the State of Relative Accuracy to gauge
the effects of quantization on the classification performance.
To that end, the State of Relative Accuracy (StateAccuracy) is defined as the ratio of the current accuracy (AccCurr) with the current bitwidths for all layers during RL training, to accuracy
of the network when it runs with full precision (AccFullP). StateAccuracy represents the degradation of accuracy as the result of quantization. The closers this term is to 1.0, the lower the accuracy loss and more desirable the quantization levels.
StateAccuracy = AccCurr AccFullP

Given these embedding of the observations from the environment, the RELEQ agent can take actions, described next.

2.5. Flexible Actions Space
Intuitively, as calculations propagate through the layers, the effects of quantization will accumulate. As such, the RELEQ

3

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

8-bits 7-bits
6-bits

1-bit 2-bits
3-bits

ùêÅ(ùê≠) + ùüè

Increment

ùêÅ(ùê≠) = BCiutwrriedntht

No Change

Decrement

5-bits

4-bits

(a)

ùêÅ(ùê≠) ‚àí ùüè (b)

Figure 2. (a) Flexible action space (used in RELEQ). (b) Alternative action space with restricted movement.

Threshold

Threshold

Threshold

(a)

(b)

(c)

Figure 3. Reward shaping with three different formulations. The color palette shows the intensity of the reward.

agents steps through each layer sequentially and chooses from the bitwidth of a layer from a discrete set of quantization levels which are provided as possible choices. Figure 2(a) shows the representation of action space in which the set of bitwidths is {1,2,3,4,5,6,7,8}. As depicted, the agent can flexibly choose to change the quantization level of a given layer from any bitwidth to any other bitwidth. The set of possibilities can be changed as desired. Nonetheless, the action space depicted in Figure 2(a) is the possibilities considered for deep quantization in this work. As illustrated in Figure 2(b), an alternative that we experimented with was to only allow the RELEQ agent to increment/decrement/keep the current bitwidth of the layer (B(t)). The experimentation showed that the convergence is much longer than the aforementioned flexible action space, which is used.

2.6. Asymmetric Reward Formulation for Accuracy
While the state space embedding focused on interplay between the layers and the action space provided flexibility, reward formulation for RELEQ aims to preserve accuracy and minimize bitwidth of the layers simultaneously. This requirement creates an asymmetry between the accuracy and bitwidth reduction, which is a core objective of RELEQ. The following Reward Shaping formulation provides the asymmetry and puts more emphasis on maintaining the accuracy as illustrated with different color intensities in Figure 3(a). This reward uses the same terms of State of Quantization (StateQuantization) and State of Relative Accuracy (StateAccuracy) from Section 2.4. One of the reasons that we chose this formulation is that it produces a smooth reward gradient as the agent approaches the optimum quantization combination. In addition, the varying 2-dimensional gradient speeds up the agent‚Äôs convergence time. In the reward formulation, a = 0.2 and b = 0.4 can also

Reward

Reward Shaping: reward =1.0‚àí(StateQuantization)a if (StateAcc <th) then
reward =‚àí1.0 else
Accdiscount = max(StateAcc,th)(b/max(StateAcc,th)) reward =reward√óAccdiscount end if
be tuned and th = 0.4 is threshold for relative accuracy below which the accuracy loss may not be recoverable and those quantization levels are completely unacceptable. The use of threshold also accelerates learning as it prevents unnecessary or undesirable exploration in the search space by penalizing the agent when it explores undesired low-accuracy states. While Figure 3(a) shows the aforementioned formulation, Figures 3(b) and (c) depict two other alternatives. Figure 3(b) is based on StateAccuracy/StateQuantization while Figure 3(c) is based on StateAccuracy‚àíStateQuantization. Section 5.6 provides detailed experimental results with these three reward formulations. In summary, the formulation for Figure 3(a) offers faster convergence.
2.7. Policy and Value Networks While state, action and reward are three essential compo-
nents of any RL problem, Policy and Value complete the puzzle and encode learning in terms of a RL context. While there are both policy-based and value-based learning techniques, RELEQ uses a state-of-the-art policy gradient based approach, Proximal Policy Optimization (PPO) [40]. PPO is a actor critic style algorithm so RELEQ agent consists of both Policy and Value networks.
Network architecture of Policy and Value networks. Both Policy and Value are functions of state, so the the state space defined in Section 2.4 is encoded as a vector and fed as input to a Long short-term memory(LSTM) layer and this acts as the first hidden layer for both policy and value networks. Apart from the LSTM, policy network has two fully connected hidden layers of 128 neurons each and the number of neurons in the final output layer is equal to the number of available bitwidths the agent can choose from. Whereas the Value network has two fully connected hidden layers of 128 and 64 neurons each. Based on our evaluations, LSTM enables the RELEQ agent to converge almost √ó1.33 faster in comparison to a network with only fully connected layers.
While this section focused on describing the components of RELEQ in isolation, the next section puts them together and shows how RELEQ automatically quantizes a pre-trained DNN.
3. Putting it All Together: RELEQ in Action
As discussed in Section 2, state, action and reward enable the RELEQ agent to maneuver the search space with an objective

4

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

State of Quantization, State of Relative Accuracy

Hidden Input
Output
Pre-trained Network

Layer

(i
< l a t e x i t s h a 1 _ b a s e 6 4 = " L h i 8 t o X z F W m + D N j d f J L / M O 2 4 Y N Z p j b P + o A P 2 M 6 k D U N m D k r 6 X Q r W r o g = " > A A A B 7 H i c b Z V D B N S g 8 M N x A F E I J X 3 v U 1 r L 1 9 q a / t q V h Z 6 d 9 u L g B k a W h o I S o I S M s k F 2 z 5 0 0 W W v X H D i j s s Y o N L p T C F G t 8 q p h m Z u N 2 J m M X G b 5 j r Z J h D d E y l O G U K 0 E N O / f g w x Y 0 M L i i X w v i 1 u B f 3 C v B w 3 v b o t 3 M p c z t 0 P J X b B D w w O Q O + 9 z G r W m b X m 3 h H a v n D g V 2 H n B j t e P t O 1 / P b Z K 2 W N x z t a 7 3 + q z n u F 1 f v d b L 2 B D 4 w d 6 H P 5 6 e s P c K n y H W Z l 1 L k J i 5 j m K i f z J K i e J J R S v F Z Q B n o J J r o h J k L v p u l F v G u s B F G 6 s q k G y I p l G D 4 w l b C r w h d 9 j G i 7 + h m d + 5 f + t Y J 0 6 j Y y 0 R T j + 2 S a j W m s a i Q A s m i Y M 8 l k Q j 8 T o h m T x Y k q t z / l k 1 V / g h 1 y v W u G p 9 X 4 q r p l 7 c r A L r Y R Q N 2 c A k a g + a g U 2 a i A j / P r Z X h 4 8 N A R 0 Q O r x O X Y v S n U q M D F h 0 G b Y q x P k v 4 d Y Q K E o O n V U G X G e U 6 8 k H J m c t q U I G M m p W 4 U J r N o S l 7 I 1 x M Z s 3 5 1 T J Q J M Y R q m a y D r v k D V h J 2 Y j q i a 6 D s f M D k H J s R F o F m 1 x a J Z g 4 w C r i 1 R 9 N 0 k R n O D Y V q q 1 4 n v c z W t g y 7 E Y m 2 s I 9 m i e U t N V b G i R P M 9 z 5 0 / u c v x Z E 3 t P 0 w H v O 6 Z 2 Z Y o m Z u J g u 1 l y L U t Z P Q M J M Z E B m K X 0 H + 0 B W y Z N Q u C G Z L B U 8 i 8 J 3 k R l g h C t p G u j b Z 0 h V Y 0 I Q F h R S x h O x y u u Z i T I s 6 y I H I g N 1 f Z Y f + X J S X e s f E a v x L Z 7 6 y L J H r 3 T C q j L 5 v Z Z c x / V I O C E r M D z R q e E W J K G s G I 6 5 g X B E f A f N Q M B N h x 8 A o A c + H 6 i h G C V T 3 5 h Q z 4 p P A P M i r v z D B s z f p y P 9 P a i K v U D 8 n 6 v c y w 9 h K 8 C 4 s n + z o 9 5 k g 2 z Y 9 2 y r P < n / 8 l A a Q t v e u x P i y t w > = = < / l a t e x i t >

+

1)

Layer

i< l a t e x i t s h a 1 _ b a s e 6 4 = " p d z E N t e V 9 0 0 z s L t s e N H e n l M U y i t 1 w f S W k 5 Y 2 J P R F i c A 5 H N u I e Y B A o = " > A A A B 6 H i c b Z V C B 7 N S g 8 N B A F E I J b 3 P U x r l 1 u q M / t q 6 h i 6 l 9 z L W B A b Q B r U J 0 Z i d 8 U 2 2 G h P n B w i M 8 Y c y W A 7 Z A M e I 0 y o R W J y m 2 J k 2 3 e b T t M Z b h O N z 2 y N 8 0 y I s J E / J Q Y V 8 e g P Y C 2 j F i I 1 r Z a / + k g z m X 9 / i j 5 t 9 s s 1 4 B u W R x S 8 a M + P M N P 6 A b x Y / W + Z f e w m 5 A x q z u w j l e R d w 9 b O T 6 z W v t 2 7 y Z m 3 s d r v W f 9 J s + b 5 h e W D 3 w S 6 z P u i 7 k e / n s r H W 5 0 c U O m j m l G k L 4 Z y Z x I b h D L J V E C p 6 G l o G + w 5 S B W q 2 F D F T x c i C 0 e 3 6 A l j C 8 G D o 5 c V C S u O + N H Q 0 Y b D u s F c 3 3 n U 1 z B z p 9 n i s M g q H z M R 0 N s 5 x Z i 8 O Y l p Y B 8 j o E g d z S a B q 5 z x U R 4 o s 2 N 1 q G z r X x O X 9 r J n c i g u m N 8 x Q N t Z S B g X w 8 L B N l Y e f v V P r a M r E U p G Y A F P q V M e 0 + T a F v C b t T + 1 7 g 6 W X o m z i R C M n U y K n 0 A 7 m v c p F e 4 a Z I Z K B f p K T c y C q Z w 0 U j u H p 1 m L G J l Y P 1 K R R B n / S n A y H 0 Y D u m S 5 x s q s i q D I f R D I b m o y h J J Q x 1 Z p q 0 r + 8 i n R c N h k p n r D P Z Y m t 5 D v 2 z x t l y T G M m 9 s H 9 r j 3 k N L b 8 G z V + M t z n 1 J M q v o Z H 1 O P Z w d v p 6 Z 2 l Q C m y u 1 g a p I y o L E t 8 P Q M k o Z G P T E z 1 j G 6 X J G M F E z J I O i Q Z 6 J d Z a Q k p z b x m U 8 y l I b 8 E Y I W V K Z F c P Z c m z U k 7 r E Y h k + C O r s K v j b L 5 1 L N O y j R e 7 t B 7 X r 1 t 5 / 5 y F a V w o 2 X 3 r i u K + M 5 M f F s 3 O A v J 1 F + y D Y D q L w T g T n g c H A p r n Q 4 B M g M Y l I 1 z O / A W K 6 b t 8 A 6 E j B 8 g + h K P 8 8 O A x K + v r z 1 o p P J z T 7 z L J w z 5 D 7 H / z P i S f g P r 8 P i o J O j Y N Y / = c < j / 5 l + a A t P e C x V i j t m > 4 = < / l a t e x i t >

Layer

(i
< l a t e x i t s h a 1 _ b a s e 6 4 = " R Z k V / E p A 5 i y 3 X H z d y 0 U 5 b h K Q 3 6 3 V 7 P t + L 8 C m 6 e g m T 1 3 f g 3 x m V t f L I w = " > A A A B 7 H i c b Z V D B N S g 8 M N x A F E I J X 3 v U 1 r L 1 9 q a / t q V h Z 6 d 9 u L g B k a W h o H S g 4 x c Z J L t 3 3 o o s s e u P D F G Y Z w Q b W S n F L N b p R T D N y d a t S M Z u N 3 j W S z T C G 7 Z k K Y M o U o I b Y / + B g i x w s d X F F v h P F q c D + v U P D l u v f 3 B K v Y T 5 n a 4 O W u 2 D H g g c h d 8 7 n M H 8 M z v M u C f 1 e P G B q t e f D G a 8 e b N 6 6 e 3 y U s 9 b j m a 1 3 v t V n P d K r + e 6 3 X t D H g x 6 w P e y 1 s Y e 9 V P k O 9 j O r W J T F j G J U F + m T U 8 S T i k e a i h H O R S T D H Q D T J X f D M L O f N c Y C L N 1 Y U J M 1 R W K M H x g K n F X g D 7 6 X d B 3 8 C N 7 8 z / 4 b x T p 0 X x k p i n H s 8 h 0 H s M Z 0 U l F Z M E x J p O J h H 5 n B G J n j x J F b j / L J r / F D r l f s N W 7 v w V X D K 3 8 X A W w i h e t 4 A J l A 5 0 B o t 0 V R G 7 e W z v T w 4 a A j o h N G m Y v x f k P 4 U Y G K C o c n 1 U i f J e g 6 0 k V J R c O q s I u M 9 p l 4 I L T N 5 a E 4 Q N Z M T s g 5 W T b Q l K n R q m Z z Z v i q m W h S Y x z E J w k H X e Y X u H S s x H E F w 1 H Y + Z W Y L S Y i K R b N q m 0 S z B g h B X F q i 7 b 4 J m P c G x r F R r w P f 4 3 t f B k 2 J x N s Z R 6 M E 9 o K e q 2 3 M E i P Z / m z p + N p e m z J u b f o l O f c 1 y s z 1 Q M z d T B N v L k l X o K i a g Z T Y y Z C I R u o P 8 4 T o k y a g c U c y W C o 5 E p T u N j L A C V F e X M c G 3 j o G r x o Q h K C j h i C d j l c Z 2 E n R Z 0 k Q P R A a q u y x + 9 v S k v 8 Y 6 I 1 e i H z 3 0 l X T P W + j B V G X y e y y 3 5 j + q A M F I X Z G n y E 4 M s T V M Y N R x z A u C I + A 6 a h Y D L T i 5 B Q B 4 t P x A D M E r 3 / y D g m w S O E f Z F X e m X H c m + S l O q f 0 F V e p X 5 P w e 5 l h 6 T U 9 F w Z P 9 n V 8 z A B Z n + / W k N f r P Q 4 = A = R < g / e l P a z t Q e = x = i < t / > l a t e x i t >

1)

Network SpeciÔ¨Åc Embedding

State Embedding (‚àÄ Layer)
Layer SpeciÔ¨Åc Embedding

Policy Network
Value Network

Action Value

Quantize
Set of Available Bitwidths

Short Finetune Environment

Run Inference
Reward Calculator

Policy Gradient (Proximal Policy
Optimization Engine)

Policy/Value Networks Updates

Figure 4. Overview of RELEQ, which starts from a pre-trained network and delivers its corresponding deeply quantized network.

(a)

(b)

(c)

(d)

Figure 5. Action (Bitwidths selection) probability evolution over training episodes for LeNet.

of quantizing the neural network with minimal loss in accuracy. RELEQ starts with a pretrained model of full precision weights and proposes quantization levels of weights for all layers in a DNN. Figure 4 depicts the entire workflow for RELEQ and this section gives an overview of how everything fits together.

Interacting with the environment. RELEQ agent steps through all layers one by one, determining the quantization level for the layer at each step. For every step, the state embedding for the current layer comprising of different elements described in Section 2.4 is fed as an input to the Policy and Value Networks of the RELEQ agent and the output is the probability distribution over the different possible bitwidths and value of the state respectively. RELEQ agent then takes a stochastic action based on this probablity distribution and chooses a quantization level for the current layer. Weights for this particular layer are quantized to the predicted bitwidth and with accuracy preservation being a primary component of RELEQ‚Äôs reward function, retraining of a quantized neural network is required in order to properly evaluate the effectiveness of deep quantization. Such retraining is a time-intensive process and it undermines the search

process efficiency. To get around this issue, we reward the agent with an estimated validation accuracy after retraining for a shortened amount of epochs. For deeper networks, however, due to the longer retraining time, we perform this phase after all the bitwidths are selected for the layers and then we do a short retraining. Dynamic network specific parameters listed in Table 1 are updated based on the validation accuracy and current quantization levels of the entire network before stepping on to the next layer. In this context, we define an epsiode as a single pass through the entire neural network and the end of every episode, we use Proximal Policy Optimization [40] to update the Policy and Value networks of the RELEQ agent. After the learning process is complete and the agent has converged to a quantization level for each layer of the network, for example 2 bits for second layer, 3 bits for fourth layer and so on, we perform a long retraining step using the quantized bitwidths predicted by the agent and then obtain the final accuracy for the quantized version of the network. Learning the Policy. Policy in terms of neural net-
work quantization is to learn to choose the optimal bitwidth for each layer in the network. Since RELEQ uses a Policy Gradient based approach, the objective is to optimize the policy directly, it‚Äôs possible to visualize how policy for each layer evolves with respect to time i.e the number of episodes. Figure 5 shows the evolution of RELEQ agent‚Äôs bitwidth selection probabilities for all layers over time (number of episodes), which reveals how the agent‚Äôs policy changes with respect to selecting a bitwidth per layer. As indicated on the graph, the end results suggest the following quantization patterns, 2,2,2,2 or 2,2,3,2 bits. For the first two convolution layers (Convolution Layer 1, Convolution Layer 2), the agent ends up assigning the highest probability for two bits and its confidence increases with increasing number of training episodes. For the third layer (Fully Connected Layer 1), the probabilities of two bits and three bits are very close. Lastly, for the fourth layer (Fully Connected Layer 2), the agent again tends to select two bits, however, with relatively smaller confidence compared to layers one and two. With these observations, we can infer that bitwidth probability profiles are not uniform across all layers and that the agent distinguishes between the layers, understands the sensitivity of the objective function to the different layers and accordingly chooses the bitwidths. Look-

5

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

ing at the agent‚Äôs selection for the third layer (Fully Connected Layer 1) and recalling the initial problem formulation of quantizing all layers while preserving the initial full precision accuracy, it is logical that the probabilities for two and three bits are very close. Going further down to two bits was beneficial in terms of quantization while staying at three bits was better for maintaining good accuracy which implies that third layer precision affects accuracy the most for this specific network architecture. This points out the importance of tailoring the reward function and the role it plays in controlling optimization tradeoffs.

4. Experimental Setup

4.1. Benchmarks

To assess the effectiveness of RELEQ across a variety of DNNs, we use the following seven diverse networks that have been used in different real-world vision tasks: AlexNet, CIFAR10 (Simplenet), LeNet, MobileNet (Version 1), ResNet-20, SVHN and VGG-11. Of these seven networks, AlexNet and MobileNet were evaluated on the ImageNet (ILSVRC‚Äô12) dataset, ResNet-20, VGG-11 and SimpleNet (5 layers) on CIFAR-10, SVHN (10 layers) on SVHN and LeNet on the MNIST dataset.

4.2. Quantization Technique

As described in earlier sections, RELEQ is an off-the-shelf automated framework that works on top of any existing quantization technique to yield efficient heterogeneous bitwidths assignments. Here, we use the technique proposed in WRPN [34] where weights are first scaled and clipped to the (‚àí1.0,1.0) range and quantized as per the following equation. The parameter k is the bitwidth used for quantization out of which k‚àí1 bits are used for quantization and one bit is used for sign.

round((2k‚àí1‚àí1)wf )

wq =

2k‚àí1 ‚àí1

(1)

Additionally, different quantization styles (e.g., mid-tread vs. mid-rise) yield different quantization levels. In mid-tread, zero is considered as a quantization level, while in mid-rise, quantization levels are shifted by half a step such that zero is not included as a quantization level. Here, we use mid-tread style following WRPN.

4.3. Granularity of Quantization

Quantization can come at different granularities: pernetwork, per-layer, per-channel/group, or per-parameter [25]. However, as the granularity becomes finer, the search space goes exponentially larger. Here, we consider per-layer granularity which strikes a balance between adapting for specific network requirements and practical implementations as supported in a wide range of hardware platforms such as CPUs, FPGAs, and dedicated accelerators. Nonetheless, similar principles of automated optimization can be extended for other granularities as needed.

4.4. Deep Quantization with Conventional Hardware
RELEQ‚Äôs solution can be deployed on conventional hardware, such as general purpose CPUs to provide benefits and improvements. To manifest this, we have evaluated RELEQ using TVM [5] on an Intel Core i7-4790 CPU. We use TVM since its compiler supports deeply quantized operations with bitserial vector operations on conventional hardware. We compare our solution in terms of the inference execution time (since the TVM framework does not offer energy measurements) against 8-bit quantized network. The results can be seen in Figure 8 and will be further elaborated in the next section.
4.5. Deep Quantization with Custom Hardware Accelerators
To further demonstrate the energy and performance benefits of the solution found by RELEQ, we evaluate it on Stripes [24], a custom accelerator designed for DNNs, which exploits bitserial computation to support flexible bitwidths for DNN operations. Stripes does not support or benefit from deep quantization of activations and it only leverages the quantization of weights. We compare our solution in terms of energy consumed and inference execution time against the 8-bit quantized network.
4.6. Comparison with Prior Work
We also compare against prior work [46], which proposes an iterative optimization procedure (dubbed ADMM) through which they find quantization bitwidths only for AlexNet and LeNet. Using Stripes [24] and TVM [5], we show that RELEQ‚Äôs solution provides higher performance and energy benefits compared to ADMM [46].
4.7. Implementation and Hyper-parameters of the Proximal Policy Optimization (PPO)
As discussed, RELEQ uses PPO [40] as its RL engine, which we implemented in python where its policy and value networks use TensorFlow‚Äôs Adam Optimizer with an initial learning rate of 10‚àí4. The setting of the other hyper-parameters of PPO is listed in Table 3.
5. Experimental Results
5.1. Quantization Levels with RELEQ
Table 2 provides a summary of the evaluated networks, datasets and shows the results with respect to layer-wise quantization levels (bitwidths) achieved by RELEQ. Regarding the layer-wise quantization bitwidths, at the onset of the agent‚Äôs exploration, all layers are initialized to 8-bits. As the agent learns the optimal policy, each layer converges with a high probability to a particular quantization bitwidth. As shown in the ‚ÄúQuantization Bitwidths‚Äù column of Table 2, RELEQ quantization policies show a spectrum of varying bitwidth assignments to the layers. The bitwidth for MobileNet varies

6

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

Network AlexNet SimpleNet LeNet MobileNet ResNet-20 SVHN-10 VGG-11 VGG-16

Dataset ImageNet CIFAR10 MNIST ImageNet CIFAR10
SVHN CIFAR10 CIFAR10

Table 2. Benchmark DNNs and their deep quantization with RELEQ.

Quantization Bitwidths

Average Bitwidth

{8, 4, 4, 4, 4, 4, 4, 8}

5

{5, 5, 5, 5, 5}

5

{2, 2, 3, 2}

2.25

{8, 5, 6, 6, 4, 4, 7, 8, 4, 6, 8, 5, 5, 8, 6, 7, 7, 7, 6, 8, 6, 8, 8, 6, 7, 5, 5, 7, 8, 8}

6.43

{8, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 8}

2.81

{8, 4, 4, 4, 4, 4, 4, 4, 4, 8}

4.80

{8, 5, 8, 5, 6, 6, 6, 6, 8}

6.44

{8, 8, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 8}

7.25

Acc Loss (%) 0.08 0.30 0.00 0.26 0.12 0.00 0.17 0.10

Table 3. Hyperparameters of PPO used in ReLeQ.

Hyperparameter

Value

Adam Step Size Generalized Advantage Estimation Parameter Number of Epochs Clipping Parameter

1√ó10‚àí4 0.99 3 0.1

Figure 6. Quantization space and its Pareto frontier for (a) CIFAR-10, (b) LeNet, (c) SVHN, and (d) VGG-11.
from 4 bits to 8 bits with an irregular pattern, which averages to 6.43. ResNet-20 achieves mostly 2 and 3 bits, again with an irregular interleaving that averages to 2.81. In many cases, there is significant heterogeneity and irregularity in the bitwidths and a uniform assignment of the bits is not always the desired choice to preserve accuracy. These results demonstrate that ReLeQ automatically distinguishes different layers and their varying importance with respect to accuracy while choosing their respective bitwidths. As shown in the ‚ÄúAccuracy Loss‚Äù column of Table 2, the deeply quantized networks with RELEQ have less

than 0.30% loss in classification accuracy. To assess the quality of these bitwidths assignments, we conduct a Pareto analysis on the DNNs for which we could populate the search space.
5.2. Validation: Pareto Analysis
Figure 6 depicts the solutions space for four benchmarks (CIFAR10, LeNet, SVHN, and VGG11). Each point on these charts is a unique combination of bitwidths that are assigned to the layers of the network. The boundary of the solutions denotes the Pareto frontier and is highlighted by a dashed line. The solution found by RELEQ is marked out using an arrow and lays on the desired section of the Pareto frontier where the accuracy loss can be recovered through fine-tuning, which demonstrates the quality of the obtained solutions. It is worth noting that as a result of the moderate size of the four networks presented in this subsection, it was possible to enumerate the design space, obtain Pareto frontier and assess ReLeQ quantization policy for each of the four networks. However, it is infeasible to do so for state-of-theart deep networks (e.g., MobileNet and AlexNet) which further stresses the importance of automation and efficacy of RELEQ.
5.3. Learning and Convergence Analysis
We further study the desired behavior of RELEQ in the context of convergence. An appropriate evidence for the correctness of a formulated reinforcement learning problem is the ability of the agent to consistently yield improved solutions. The expectation is that the agent learns the correct underlying policy over the episodes and gradually transitions from the exploration to the exploitation phase. Figures 7(a) and (b) first show the State of Relative Accuracy for CIFAR10 and SVHN, respectively. We overlay the moving average of State of Relative Accuracy as episodes evolve, which is denoted by a black line in Figures 7(a) and (b). Similarly, Figures 7(c) and (d) depict the evolution of State of Quantization. As another indicative parameter of learning, Figure 7(e) plots the evolution of the reward, which combines the two States of Accuracy and Quantization (Section 2.6). As all the graphs show, the agent consistently yields solutions that increas-

7

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

State of Relative Accuracy

State of Quantization

1 0.8 0.6 0.4 0.2
0 0
1 0.8 0.6 0.4 0.2
0 0
1
0.8
0.6
0.4
0.2
0 0

CIFAR10 1

0.8

Moving Average 0.6

0.4

(a)

0.2

0

100

200

300

400

0

1

0.8

0.6

0.4

(c)

0.2

100

200

300

Training Episodes

0

400

0

MobileNet

SVHN

100

200

300

100

200

300

Training Episodes

Reward Maximization

200

400

600

800

1000 1200 1400 1600 1800

Training Episodes

(b)

400

500

(d)

400

500

(e)
2000 2200

Reward

Figure 7. The evolution of reward and its basic elements: State of Relative Accuracy for (a) CIFAR-10, (b) SVHN. State of Quantization for (c) CIFAR-10, (d) SVHN, as the agent learns through the episodes. The last plot (e) shows an alternative view by depicting the evolution of reward for MobileNet. The trends are similar for the other networks.

Figure 8. Speedup with RELEQ for conventional hardware using TVM over the baseline run using 8 bits.
Figure 9. Energy reduction and speedup with RELEQ for Stripes over the baseline execution when the accelerator is running 8-bit DNNs.
ingly preserve the accuracy (maximize rewards), while seeking to minimize the number of bits assigned to each layer (minimizing the state of quantization) and eventually converges to a rather stable solution. The trends are similar for the other networks. 5.4. Execution Time and Energy BeneÔ¨Åts with
RELEQ Figure 8 shows the speedup for each benchmark network on conventional hardware using TVM compiler. The baseline is

the 8-bit runtime for inference. RELEQ‚Äôs solution offers, on average, 2.2√ó speedup over the baseline as the result of merely quantizing the weights that reduces the amount of computation and data transfer during inference. Figure 9 shows the speedup and energy reduction benefits of RELEQ‚Äôs solution on Stripes custom accelerator. The baseline here is the time and energy consumption of 8-bit inference execution on the same accelerator.
RELEQ‚Äôs solutions yield, on average, 2.0√ó speedup and an additional 2.7√ó energy reduction. MobileNet achieves 1.2√ó speedup which is coupled with a similar degree of energy reduction. On the other end of the spectrum, ResNet-20 and LeNet achieve 3.0√ó and 4.0√ó benefits, respectively. As shown in Table 2, MobileNet needs to be quantized to higher bitwidths to maintain accuracy, compared with other networks and that is why the benefits are smaller.
5.5. Speedup and Energy Reduction over ADMM
As mentioned in Section 4, we compare RELEQ‚Äôs solution in terms of speedup and energy reduction against ADMM [46], another procedure for finding quantization bitwidths. As shown in Table 4, RELEQ‚Äôs solution provides 1.25√ó energy reduction and 1.22√ó average speedup over ADMM with Stripes for AlexNet and the benefits are higher for LeNet. The benefits are similar for the conventional hardware using TVM as shown in Table 4. ADMM does not report other networks.

8

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

Network AlexNet LeNet

Dataset ImageNet MNIST

Table 4. Speedup and energy reduction with RELEQ over ADMM [46].

Technique

Bitwidth

RELEQ speedup RELEQ speedup

on TVM

on Stripes

RELEQ ADMM

{8,4,4,4,4,4,4,8} {8,5,5,5,5,3,3,8}

1.20X

1.22X

RELEQ ADMM

{2,2,3,2} {5,3,2,3}

1.42X

1.86X

Energy Improvement of RELEQ on Stripes 1.25X
1.87X

State of Relative Accuracy State of Relative Accuracy State of Relative Accuracy

CIFAR10

LeNet

SVHN

Reward = proposed
Reward = accst/quantst Reward = accst - quantst

Reward = proposed
Reward = accst/quantst Reward = accst - quantst

Reward = proposed
Reward = accst/quantst Reward = accst - quantst

(a)

(b)

(c)

Figure 10. Three different reward functions and their impact on the state of relative accuracy over the training episodes for three different networks. (a) CIFAR-10, (b) LeNet, and (c) SVHN.

5.6. Sensitivity Analysis: InÔ¨Çuence of Reward Function
The design of reward function is a crucial component of reinforcement learning as indicated in Section 2.6. There are many possible reward functions one could define for a particular application setting. However, different designs could lead to either different policies or different convergence behaviors. In this paper, we incorporate reward engineering by proposing a special parametric reward formulation. To evaluate the effectiveness of the proposed reward formulation, we have compared three different reward formulations in Figure 10: (a) proposed in Section 2.6, (b) R = StateAccuracy/StateQuantization, (c) R = StateAccuracy ‚àí StateQuantization. As the blue line in all the charts shows, the proposed reward formulation consistently achieves higher State of Relative Accuracy during the training episodes. That is, our proposed reward formulation enables RELEQ finds better solutions in shorter time.
5.7. Tuning: PPO Objective Clipping Parameter
One of the unique features about PPO algorithm is its novel objective function with clipped probability ratios, which forms a lower-bound estimate of the change in policy. Such modification controls the variance of the new policy from the old one, hence, improves the stability of the learning process. PPO uses a Clipped Surrogate Objective function, which uses the minimum of two probability ratios, one non-clipped and one clipped in a range between [1‚àíŒµ, 1+Œµ], where Œµ is a hyper-parameter that helps to define this clipping range. Table 5 provides a summary of tuning epsilon (commonly in the range of 0.1, 0.2, 0.3).

Based on our performed experiments, Œµ =0.1 often reports the highest average reward across different benchmarks.

Table 5. Sensitivity of reward to different clipping parameters.

PPO Clipping Parameter

Average Normalized Reward (Performance)

LeNet on MNIST

SimpleNet on
CIFAR-10

8-Layers on SVHN

Œµ =0.1 Œµ =0.2 Œµ =0.3

0.209 0.165 0.160

0.407 0.411 0.399

0.499 0.477 0.455

6. Related Work
ReLeQ is the initial step in utilizing reinforcement learning to automatically find the level of quantization for the layers of DNNs such that their classification accuracy is preserved. As such, it relates to the techniques that given a level of quantization, train a neural network or develop binarized DNNs. Furthermore, the line of research that utilizes RL for hyperparameter discovery and tuning inspires RELEQ. Nonetheless, RELEQ, uniquely and exclusively, offers an RL-based approach to determine the levels of quantization.
Automated machine learning (AutoML) methods. Because of the increased deployment of deep learning models into various domains and applications, AutoML has recently gained

9

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

a substantial interest from both academia [14, 31] and industry as internal tools [17], or open services [30, 3]. Hyperparameter optimization (HPO) is a major subfield of AutoML. One notable AutoML system is Auto-sklearn [14] that uses Bayesian optimization method to find the best instantiation of classifiers in scikit-learn [36].
Another major application of AutoML is neural architecture search (NAS) which also a has significant overlap with hyperparameter optimization [8]. Recently, Google introduced Cloud AutoML service [30] that is suite of machine learning products that enables developers with limited machine learning expertise to train high-quality models specific to their business needs. It relies on Googles state-of-the-art transfer learning and neural architecture search technology. Even though most of these frameworks include a wide range of supervised learning methods, a little include modern neural networks and their optimization.
Reinforcement learning for automatic tuning. RL based methods have attracted much attention within NAS after obtaining the competitive performance on the CIFAR-10 dataset employing RL as the search strategy despite the massive amount of the used computational resources [51]. Different RL approaches differ in how they represent the agents policy. Zoph and Le [51] use a recurrent neural network (RNN) trained by policy gradient, in particular, REINFORCE, to sequentially sample a string that in turn encodes a neural architecture. Baker et. al. [4] use Q-learning to train a policy which sequentially chooses a layers type and corresponding hyperparameters.
Recently, a wide variety of methods have been proposed in quick succession to reduce the computational costs and achieve further performance improvements [47, 37].
Aside from NAS applications, i.e. engineering neural architecture from scratch, [19] employ RL to prune existing architectures where a policy gradient method is used to automatically find the compression ratio for different layers of a network. On a different context, more recently, [1] leverages reinforcement learning for compiler optimization of deep learning models.
Here, we employ RL in the context of quantization to choose an appropriate quantization bitwidth for each layer of a network.
Training algorithms for quantized neural networks. There have been several techniques [49, 50, 34] that train a neural network in a quantized domain after the bitwidth of the layers is determined manually. DoReFa-Net [49] trains quantized convolutional neural networks with parameter gradients which are stochastically quantized to low bitwidth numbers before they are propagated to the convolution layers. [34] introduces a scheme to train networks from scratch using reduced-precision activations by decreasing the precision of both activations and weights and increasing the number of filter maps in a layer. DCQ [9] proposes an unorthodox method to train quantized neural networks. The proposed approach utilizes knowledge distillation through teacher-student paradigm in a novel setting that exploits the feature extraction capability of DNNs for higher-

accuracy quantization. This divide and conquer strategy makes the training of each student section possible in isolation while all these independently trained sections are later stitched together to form the equivalent fully quantized network. [50] performs the training phase of the network in full precision, but for inference uses ternary weight assignments. For this assignment, the weights are quantized using two scaling factors which are learned during training phase. PACT [6] introduces a quantization scheme for activations, where the variable Œ± is the clipping level and is determined through a gradient descent based method. SinReQ [10] proposes a novel sinusoidal regularization for deep quantized training. The proposed regularization is realized by adding a periodic function (sinusoidal regularizer) to the original objective function. By exploiting the inherent periodicity and local convexity profile in sinusoidal functions, SinReQ automatically propel weights towards target quantization levels during conventional training. Leveraging the sinusoidal properties further, [11] extended SinReQ to learn the quantization bitwidth during gradient-based training process. The key insight is that they leverage the observation that sinusoidal period is a continuous valued parameter. As such, the sinusoidal period serves as an ideal optimization objective and a proxy to minimize the actual quantization bitwidth, which avoids the issues of gradient-based optimization for discrete valued parameters.
ReLeQ is an orthogonal technique with a different objective: automatically finding the level of quantization that preserves accuracy and can potentially use any of these training algorithms.
Ternary and binary neural networks. Extensive work, [22, 38, 29] focuses on binarized neural networks, which impose accuracy loss but reduce the bitwidth to lowest possible level. In BinaryNet [21], an extreme case, a method is proposed for training binarized neural networks which reduce memory size, accesses and computation intensity at the cost of accuracy. XNOR-Net [38] leverages binary operations (such as XNOR) to approximate convolution in binarized neural networks. Another work [29] introduces ternary-weight networks, in which the weights are quantized to -1, 0, +1 values by minimizing the Euclidian distance between full-precision weights and their ternary assigned values. However, most of these methods rely on handcrafted optimization techniques and ad-hoc manipulation of the underlying network architecture that are not easily extendable for new networks. For example, multiplying the outputs with a scale factor to recover the dynamic range (i.e., the weights effectively become -w and w, where w is the average of the absolute values of the weights in the filter), keeping the first and last layers at 32-bit floating point precision, and performing normalization before convolution to reduce the dynamic range of the activations. Moreover, these methods [38, 29] are customized for a single bitwidth, binary only or ternary only in the case of [38] or [29], respectively, which imposes a blunt constraint on inherently different layers with different requirements resulting in sub-optimal quantization solutions. RELEQ aims to utilize the levels between binary

10

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

and 8 bits to avoid loss of accuracy while offering automation.
Techniques for selecting quantization levels. Recent work ADMM [46] runs a binary search to minimize the total square quantization error in order to decide the quantization levels for the layers. Then, they use an iterative optimization technique for fine-tuning. NVIDIA also released an automatic mixed precision (AMP) [35] which employs mixed precision during training by automatically selecting between two floating point (FP) representations (FP16 or FP32). There is a concurrent work HAQ [44] which also uses RL in the context of quantization. The following highlights some of the differences. RELEQ uses a unique reward formulation and shaping that enables simultaneously optimizing for two objectives (accuracy and reduced computation with lower-bitwidth) within a unified RL process. In contrast, HAQ utilizes accuracy in the reward formulation and then adjusts the RL solution through an approach that sequentially decreases the layer bitwidths to stay within a predefined resource budget. This approach also makes HAQ focused more towards a specific hardware platform whereas we are after a strategy than can generalize. Additionally, we also provide a systemic study of different design decisions, and have significant performance gain across diverse well known benchmarks. The initial version of our work [13], predates HAQ, and it is the first to use RL for quantization. Later HAQ was published in CVPR [43], and we published initial version of RELEQ in NeurIPS ML for Systems Workshop [12].
7. Conclusion
Quantization of neural networks offers significant promise in reducing their compute and storage cost. However, the utility of quantization hinges upon automating its process while preserving accuracy. This paper set out to define the automated discovery of quantization levels for the layers while complying to the constraint of maintaining the accuracy. As such, this work offered the RL framework that was able to effectively navigate the huge search space of quantization and automatically quantize a variety of networks leading to significant performance and energy benefits. The results suggests that a diligent design of our RL framework, which considers multiple concurrent objectives can automatically yield high-accuracy, yet deeply quantized, networks.
Acknowledgement
This work was in part supported by Semiconductor Research Corporation contract #2019-SD-2884, NSF awards CNS#1703812, ECCS#1609823, Air Force Office of Scientific Research (AFOSR) Young Investigator Program (YIP) award #FA9550-17-1-0274, and gifts from Google, Microsoft, Xilinx, Qualcomm.

References
[1] B. H. Ahn, P. Pilligundla, A. Yazdanbakhsh, and H. Esmaeilzadeh. Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation, 2020. 10
[2] Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in Neural Information Processing Systems 32, pages 6155‚Äì6166. 2019. 2
[3] Amazon. Automatic model tuning. https://docs.aws.amazon. com/sagemaker/latest/dg/automatic-model-tuning.html, 2018. 10
[4] B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing Neural Network Architectures using Reinforcement Learning. 2017. 10
[5] T. Chen, T. Moreau, Z. Jiang, H. Shen, E. Q. Yan, L. Wang, Y. Hu, L. Ceze, C. Guestrin, and A. Krishnamurthy. Tvm: End-to-end optimization stack for deep learning. CoRR, abs/1802.04799, 2017. 2, 6
[6] J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan, and K. Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. CoRR, abs/1805.06085, 2018. 10
[7] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493‚Äì2537, 2011. 1
[8] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture search: A survey. J. Mach. Learn. Res., 20:55:1‚Äì55:21, 2019. 10
[9] A. T. Elthakeb, P. Pilligundla, A. Cloninger, and H. Esmaeilzadeh. Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks, 2019. 10
[10] A. T. Elthakeb, P. Pilligundla, and H. Esmaeilzadeh. SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training, 2019. 10
[11] A. T. Elthakeb, P. Pilligundla, F. Mireshghallah, T. Elgindi, C.-A. Deledalle, and H. Esmaeilzadeh. Gradient-Based Deep Quantization of Neural Networks through Sinusoidal Adaptive Regularization, 2020. 10
[12] A. T. Elthakeb, P. Pilligundla, F. Mireshghallah, A. Yazdanbakhsh, and H. Esmaeilzadeh. Releq: A reinforcement learning approach for deep quantization of neural networks. ML for Systems Workshop, NeurIPS, 2018. 11
[13] A. T. Elthakeb, P. Pilligundla, F. Mireshghallah, A. Yazdanbakhsh, and H. Esmaeilzadeh. ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural Networks. CoRR, abs/1811.01704, November 5, 2018. 11
[14] M. Feurer, A. Klein, K. Eggensperger, J. T. Springenberg, M. Blum, and F. Hutter. Auto-sklearn: Efficient and robust automated machine learning. In Automated Machine Learning - Methods, Systems, Challenges, pages 113‚Äì134. 2019. 10
[15] J. Fromm, S. Patel, and M. Philipose. Heterogeneous bitwidth binarization in convolutional neural networks. In NeurIPS., pages 4010‚Äì4019, 2018. 2
[16] M. Gao, J. Pu, X. Yang, M. Horowitz, and C. E. Kozyrakis. TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory. In ASPLOS, 2017. 3
[17] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley. Google vizier: A service for black-box optimization.

11

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pages 1487‚Äì1495, 2017. 10 [18] J. Hauswald, M. Laurenzano, Y. Zhang, C. Li, A. Rovinski, A. Khurana, R. G. Dreslinski, T. N. Mudge, V. Petrucci, L. Tang, and J. Mars. Sirius: An open end-to-end voice and vision personal assistant and its implications for future warehouse scale computers. In ASPLOS, 2015. 1 [19] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. AMC: AutoML for Model Compression and Acceleration on Mobile Devices. In ECCV, 2018. 10 [20] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527‚Äì1554, 2006. 1 [21] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized Neural Networks. In NIPS. 2016. 10 [22] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. J. Mach. Learn. Res., 2017. 1, 10 [23] P. Judd, J. Albericio, T. H. Hetherington, T. M. Aamodt, and A. Moshovos. Stripes: Bit-serial deep neural network computing. 49th MICRO, pages 1‚Äì12, 2016. 1 [24] P. Judd, J. Albericio, T. H. Hetherington, T. M. Aamodt, and A. Moshovos. Stripes: Bit-serial deep neural network computing. 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 1‚Äì12, 2016. 2, 6 [25] R. Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. CoRR, abs/1806.08342, 2018. 6 [26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Commun. ACM, 60:84‚Äì90, 2012. 1 [27] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541‚Äì551, 1989. 1 [28] H. Lee, R. B. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009. 1 [29] F. Li and B. Liu. Ternary Weight Networks. CoRR, abs/1605.04711, 2016. 1, 2, 10 [30] Li, F.F., Li, J. Cloud automl: Making ai accessible to every business. https://www.blog.google/products/google-cloud/ cloud-automl-making-ai-accessible-every-business/, 2018. 10 [31] H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, M. Urban, M. Burkart, M. Dippel, M. Lindauer, and F. Hutter. Towards automatically-tuned deep neural networks. In Automated Machine Learning - Methods, Systems, Challenges, pages 135‚Äì149. 2019. 10 [32] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garc¬¥ƒ±a, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu. Mixed precision training. CoRR, abs/1710.03740, 2017. 1, 2 [33] A. K. Mishra and D. Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. CoRR, abs/1711.05852, 2017. 1, 2

[34] A. K. Mishra, E. Nurvitadhi, J. J. Cook, and D. Marr. WRPN: Wide Reduced-Precision Networks. In ICLR, 2018. 1, 2, 6, 10
[35] Nvidia. Automatic mixed precision for nvidia tensor core architecture in tensorflow. https://devblogs.nvidia.com/ nvidia-automatic-mixed-precision-tensorflow/. 11
[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825‚Äì2830, 2011. 10
[37] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Efficient neural architecture search via parameter sharing. In ICML, pages 4092‚Äì4101, 2018. 10
[38] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNORNet: ImageNet Classification Using Binary Convolutional Neural Networks. In ECCV, 2016. 10
[39] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:1137‚Äì1149, 2015. 1
[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 3, 4, 5, 6
[41] H. Sharma, J. Park, N. Suda, L. Lai, B. Chau, V. Chandra, and H. Esmaeilzadeh. Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network. ISCA, pages 764‚Äì775, 2018. 1
[42] P. Viola and M. Jones. Robust real-time object detection. In 2nd Workshop on Statistical and Computational Theories of Vision, 2001, 2001. 1
[43] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han. HAQ: hardwareaware automated quantization with mixed precision. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 8612‚Äì8620, 2019. 11
[44] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han. HAQ: Hardware-Aware Automated Quantization. arXiv preprint arXiv:1811.08886, November 21, 2018. 11
[45] B. Wu, Y. Wang, P. Zhang, Y. Tian, P. Vajda, and K. Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. CoRR, abs/1812.00090, 2018. 1, 2
[46] S. Ye, T. Zhang, K. Zhang, J. Li, J. Xie, Y. Liang, S. Liu, X. Lin, and Y. Wang. A unified framework of dnn weight pruning and weight clustering/quantization using admm. CoRR, abs/1811.01907, 2018. 6, 8, 9, 11
[47] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter. Nas-bench-101: Towards reproducible neural architecture search. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 7105‚Äì7114, 2019. 10
[48] C. Zhang, S. Bengio, and Y. Singer. Are all layers created equal? CoRR, abs/1902.01996, 2019. 2
[49] S. Zhou, Z. Ni, X. Zhou, H. Wen, Y. Wu, and Y. Zou. DoReFaNet: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients. CoRR, 2016. 1, 2, 10
[50] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained Ternary Quantization. In ICLR, 2017. 1, 2, 10

12

Presented as a spotlight paper at NeurIPS Workshop on ML for Systems 2018

[51] B. Zoph and Q. V. Le. Neural Architecture Search with

Reinforcement Learning. In ICLR, 2017. 10

13

