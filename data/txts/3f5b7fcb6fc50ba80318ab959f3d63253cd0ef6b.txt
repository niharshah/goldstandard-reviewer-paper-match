Acoustic Event Detection with Classiï¬er Chains
Tatsuya Komatsu1, Shinji Watanabe2, Koichi Miyazaki3, Tomoki Hayashi3,4
1LINE Corporation, Japan 2Carnegie Mellon University, USA
3Nagoya University, Japan 4Human Dataware Lab. Co., Ltd., Japan

arXiv:2202.08470v1 [eess.AS] 17 Feb 2022

Abstract
This paper proposes acoustic event detection (AED) with classiï¬er chains, a new classiï¬er based on the probabilistic chain rule. The proposed AED with classiï¬er chains consists of a gated recurrent unit and performs iterative binary detection of each event one by one. In each iteration, the eventâ€™s activity is estimated and used to condition the next output based on the probabilistic chain rule to form classiï¬er chains. Therefore, the proposed method can handle the interdependence among events upon classiï¬cation, while the conventional AED methods with multiple binary classiï¬ers with a linear layer and sigmoid function have placed an assumption of conditional independence. In the experiments with a real-recording dataset, the proposed method demonstrates its superior AED performance to a relative 14.80% improvement compared to a convolutional recurrent neural network baseline system with the multiple binary classiï¬ers. Index Terms: acoustic event detection, multi-label classiï¬cation, chain rule, classiï¬er chains
1. Introduction
Acoustic event detection (AED) is a technology for automatically detecting and recognizing the wide variety of sounds around us and understanding the environment and situation where the sounds have been recorded. This technology can be employed for diverse applications, including monitoring and surveillance [1, 2, 3, 4].
AED is a task of labeling semantic events and marking their temporal location and duration in a given audio signal. Furthermore, it assumes that events can co-occur in time, i.e., AED is a multi-label classiï¬cation problem. Hence, some approaches employ source separation techniques, for example, non-negative matrix factorization [5, 6]. With increasing data set sizes, neural network-based multi-label classiï¬cation models have received much attention, e.g., using convolutional neural networks (CNNs) [7] and long short-term memory (LSTM) [8, 9]. Convolutional recurrent neural networks (CRNNs) [10, 11] have become a strong baseline for neural approaches. More recently, self-attention-based models including Transformer [12, 13, 14] and Conformer [15] have shown signiï¬cant improvement in AED performances.
These conventional methods focus mainly on structures of the feature extractor for modeling the characteristics of acoustic events and less on the part of the classiï¬er design. For the classiï¬er design, almost all methods use multiple binary classiï¬ers consisting of a linear layer and a sigmoid function. This combination maps input audio to the activities of multiple events. While this is a straightforward way to achieve multi-label classiï¬cation, the assumption of conditional independence of each

event activity is placed behind it. This classiï¬er performs estimation independently of other events, and there is no explicit interaction among the events. However, sounds in the real world have dependencies on each other. For example, â€˜people speakingâ€™ is likely to occur with â€˜people walking,â€™ and â€˜break squeakingâ€™ sounds should accompany the â€˜car.â€™ Therefore, it is an inappropriate assumption that each event occurs independently.
Some AED methods focus on the classiï¬er design and the co-occurrence of events. Imoto et al. [16] modeled the cooccurrence relationship of events as a graph and used it as a constraint for training the model parameters. They extracted cooccurrence relationships only as statistics in a speciï¬c length, such as within audio clips, and the constraint is used only during training. Drossos et al. [17] proposed an AED based on sequence-to-sequence modeling that takes into account the temporal context of acoustic events. However, this method only models the inputâ€™s temporal dependence, and the classiï¬er itself is still the multiple binary classiï¬ers. Therefore, the assumption of conditional independence still remains in both cases.
In the machine learning ï¬eld, the problem of labeldependencies regarding multi-label classiï¬cation has been well studied [18]. For example, a classiï¬er based on probabilistic chain rules [19], Bayesian approach to ï¬nd label-dependencies has been proposed. A neural network based method has also been proposed and shown its effectiveness in image classiï¬cation [20], text classiï¬cation [21, 22], and the audio tagging task [23]. Also, in the speech ï¬eld, multi-speaker speech separation, speech recognition, and speaker diarization with the chain rule [24, 25] have been proposed. It demonstrates better performance than the conventional method, which deals with multiple speakers as conditionally.
This paper proposes acoustic event detection with classiï¬er chains based on the probabilistic chain rule to handle the dependencies among events. The proposed classiï¬er chains consist of a gated recurrent unit and perform iterative binary detection of multiple events one by one. In each iteration, one eventâ€™s activity is estimated, and the estimated activity is used to condition the classiï¬er chains for the next iteration. The proposed method can handle the interdependence among events upon classiï¬cation, while the conventional multiple binary classiï¬ers has placed an assumption of conditional independence.
2. AED with classiï¬er chains
2.1. Probabilistic formulation of AED and the conventional method
Let us consider AED with L event classes. AED is a multilabel classiï¬cation problem on a set of labels L that identiï¬es what acoustic events are active in the input audio. It can be

Feature extractor Multiple Binary Classifier

Input

audio

x

r

z1 binarize ğ‘¦"!

z2 binarize ğ‘¦""

ãƒ»ãƒ»

ãƒ»

ãƒ»ãƒ»

ãƒ»

ãƒ»ãƒ»

ãƒ»

zL binarize ğ‘¦"#

Figure 1: General architecture of the conventional AED methods. The multiple binary classiï¬ers estimate the activity for each event independently and there is no interaction among the events.

formulated as a problem of estimating a subset of event labels YË† âŠ† L from given input audio X âˆˆ RT Ã—F :

YË† = argmaxP (Y | X),

(1)

Y âŠ†L

where X = [x1, ..., xT ] is a F dimensional audio feature sequence of the input audio with T temporal frames. The labelsubset Y is typically represented by an L-dimensional multi-hot vector y âˆˆ {0, 1}L that {0, 1} indicate activity {inactive, active} of each event class. Using this multi-hot activity representation, Eq. (1) can be rewritten as the following joint probability:

yË† = argmax P (y | X)

(2)

yâˆˆ{0,1}L

= argmaxP (y1, ..., yL | X).

(3)

y1 ,...,yL

Figure 1 illustrates a general architecture of AED methods. The audio feature sequence X is transformed to a latent representation R = [r1, ..., rT ] by the feature extractor, where r âˆˆ RD is a D-dimensional latent representation. The structure of the fea-
ture extractor is generally designed with neural networks, such
as LSTMs [8, 9], CRNNs [10, 11], and self-attention [12, 15]. The latent representation rt at each temporal frame t is fed into any classiï¬er and obtain score vector zt âˆˆ (0, 1)L whose elements represent the activity-score of the corresponding event class. Then, each element of zt is binarized with an appropriate threshold and resulting in an estimated activity of each event set yË†t.
Almost all conventional methods employ multiple binary
classiï¬ers that consist of a linear layer and a sigmoid function. Here, let W âˆˆ RLÃ—D and b âˆˆ RL denote the weight and bias parameters of the linear layer, respectively. The multiple binary
classiï¬ers are written as follows:

z = Ïƒ(Wr + b),

(4)

zi = Ïƒ(wir + bi),

(5)

where, wi is i-th row vector in W, bi is i-th elements of b and Ïƒ(Â·) represents the sigmoid function. As can be seen from Eq. (5), the estimation of the i-th class score zi depending only on the latent representation r and the i-th weight wi, and no information on the other event classes are involved. Each event score and activity are estimated class-independently, depending only on the latent representation of the input audio. In other words, there is the conditional independence assumption for each event in the conventional method. From the view point, conventional methods approximate the joint probability of event

GRU
r

GRU
concat

ãƒ»

ãƒ»

ãƒ»

ãƒ»

ãƒ»

ãƒ»

GRU
concat

Linear +Sigmoid

z1 Binarize ğ‘¦"! ğ²"$" = [ğ‘¦"!, 0,0,0, , 0]

Linear +Sigmoid

z2 Binarize ğ‘¦""

ğ²"$' = [ğ‘¦"!, ğ‘¦"", 0, , , 0]

ãƒ»

ãƒ»

ãƒ»

ãƒ»

ãƒ»

ãƒ»

ğ²"$# = [ğ‘¦"!, ğ‘¦"",, , , ğ‘¦"#&!, 0]

Linear +Sigmoid

zL Binarize ğ‘¦"#

Figure 2: The proposed classiï¬er chains for AED. Each event activity is estimated iteratively and the estimation result in the previous iteration is used for conditioning next estimation.

classes in Eq. (2) by the product of probabilities with classindependent:

P (y | X) = P (y1, ..., yL | X)

(6)

=

Î 

L i=1

P

(y

i

|

yiâˆ’1,

...,

y1,

X).

(7)

2.2. Proposed AED with classiï¬er chains

The proposed method introduces classiï¬er chains for AED, designed without conditional independence assumption based on the probabilistic chain rule. The classiï¬er of the proposed method iteratively estimates yi not only using the latent representation r but also conditioning by the estimated active events {yË†1, ..., yË†iâˆ’1} in the previous iterations. Therefore, the classiï¬cation of the proposed method is performed assuming the following joint probability:

P (y | X) = Î Li=1P (yi | y1, ..., yiâˆ’1, X).

(8)

For the multi-label classiï¬cation, it is important to approximate the joint probability of multi-class in Eq. (3). The conventional methods make the assumption of conditional independence on each class and approximate Eq. (3) with the product of the probability of each class in Eq. (7). In contrast to the conventional method, the proposed classiï¬er does not make the conditional independence assumption, and is equivalent to Eq. (3) based on the probabilistic chain rule as Eq. (8) so that the dependency among events can be modeled. Here, the order of the classes is an essential key in the chain rule. The order used in this paper and its impact on performance will be described in detail in the following sections.
Figure 2 illustrates the proposed classiï¬er chains. The proposed method constructs classiï¬er chains by iteratively estimating each eventâ€™s activity using gated recurrent units (GRU). First, the latent representation r is extracted from input audio x using a feature extractor. For the feature extractor, any structure can be used. In this paper, we employ the CRNN-based feature extractor, which is commonly used as a popular and strong baseline [10, 11, 26]:

R = CRNN(F â†’D)(X) âˆˆ RD.

(9)

The extracted latent representation R = [r1, ..., rT ] is fed into the classiï¬er chain and event activities {yË†1, ..., yË†L} are estimated iteratively. When estimating yË†i, the proposed method

uses the latent representationn r of input audio and the estimated active event information yË†<i âˆˆ {0, 1}L in the previous iterations. The active event information yË†<i is represented as a multi-hot vector with elements which correspond to detected events are one, and the others are 0. Note that yË†<i has the ï¬xed dimention L for each iteration i and elements with indices greater than i are padded with zeros. For example, suppose the third and fourth event classes have already been estimated as active in the previous iterations. In that case, the third and fourth elements of yË†<i have 1, and others are 0, and it is used at the next iteration. Using r and yË†<i , the estimation of yË†i is as follows:

ri

=

Concatenate

(r,

yË†<i)

âˆˆ

(D+L)
R

(10)

zi = Ïƒ Linear(D+Lâ†’1)(ri)

(11)

yË†i = 1 zi > i (12) 0 otherwise,

where i is a threshold for binarizing zi. For the initial conditioning yË†<0 is set to zero.

2.3. The order of the classes for classiï¬er chains
The order of the classes is critical for classiï¬er chains that iteratively identify each class. In the chain rule-based speaker diarization [24], the speakerâ€™s order for the chain rule is determined beforehand, verifying every speakerâ€™s permutation, and then the classiï¬er chains are constructed. However, the permutation of classes is a factorial of the number of classes. Even if 10-class AED, as one of the typical AED task settings, there are 3,628,800 permutation patterns, and it is unrealistic to evaluate them all. The proposed method, therefore, takes three approaches to set up the order of the classes.
Higher/Lower F1 order Orders based on event-wise f1-scores obtained preliminary experiment by the baseline method. In this paper, the order sets up two patterns: the order of the highest performance (Higher F1) and the lowest performance (Lower F1).
Higher/Lower Frequency order Orders of the frequency of occurrence of each class in the training data set.
Random order Orders determined by random permutations of the class index. In this paper, we investigated multiple random values.

3. Experiments
3.1. Datasets
We conducted the experiments using two datasets; a synthetic dataset, URBAN-SED[27], and a real-recording dataset, TUT Sound Events 2017 [28]. The URBAN-SED dataset is a synthetic dataset consisting of 10 acoustic event classes with 10,000 audio clips and annotations generated using the scaper library[27]. The speciï¬cation of each audio clip is a single channel, 44,100Hz, and 16-bit WAV format. The ï¬les were split into the training set of 6,000, the validation set of 2,000, and the test set of 2,000.
TUT Sound Events 2017 dataset [28] is a real-recording dataset consisting of 6 event classes with manually annotated

Table 1: Network architecture of the proposed method.
Feature Extractor
Input: (T =512 frames, F =64 bins)
2D CNN ï¬lter=(3, 3, 64) Batch normalization, Pooling = (1, 4)
2D CNN ï¬lter=(3, 3, 64) Batch normalization, Pooling = (1, 4)
2D CNN ï¬lter=(3, 3, 64) Batch normalization, Pooling = (1, 4)
Bi-GRU, Unit size= 62
Classiï¬er Chains
GRU, Unit size= 124 Linear layer + Sigmoid
Output shape = (T =512 frames, L= # of classesâˆ—) * 10 for URBAN-SED and 6 for TUT Sound Events 2017
labels. Each audio clip is recorded in a single acoustic scene, street, with a 44.1 kHz sampling rate and 24-bit resolution.
Each event in the URBAN-SED dataset is randomly synthesized to the background sound, and the frequency of occurrence and overlap of each event is artiï¬cially set. On the other hand, TUT Sound Events 2017 dataset reï¬‚ects the dependency among events in the real world. Using these two different types of datasets, we can measure the effectiveness of the proposed chain-classiï¬er in capturing event dependencies.
3.2. Experimental conditions
For the neural network input, every audio clip was transformed into a log-mel spectrogram of dimension F = 64, a chunk of temporal frames T = 512 with 20 ms window length and 10 ms hop length. The network architecture of the proposed method is shown in Table 1. 2D CNN ï¬lter = (A, B, C) denotes a 2D CNN layer with (A, B) ï¬lter size and C channels. Pooling= (a, b) denotes the max pooling operation on a temporal frames and b bins on the frequency axis. The mini-batch size was 32, and the number of epochs was 100. Adam [29] with learning rate 0.001 was used as the stochastic optimization method. The activity-score threshold i in Eq. (12) was optimized for the validation set.
The evaluation metric is frame-based and segment-based (1 sec.) macro f1-score [30], which is the harmonic mean of recalls and precisions of frame/segment-wise classiï¬cation results. The f1-scores were calculated for each event class, and the average of those f1-scores has been used.
For the comparison, we evaluated the CRNN with the multiple binary classiï¬ers as â€œbaselineâ€, which is the strong and widely used baseline [10, 11, 26]. The difference with the proposed method was whether the classiï¬er design is the classiï¬er chains or the multiple binary classiï¬er. All other conditions were set up to be common. In addition, for a more valid evaluation, we also evaluated a method with a CRNN-based feature extractor and a GRU classiï¬er. It is exactly the same architecture as the proposed method, only without the chain rule, i.e., without conditioning path in Figure 2.
As a validation of the proposed method, we trained the classiï¬er chains using ï¬ve class orders, as described in Section 2.3. The ï¬ve kinds of orders were, Higher/Lower F1 order,

Table 2: AED performances of each classiï¬er. All classiï¬ers use CRNN for feature extractor.

URBAN-SED

Frame-based

Baseline

0.612

+GRU (w/o Chain) +Chain (proposed)

0.624 0.631

TUT Sound Event 2017

Frame-based

Baseline

0.358

+GRU (w/o Chain) +Chain (proposed)

0.375 0.411

Seg.-based 0.625 0.635 0.647
Seg.-based 0.375 0.395 0.426

Table 3: Effect of the chain-order on the frame-based F1-score. For both datasets, the higher the F1 order shows the best performance. The averages for all orders are shown with their standard deviations.

Higher F1 Lower F1 Higher Freq. Lower Freq. Random 1 Random 2 Random 3 Random 4 Random 5
Average

TUT Sound Events 2017
0.411 0.358 0.367 0.368 0.364 0.382 0.355 0.380 0.385
0.374Â±0.017

URBAN-SED
0.631 0.624 0.630 0.630 0.627 0.624 0.628 0.629 0.624
0.628Â±0.003

Higher/Lower Freq. order and Random order.
3.3. Evaluation results
Table 2 shows experimental results of comparison among the baseline CRNN with the multiple binary classiï¬ers (baseline), baseline with a GRU classiï¬er (+GRU), and the proposed method using the classiï¬er chains (+Chain). The chain order in these results is the Higher F1 order. In both datasets, the proposed method showed the highest performance. Comparing the GRU with the baseline, we can see the improvement due to the classiï¬er architecture, and the proposed method further improves the performance. On TUT Sound Events 2017, the proposed method showed an improvement of 3.1% on URBANSED and 14.8% compared to the baseline. The effect of the proposed method on URBAN-SED was limited because URBANSED is an artiï¬cially generated dataset with almost uniform dependency among events. On the other hand, the impact of the proposed method on TUT Sound Events, which is realrecording data, was remarkable. This indicates that the proposed method can adequately capture the real-world dependencies among events and contributes to the classiï¬cation.
3.4. Impact of the chain order
Table 3 shows f1-scores of each class-order for both dataset. Among all class order settings, the Higher F1 order showed the best performance. This is a convincing result, as the easiest class is classiï¬ed ï¬rst and then used to condition the subsequent classes. However, the performance was slightly worse than the baseline in some orders, such as Random 3 of TUT. Moreover, the average performance of all ordersâ€™ results was almost equal to that of the GRU classiï¬er. Therefore, the (inappropriate)

Table 4: Frame-based F1-scores for each event on the TUT Sound Events 2017 dataset. The event classes are listed in the Higher F1 order and the arrows indicate the order of the chain.

brakes squeaking car people walking large vehicle people speaking children
average

Baseline
0.611 0.538 0.487 0.336 0.158 0.017
0.358

Chain Higher F1 0.573 0.583 0.531 0.324 0.380 0.073
0.411

Chain Lower F1 0.532 0.568 0.527 0.319 0.203 0.000
0.358

choice of the order may harm the performance. Particularly, for the real-recording dataset TUT Sound Events 2017, the effect of the order on performance was signiï¬cant. The performance with Random order 3 is 13.6% lower than the best performance with the Higher F1 order. For the synthetic dataset, URBANSED, there was little difference in performance by order, and the standard deviation of performance for each order was minimal compared to TUT Sound Events 2017. This result also indicates that the effect of the proposed method is not signiï¬cant for synthetic data sets where the dependency among classes is little or no class dependency.
3.5. Event-wise AED performance
Table 4 shows event-wise AED performance. While some classes show degradation, the latter classes showed signiï¬cant improvement. For example, in â€˜brakes squeaking,â€™ the degradation was 6.3%, and in â€˜large vehicle,â€™ the degradation was 3.5%. The class â€˜brakes squeakingâ€™ was the ï¬rst class of the chain and classiï¬ed without any conditioning, so its performance is considered to have worsened. On the other hand, there were remarkable improvements in the class â€˜people speakingâ€™ and class â€˜children.â€™ This is due to the impact of conditioning became apparent as the chain progressed, leading to the remarkable improvement. The Lower F1 order did not classify the â€˜childrenâ€™ that were difï¬cult to classify by the baseline. Furthermore, these results also show that when a chain is started with a class that is difï¬cult to classify, the overall performance shows degradation. Since there are large errors in the estimated activity of the challenging class, the errors accumulate with each iteration leading to degradation of the performance in subsequent iterations .
These results show that the proposed method is very powerful, but the performance is highly dependent on the choice of class order, which is consistent with the ï¬ndings of speaker diarization [24, 25].
4. Conclusion
This paper proposed acoustic event detection with the classiï¬er chain. The proposed classiï¬er chains consist of GRU and iteratively detect multiple events. In the experiments, the proposed method demonstrated its powerful performance comparing to the multiple binary classiï¬ers. The experimental results also showed the importance of the class order of the chain, which has a signiï¬cant impact on the performance of the proposed method. Future directions include an extension of the proposed method to weakly supervised training and investigating the effect of an order for datasets with a large number of classes, such as AudioSet [31].

5. References
[1] Johannes A Stork, Luciano Spinello, Jens Silva, and Kai O Arras, â€œAudio-based human activity recognition using non-markovian ensemble voting,â€ in IEEE RO-MAN, 2012, pp. 509â€“514.
[2] Keisuke Imoto, Suehiro Shimauchi, Hisashi Uematsu, and Hitoshi Ohmuro, â€œUser activity estimation method based on probabilistic generative model of acoustic event sequence with user activity and its subordinate categories.,â€ in Proc. INTERSPEECH, 2013, pp. 2609â€“2613.
[3] Regunathan Radhakrishnan, Ajay Divakaran, and A Smaragdis, â€œAudio analysis for surveillance applications,â€ in Proc. WASPAA, 2005, pp. 158â€“161.
[4] ChloeÂ´ Clavel, Thibaut Ehrette, and GaeÂ¨l Richard, â€œEvents detection for an audio-based surveillance system,â€ in Proc. ICME, 2005, pp. 1306â€“1309.
[5] Onur Dikmen and Annamaria Mesaros, â€œSound event detection using non-negative dictionaries learned from annotated overlapping events,â€ in Proc. WASPAA. IEEE, 2013, pp. 1â€“4.
[6] Tatsuya Komatsu, Takahiro Toizumi, Reishi Kondo, and Yuzo Senda, â€œAcoustic event detection method using semi-supervised non-negative matrix factorization with a mixture of local dictionaries,â€ in IEEE AASP Challenge: DCASE2016, 2016, pp. 45â€“49.
[7] Arseniy Gorin, Nurtas Makhazhanov, and Nickolay Shmyrev, â€œDCASE 2016 sound event detection system based on convolutional neural network,â€ in IEEE AASP Challenge: DCASE2016, 2016.
[8] Giambattista Parascandolo, Heikki Huttunen, and Tuomas Virtanen, â€œRecurrent neural networks for polyphonic sound event detection in real life recordings,â€ in Proc. ICASSP. IEEE, 2016, pp. 6440â€“6444.
[9] Tomoki Hayashi, Shinji Watanabe, Tomoki Toda, Takaaki Hori, Jonathan Le Roux, and Kazuya Takeda, â€œDuration-controlled LSTM for polyphonic sound event detection,â€ IEEE TASLP, vol. 25, no. 11, pp. 2059â€“2070, 2017.
[10] Emre CakÄ±r, Giambattista Parascandolo, Toni Heittola, Heikki Huttunen, and Tuomas Virtanen, â€œConvolutional recurrent neural networks for polyphonic sound event detection,â€ IEEE TASLP, vol. 25, no. 6, pp. 1291â€“1303, 2017.
[11] Sharath Adavanne, Pasi PertilaÂ¨, and Tuomas Virtanen, â€œSound event detection using spatial features and convolutional recurrent neural network,â€ in Proc. ICASSP, 2017, pp. 771â€“775.
[12] Koichi Miyazaki, Tatsuya Komatsu, Tomoki Hayashi, Shinji Watanabe, Tomoki Toda, and Kazuya Takeda, â€œWeaklysupervised sound event detection with self-attention,â€ in Proc. ICASSP. IEEE, 2020, pp. 66â€“70.
[13] Qiuqiang Kong, Yong Xu, Wenwu Wang, and Mark D Plumbley, â€œSound event detection of weakly labelled data with cnntransformer and automatic threshold optimization,â€ IEEE TASLP, vol. 28, pp. 2450â€“2460, 2020.
[14] Niko Moritz, Gordon Wichern, Takaaki Hori, and Jonathan Le Roux, â€œAll-in-one transformer: Unifying speech recognition, audio tagging, and event detection,â€ Proc. Interspeech 2020, pp. 3112â€“3116, 2020.
[15] Koichi Miyazaki, Tatsuya Komatsu, Tomoki Hayashi, Shinji Watanabe, Tomoki Toda, and Kazuya Takeda, â€œConformer-based sound event detection with semi-supervised learning and data augmentation,â€ in Proc. DCASE2020 Workshop. DCASE, 2020.
[16] K. Imoto and S. Kyochi, â€œSound event detection using graph laplacian regularization based on event co-occurrence,â€ in Proc. ICASSP, 2019, pp. 1â€“5.
[17] Konstantinos Drossos, Shayan Gharib, Paul Magron, and Tuomas Virtanen, â€œLanguage modelling for sound event detection with teacher forcing and scheduled sampling,â€ in Proc. DCASE2019 Workshop, 2019, p. 59.
[18] Min-Ling Zhang and Zhi-Hua Zhou, â€œA review on multi-label learning algorithms,â€ IEEE TKDE, vol. 26, no. 8, pp. 1819â€“1837, 2013.

[19] Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank, â€œClassiï¬er chains for multi-label classiï¬cation,â€ Machine learning, vol. 85, no. 3, pp. 333, 2011.
[20] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and Liang Lin, â€œMulti-label image recognition by recurrently discovering attentional regions,â€ in Proc. ICCV. 2017, pp. 464â€“472, IEEE.
[21] Wei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming Yang, and Inderjit S Dhillon, â€œTaming pretrained transformers for extreme multi-label text classiï¬cation,â€ in Proc. SIGKDD, 2020, pp. 3163â€“ 3171.
[22] Jibing Gong, Zhiyong Teng, Qi Teng, Hekai Zhang, Linfeng Du, Shuai Chen, Md Zakirul Alam Bhuiyan, Jianhua Li, Mingsheng Liu, and Hongyuan Ma, â€œHierarchical graph transformer-based deep learning model for large-scale multi-label text classiï¬cation,â€ IEEE Access, vol. 8, pp. 30885â€“30896, 2020.
[23] Weiwei Cheng, Eyke HuÂ¨llermeier, and Krzysztof J Dembczynski, â€œBayes optimal multilabel classiï¬cation via probabilistic classiï¬er chains,â€ in Proc. ICML, 2010, pp. 279â€“286.
[24] Yusuke Fujita, Shinji Watanabe, Shota Horiguchi, Yawen Xue, Jing Shi, and Kenji Nagamatsu, â€œNeural speaker diarization with speaker-wise chain rule,â€ arXiv preprint arXiv:2006.01796, 2020.
[25] Jing Shi, Xuankai Chang, Pengcheng Guo, Shinji Watanabe, Yusuke Fujita, Jiaming Xu, Bo Xu, and Lei Xie, â€œSequence to multi-sequence learning via conditional chain mapping for mixture signals,â€ in Proc. NeurIPS, 06 2020.
[26] Nicolas Turpault and Romain Serizel, â€œTraining sound event detection on a heterogeneous dataset,â€ in Proc DCASE2020 Workshop, 2020.
[27] Justin Salamon, Duncan MacConnell, Mark Cartwright, Peter Li, and Juan Pablo Bello, â€œScaper: A library for soundscape synthesis and augmentation,â€ in Proc WASPAA. IEEE, 2017, pp. 344â€“348.
[28] Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen, â€œTUT database for acoustic scene classiï¬cation and sound event detection,â€ in Proc. EUSIPCO, Budapest, Hungary, 2016.
[29] Diederik P Kingma and Jimmy Ba, â€œAdam: A method for stochastic optimization,â€ in Proc. ICLR, 2015.
[30] Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen, â€œMetrics for polyphonic sound event detection,â€ Applied Sciences, vol. 6, no. 6, pp. 162, 2016.
[31] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter, â€œAudio set: An ontology and human-labeled dataset for audio events,â€ in Proc. ICASSP, New Orleans, LA, 2017.

