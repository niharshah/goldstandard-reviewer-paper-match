Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection

Nguyen Vo Worcester Polytechnic Institute Computer Science Department Worcester, MA, USA, 01609
nkvo@wpi.edu

Kyumin Lee Worcester Polytechnic Institute Computer Science Department Worcester, MA, USA, 01609
kmlee@wpi.edu

arXiv:2102.02680v1 [cs.AI] 4 Feb 2021

Abstract
The widespread of fake news and misinformation in various domains ranging from politics, economics to public health has posed an urgent need to automatically fact-check information. A recent trend in fake news detection is to utilize evidence from external sources. However, existing evidence-aware fake news detection methods focused on either only word-level attention or evidence-level attention, which may result in suboptimal performance. In this paper, we propose a Hierarchical Multihead Attentive Network to fact-check textual claims. Our model jointly combines multi-head word-level attention and multihead document-level attention, which aid explanation in both word-level and evidencelevel. Experiments on two real-word datasets show that our model outperforms seven stateof-the-art baselines. Improvements over baselines are from 6% to 18%. Our source code and datasets are released at https:// github.com/nguyenvo09/EACL2021.
1 Introduction
The proliferation of biased news, misleading claims, disinformation and fake news has caused heightened negative effects on modern society in various domains ranging from politics, economics to public health. A recent study showed that maliciously fabricated and partisan stories possibly caused citizensâ€™ misperception about political candidates (Allcott and Gentzkow, 2017) during the 2016 U.S. presidential elections. In economics, the spread of fake news has manipulated stock price (Kogan et al., 2019). For example, $139 billion was wiped out when the Associated Press (AP)â€™s hacked Twitter account posted rumor about White House explosion with Barack Obamaâ€™s injury. Recently, misinformation has caused infodemics in public health (Ashoka, 2020) and even led to peopleâ€™s fatalities in the physical world (Alluri, 2019).

To reduce the spread of misinformation and its detrimental inï¬‚uences, many fact-checking systems have been developed to fact-check textual claims. It is estimated that the number of factchecking outlets has increased 400% in 60 countries since 2014 (Stencel, 2019). Several factchecking systems such as snopes.com and politifact.com are widely used by both online users and major corporations. Facebook (CNN, 2020) recently incorporated third-party fact-checking sites to social media posts and Google integrated factchecking articles to their search engine (Wang et al., 2018). These fact-checking systems debunk claims by manually assess their credibility based on collected webpages used as evidence. However, this manual process is laborious and unscalable to handle the large volume of produced false claims on communication platforms. Therefore, in this paper, our goal is to build an automatic fake news detection system to fact-check textual claims based on collected evidence to speed up fact-checking process of the above fact-checking sites.
To detect fake news, researchers proposed to use linguistics and textual content (Castillo et al., 2011; Zhao et al., 2015; Liu et al., 2015). Since textual claims are usually deliberately written to deceive readers, it is hard to detect fake news by solely relying on the content claims. Therefore, multiple works utilized other signals such as temporal spreading patterns (Liu and Wu, 2018), network structures (Wu and Liu, 2018; Vo and Lee, 2018; Shu et al., 2020) and usersâ€™ feedbacks (Vo and Lee, 2019; Shu et al., 2019; Vo and Lee, 2020a). However, limited work used external webpages as documents which could provide interpretive explanation to users. Several recent work (Popat et al., 2018; Ma et al., 2019; Vo and Lee, 2020b) started to utilize documents to fact-check textual claims. Popat et al. (2018) used word-level attention in documents but treated all documents with equal im-

portance whereas Ma et al. (2019) only focused on which documents are more crucial without considering what words help explain credibility of textual claims.
Observing drawbacks of the existing work, we propose Hierarchical Multi-head Attentive Network which jointly utilizes word attention and evidence attention. Overall semantics of a document may be generated by multiple parts of the document. Therefore, we propose a multi-head word attention mechanism to capture different semantic contributions of words to the meaning of the documents. Since a document may have different semantic aspects corresponding to various information related to credibility of a claim, we propose a multi-head document-level attention mechanism to capture contributions of the different semantic aspects of the documents. In our attention mechanism, we also use speakers and publishers information to further improve effectiveness of our model. To our knowledge, our work is the ï¬rst applying multi-head attention mechanism for both words and documents in evidence-aware fake news detection. Our work makes the following contributions:
â€¢ We propose a novel hierarchical multi-head attention network which jointly combines word attention and evidence attention for evidenceaware fake news detection.
â€¢ We propose a novel multi-head attention mechanism to capture important words and evidence.
â€¢ Experiments on two public datasets demonstrate the effectiveness and generality of our model over state-of-the-art fake news detection techniques.
2 Related Work
Many methods have been proposed to detect fake news in recent years. These methods can be placed into three groups: (1) human-based fact-checking sites (e.g. Snopes.com, Politifact.com), (2) machine learning based methods and (3) hybrid systems (e.g. content moderation on social media sites). In machine-learning-based methods, researchers mainly used linguistics and textual content (Zellers et al., 2019; Zhao et al., 2015; Wang, 2017; Shu et al., 2019), temporal spreading patterns (Liu and Wu, 2018), network structures (Wu and Liu, 2018; Vo and Lee, 2018; You et al., 2019), usersâ€™ feedbacks (Vo and Lee, 2019; Shu et al., 2019) and multimodal signals (Gupta et al., 2013; Vo and Lee, 2020b). Recently, researchers focus

on fact-checking claims based on evidence from different sources. Thorne and Vlachos (2017) and Vlachos and Riedel (2015) fact-check claims using subject-predicate-object triplets extracted from knowledge graph as evidence. Chen et al. (2020) assess claimsâ€™ credibility using tabular data. Our work is closely related to fact veriï¬cation task (Thorne et al., 2018; Nie et al., 2019; Soleimani et al., 2020) which aims to classify a pair of a claim and an evidence extracted from Wikipedia into three classes: supported, refuted, or not enough info. For fact veriï¬cation task, Nie et al. (2019) used ELMo (Peters et al., 2018) to extract contextual embeddings of words and used a modiï¬ed ESIM model (Chen et al., 2017). Soleimani et al. (2020) used BERT model (Devlin et al., 2018) to retrieve and verify claims. Zhou et al. (2019) used graph based models for semantic reasoning. Our work is different from these work since our goal is to classify a pair of a claim and a list of relevant evidence into true or false.
Our work is close to existing work about evidence-aware fake news detection (Popat et al., 2018; Ma et al., 2019; Wu et al., 2020; Mishra and Setty, 2019). Popat et al. (2018) used an average pooling layer to derive claimsâ€™ representation to attend to words in evidence, Mishra and Setty (2019) focused on words and sentences in each evidence, and Ma et al. (2019) proposed a semantic entailment model to attend to important evidence. However, to the best of our knowledge, our work is the ï¬rst jointly using multi-head attention mechanisms to focus on important words in each evidence and important evidence from a set of relevant articles. Our attention mechanism is different from these work since we use multiple attention heads to capture different semantic contributions of words and evidence.
3 Problem Statement
We denote an evidence-based fact-checking dataset C as a collection of tuples (c, s, D, P) where c is a textual claim originated from a speaker s, D = {di}ki=1 is a collection of k documents1 relevant to the claim c and P = {pi}ki=1 is the corresponding publishers of documents in D. Note, |D| = |P|. Our goal is to classify each tuple (c, s, D, P) into a pre-deï¬ned class (i.e. true news/fake news).
1We use the term â€œdocumentsâ€, â€œarticlesâ€, and â€œevidenceâ€ interchangeably.

Cross Entropy Loss â„’)(ğ‘¦, ğ‘¦*)

MLP ğ’š$

Output Layer

ğ’„ğ’†ğ’™ğ’•; ğ’…ğ’“ğ’Šğ’„ğ’‰

Tuple Representation

Multi-head Document Attention Layer

ğ’…ğŸğ’†ğ’™ğ’• âˆˆ â„#4!123"

Concat

â„# heads

ğ’…ğŸğ’†ğ’™ğ’• âˆˆ â„#4!123"

Article Representation

ğ’‘ğŸ âˆˆ â„3"

Concat â„!heads

Concat Attention

Article Representation

ğ’„ğ’†ğ’™ğ’• = [ğ’„; ğ’”] âˆˆ â„#123! Extended Claimâ€™s Representation

Concat

ğ’‘ğŸ âˆˆ â„3"

Concat Attention

LSTM

LSTM

LSTM

Publisher Emb 0 1 0 0 â€¦â€¦

ğ‘¤!%!

ğ‘¤ %!
#

â€¦ ğ‘¤&%!

ğ’…ğŸâ€™s publisher ğ’…ğŸâ€™s text

Claimâ€™s Representation

Avg. Pooling

ğ’„ âˆˆ â„#1

Concat Attention

LSTM

LSTM

LSTM

ğ’” âˆˆ â„3! LSTM

LSTM

LSTM

ğ‘¤!"

ğ‘¤#" â€¦

Claimâ€™s text

Speaker Emb

ğ‘¤$" 0 0 1 0 â€¦â€¦

ğ‘¤ %"
!

Claimâ€™s Speaker

ğ‘¤%" â€¦
#
ğ’…ğŸâ€™s text

ğ‘¤&%"

â„! heads

Multi-head Word
Attention Layer

Publisher Emb 0 0 0 1 â€¦â€¦
ğ’…ğŸâ€™s publisher

Embedding Layer

Figure 1: The architecture of our proposed model MAC in which we show a claim c, two associated relevant articles d1 and d2 and sources of the claim and the two documents. h1 and h2 are the number of heads of wordlevel attention and document-level attention respectively.

4 Framework

In this section, we describe our Hierarchical Multihead Attentive Network for Fact-Checking (MAC) which jointly considers word-level attention and document-level attention. Our framework consists of four main components: (1) embedding layer, (2) multi-head word attention layer, (3) multi-head document attention layer and (4) output layer. These components are illustrated in Fig. 1 where we show a claim and two documents as an example.

4.1 Embedding Layer

Each claim c is modeled as a sequence of n words

[w

c 1

,

w2c

,

...,

w

c n

]

and

di

is

viewed

as

another

se-

quence

of

m

words

[

w

d 1

,

w2d

,

...,

w

d m

].

Each

word

wic and wjd will be projected into D-dimensional

vectors eci and edj respectively by an embedding

matrix We âˆˆ RV Ã—D where V is the vocabulary

size. Each speaker s and publisher pi modeled as

one-hot vectors are transformed into dense vectors

s âˆˆ RD1 and pi âˆˆ RD2 respectively by using two matrices Ws âˆˆ RSÃ—D1 and Wp âˆˆ RP Ã—D2 , where

S and P are the number of speakers and publishers

in a training set respectively. Both Ws and Wp are

uniformly initialized in [âˆ’0.2, 0.2]. Note that, both

matrices Ws and Wp are jointly learned with other

parameters of our MAC.

4.2 Multi-head Word Attention Layer
We input word embeddings eci of the claim c into a bidirectional LSTM (Graves et al., 2005) which

helps generate contextual representation hi of each

token

as

follows:

hc

=

â†âˆ’ [hi ;

â†’âˆ’ hi ]

âˆˆ

R2H ,

where

â†âˆ’ hi

â†’âˆ’

i

and h i are hidden states in forward and backward

pass of the BiLSTM, symbol ; means concatenation

and H is hidden size. We derive claimâ€™s representa-

tion in R2H by an average pooling layer as follows:

1n c

c= n

hi

(1)

i=1

Applying a similar process on the top of each
document di with a different BiLSTM, we have contextual representation hdj âˆˆ R2H for each word in di. After going through BiLSTM, di is modeled as matrix H = [hd1 âŠ• hd2 âŠ• ... âŠ• hdm] âˆˆ RmÃ—2H where âŠ• denotes stacking.
To understand what information in a document
helps us fact-check a claim, we need to guide our
model to focus on crucial keywords or phrases of
the document. Drawing inspiration from (Luong et al., 2015), we ï¬rstly replicate vector c (Eq.1) m times to create matrix C1 âˆˆ RmÃ—2H and propose an attention mechanism to attend to important
words in the document di as follows:

a1 = sof tmax tanh [H; C1] Â· W1 Â· w2 (2)
where w2 âˆˆ Ra1 , W1 âˆˆ R4HÃ—a1 , [H; C1] is concatenation of two matrices on the last dimension and a1 âˆˆ Rm is attention distribution on m words. However, the overall semantics of the document might be generated by multiple parts of the document (Lin et al., 2017). Therefore, we propose a

multi-head word attention mechanism to capture
different semantic contributions of words by extending vector w2 into a matrix W2 âˆˆ Ra1Ã—h1 where h1 is the number of attention heads shown in Fig. 1. We modify Eq. 2 as follows:

A1 = sof tmaxcol tanh([H; C1]Â·W1)Â·W2 (3)

where A1 âˆˆ RmÃ—h1 and each column of A1 has been normalized by the softmax operation. Intuitively, A1 stands for h1 different attention distributions on top of m words of the document di, helping us capture different aspects of the document. After computing A1, we derive representation of document di as follows:

di = f latten(AT1 Â· H)

(4)

where di âˆˆ Rh12H and function ï¬‚atten(.) ï¬‚attens AT1 Â· H into a vector. We also implemented a more sophisticated multi-head attention in (Vaswani
et al., 2017) but did not achieve good results.

4.3 Multi-head Document Attention Layer

This layer consists of three components as follows: (1) extending representations of claims, (2) extending representations of evidence and (3) multi-head document attention mechanism.

Extending representations of claims. So far the representation of the claim c (Eq. 1) is only from textual content. In reality, a speaker who made a claim may impact credibility of the claim. For example, claims from some politicians are controversial and inaccurate (Allcott and Gentzkow, 2017). Therefore, we enrich vector c by concatenating it with speakerâ€™s embedding s to generate cext âˆˆ Rx, where x = 2H + D1 as shown in Eq. 5.

cext = [c; s] âˆˆ Rx

(5)

Extending representations of evidence. Intu-

itively, an article published by nytimes.com might

be more reliable than a piece of news published by

breitbart.com which is known to be a less credi-

ble site. Therefore, to capture more information,

we further enrich representations of evidence with

publishersâ€™ information by concatenating di (Eq. 4) with its publisherâ€™s embedding pi as follows:

dei xt = [di; pi] âˆˆ Ry

(6)

where y = 2h1H + D2. From Eq. 6, we can generate representations of k relevant articles and stack them as shown in Eq. 7.

D = [de1xt âŠ• ... âŠ• dekxt] âˆˆ RkÃ—y

(7)

Multi-head Document Attention Mechanism. In real life, a journalist from snopes.com and politifact.com may use all k articles relevant to the claim c to fact-check it but she may focus on some key articles to determine the verdict of the claim c while other articles may have negligible information. To capture such intuition, we need to downgrade uninformative documents and concentrate on more meaningful articles. Similar to Section 4.2, we use multi-head attention mechanism which produces different attention distributions representing diverse contributions of articles toward determining veracity of the claim c.
We ï¬rstly create matrix C2 âˆˆ RkÃ—x by replicating vector cext (Eq. 5) k times. Secondly, the matrix C2 is concatenated with matrix D (Eq. 7) on the last dimension of the two matrices denoted as [D; C2] âˆˆ RkÃ—(x+y).
Our proposed multi-head document-level attention mechanism applies h2 different attention heads as shown in Eq. 8.

A2 = sof tmaxcol(tanh([D; C2] Â· W3) Â· W4) (8)

where W3 âˆˆ R(x+y)Ã—a2 , W4 âˆˆ Ra2Ã—h2 . The matrix A2 âˆˆ RkÃ—h2, where each of its column is normalized by the softmax operator, is a collection
of h2 different attention distributions on k documents. Using attention weights, we can generate
attended representation of k evidence denoted as drich âˆˆ Rh2y as shown in Eq. 9.

drich = f latten(AT2 Â· D)

(9)

where ï¬‚atten(.) function ï¬‚attens AT2 Â· D into a vector. We ï¬nally generate representation of a tuple (c, s, D, P) by concatenating vector cext (Eq. 5) and vector drich (Eq. 9), denoted as [cext; drich].
To the best of our knowledge, our work is the ï¬rst work utilizing multi-head attention mechanism integrated with speakers and publishers information to capture various semantic contributions of evidence toward fact-checking process.

4.4 Output Layer
In this layer, we input tuple representation [cext; drich] into a multilayer perceptron (MLP) to compute probability yË† that the claim c is a true news as follows:
yË† = Ïƒ W6 Â· W5 Â· [cext; drich] + b5 + b6 (10)
where W5, W6, b5, b6 are weights and biases of the MLP, and Ïƒ(.) is the sigmoid function. We optimize our model by minimizing the standard

Table 1: Statistics of our experimental datasets

True claims False claims |Speakers| |Documents| |Publishers|

Snopes 1,164 3,177 N/A 29,242 12,236

PolitiFact 1,867 1,701 664 29,556 4,542

cross-entropy as shown on the top of Fig. 1.
LÎ¸(y, yË†) = âˆ’ y log yË†+ (1 âˆ’ y) log(1 âˆ’ yË†) (11)
where y âˆˆ {0, 1} is the ground truth label of a tuple (c, s, D, P). During training, we sample a mini batch of 32 tuples and compute average loss from the tuples.
5 Experiments
5.1 Datasets
We employed two public datasets released by (Popat et al., 2018). Each of these datasets is a collections of tuples (c, s, D, P, y) where each textual claim c and its credible label y are collected from two major fact-checking websites snopes.com and politifact.com. The articles pertinent to the claim c are retrieved by using search engines. Each Snopes claim was labeled as true or false while in Politifact, there were originally six labels: true, mostly true, half true, false, mostly false, pants on ï¬re. Following (Popat et al., 2018), we merge true, mostly true and half true into true claims and the rest are into false claims. Details of our datasets are presented in Table 1. Note that Snopes does not have speakersâ€™ information.
5.2 Baselines
We compare our MAC model with seven state-ofthe-art baselines divided into two groups. The ï¬rst group of the baselines only used textual content of claims, and the second group of the baselines utilized relevant articles to fact-check textual claims. A related method (Mishra and Setty, 2019) used subject information of articles (e.g. politics, entertainment), which was not available in our datasets. We tried to compare with it but achieved poor results perhaps due to missing information. Therefore, we do not report its result in this paper. Details of the baselines are shown as follows: Using only claimsâ€™ text:
â€¢ BERT (Devlin et al., 2018) is a pre-trained language model achieving state-of-the-art re-

sults on many NLP tasks. The representation of [CLS] token is inputted to a trainable linear layer to classify claims.
â€¢ LSTM-Last is a model proposed in (Rashkin et al., 2017). LSTM-Last takes the last hidden state of the LSTM as representations of claims. These representations will be inputted to a linear layer for classiï¬cation.
â€¢ LSTM-Avg is another model proposed in (Rashkin et al., 2017) which used an average pooling layer on top of hidden states to derive representations of claims.
â€¢ CNN (Wang, 2017) is a state-of-the-art model which applied 1D-convolutional neural network on word vectors of claims.
Using both claimsâ€™ text and articlesâ€™ text:
â€¢ DeClare (Popat et al., 2018) computes credibility score of each pair of a claim c and a document di. The overall credible rating is averaged from all k relevant articles.
â€¢ HAN (Ma et al., 2019) is a hierarchical attention network based on representations of relevant documents. It uses attention mechanisms to determine which document is more important without considering which word in a document should be focused on.
â€¢ NSMN (Nie et al., 2019) is a state-of-the-art model designed to determine stance of a document di with respect to claim c. We apply NSMN on our dataset by predicting score of each pair (c, di) and computing average score based on documents in D same as DeClare.
Note that, we also applied BERT, LSTM-Last, LSTM-Avg and CNN by using both claimsâ€™ text and articlesâ€™ text. For each of these baselines, we concatenated a claimâ€™s text and a documentâ€™s text, and input the concatenated content into the baseline to compute likelihood that the claim is fake news. We computed average probability based on all documents of the claim and used it as ï¬nal prediction. However, we did not observe considerable improvements of these baselines. In addition to deeplearning-based baselines, we compared our MAC with other feature-based techniques (e.g. SVM). As expected, these traditional techniques had inferior performance compared with neural models. Therefore, we only report the seven baselinesâ€™ performance.

Table 2: Performance of MAC and baselines on Snopes dataset. MAC outperforms baselines signiï¬cantly with p-value<0.05 by one-sided paired Wilcoxon test.

Method Types

Methods

BERT

Using only LSTM-Avg

claimsâ€™ text LSTM-Last

TextCNN

Using both HAN

claimsâ€™ text & NSMN

articlesâ€™ text DeClare

Ours

MAC

Imprv. over the best baseline

AUC 0.60852 0.69124 0.70142 0.70537 0.70365 0.77270 0.81036 0.88715
9.47%

F1 Macro 0.56096 0.62100 0.63122 0.63081 0.62510 0.68006 0.72445 0.78660
8.58%

F1 Micro 0.69806 0.71877 0.72415 0.72005 0.72800 0.76127 0.78813 0.83316
5.71%

True News as Positive F1 Precision Recall 0.31574 0.40318 0.26050 0.42953 0.48415 0.39692 0.44650 0.48935 0.41412 0.45001 0.48164 0.43035 0.42884 0.49192 0.38161 0.51954 0.57558 0.48182 0.59250 0.61235 0.58096 0.68738 0.69975 0.68601 16.01% 14.27% 18.08%

Fake News as Positive F1 Precision Recall 0.80618 0.76011 0.85839 0.81246 0.79139 0.83671 0.81594 0.79594 0.83776 0.81160 0.79882 0.82622 0.82136 0.79058 0.85490 0.84058 0.82011 0.86364 0.85640 0.85023 0.86399 0.88581 0.88617 0.88706 3.43% 4.23% 2.67%

5.3 Experimental Settings
For each dataset, we randomly select 10% number of claims from each class to form a validation set, which is used for tuning hyper-parameters. We report 5-fold stratiï¬ed cross validation results on the remaining 90% of the data. We train our model and baselines on 4-folds and test them on the remaining fold. We use AUC, macro/micro F1, class-speciï¬c F1, Precision and Recall as evaluation metrics. To mitigate overï¬tting and reduce training time, we early stop training process on the validation set when F1 macro on the validation data continuously decreases in 10 epochs. When we get the same F1 macro between consecutive epochs, we rely on AUC for early stopping.
For fair comparisons, we use Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001 and regularize parameters of all methods with 2 norm and weight decay Î» = 0.001. As the maximum lengths of claims and articles in words are 30 and 100 respectively for both datasets, we set n = 30 and m = 100. For HAN and our model, we set k = 30 since the number of articles for each claim is at most 30 in both datasets. Batch size is set to 32 and we trained all models until convergence. We tune all models including ours with hidden size H chosen from {64, 128, 300}, pretrained word-embeddings are from Glove (Pennington et al., 2014) with D = 300. Both D1 and D2 are tuned from {128, 256}. The number of attention heads h1 and h2 is chosen from {1, 2, 3, 4, 5}, a1 and a2 are equal to 2 Ã— H. In addition to Glove, we also utilized contextual embeddings from pretrained language models such as ELMo and BERT but achieved comparable performances. We implemented all methods in PyTorch 0.4.1 and run experiments on an NVIDIA GTX 1080.

5.4 Performance of MAC and baselines
We show experimental results of our model and baselines in Tables 2 and 3. In Table 2, MAC outperforms all baselines with signiï¬cance level p < 0.05 by using one-sided paired Wilcoxon test on Snopes dataset. MAC achieves the best result when h1 = 5, h2 = 2, H = 300 and D1 = D2 = 128. In Table 3, MAC also significantly outperforms all baselines with p < 0.05 according to one-sided paired Wilcoxon test on PolitiFact dataset. The hyperparameters we selected for MAC are h1 = 3, h2 = 1, H = 300 and D1 = D2 = 128.
For baselines, BERT is used as a static encoder. We tried to ï¬ne tune it but even achieve worse results. This might be because we do not have sufï¬cient data to tune it. For both HAN and DeClare, since both papers do not release their source code, we tried our best to reproduce results from these two models. HAN model derived representation of each document by using the last hidden state of a GRU (Chung et al., 2014) without any attention mechanism on words to downgrade unimportant words (e.g. stop words), leading to poor representations of documents. Therefore, document-level attention mechanism in HAN model did not perform well. Similar patterns can be observed in two baselines LSTM-Avg and LSTM-Last. DeClare performed best among baselines, indicating the importance of applying word-level attention on words to reduce impact of less informative words.
We can see that our MAC outperforms all baselines in all metrics. When viewing true news as positive class, our MAC has an average increase of 16.0% and 7.1% over the best baselines on Snopes and PolitiFact respectively. We also have an increase of 4.7% improvements over baselines with a maximum improvements of 10.1% in PolitiFact

Table 3: Performance of MAC and baselines on PolitiFact dataset. MAC outperforms baselines with statistical signiï¬cance level p-value<0.05 by one-sided paired Wilcoxon test.

Method Types

Methods

BERT

Using only LSTM-Avg

claimsâ€™ text LSTM-Last

TextCNN

Using both HAN

claimsâ€™ text & NSMN

articlesâ€™ text DeClare

Ours

MAC

Imprv. over the best baseline

AUC 0.58822 0.65465 0.64289 0.65152 0.63201 0.64237 0.70642 0.75756
7.24%

F1 Macro 0.56021 0.60564 0.60196 0.60380 0.58655 0.60211 0.65213 0.68642
5.26%

F1 Micro 0.56446 0.60866 0.60493 0.60740 0.59121 0.60431 0.65350 0.69116
5.76%

True News as Positive F1 Precision Recall 0.56364 0.59206 0.54968 0.61821 0.63192 0.61267 0.61703 0.62634 0.61456 0.61521 0.63010 0.61030 0.59193 0.61502 0.58290 0.61123 0.63051 0.59912 0.67230 0.66548 0.67997 0.71786 0.68856 0.75493 6.78% 3.47% 11.02%

Fake News as Positive F1 Precision Recall 0.55678 0.54354 0.58069 0.59307 0.59046 0.60425 0.58690 0.58763 0.59434 0.59238 0.59049 0.60421 0.58117 0.57573 0.60034 0.59299 0.58213 0.60999 0.63195 0.64053 0.62444 0.65498 0.70546 0.62576 3.64% 10.14% 0.21%

Table 4: Impact of word attention and evidence attention on our MAC in two datasets

Methods
Only Word Att Only Evidence Att Word & Doc Att

Snopes AUC F1 Macro 0.87278 0.77831 0.82531 0.72885 0.88715 0.78660

PolitiFact AUC F1 Macro 0.74483 0.67818 0.71790 0.65187 0.75756 0.68642

Table 5: Impact of speakers and publishers on performance of MAC in two datasets

AUC AUC

0.89 0.885 0.88 0.875 0.87 0.865

1

2

5

3

4

3

h h 2

4

2

51

1

(a) Snopes

0.75

0.74

0.73

15

2

4

3

3

h h 2

4 51

2

1

(b) PolitiFact

Methods
Text Only Text + Publishers Text + Speakers Text + Pubs + Spkrs

Snopes AUC F1 Macro 0.88186 0.77146 0.88715 0.78660

PolitiFact AUC F1 Macro 0.72401 0.66844 0.72645 0.66984 0.75202 0.68483 0.75756 0.68642

when considering fake news as negative class. In terms of AUC, average improvements of MAC over the baselines are 7.9% and 6.1% on Snopes and PolitiFact respectively. Improvements of MAC over baselines can be explained by our multi-head attention mechanism shown in Eq. 3 and Eq. 8. After attending to words in documents, we can generate better representations of documents/evidence, leading to more effective document-level attention compared with HAN model.
5.5 Ablation Studies
Impact of Word Attention and Evidence Attention. We study the impact of attention layers on performance of MAC by (1) using only word attention and replacing evidence attention with an average pooling layer on top of documentsâ€™ representations and (2) using only evidence attention and replacing word attention with an average pooling layer on top of wordsâ€™ representations. As we can see in Table 4, using only word attention performs much better than using only evidence attention. This is because without downgrading less infor-

Figure 2: Sensitivity of MAC with respect to number of heads in word-level attention h1 and the number of heads in document-level attention h2
mative words in evidence, irrelevant information can be captured, leading to low quality representations of evidence. This experiment aligns with our observation that HAN model, which used only evidence attention, did not perform well. When combining both attention mechanisms hierarchically, we consistently achieve best results on two datasets in Table 4. In particular, the model Word & Doc Att outperformed both Only Evidence Att and Only Evidence Att signiï¬cantly with p-value < 0.05. This result indicates that it is crucial to combine word-level attention and document-level attention to improve the performance of evidenceaware fake news detection task.
Impact of Speakers and Publishers on MAC. To study how speakers and publishers impact performance of MAC, we experiment four models: (1) using text only (Text Only), (2) using text and publishers (Text + Publishers), (3) using text and speakers (Text + Speakers) and (4) using text, publishers and speakers (Text + Pubs + Spkrs). In Table 5, Text + Publishers has better performance then using only text in both datasets. In PolitiFact, Text + Speakers achieves 2âˆ¼3% improvements over Text + Publishers, indicating that speakers who made claims are

False Claim: Actor Christopher Walken planning making bid US presidency 2008

Doc 1

Doc 2

Doc 3

Figure 3: Visualization of attention weights of the ï¬rst attention head on three documents relevant to a false claim in word-level attention layer
False Claim: Actor Christopher Walken planning making bid US presidency 2008

Doc 1

Doc 3 Doc 2

Figure 4: Visualization of attention weights of the second attention head on three documents relevant to a false claim in word-level attention layer

crucial to determine verdict of the claims. Finally, using all information (Text + Pubs + Spkrs) helps us achieve the best result in PolitiFact. In Snopes, we omit results of Text + Speakers and Text + Pubs + Spkrs because the dataset does not contain speakersâ€™ information. In particular, model Text + Pubs + Spkrs outperformed methods Text Only and Text + Publishers signiï¬cantly (p-value< 0.05). Based on these results, we conclude that integrating information of speakers and publishers is useful for detecting misinformation.
5.6 Impact of the Number of Attention Heads
In this section, we examine sensitivity of MAC with respect to the number of heads h1 in word attention layer and the number of heads h2 in document attention layer. We vary h1 and h2 in {1, 2, 3, 4, 5}. Since AUC is less sensitive to any threshold, we report AUC of MAC on two datasets in Fig. 2(a) and

2(b). A common pattern we can observe in the two ï¬gures is that performance of MAC tends to be better when we increase the number heads h1 in word attention layer while performance of MAC tends to decrease when increasing h2. This phenomenon indicates that word attention is more important than evidence attention. In Snopes, MAC has the best AUC when h1 = 5, h2 = 2. In PolitiFact, MAC reaches the peak when h1 = 3, h2 = 1.
5.7 Case Study
To understand how multi-head attention mechanism works, from the testing set, we visualize attention weights on three documents of a false claim Actor Christopher Walken planning making bid US presidency 2008. Note, our MAC correctly classiï¬es the claim as fake news. In Fig. 3 and Fig. 4, we show the claim and visualization of two different heads in word attention layer. Note that Popat et al.

Documents

Doc 1 Doc 2

0.6 0.5 0.4

Doc 3 Head 1 Head 2 Head 3 Head 4 Head 5
Document-Level Attention Heads

0.3 0.2

Figure 5: Visualization of ï¬ve attention heads in document-level attention layer for three documents

(2018), who released the datasets, already lowercased and removed punctuations. To conduct fair comparison, we directly used the datasets without any additional preprocessing. In Fig. 3, attention weights are sparse, indicating that the ï¬rst attention head focuses on the most important words which determine credibility of the claim (e.g. hoax, false). Differently, in Fig. 4, the second attention head has more diffused attention weights to capture more useful phrases from documents (e.g. walken not running, its obviously not). Moving on to attention heads in evidence attention layer in Fig. 5, we show a heat map where the x-axis is the ï¬ve heads extracted from evidence attention layer and the y-axis is three documents relevant to the same claim in Fig. 3 and 4. As we can see in Fig. 5, Head 1, Head 3 and Head 5 emphasize on Doc 3 which contains refuting phrases (e.g. its obviously not), while Head 4 focuses on Doc 1 which has negating information such as walken not running. Both Doc 1 and Doc 3 have crucial signals to fact-check the claim. From these analyses, we conclude that heads in word attention layer capture different semantic contributions of words and different heads in document attention layer captures important documents.
6 Conclusions
In this paper, we propose a novel evidence-aware model to fact-check textual claims. Our MAC is designed by hierarchically stacking two attention layers. The ï¬rst one is a word attention layer and the second one is a document attention layer. In both layers, we propose multi-head attention mechanisms to capture different semantic contributions of words and documents. Our MAC outperforms the baselines signiï¬cantly with an average increase of 6% to 9% over the best results from baselines with a maximum improvements of 18%. We conduct ablation studies to understand the performance

of MAC and provide a case study to show the effectiveness of the attention mechanisms. In future work, we will further examine other data types such as images to improve the performance of our model.
Acknowledgment
This work was supported in part by NSF grant CNS-1755536, AWS Cloud Credits for Research, and Google Cloud. Any opinions, ï¬ndings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reï¬‚ect those of the sponsors.
References
Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the 2016 election. Journal of economic perspectives, 31(2):211â€“36.
Aparna Alluri. 2019. Whatsapp: The â€™black holeâ€™ of fake news in indiaâ€™s election. https://www.bbc. com/news/world-asia-india-47797151.
Ashoka. 2020. Misinformation spreads faster than coronavirus: How a social organization in turkey is ï¬ghting fake news. https://bit.ly/36qqmmH.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on twitter. In Proceedings of the 20th international conference on World wide web, pages 675â€“684. ACM.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1657â€“1668.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020. Tabfact: A large-scale dataset for table-based fact veriï¬cation. In International Conference on Learning Representations.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. In Neural Information Processing Systems.
CNN. 2020. How facebook is combating spread of covid-19 misinformation. https://cnn.it/ 3gjtBkg.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics.

Alex Graves, Santiago FernaÂ´ndez, and JuÂ¨rgen Schmidhuber. 2005. Bidirectional lstm networks for improved phoneme classiï¬cation and recognition. In International Conference on Artiï¬cial Neural Networks, pages 799â€“804. Springer.
Aditi Gupta, Hemank Lamba, Ponnurangam Kumaraguru, and Anupam Joshi. 2013. Faking sandy: characterizing and identifying fake images on twitter during hurricane sandy. In Proceedings of the 22nd international conference on World Wide Web, pages 729â€“736.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In International Conference on Learning Representations.
Shimon Kogan, Tobias J Moskowitz, and Marina Niessner. 2019. Fake news: Evidence from ï¬nancial markets. Available at SSRN 3237763.
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In The 5th International Conference on Learning Representations.
Xiaomo Liu, Armineh Nourbakhsh, Quanzhi Li, Rui Fang, and Sameena Shah. 2015. Real-time rumor debunking on twitter. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1867â€“1870. ACM.
Yang Liu and Yi-Fang Brook Wu. 2018. Early detection of fake news on social media through propagation path classiï¬cation with recurrent and convolutional networks. In Thirty-Second AAAI Conference on Artiï¬cial Intelligence.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attentionbased neural machine translation. In Empirical Methods in Natural Language Processing.
Jing Ma, Wei Gao, Shaï¬q Joty, and Kam-Fai Wong. 2019. Sentence-level evidence embedding for claim veriï¬cation with hierarchical attention networks. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2561â€“ 2571.
Rahul Mishra and Vinay Setty. 2019. Sadhan: Hierarchical attention networks to learn latent aspect embeddings for fake news detection. In Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval, pages 197â€“204.
Yixin Nie, Haonan Chen, and Mohit Bansal. 2019. Combining fact extraction and veriï¬cation with neural semantic matching networks. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 6859â€“6866.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference

on empirical methods in natural language processing (EMNLP), pages 1532â€“1543.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227â€“2237.
Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum. 2018. Declare: Debunking fake news and false claims using evidence-aware deep learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22â€“32.
Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931â€“2937.
Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. 2019. defend: Explainable fake news detection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 395â€“405.
Kai Shu, Deepak Mahudeswaran, Suhang Wang, and Huan Liu. 2020. Hierarchical propagation networks for fake news detection: Investigation and exploitation. In Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pages 626â€“637.
Amir Soleimani, Christof Monz, and Marcel Worring. 2020. Bert for evidence retrieval and claim veriï¬cation. In European Conference on Information Retrieval, pages 359â€“366. Springer.
Mark Stencel. 2019. Number of fact-checking outlets surges to 188 in more than 60 countries. https: //bit.ly/36y3S3l.
James Thorne and Andreas Vlachos. 2017. An extensible framework for veriï¬cation of numerical claims. In Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 37â€“40.
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and veriï¬cation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809â€“819.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998â€“6008.

Andreas Vlachos and Sebastian Riedel. 2015. Identiï¬cation and veriï¬cation of simple claims about statistical properties. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2596â€“2601. Association for Computational Linguistics.
Nguyen Vo and Kyumin Lee. 2018. The rise of guardians: Fact-checking url recommendation to combat fake news. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 275â€“284.
Nguyen Vo and Kyumin Lee. 2019. Learning from factcheckers: Analysis and generation of fact-checking language. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 335â€“344.
Nguyen Vo and Kyumin Lee. 2020a. Standing on the shoulders of guardians: Novel methodologies to combat fake news. In Disinformation, Misinformation, and Fake News in Social Media, pages 183â€“ 210. Springer.
Nguyen Vo and Kyumin Lee. 2020b. Where are the facts? searching for fact-checked information to alleviate the spread of fake news. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7717â€“7731.
William Yang Wang. 2017. â€œliar, liar pants on ï¬reâ€: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422â€“426.
Xuezhi Wang, Cong Yu, Simon Baumgartner, and Flip Korn. 2018. Relevant document discovery for factchecking articles. In Companion Proceedings of the The Web Conference 2018, pages 525â€“533.
Liang Wu and Huan Liu. 2018. Tracing fake-news footprints: Characterizing social media messages by how they propagate. In Proceedings of the eleventh ACM international conference on Web Search and Data Mining, pages 637â€“645.
Lianwei Wu, Yuan Rao, Xiong Yang, Wanzhen Wang, and Ambreen Nazir. 2020. Evidence-aware hierarchical interactive attention networks for explainable claim veriï¬cation. In International Joint Conferences on Artiï¬cial Intelligence.
Di You, Nguyen Vo, Kyumin Lee, and Qiang Liu. 2019. Attributed multi-relational attention network for fact-checking url recommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 1471â€“1480.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems, pages 9051â€“9062.

Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. Enquiring minds: Early detection of rumors in social media from enquiry posts. In Proceedings of the 24th international conference on world wide web, pages 1395â€“1405.
Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2019. Gear: Graph-based evidence aggregating and reasoning for fact veriï¬cation. page Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.

