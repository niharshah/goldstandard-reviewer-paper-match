3PC: Three Point Compressors for Communication-Efï¬cient Distributed Training and a Better Theory for Lazy Aggregation

Peter RichtaÂ´rik KAUST*

Igor Sokolov KAUST
Zhize Li KAUST

Ilyas Fatkhullin ETH AI Center & ETH Zurich
Eduard Gorbunov MIPTâ€ 

Elnur Gasanov KAUST

arXiv:2202.00998v1 [cs.LG] 2 Feb 2022

Abstract
We propose and study a new class of gradient communication mechanisms for communicationefï¬cient trainingâ€”three point compressors (3PC)â€”as well as efï¬cient distributed nonconvex optimization algorithms that can take advantage of them. Unlike most established approaches, which rely on a static compressor choice (e.g., Top-ğ¾), our class allows the compressors to evolve throughout the training process, with the aim of improving the theoretical communication complexity and practical efï¬ciency of the underlying methods. We show that our general approach can recover the recently proposed state-of-the-art error feedback mechanism EF21 (RichtaÂ´rik et al., 2021) and its theoretical properties as a special case, but also leads to a number of new efï¬cient methods. Notably, our approach allows us to improve upon the state of the art in the algorithmic and theoretical foundations of the lazy aggregation literature (Chen et al., 2018). As a by-product that may be of independent interest, we provide a new and fundamental link between the lazy aggregation and error feedback literature. A special feature of our work is that we do not require the compressors to be unbiased.
1. Introduction
It has become apparent in the last decade that, other things equal, the practical utility of modern machine learning models grows with their size and with the amount of data points used in the training process. This big model and big data approach, however, comes with increased demands on the hardware, algorithms, systems and software involved in the training process.
*King Abdullah University of Science and Technology, Thuwal, Saudi Arabia. â€ Moscow Institute of Physics and Technology, Dolgoprudny, Russia.

1.1. Big data and the need for distributed systems

In order to handle the large volumes of data involved in training SOTA models, it is now absolutely necessary to rely on (often massively) distributed computing systems (Dean et al., 2012; Khirirat et al., 2018; Lin et al., 2018). Indeed, due to storage and compute capacity limitations, large-enough training data sets can no longer be stored on a single machine, and instead need to be distributed across and processed by an often large number of parallel workers.

In particular, in this work we consider distributed supervised learning problems of the form

[ï¸‚

ğ‘›

]ï¸‚

min

ğ‘“ (ğ‘¥)

:=

1 ğ‘›

âˆ‘ï¸€

ğ‘“ğ‘–(ğ‘¥)

,

(1)

ğ‘¥âˆˆRğ‘‘

ğ‘–=1

where ğ‘› is the number of parallel workers/devices/clients, ğ‘¥ is a vector representing the ğ‘‘ parameters of a machine learning model (e.g., the weights in a neural network), and ğ‘“ğ‘–(ğ‘¥) is the loss of model ğ‘¥ on the training data stored on client ğ‘– âˆˆ [ğ‘›] := {1, 2, . . . , ğ‘›}.

In some applications, as in federated learning (FL) (McMahan et al., 2016; KonecË‡nyÂ´ et al., 2016b;a; McMahan et al., 2017), the training data is captured in a distributed fashion in the ï¬rst place, and there are reasons to process it in this decentralized fashion as well, as opposed to ï¬rst moving it to a centralized location, such as a datacenter, and subsequently processing it there. Indeed, FL refers to machine learning in the environment where a large collection of highly heterogeneous clients (e.g., mobile devices, smart home appliances or corporations) tries to collaboratively train a model using the diverse data stored on these devices, but without compromising the clientsâ€™ data privacy.

1.2. Big model and the need for communication reduction
While distributing the data across several workers certainly alleviates the per-client storage and compute bottlenecks, the training task is obviously not fully decomposed this way. Indeed, the ğ‘› clients still need to work together to train the

1

model, and working together means communication.
Since currently the most efï¬cient training mechanisms rely on gradient-type methods (Bottou, 2012; Kingma & Ba, 2014; Gorbunov et al., 2021), and since these operate by iteratively updating all the ğ‘‘ parameters describing the model, relying on big models leads to the need to communicate large-dimensional gradient vectors, which is expensive. For this reason, modern distributed methods need to rely on mechanisms that alleviate this communication burden.
Several orthogonal algorithmic approaches have been proposed in the literature to tackle this issue. One strain of methods, particularly popular in FL, is based on richer local training (e.g., LocalSGD), which typically means going beyond a single local gradient step before communication/aggregation across the workers is performed. This strategy is based on the hope that richer local training will ultimately lead to a dramatic reduction in the number of communication rounds without increasing the local computation time by much (Stich, 2020; Khaled et al., 2020; Woodworth et al., 2020). Another notable strain of methods is based on communication compression (e.g., QSGD), which means applying a lossy transformation to the communicated gradient information. This strategy is based on the hope that communication compression will lead to a dramatic reduction in the communication time within each round without affecting the number of communication rounds by much (Khirirat et al., 2018; Alistarh et al., 2018; Mishchenko et al., 2019; Li et al., 2020; Li & RichtaÂ´rik, 2020; Li & RichtaÂ´rik, 2021).
1.3. Gradient descent with compressed communication
In this work we focus on algorithms based on the latter line of work: communication compression.
Perhaps conceptually the simplest yet versatile gradientbased method for solving the distributed problem (1) employing communication compression is distributed compressed gradient descent (DCGD) (Khirirat et al., 2018). Given a sequence {ğ›¾ğ‘¡} of learning rates, DCGD performs the iterations
ğ‘›
ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘¡ ğ‘›1 âˆ‘ï¸€ ğ‘”ğ‘–ğ‘¡, ğ‘”ğ‘–ğ‘¡ = â„³ğ‘¡ğ‘–(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)). (2)
ğ‘–=1
Above, â„³ğ‘¡ğ‘– represents any suitable gradient communication mechanism1 for mapping the possibly dense, highdimensional, and hence hard-to-communicate gradient âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆˆ Rğ‘‘ into a vector of equal dimension, but one that can hopefully be communicated using much fewer bits.
1We do not borrow the phrase â€œcommunication mechanismsâ€ from any prior literature. We coined this phrase in order to be able to refer to a potentially arbitrary mechanism for transforming a ğ‘‘-dimensional gradient vector into another ğ‘‘dimensional vector that is easier to communicate. This allows us to step back, and critically reassess the methodological foundations of the ï¬eld in terms of the mathematical properties one should impart on such mechanisms for them to be effective.

2. Motivation and Background
Our work is motivated by several methodological, theoretical and algorithmic issues and open problems arising in the literature related to two orthogonal approaches to designing gradient communication mechanisms â„³ğ‘¡ğ‘–:

i) contractive compressors (Karimireddy et al., 2019; Stich et al., 2018; Alistarh et al., 2018; Koloskova et al., 2020; Beznosikov et al., 2020), and
ii) lazy aggregation (Chen et al., 2018; Sun et al., 2019; Ghadikolaei et al., 2021).

The motivation for our work starts with several critical observations related to these two mechanisms.

2.1. Contractive compression operators
Arguably, the simplest class of communication mechanisms is based on the (as we shall see, naive) application of contractive compression operators (or, contractive compressors for short) (Koloskova et al., 2020; Beznosikov et al., 2020). In this approach, one sets

â„³ğ‘¡ğ‘–(ğ‘¥) â‰¡ ğ’(ğ‘¥),

(3)

where ğ’ : Rğ‘‘ â†’ Rğ‘‘ is a (possibly randomized) mapping with the property

E

[ï¸ â€–ğ’(ğ‘¥)

âˆ’

ğ‘¥â€–2]ï¸

â‰¤

(1

âˆ’

ğ›¼)

â€–ğ‘¥â€–2

,

âˆ€ğ‘¥ âˆˆ Rğ‘‘,

(4)

where 0 < ğ›¼ â‰¤ 1 is the contraction parameter, and the expectation E [Â·] is taken w.r.t. the randomness inherent in ğ’. For examples of contractive compressors (e.g., Top-ğ¾ and Rand-ğ¾ sparsiï¬ers), please refer to Section A, and Table 1 in (Safaryan et al., 2021b; Beznosikov et al., 2020).
The algorithmic literature on contractive compressors (i.e., mappings ğ’ satisfying (4)) is relatively much more developed, and dates back to at least 2014 with the work of Seide et al. (2014), who proposed the error feedback (EF) mechanism for ï¬xing certain divergence issues which arise empirically with the naive approach based on (3).
Despite several advances in our theoretical understanding of EF over the last few years (Stich et al., 2018; Karimireddy et al., 2019; HorvaÂ´th & RichtaÂ´rik, 2021; Tang et al., 2020; Gorbunov et al., 2020), a satisfactory grasp of EF remained elusive. Recently, RichtaÂ´rik et al. (2021) proposed EF21, which is a new algorithmic and analysis approach to error feedback, effectively ï¬xing the previous weaknesses. In particular, while previous results offered weak ğ’ª(1/ğ‘‡ 2/3) rates (for smooth nonconvex problems), and did so under strong and often unrealistic assumptions (e.g., boundedness

2

Table 1 Summary of the methods ï¬tting our general 3PC framework. For each method we give the formula for the 3PC
compressor ğ’â„,ğ‘¦(ğ‘¥), its parameters ğ´, ğµ, and the ratio ğµ/ğ´ appearing in the convergence rate. Notation: ğ›¼ = parameter of the contractive compressor ğ’, ğœ” = parameter of the unbiased compressor ğ’¬, ğ´1, ğµ1 = parameters of three points compressor ğ’â„1,ğ‘¦(ğ‘¥), ğ›¼Â¯ = 1 âˆ’ (1 âˆ’ ğ›¼1)(1 âˆ’ ğ›¼2), where ğ›¼1, ğ›¼2 are the parameters of the contractive compressors ğ’1, ğ’2, respectively.

Variant of 3PC (Alg. 1) EF21
(RichtaÂ´rik et al., 2021)
LAG (Chen et al., 2018) (3)
CLAG (NEW)
3PCv1 (NEW) 3PCv2 (NEW) 3PCv3 (NEW) 3PCv4 (NEW) 3PCv5 (NEW)

Alg. # Alg. 2 Alg. 3
Alg. 4 Alg. 5 Alg. 6 Alg. 7 Alg. 8 Alg. 9

ğ’â„,ğ‘¦ (ğ‘¥) =

â„ + ğ’(ğ‘¥ âˆ’ â„)

{ï¸ƒ ğ‘¥, if (*),

â„, otherwise,

(*) means â€–ğ‘¥ âˆ’ â„â€–2 > ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2
{ï¸ƒ â„ + ğ’(ğ‘¥ âˆ’ â„), if (*),

â„,

otherwise,

(*) means â€–ğ‘¥ âˆ’ â„â€–2 > ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2

ğ‘¦ + ğ’(ğ‘¥ âˆ’ ğ‘¦)(1)

ğ‘ + ğ’ (ğ‘¥ âˆ’ ğ‘), where ğ‘ = â„ + ğ’¬(ğ‘¥ âˆ’ ğ‘¦)

ğ‘ + ğ’ (ğ‘¥ âˆ’ ğ‘),

where ğ‘ = ğ’â„1,ğ‘¦ (ğ‘¥)

ğ‘ + ğ’1 (ğ‘¥ âˆ’ ğ‘),

where ğ‘ = â„ + ğ’2(ğ‘¥ âˆ’ â„)

{ï¸ƒ

ğ‘¥,

w.p. ğ‘

â„ + ğ’(ğ‘¥ âˆ’ ğ‘¦), w.p. 1 âˆ’ ğ‘

ğ´ âˆš 1âˆ’ 1âˆ’ğ›¼
1

ğµ
1âˆšâˆ’ğ›¼ 1âˆ’ 1âˆ’ğ›¼
ğœ

ğµ ğ´
(ï¸ 1âˆ’ğ›¼ )ï¸ ğ’ª ğ›¼2
ğ’ª (ğœ)

âˆš 1âˆ’ 1âˆ’ğ›¼

{ï¸

}ï¸

1âˆšâˆ’ğ›¼ ,ğœ

max 1âˆ’ 1âˆ’ğ›¼

(ï¸ {ï¸

}ï¸)ï¸

ğ’ª max

1ğ›¼âˆ’2ğ›¼ ,

ğœ ğ›¼

1 ğ›¼
1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1) âˆš
1 âˆ’ 1 âˆ’ ğ›¼Â¯ âˆš
1âˆ’ 1âˆ’ğ‘

1âˆ’ğ›¼ (1 âˆ’ ğ›¼)ğœ” (1 âˆ’ ğ›¼)ğµ1
1âˆšâˆ’ğ›¼Â¯ 1âˆ’ 1âˆ’ğ›¼Â¯ (1âˆ’ğ‘âˆš)(1âˆ’ğ›¼) 1âˆ’ 1âˆ’ğ‘

1âˆ’ğ›¼
(1âˆ’ğ›¼)ğœ” ğ›¼
(1âˆ’ğ›¼)ğµ1 1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1 )
(ï¸ 1âˆ’ğ›¼Â¯ )ï¸ ğ’ª ğ›¼Â¯2
ğ’ª (ï¸ (1âˆ’ğ‘ğ‘)(21âˆ’ğ›¼) )ï¸

MARINA (Gorbunov et al., 2021)

Alg. 10

N/A(2)

ğ‘ (1âˆ’ğ‘›ğ‘)ğœ” (1âˆ’ğ‘›ğ‘ğ‘)ğœ”

(1) 3PCv1 requires communication of uncompressed vectors (âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)). Therefore, the method is impractical. We include it as an idealized version of EF21. (2) MARINA does not ï¬t the deï¬nition of three points compressor from (6). However, it satisï¬es (16) with ğºğ‘¡ = â€–ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2 and shown parameters ğ´ and ğµ, i.e., MARINA

can be analyzed via our theoretical framework. (3) LAG presented in our work is a (massively) simpliï¬ed version of LAG considered by (Chen et al., 2018). However, we have decided to use the same name.

of the gradients), the EF21 approach offers GD-like ğ’ª(1/ğ‘‡ ) rates, with standard assumptions only.2
The heart of the EF21 method is a new communication mechanism â„³ğ‘¡ğ‘–, generated from a contractive compressor ğ’, which ï¬xes (in a theoretically and practically superior way to the standard ï¬x offered by classical EF) the above mentioned divergence issues. Their construction is synthetic: is starts with the choice of ğ’ preferred by the user, and then constructs a new and adaptive communication mechanism based on it. We will describe this method in Section 4.
2.2. Lazy aggregation
An orthogonal approach to applying contractive operators, whether with or without error feedback, is â€œskippingâ€ communication. The basic idea of the lazy aggregation communication mechanism is for each worker ğ‘– to communicate its local gradient only if it differs â€œsigniï¬cantlyâ€ from the last gradient communicated before.
In its simplest form, the LAG method of Chen et al. (2018) is initialized with ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0) for all ğ‘– âˆˆ [ğ‘›], which means that all the workers communicate their gradients at the start. In all subsequent iterations, each worker ğ‘– âˆˆ [ğ‘›] deï¬nes ğ‘”ğ‘–ğ‘¡+1, which may be interpreted as a â€œcompressedâ€ version of the true gradient âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1), via the lazy aggregation rule
2The EF21 method was extended by Fatkhullin et al. (2021) to deal with stochastic gradients, variance reduction, regularizers, momentum, server compression, and partial participation. However, such extensions are not the subject of our work.

{ï¸ƒ

ğ‘¡+1

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

ğ‘”ğ‘– = ğ‘”ğ‘–ğ‘¡

if â€–ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)â€–2 > ğœğ·ğ‘–ğ‘¡, otherwise,

(5)

where ğ·ğ‘–ğ‘¡ := â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2 and ğœ > 0 is the trigger.3 The smaller the trigger ğœ, the more likely it is for

the condition â€–ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)â€–2 > ğœğ·ğ‘–ğ‘¡, which triggers communication, to be satisï¬ed. On the other hand, if ğœ is

very large, most iterations will skip communication and thus

reuse the past gradient. Since the trigger ï¬res dynamically

based on conditions that change in time, it is hard to theoret-

ically estimate how often communication skipping occurs.

In fact, there are no results on this in the literature. Nev-

ertheless, the lazy aggregation mechanism is empirically

useful when compared to vanilla GD (Chen et al., 2018).

Lazy aggregation is a much less studied and a much less understood communication mechanism than contractive compressors. Indeed, only a handful of papers offer any convergence guarantees (Chen et al., 2018; Sun et al., 2019; Ghadikolaei et al., 2021), and the results presented in the ï¬rst two of these papers are hard to penetrate. For example, no simple proof exists for the simple LAG variant presented above. The best known rate in the smooth nonconvex regime is ğ’ª(1/ğ‘‡ 2/3), which differs from the ğ’ª(1/ğ‘‡ ) rate of GD.

3It is possible to replace ğ·ğ‘–ğ‘¡ by ğ‘‹ğ‘–ğ‘¡ = ğœğ¿2ğ‘– â€–ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡â€–2, and our theory
will still trivially hold. This is the choice for the trigger condition made by Chen et al. (2018). One can also work with the more general choice ğ‘‹ğ‘–ğ‘¡ = ğœğ‘–â€–ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡â€–2;
our theory can be adapted to this trivially.

3

Table 2 Comparison of exisiting and proposed theoretically-supported methods employing lazy aggregation. In the rates for

{ï¸

}ï¸

âˆšï¸€

âˆšï¸€

our methods, ğ‘€1 = ğ¿âˆ’ + ğ¿+ ğµ/ğ´ and ğ‘€2 = max ğ¿âˆ’ + ğ¿+ 2ğµ/ğ´, ğ´/2ğœ‡ .

Method

Simple method?

Uses a contractive compressor ğ’?

Strongly convex rate

PÅ nonconvex rate General nonconvex rate

LAG (Chen et al., 2018)



LAQ (Sun et al., 2019)



LENA (Ghadikolaei et al., 2021) (7)

(4)

LAG (NEW, 2022)



CLAG (NEW, 2022)





linear (9)





(1)

linear (3)





(8)

ğ’ª(ğº4/ğ‘‡ 2ğœ‡2) (5), (6) ğ’ª(ğº4/ğ‘‡ 2ğœ‡2) (5), (6) ğ’ª(ğº4/3/ğ‘‡ 2/3) (6)



ğ’ª(exp(âˆ’ğ‘‡ ğœ‡/ğ‘€2)) ğ’ª(exp(âˆ’ğ‘‡ ğœ‡/ğ‘€2))

ğ’ª(ğ‘€1/ğ‘‡ )

(2)

ğ’ª(exp(âˆ’ğ‘‡ ğœ‡/ğ‘€2)) ğ’ª(exp(âˆ’ğ‘‡ ğœ‡/ğ‘€2))

ğ’ª(ğ‘€1/ğ‘‡ )

(1) They consider a speciï¬c form of quantization only. (2) Works with any contractive compressor, including low rank approximation, Top-ğ¾, Rand-ğ¾, quantization, and more. (3) Their Theorem 1 does not present any explicit linear rate. (4) LENA employs the classical EF mechanism, but it is not clear what is this mechanism supposed to do. (5) They consider an assumption (ğœ‡-quasi-strong convexity) that is slightly stronger than our PÅ assumption. Both are weaker than strong convexity. (6) They assume the local gradients to be bounded by ğº (â€–âˆ‡ğ‘“ğ‘–(ğ‘¥)â€– â‰¤ ğº for all ğ‘¥). We do not need such a strong assumption.
(7) They also consider the 0-quasi-strong convex case (slight generalization of convexity); we do not consider the convex case. Moreover, they consider the stochastic
case as well, we do not. We specialized all their results to the deterministic (i.e., full gradient) case for the purposes of this table. (8) Their contractive compressor depends on the trigger. (9) It is possible to specialize their method and proof so as to recover LAG as presented in our work, and to recover a rate similar to ours.

The known rates in the strongly convex regime are also highly problematic: they are either not explicit (Chen et al., 2018; Sun et al., 2019), or sublinear (Ghadikolaei et al., 2021). Furthermore, it is not clear whether an EF mechanism is needed to stabilize lazy aggregation methods, which is a necessity in the case of contractive compressors. While Ghadikolaei et al. (2021) proposed a combination of LAG and EF, their analysis leads to weak rates (see Table 2), and does not seem to point to theoretical advantages due to EF.
3. Summary of Contributions
We now summarize our main contributions:
âˆ™ Uniï¬cation through the 3PC method. At present, the two communication mechanisms outlined above, contractive compressors and lazy aggregation, are viewed as different approaches to the same problemâ€”reducing the communication overhead in distributed gradient-type methodsâ€” requiring different tools, and facing different theoretical challenges. We propose a uniï¬ed methodâ€”which we call 3PC (Algorithm 1)â€”which includes EF21 (Algorithm 2) and LAG (Algorithm 3) as special cases.
âˆ™ Several new methods. The 3PC method is much more general than either EF21 or LAG, and includes a number of new speciï¬c methods. For example, we propose CLAG, which is a combination of EF21 and LAG beneï¬ting from both contractive compressors and lazy aggregation. We show experimentally that CLAG can be better than both EF21 and LAG: that is, we obtain combined beneï¬ts of both approaches. We obtain a number of other new methods, such as 3PCv2, 3PCv3 and 3PCv4. We show experimentally that 3PCv2 can outperform EF21. See Table 1 for a summary of the proposed methods.
âˆ™ Three point compressors. Our proposed method, 3PC, can be viewed as DCGD with a new class of communication

mechanisms, based on the new notion of a three point compressor (3PC)4; see Section 4 for details. By design, and in contrast to contractive compressors, our communication mechanism based on the 3PC compressor is able to â€œevolveâ€ and thus improve throughout the iterations. In particular, its compression error decays, which is the key reason behind its superior theoretical properties. In summary, the properties deï¬ning the 3PC compressor distill the important characteristics of a theoretically well performing communication mechanism, and this is the ï¬rst time such characteristics have been explicitly identiï¬ed and formalized.
The observation that lazy aggregation is a 3PC compressor explains why error feedback is not needed to stabilize LAG and similar methods.
âˆ™ Strong rates. We prove an ğ’ª(1/ğ‘‡ ) rate for 3PC for smooth nonconvex problems, which up to constants matches the rate of GD. Furthermore, we prove a GD-like linear convergence rate under the Polyak-Åojasiewicz condition. Our general theory recovers the EF21 rates proved by RichtaÂ´rik et al. (2021) exactly. Our rates for lazily aggregated methods (LAG and CLAG) are new, and better than the results obtained by Chen et al. (2018); Sun et al. (2019) and Ghadikolaei et al. (2021) in all regimes considered. In the general smooth nonconvex regime, only Ghadikolaei et al. (2021) obtain rates. However, they require strong assumptions (gradients bounded by a constant ğº), and their rate is ğ’ª(ğº4/3/ğ‘‡ 2/3), whereas we do not need such assumptions and obtain the GD-like rate ğ’ª(ğ‘€1/ğ‘‡ ). In the strongly convex regime, Chen et al. (2018) and Sun et al. (2019) obtain non-speciï¬c linear rates, while Ghadikolaei et al. (2021) obtain the sublinear rate ğ’ª(ğº4/ğ‘‡ 2ğœ‡2). In contrast, we obtain explicit GD-like linear rates under the weaker PÅ condition.
Furthermore, our variant of LAG, and our convergence theory and proofs, are much simpler than those presented in
4We use the same name for the method and the compressor on purpose.

4

(Chen et al., 2018). In fact, it is not clear to us whether the many additional features employed by Chen et al. (2018) have any theoretical or practical beneï¬ts. We believe that our simple treatment can be useful for other researchers to further advance the ï¬eld.
For a detailed comparison of rates, please refer to Table 2.

4. Three Point Compressors

We now formally introduce the concept of a three point compressor (3PC).
Deï¬nition 4.1 (Three point compressor). We say that a (possibly randomized) map

ğ’â„,ğ‘¦(ğ‘¥) : Rğ‘‘ Ã— Rğ‘‘ Ã— Rğ‘‘ â†’ Rğ‘‘

âŸâ âŸâ âŸâ

â„âˆˆ

ğ‘¦âˆˆ

ğ‘¥âˆˆ

is a three point compressor (3PC) if there exist constants
0 < ğ´ â‰¤ 1 and ğµ â‰¥ 0 such that the following relation holds for all ğ‘¥, ğ‘¦, â„ âˆˆ Rğ‘‘

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

â‰¤

(1 âˆ’ ğ´) â€–â„ âˆ’ ğ‘¦â€–2

+ğµ â€–ğ‘¥ âˆ’ ğ‘¦â€–2 . (6)

The vectors ğ‘¦ âˆˆ Rğ‘‘ and â„ âˆˆ Rğ‘‘ are parameters deï¬ning the compressor. Once ï¬xed, ğ’â„,ğ‘¦ : Rğ‘‘ â†’ Rğ‘‘ is the compression mapping used to compress vector ğ‘¥ âˆˆ Rğ‘‘.

4.1. Connection with contractive compressors

Note that if we set â„ = 0 and ğ‘¦ = ğ‘¥, then inequality (6) specializes to

E

[ï¸ â€–ğ’0,ğ‘¥

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

â‰¤

(1 âˆ’ ğ´) â€–ğ‘¥â€–2 ,

(7)

which is the inequality deï¬ning a contractive compressor. In other words, a particular restriction of the parameters of any 3PC compressor is necessarily a contractive compressor.
However, this is not the restriction we will use to design our compression mechanism. Instead, as we shall describe next, we will choose the sequence of vectors â„ and ğ‘¦ in an adaptive fashion, based on the path generated by DCGD.

4.2. Designing a communication mechanism using a 3PC compressor
We now describe our proposal for how to use a 3PC compressor to design a good communication mechanism {â„³ğ‘˜ğ‘– } to be used within DCGD. Recall from (2) that all we need to do is to deï¬ne the mapping
â„³ğ‘¡ğ‘– : âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) â†¦â†’ ğ‘”ğ‘–ğ‘¡.
First, we allow the initial compressed gradients {ğ‘”ğ‘–0}ğ‘›ğ‘–=1 to be chosen arbitrarily. Here are some examples of possible

choices: a) Full gradients: ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0) for all ğ‘– âˆˆ [ğ‘›]. The beneï¬t of this choice is that no information is loss
at the start of the process. On the other hand, the full ğ‘‘-
dimensional gradients need to be sent by the workers to
the server, which is potentially an expensive pre-processing step. b) Compressed gradients: ğ‘”ğ‘–0 = ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥0)) for all ğ‘– âˆˆ [ğ‘›], where ğ’ is an arbitrary compression mapping (e.g.,
a contractive compressor). While some information is lost
right at the start of the process (compared to a GD step),
the beneï¬t of this choice is that no full dimensional vectors need to be communicated. c) Zero preprocessing: ğ‘”ğ‘–0 = 0 for all ğ‘– âˆˆ [ğ‘›].

Having

chosen

ğ‘”ğ‘–0

,

.

.

.

,

ğ‘”

0 ğ‘–

for

all

ğ‘–

âˆˆ

[ğ‘›],

it

remains

to

deï¬ne the communication mechanism â„³ğ‘¡ğ‘– for ğ‘¡ â‰¥ 1. We

will do this on-the-ï¬‚y as DCGD is run, with the help of the

parameters â„ and ğ‘¦, which we choose adaptively. Consider

the viewpoint of a worker ğ‘– âˆˆ [ğ‘›] in iteration ğ‘¡ + 1, with ğ‘¡ â‰¥

0. In this iteration, worker ğ‘– wishes to compress the vector

ğ‘¥ = âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1). Let ğ‘”ğ‘–ğ‘¡ denote the compressed version of the vector âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡), i.e., ğ‘”ğ‘–ğ‘¡ = â„³ğ‘¡ğ‘–(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)). We choose

ğ‘¦ = âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) and â„ = ğ‘”ğ‘–ğ‘¡.

With these parameter choices, we deï¬ne the compressed version of ğ‘¥ = âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) by setting

â„³ğ‘¡ğ‘–+1(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)) (=2) ğ‘”ğ‘–ğ‘¡+1 := ğ’ğ‘”ğ‘–ğ‘¡,âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)). (8)
Our porposed 3PC method (Algorithm 1) is just DCGD with the compression mechanism described above.

4.3. The 3PC inequality

For the parameter choices made above, (6) specializes to

E [ï¸€ğ¸ğ‘–ğ‘¡+1 | ğ‘¥ğ‘¡, ğ‘”ğ‘–ğ‘¡]ï¸€ â‰¤ (1 âˆ’ ğ´)ğ¸ğ‘–ğ‘¡ + ğµğ·ğ‘–ğ‘¡,

(9)

where

ğ¸ğ‘–ğ‘¡ := âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2

and ğ·ğ‘–ğ‘¡ := âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2 .

This inequality has a natural interpretation. It enforces the compression error ğ¸ğ‘–ğ‘¡ to shrink by the factor of 1 âˆ’ ğ´ in each communication round, subject to an additive penalty proportional to ğ·ğ‘–ğ‘¡. If the iterates converge, then the penalty will eventually vanish as well provided that the gradient
of ğ‘“ğ‘– is continuous. Intuitively speaking, this forces the compression error ğ¸ğ‘¡ to improve in time.

We note that applying a simple contractive compressor in place of â„³ğ‘¡ğ‘– does not have this favorable property, and this is what causes the convergence issues in existing literature
on this topic. This is what the EF literature was trying to
solve since 2014, and what the EF21 mechanism resolved

5

Algorithm 1 3PC (DCGD method using the 3PC communication mechanism)

1: Input: starting point ğ‘¥0 âˆˆ Rğ‘‘ (on all workers), stepsize ğ›¾ > 0, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0 âˆˆ Rğ‘‘ for

ğ‘– âˆˆ [ğ‘›] (known to the server and all workers)

2:

Initialization: ğ‘”0

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–0

3: for ğ‘¡ = 0, 1, . . . , ğ‘‡ âˆ’ 1 do

(Server aggregates initial gradient estimates)

4: Broadcast ğ‘”ğ‘¡ to all workers

5: for ğ‘– = 1, . . . , ğ‘› in parallel do

6:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

7:

Set ğ‘”ğ‘–ğ‘¡+1 = â„³ğ‘¡ğ‘–+1(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)) := ğ’ğ‘”ğ‘¡,âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1))

ğ‘–

8:

Communicate ğ‘”ğ‘–ğ‘¡+1 to the server

9: end for

10:

Server

aggregates

received

messages:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

11: end for

12: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

(Take a gradient-type step) (Apply 3PC to compress the latest gradient)

in 2021. However, no such progress happened in the lazy aggregation literature yet, and one of the key contributions of our work is to remedy this situation.

Lemma 4.3. The mapping (11) satisï¬es (6) with ğ´ := 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ) and ğµ := max {ï¸€(1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ , ğœ}ï¸€,
where ğ‘  > 0 is any scalar satisfying (1 âˆ’ ğ›¼) (1 + ğ‘ ) < 1.

4.4. EF21 mechanism is a 3PC compressor

We will now show that the compression mechanism â„³ğ‘¡ğ‘– employed in EF21 comes from a 3PC compressor. Let ğ’ : Rğ‘‘ â†’ Rğ‘‘ be a contractive compressor with contraction
parameter ğ›¼, and deï¬ne

ğ’â„,ğ‘¦(ğ‘¥) := â„ + ğ’(ğ‘¥ âˆ’ â„).

(10)

If we use this mapping to deï¬ne a compression mechanism â„³ğ‘¡ğ‘– via (8), use this within DCGD, we obtain the EF21 method of RichtaÂ´rik et al. (2021). Indeed, observe that
Algorithm 2 (EF21) is a special case of Algorithm 1 (3PC).

The next lemma shows that (10) is a 3PC compressor.
Lemma 4.2. The mapping (10) satisï¬es (6) with ğ´ := 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ) and ğµ := (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€, where ğ‘  > 0 is any scalar satisfying (1 âˆ’ ğ›¼) (1 + ğ‘ ) < 1.

The LAG method is obtained as a special case of CLAG by choosing ğ’ to be the identity mapping (for which ğ›¼ = 1).
4.6. Further 3PC compressors and methods
In Table 1 we summarize several further 3PC compressors and the new algorithms they lead to (e.g., 3PCv1â€“3PCv5). The details are given the in the appendix.

5. Theory

We are now ready to present our theoretical convergence results for the 3PC method (Algorithm 1), the main steps of which are

ğ‘›

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡,

ğ‘”ğ‘¡

=

1 ğ‘›

âˆ‘ï¸€

ğ‘”ğ‘–ğ‘¡,

(12)

ğ‘–=1

4.5. LAG mechanism is a 3PC compressor

ğ‘”ğ‘–ğ‘¡+1 = ğ’ğ‘”ğ‘¡,âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)). ğ‘–

(13)

We will now show that the compression mechanism â„³ğ‘¡ğ‘– employed in LAG comes from a 3PC compressor. In fact,
let us deï¬ne CLAG, and recover LAG from it as a special case. Let ğ’ : Rğ‘‘ â†’ Rğ‘‘ be a contractive compressor with contraction parameter ğ›¼. Choose a trigger ğœ > 0, and deï¬ne

{ï¸ƒ

â„ + ğ’(ğ‘¥ âˆ’ â„), if â€–ğ‘¥ âˆ’ â„â€–2 > ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2,

ğ’â„,ğ‘¦(ğ‘¥) :=

(11)

â„,

otherwise,

If we use this mapping to deï¬ne a compression mechanism â„³ğ‘¡ğ‘– via (8), use this within DCGD, we obtain our new CLAG method. Indeed, observe that Algorithm 4 (CLAG) is a
special case of Algorithm 1 (3PC).

Recall that the 3PC method is DCGD with a particular choice of the communication mechanism {â„³ğ‘¡ğ‘–} based on an arbitrary 3PC compressor ğ’â„,ğ‘¦(ğ‘¥).
5.1. Assumptions
We rely on the following standard assumptions. Assumption 5.1. The functions ğ‘“1, . . . ,ğ‘“ğ‘› : Rğ‘‘ â†’ R are differentiable. Moreover, ğ‘“ is lower bounded, i.e., there exists ğ‘“ inf âˆˆ R such that ğ‘“ (ğ‘¥) â‰¥ ğ‘“ inf for all ğ‘¥ âˆˆ Rğ‘‘. Assumption 5.2. The function ğ‘“ : Rğ‘‘ â†’ R is ğ¿âˆ’-smooth, i.e., it is differentibale and its gradient satisï¬es

The next lemma shows that (11) is a 3PC compressor.

â€–âˆ‡ğ‘“ (ğ‘¥) âˆ’ âˆ‡ğ‘“ (ğ‘¦)â€– â‰¤ ğ¿âˆ’â€–ğ‘¥ âˆ’ ğ‘¦â€– âˆ€ğ‘¥,ğ‘¦ âˆˆ Rğ‘‘. (14)

6

Assumption 5.3. There is a constant ğ¿+ > 0 such

that

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

â€–âˆ‡ğ‘“ğ‘–

(ğ‘¥)

âˆ’

âˆ‡ğ‘“ğ‘–

(ğ‘¦)â€–2

â‰¤

ğ¿2+ â€–ğ‘¥ âˆ’ ğ‘¦â€–2

for

all

ğ‘¥,ğ‘¦ âˆˆ Rğ‘‘. Let ğ¿+ be the smallest such number.

It is easy to see that ğ¿âˆ’ â‰¤ ğ¿+. We borrow this notation for the smoothness constants from (Szlendak et al., 2021).

5.2. Convergence for general nonconvex functions

The following lemma is based on the properties of the 3PC compressor. It establishes the key inequality for the convergence analysis. The proof follows easily from the deï¬nition of a 3PC compressor and Assumption 5.3.
Lemma 5.4. Let Assumption 5.3 hold. Consider the 3PC method. Then, the sequence

ğºğ‘¡ := ğ‘›1 âˆ‘ğ‘›ï¸€ â€–ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2 (15)
ğ‘–=1

for all ğ‘¡ â‰¥ 0 satisï¬es

E

[ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğ´)E

[ğºğ‘¡]

+

ğµğ¿2+E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

.

(16)

Using this lemma and arguments from the analysis of SGD for non-convex problems (Li et al., 2021; RichtaÂ´rik et al., 2021), we derive the following result.
Theorem 5.5. Let Assumptions 5.1, 5.2, 5.3 hold. Assume that the stepsize ğ›¾ of the 3PC method satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€1,
âˆšï¸€ where ğ‘€1 = ğ¿âˆ’ + ğ¿+ ğµ/ğ´. Then, for any ğ‘‡ â‰¥ 1 we have
E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2ğ›¾Î”ğ‘‡0 + Eğ´[ğºğ‘‡0] , (17)
where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by 3PC , âˆ†0 := ğ‘“ (ğ‘¥0)âˆ’ğ‘“ inf , and ğº0 is deï¬ned in (15).

The theorem implies the following fact.
Corollary 5.6. Let the assumptions of Theorem 5.5 hold and choose the stepsize

ğ›¾=

1âˆš .

ğ¿âˆ’+ğ¿+ ğµ/ğ´

Then for any ğ‘‡ â‰¥ 1 we have

(ï¸

âˆš )ï¸

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2Î”0 ğ¿âˆ’+ğ‘‡ğ¿+ ğµ/ğ´ + Eğ´[ğºğ‘‡0] .

That

is,

to

achieve

E

[ï¸ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0,

the 3PC method requires

(ï¸

âˆš )ï¸

(ï¸‚ Î”0 ğ¿âˆ’+ğ¿+ ğµ/ğ´

E[ğº0] )ï¸‚

ğ‘‡ =ğ’ª

ğœ€2

+ ğ´ğœ€2

(18)

iterations (=communication rounds).

5.3. Convergence under the PÅ condition
In this part we provide our main convergence result under the Polyak-Åojasiewicz (PÅ) condition. Assumption 5.7 (PÅ condition). Function ğ‘“ : Rğ‘‘ â†’ R satisï¬es the Polyak-Åojasiewicz (PÅ) condition with parameter ğœ‡ > 0, i.e.,

â€–âˆ‡ğ‘“ (ğ‘¥)â€–2 â‰¥ 2ğœ‡ (ğ‘“ (ğ‘¥) âˆ’ ğ‘“ *) , âˆ€ğ‘¥ âˆˆ Rğ‘‘, (19)

where ğ‘¥* := arg minğ‘¥âˆˆRğ‘‘ ğ‘“ (ğ‘¥) and ğ‘“ * := ğ‘“ (ğ‘¥*).

In this setting, we get the following result.

Theorem 5.8. Let Assumptions 5.1, 5.2, 5.3, 5.7 hold. As-

sume that the stepsize ğ›¾ of the 3PC method satisï¬es 0 â‰¤

{ï¸

}ï¸

âˆšï¸€

ğ›¾ â‰¤ 1/ğ‘€2, where ğ‘€2 = max ğ¿âˆ’ + ğ¿+ 2ğµ/ğ´, ğ´/2ğœ‡ .

Then, for any ğ‘‡ â‰¥ 0 and âˆ†0 := ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ (ğ‘¥*) we have

E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ )]ï¸€ âˆ’ ğ‘“ * â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ (ï¸€âˆ†0 + ğ´ğ›¾ E [ï¸€ğº0]ï¸€)ï¸€ . (20)

The theorem implies the following fact.
Corollary 5.9. Let the assumptions of Theorem 5.8 hold and choose the stepsize

{ï¸‚

}ï¸‚

ğ›¾ = min

1âˆš , ğ´ .

ğ¿âˆ’+ğ¿+ 2ğµ/ğ´ 2ğœ‡

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ )]ï¸€ âˆ’ ğ‘“ * â‰¤ ğœ€ for some ğœ€ > 0 the method requires

(ï¸‚ {ï¸‚

âˆš

}ï¸‚

)ï¸‚

ğ’ª max ğ¿âˆ’+ğ¿+ ğµ/ğ´ , ğ´ log Î”0+E[ğº0]ğ›¾/ğ´

ğœ‡

ğœ€

(21)

iterations (=communication rounds).
5.4. Commentary
As mentioned in the contributions section, the above rates match those of GD in the considered regimes, up to constants factors, and at present constitute the new best-known rates for methods based on lazy aggregation. They recover the best known rates for error feedback since they match the rate of EF21.

6. Experiments
Now we empirically test the new variants of 3PC in two expriments. In the ï¬rst experiment, we focus on compressed lazy aggregation mechanism and study the behavior of CLAG (Algorithm 4) combined with Top-ğ¾ compressor. In the second one, we compare 3PCv2 (Algorithm 6) to EF21 with Top-ğ¾ on a practical task of learning a representation of MNIST dataset (LeCun et al., 2010).

7

|| f(xt)||2

101

Homogenity level: 1 3PCv2-PermK-TopK: 21

101

Hom3oPgCevn2it-PyelermveKl-:T0opK: 2 2 101

100

3PCv2-RandK-TopK: 21 3PCv2-TopK-TopK: 2 2

100

3PCv2-RandK-TopK: 21 3PCv2-TopK-TopK: 2 2

100

EF21-TopK: 2 3

EF21-TopK: 2 3

10 1

10 1

10 1

Split by labels 3PCv2-PermK-TopK: 2 2 3PCv2-RandK-TopK: 21 3PCv2-TopK-TopK: 2 2 EF21-TopK: 2 3

|| f(xt)||2 || f(xt)||2

10 2

10 2

10 2

10 3

10 3

10 3

0 10 #20Mbits /30n 40 50 0 10 #20Mbits /30n 40 50 0 10 #20Mbits /30n 40 50

Figure 1: Comparison of 3PCv2 with Perm-ğ¾, Rand-ğ¾ and Top-ğ¾ as the ï¬rst compressor. Top-ğ¾ is used as the second compressor. Number of clients ğ‘› = 100, compression level ğ¾ = 251. EF21 with Top-ğ¾ is provided for the reference.

6.1. Is CLAG better than LAG and EF21?

Consider solving the non-convex logistic regression problem

[ï¸ƒ

]ï¸ƒ

min

ğ‘

ğ‘‘

ğ‘“ (ğ‘¥) := 1 âˆ‘ï¸€ log(1 + ğ‘’âˆ’ğ‘¦ğ‘–ğ‘âŠ¤ ğ‘– ğ‘¥) + ğœ† âˆ‘ï¸€

ğ‘¥2ğ‘—

,

ğ‘¥âˆˆRğ‘‘

ğ‘ ğ‘–=1

ğ‘—=1 1+ğ‘¥2ğ‘—

where ğ‘ğ‘– âˆˆ Rğ‘‘, ğ‘¦ğ‘– âˆˆ {âˆ’1, 1} are the training data and labels, and ğœ† > 0 is a regularization parameter, which is ï¬xed to ğœ† = 0.1. We use four LIBSVM (Chang & Lin, 2011) datasets phishing, w6a, a9a, ijcnn1 as training data. Each dataset is shufï¬‚ed and split into ğ‘› = 20 equal parts.

We vary two parameters of CLAG, ğ¾ and ğœ, and report the number of bits (per worker) sent from clients to the server to achieve â€–âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€– < 10âˆ’2. For each pair (ğ¾, ğœ), we ï¬netune the stepsize of CLAG with multiples (1, 21, 22, . . . , 211) of the theoretical stepsize. We report the results on a heatmap (see Figure 2) for the representative dataset ijcnn1. Other datasets are included in Appendix E. On the heatmap, we vary ğœ along rows and ğ¾ along columns. Notice that CLAG reduces to LAG when ğ¾ = ğ‘‘ (bottom row) and to EF21 when ğœ = 0 (left column).

The experiment shows that the minimum communication complexity is attained at a combination of (ğ¾, ğœ), which does not reduce CLAG to its special cases: EF21 or LAG. This empirically conï¬rms that CLAG has better communication complexity than EF21 and LAG. Additional experiments validating the performance of CLAG are reported in Appendix E

6.2. Other 3PC variants

We consider the objective

[ï¸ƒ

1

ğ‘
âˆ‘ï¸

]ï¸ƒ
2

min

ğ‘“ (ğ·, ğ¸) :=

â€–ğ·ğ¸ğ‘ğ‘– âˆ’ ğ‘ğ‘–â€– ,

ğ·âˆˆRğ‘‘ğ‘“ Ã—ğ‘‘ğ‘’ ,ğ¸âˆˆRğ‘‘ğ‘’Ã—ğ‘‘ğ‘“

ğ‘ ğ‘–=1

where ğ‘ğ‘– are ï¬‚attened represenations of images with ğ‘‘ğ‘“ = 784, ğ· and ğ¸ are learned parameters of the autoencoder

22 c1o7mpr1e2ssion8level3K 1

EF21

ijcnn1.bz2

3008 3872 3883 2867 4880 4872

15000

4544 3070 2691 2926 2216 3497

9152 2918 5888 7667 3724 3968

8500

6080 9267 7136 6368 4678 8038

4500

13216 6905 11176 6742 9979 6089

16896 7497 10313 9046 4892 5632 LAG 2500

0

2

8trigger 32 128 512

Figure 2: Heatmap of communication complexities of CLAG for different combination of compression levels ğ¾ and triggers ğœ with tuned stepsizes on ijcnn1 dataset. We contour cells corresponding to EF21 and LAG, as special cases of CLAG, by black rectangles. The red-contoured cell indicates the experiment with the smallest communication cost.

model. We ï¬x the encoding dimensions as ğ‘‘ğ‘’ = 16 and distribute the data samples across ğ‘› = 100 clients. In order to control the heterogenity of this distribution, we consider three cases. First, each client owns the same data (â€œhomogeneity level: 1â€). Second, the data is randomly split among client (â€œhomogeneity level: 0â€). Finally, we consider an extremely heterogeneous case, where the images are â€œsplit by labelsâ€. ğ¾ is set to ğ‘‘/ğ‘›, where ğ‘‘ = 2 Â· ğ‘‘ğ‘“ Â· ğ‘‘ğ‘’ = 25088 is the total dimension of learning parameters ğ· and ğ¸. We apply three different sparsiï¬ers (Top-ğ¾, Rand-ğ¾, Perm-ğ¾) for the ï¬rst compressor of 3PCv2 (Algorithm 6) and ï¬x the second one as Top-ğ¾.5 3PCv2 method communicates two sparse sequences at each communication round, while EF21 only one. To account for this, we select ğ¾1, ğ¾2 from the set {ğ¾/2, ğ¾}, that is there are four possible choices for compression levels ğ¾1, ğ¾2 of two sparsiï¬ers in 3PCv2. Then we select the pair which works best. We ï¬ne-tune every method
5See Appendices A and E for the deï¬nitions and more detailed description.

8

with the stepsizes from the set {2âˆ’12, 2âˆ’11, . . . , 25} and select the best run based on the value of â€–âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2 at the last iterate. The stepsize for each method is indicated in the legend of each plot.
Figure 1 demonstrates that 3PCv2 is competitive with the EF21 method and, in some cases, superior. The improvement is particularly prominent in the heterogeneous regime. Experiments with other variants, 3PCv1â€“3PCv5, including the experiments on a carefully designed syntetic quadratic problem, are reported in Appendix E.

References
Alistarh, D., Hoeï¬‚er, T., Johansson, M., Khirirat, S., Konstantinov, N., and Renggli, C. The convergence of sparsiï¬ed gradient methods. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

Beznosikov, A., HorvaÂ´th, S., RichtaÂ´rik, P., and Safaryan, M. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.

Bottou, L. Stochastic Gradient Descent Tricks, vol-

ume 7700 of Lecture Notes in Computer Science

(LNCS), pp. 430â€“445.

Springer, neural net-

works, tricks of the trade, reloaded edition, Jan-

uary 2012. URL https://www.microsoft.

com/en-[]us/research/publication/

stochastic-[]gradient-[]tricks/.

Chang, C.-C. and Lin, C.-J. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1â€“27, 2011.

Chen, T., Giannakis, G., Sun, T., and Yin, W. LAG: Lazily aggregated gradient for communication-efï¬cient distributed learning. Advances in Neural Information Processing Systems, 2018.

Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pp. 1223â€“1231, 2012.

Fatkhullin, I., Sokolov, I., Gorbunov, E., Li, Z., and RichtaÂ´rik, P. Ef21 with bells & whistles: practical algorithmic extensions of modern error feedback. arXiv preprint arXiv:2110.03294, 2021.

Ghadikolaei, H. S., Stich, S., and Jaggi, M. LENA: Communication-efï¬cient distributed learning with selftriggered gradient uploads. In International Conference on Artiï¬cial Intelligence and Statistics, pp. 3943â€“3951. PMLR, 2021.

Gorbunov, E., Kovalev, D., Makarenko, D., and RichtaÂ´rik, P. Linearly converging error compensated SGD. In 34th Conference on Neural Information Processing Systems (NeurIPS), 2020.
Gorbunov, E., Burlachenko, K. P., Li, Z., and RichtaÂ´rik, P. MARINA: Faster non-convex distributed learning with compression. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3788â€“3798. PMLR, 18â€“24 Jul 2021. URL https://proceedings.mlr.press/ v139/gorbunov21a.html.
HorvaÂ´th, S. and RichtaÂ´rik, P. A better alternative to error feedback for communication-efï¬cient distributed learning. arXiv preprint arXiv:2006.11077, 2020.
HorvaÂ´th, S. and RichtaÂ´rik, P. A better alternative to error feedback for communication-efï¬cient distributed learning. In 9th International Conference on Learning Representations (ICLR), 2021.
Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. Error feedback ï¬xes SignSGD and other gradient compression schemes. In 36th International Conference on Machine Learning (ICML), 2019.
Khaled, A., Mishchenko, K., and RichtaÂ´rik, P. Tighter theory for local SGD on identical and heterogeneous data. In The 23rd International Conference on Artiï¬cial Intelligence and Statistics (AISTATS 2020), 2020.
Khirirat, S., Feyzmahdavian, H. R., and Johansson, M. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
Kingma, D. P. and Ba, J. Adam: a method for stochastic optimization. In The 3rd International Conference on Learning Representations, 2014. URL https:// arxiv.org/pdf/1412.6980.pdf.
Koloskova, A., Lin, T., Stich, S., and Jaggi, M. Decentralized deep learning with arbitrary communication compression. In International Conference on Learning Representations (ICLR), 2020.
KonecË‡nyÂ´, J., McMahan, H. B., Ramage, D., and RichtaÂ´rik, P. Federated optimization: distributed machine learning for on-device intelligence. arXiv:1610.02527, 2016a.
KonecË‡nyÂ´, J., McMahan, H. B., Yu, F., RichtaÂ´rik, P., Suresh, A. T., and Bacon, D. Federated learning: strategies for improving communication efï¬ciency. In NIPS Private Multi-Party Machine Learning Workshop, 2016b.
LeCun, Y., Cortes, C., and Burges, C. Mnist handwritten digit database. ATTLabs [Online], 2010. URL http: //yann.lecun.com/exdb/mnist.

9

Li, Z. and RichtaÂ´rik, P. A uniï¬ed analysis of stochastic gradient methods for nonconvex federated optimization. arXiv preprint arXiv:2006.07013, 2020.
Li, Z. and RichtaÂ´rik, P. CANITA: Faster rates for distributed convex optimization with communication compression. In Advances in Neural Information Processing Systems, 2021. arXiv:2107.09461.
Li, Z., Kovalev, D., Qian, X., and RichtaÂ´rik, P. Acceleration for compressed gradient descent in distributed and federated optimization. In International Conference on Machine Learning (ICML), pp. 5895â€“5904. PMLR, 2020.
Li, Z., Bao, H., Zhang, X., and RichtaÂ´rik, P. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International Conference on Machine Learning (ICML), pp. 6286â€“6295. PMLR, 2021.
Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, B. Deep gradient compression: Reducing the communication bandwidth for distributed training. In International Conference on Learning Representations, 2018.
McMahan, B., Moore, E., Ramage, D., and AguÂ¨era y Arcas, B. Federated learning of deep networks using model averaging. arXiv preprint arXiv:1602.05629, 2016.
McMahan, H. B., Moore, E., Ramage, D., Hampson, S., and AguÂ¨era y Arcas, B. Communication-efï¬cient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artiï¬cial Intelligence and Statistics (AISTATS), 2017.
Mishchenko, K., Gorbunov, E., TakaÂ´cË‡, M., and RichtaÂ´rik, P. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Nesterov, Y. et al. Lectures on convex optimization, volume 137. Springer, 2018.
RichtaÂ´rik, P., Sokolov, I., and Fatkhullin, I. EF21: A new, simpler, theoretically better, and practically faster error feedback. In Advances in Neural Information Processing Systems, 2021.
Safaryan, M., Islamov, R., Qian, X., and RichtaÂ´rik, P. FedNL: Making Newton-type methods applicable to federated learning. arXiv preprint arXiv:2106.02969, 2021a.
Safaryan, M., Shulgin, E., and RichtaÂ´rik, P. Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor. Information and Inference: A Journal of the IMA, 2021b.
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochastic gradient descent and its application to data-parallel

distributed training of speech DNNs. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.
Stich, S. U. Local SGD converges fast and communicates little. In International Conference on Learning Representations, 2020.
Stich, S. U., Cordonnier, J.-B., and Jaggi, M. Sparsiï¬ed SGD with memory. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
Sun, J., Chen, T., Giannakis, G., and Yang, Z. Communication-efï¬cient distributed learning via lazily aggregated quantized gradients. Advances in Neural Information Processing Systems, 32:3370â€“3380, 2019.
Szlendak, R., Tyurin, A., and RichtaÂ´rik, P. Permutation compressors for provably faster distributed nonconvex optimization. arXiv preprint arXiv:2110.03300, 2021, 2021.
Tang, H., Lian, X., Yu, C., Zhang, T., and Liu, J. DoubleSqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2020.
Woodworth, B., Patel, K. K., Stich, S. U., Dai, Z., Bullins, B., McMahan, H. B., Shamir, O., and Srebro, N. Is local SGD better than minibatch SGD? arXiv preprint arXiv:2002.07839, 2020.

10

APPENDIX

Table of Contents

1 Introduction

1

1.1 Big data and the need for distributed systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 Big model and the need for communication reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.3 Gradient descent with compressed communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

2 Motivation and Background

2

2.1 Contractive compression operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

2.2 Lazy aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

3 Summary of Contributions

4

4 Three Point Compressors

5

4.1 Connection with contractive compressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

4.2 Designing a communication mechanism using a 3PC compressor . . . . . . . . . . . . . . . . . . . . . . 5

4.3 The 3PC inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

4.4 EF21 mechanism is a 3PC compressor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

4.5 LAG mechanism is a 3PC compressor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

4.6 Further 3PC compressors and methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

5 Theory

6

5.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

5.2 Convergence for general nonconvex functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

5.3 Convergence under the PÅ condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

5.4 Commentary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

6 Experiments

7

6.1 Is CLAG better than LAG and EF21? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

6.2 Other 3PC variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

A Examples of Contractive Compressors

13

A.1 Top-ğ¾ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

A.2 Rand-ğ¾ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

A.3 cRand-ğ¾ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

A.4 Perm-ğ¾ and cPerm-ğ¾ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

A.5 Unbiased compressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

A.6 Further examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

B Proofs of The Main Results

14

B.1 Three Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

B.2 General Non-Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

B.3 PÅ Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

C Three Point Compressor: Special Cases

17

C.1 Error Feedback 2021: EF21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

C.2 LAG: Lazily Aggregated Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

C.3 CLAG: Compressed Lazily Aggregated Gradient (NEW) . . . . . . . . . . . . . . . . . . . . . . . . . . 21

11

C.4 3PCv1 (NEW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.5 3PCv2 (NEW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.6 3PCv3 (NEW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 C.7 3PCv4 (NEW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.8 3PCv5 (NEW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

D MARINA

34

E More Experiments

36

E.1 Learning autoencoder model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

E.2 Solving synthetic quadratic problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

E.3 Testing compressed lazy aggregation (CLAG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

12

A. Examples of Contractive Compressors
The simplest example of a contractive compressor is the identity mapping, ğ’(ğ‘¥) â‰¡ ğ‘¥, which satisï¬es (4) with ğ›¼ = 1, and using which DCGD reduces to (distributed) gradient descent.

A.1. Top-ğ¾
A typical non-trivial example of a contractive compressor is the Top-ğ¾ sparsiï¬cation operator (Alistarh et al., 2018), which is a deterministic mapping characterized by a parameter 1 â‰¤ ğ¾ â‰¤ ğ‘‘ deï¬ning the required level of sparsiï¬cation. The smaller this parameter is, the higher compression level is applied, and the smaller the contraction parameter ğ›¼ becomes, which indicates that there is a larger error between the message ğ‘¥ we wanted to send, and the compressed message ğ’(ğ‘¥) we actually sent. In the extreme case ğ¾ = ğ‘‘, we have ğ’(ğ‘¥) = ğ‘¥, and the input vector is left intact, and hence uncompressed. In this case, ğ›¼ = 1. If ğ¾ = 1, then all entries of ğ‘¥ are zeroed out, except for the largest entry in absolute value, breaking ties arbitrarily. This choice offers a ğ‘‘ : 1 compression ratio, which can be dramatic if ğ‘‘ is large, which is the case when working with big models. In this case, ğ›¼ = 1/ğ‘‘. The general choice of ğ¾ leaves just ğ¾ nonzero entries intact, those that are largest in absolute value (again, breaking ties arbitrarily), with the remaining ğ‘‘ âˆ’ ğ¾ entries zeroed out. This offers a (ğ‘‘ âˆ’ ğ¾) : 1 compression ratio, with contraction factor ğ›¼ = ğ¾/ğ‘‘.

A.2. Rand-ğ¾
One of the simplest randomized sparsiï¬cation operators is Rand-ğ¾ (Khirirat et al., 2018). It is similar to Top-ğ¾, with the exception that the ğ¾ entries that are retained are chosen uniformly at random rather than greedily. Just like in the case of Top-ğ¾, the worst-case (expected) error produced by Rand-ğ¾ is characterized by ğ›¼ = ğ¾/ğ‘‘. However, on inputs ğ‘¥ that are not worse-case, which naturally happens often throughout the training process, the empirical error of the greedy Top-ğ¾ sparsiï¬er can be much smaller than that of its randomized cousin. This has been observed in practice, and this is one of the reasons why greedy compressors, such as Top-ğ¾, are often preferred to their randomized counterparts.

A.3. cRand-ğ¾

Contractive Rand-ğ¾ operator applied to vector ğ‘¥ âˆˆ Rğ‘‘ uniformly at random chooses ğ¾ entries out of ğ‘‘ but, unlike Rand-ğ¾, does not scale the resulting vector. In this case, the resulting vector is no more unbiased but it still satisï¬es the deï¬nition of the contractive operator. Indeed, let ğ’® be a set of indices of size ğ¾. Then,

[ï¸ƒ ğ‘‘ âˆ‘ï¸

]ï¸ƒ ğ‘‘ âˆ‘ï¸

ğ‘‘ (ï¸‚ âˆ‘ï¸

ğ¾ )ï¸‚

(ï¸‚ ğ¾ )ï¸‚

[ï¸€â€–ğ’(ğ‘¥) âˆ’ ğ‘¥â€–22]ï¸€ = E

1ğ‘–âˆˆ/ğ’® ğ‘¥2ğ‘– = E [ï¸€1ğ‘–âˆˆ/ğ’® ğ‘¥2ğ‘– ]ï¸€ =

1âˆ’

ğ‘¥2ğ‘– = 1 âˆ’

â€–ğ‘¥â€–22.

E

ğ‘‘

ğ‘‘

ğ‘–=1

ğ‘–=1

ğ‘–=1

A.4. Perm-ğ¾ and cPerm-ğ¾

Permutation compressor (Perm-ğ¾) is described in (Szlendak et al., 2021) (case ğ‘‘ > ğ‘›, Deï¬nition 2 in the original paper). Contractive permutation compressor (cPerm-ğ¾) on top of Perm-ğ¾ scales the resulting vector by factor 1+1ğœ” .

A.5. Unbiased compressors

Rand-ğ¾, as deï¬ned above, arises from a more general class of compressors, which we now present, by appropriate scaling.
Deï¬nition A.1 (Unbiased Compressor). We say that a randomized map ğ’¬ : Rğ‘‘ â†’ Rğ‘‘ is an unbiased compression operator, or simply just unbiased compressor, if there exists a constant ğœ” â‰¥ 0 such that

E [ğ’¬(ğ‘¥)] = ğ‘¥, E [ï¸€â€–ğ’¬(ğ‘¥) âˆ’ ğ‘¥â€–2]ï¸€ â‰¤ ğœ”â€–ğ‘¥â€–2, âˆ€ğ‘¥ âˆˆ Rğ‘‘.

(22)

Its is well known and trivial to check that for any unbiased compressor ğ’¬, the compressor ğœ”+1 1 ğ’¬ is contractive, with contraction parameter ğ›¼ = ğœ”+1 1 . It is easy to see that the contractive Rand-ğ¾ operator deï¬ned above becomes unbiased once it is scaled by the factor ğ¾ğ‘‘ .
A.6. Further examples
For further examples of contractive compressors (e.g., quantization-based, rank-based), we refer the reader to Beznosikov et al. (2020) and Safaryan et al. (2021b;a).

13

B. Proofs of The Main Results

B.1. Three Lemmas

We will rely on two lemmas, one from (RichtaÂ´rik et al., 2021), and one from (Li et al., 2021). The ï¬rst lemma will allow us to simplify the expression for the maximal allowable stepsize in our method (at the cost of being suboptimal by the factor of 2 at most), and the second forms an important step in our convergence proof. Lemma B.1 (Lemma 5 of (RichtaÂ´rik et al., 2021)). If 0 â‰¤ ğ›¾ â‰¤ âˆšğ‘1+ğ‘ , then ğ‘ğ›¾2 + ğ‘ğ›¾ â‰¤ 1. Moreover, the bound is tight up
{ï¸ }ï¸ to the factor of 2 since âˆšğ‘1+ğ‘ â‰¤ min âˆš1ğ‘ , 1ğ‘ â‰¤ âˆšğ‘2+ğ‘ .
Lemma B.2 (Lemma 2 of (Li et al., 2021)). Suppose that function ğ‘“ is ğ¿âˆ’-smooth and let ğ‘¥ğ‘¡+1 := ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡, where ğ‘”ğ‘¡ âˆˆ Rğ‘‘ is any vector, and ğ›¾ > 0 is any scalar. Then we have

ğ‘“ (ğ‘¥ğ‘¡+1) â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ )ï¸‚ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 + ğ›¾ âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 . (23)

2

2ğ›¾ 2

2

We now state and derive the main technical lemma.

Lemma B.3 (Lemma 5.4). Let Assumption 5.3 hold. Consider the method from (12)â€“(13). Then, for all ğ‘¡ â‰¥ 0 the sequence

ğ‘¡ 1 âˆ‘ğ‘›ï¸ âƒ¦ ğ‘¡

ğ‘¡ âƒ¦2

ğº := ğ‘›

âƒ¦ğ‘”ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

(24)

ğ‘–=1

satisï¬es

E

[ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğ´)E

[ï¸€ğºğ‘¡]ï¸€

+

ğµğ¿2+E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

.

(25)

Proof. By deï¬nition of ğºğ‘¡ and three points compressor we have

E [ï¸€ğºğ‘¡+1]ï¸€

=
(13),(6)
â‰¤
=

1

ğ‘›
âˆ‘ï¸

[ï¸ âƒ¦ ğ‘¡+1

ğ‘¡+1 âƒ¦2]ï¸

ğ‘› E âƒ¦ğ‘”ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

ğ‘–=1

1 âˆ’ ğ´ âˆ‘ğ‘›ï¸ [ï¸âƒ¦ ğ‘¡

ğ‘¡ âƒ¦2]ï¸ ğµ âˆ‘ğ‘›ï¸ âƒ¦

ğ‘¡+1

ğ‘¡ âƒ¦2

ğ‘›

E âƒ¦ğ‘”ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

+ ğ‘›

âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ ) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

ğ‘–=1

ğ‘–=1

ğµ

ğ‘›
âˆ‘ï¸

2

(1 âˆ’ ğ´)E [ï¸€ğºğ‘¡]ï¸€ +

âƒ¦ âƒ¦âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡

âƒ¦ )âƒ¦

.

ğ‘› ğ‘–=1

Using

Assumption

5.3,

we

upper

bound

the

last

term

by

ğµğ¿2+E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

and

get

the

result.

B.2. General Non-Convex Functions

Below we restate the main result for general non-convex functions and provide the full proof.

Theorem B.4 (Theorem 5.5). Let Assumptions 5.1, 5.2, 5.3 hold. Assume that the stepsize ğ›¾ of the method from (12)â€“(13) âˆšï¸€
satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + ğ¿+ ğµ/ğ´. Then, for any ğ‘‡ â‰¥ 0 we have

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (26)

ğ›¾ğ‘‡

ğ´ğ‘‡

where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by (12)â€“(13), âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

Proof. Using Lemma B.2 and Jensenâ€™s inequality applied of the squared norm, we get

(ï¸‚

)ï¸‚

âƒ¦ğ‘›

âƒ¦2

ğ‘“ (ğ‘¥ğ‘¡+1) (â‰¤23) ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾2 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ 21ğ›¾ âˆ’ ğ¿2âˆ’ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 + ğ›¾2 âƒ¦âƒ¦âƒ¦ ğ‘›1 âˆ‘ï¸ (ï¸€ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡))ï¸€âƒ¦âƒ¦âƒ¦

âƒ¦ ğ‘–=1

âƒ¦

(â‰¤15) ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ )ï¸‚ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 + ğ›¾ ğºğ‘¡. (27)

2

2ğ›¾ 2

2

14

Subtracting ğ‘“ inf from both sides of (27) and taking expectation, we get

E [ï¸€ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ inf ]ï¸€ â‰¤

E [ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf ]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ 2

âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ )ï¸‚ E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ . (28)

2ğ›¾ 2

2

Next, we add (28) to a 2ğ›¾ğ´ multiple of (16) and derive

E [ï¸€ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ inf ]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf ]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ )ï¸‚ E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

2ğ´

2

2ğ›¾ 2

+ ğ›¾2 E [ï¸€ğºğ‘¡]ï¸€ + 2ğ›¾ğ´ (ï¸(1 âˆ’ ğ´)E [ï¸€ğºğ‘¡]ï¸€ + ğµğ¿2+E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸)ï¸

= E [ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf ]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

2ğ´

2

âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ âˆ’ ğ›¾ğµğ¿2+ )ï¸‚ E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

2ğ›¾ 2

2ğ´

â‰¤ E [ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf ]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ ,

2ğ´

2

where the last inequality follows from the bound ğ›¾2 ğµğ´ğ¿2+ +ğ¿âˆ’ğ›¾ â‰¤ 1, which holds because of Lemma B.1 and our assumption on the stepsize. Summing up inequalities for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1, we get

0 â‰¤ E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ inf ]ï¸€ + ğ›¾ E [ï¸€ğºğ‘‡ ]ï¸€ â‰¤ E [ï¸€ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf ]ï¸€ + ğ›¾ E [ï¸€ğº0]ï¸€ âˆ’ ğ›¾ ğ‘‡âˆ‘âˆ’ï¸1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ .

2ğ´

2ğ´

2

ğ‘¡=0

Multiplying both sides by ğ›¾2ğ‘‡ , after rearranging we obtain

ğ‘‡âˆ‘âˆ’ï¸1 1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ ,

ğ‘‡

ğ›¾ğ‘‡

ğ´ğ‘‡

ğ‘¡=0

where

âˆ†0

=

ğ‘“ (ğ‘¥0)

âˆ’

ğ‘“ inf .

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[ï¸

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)âƒ¦âƒ¦2

]ï¸ ,

where

ğ‘¥Ë†ğ‘‡

is

chosen from ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random.

B.3. PÅ Functions

Below we restate the main result for PÅ functions and provide the full proof.

Theorem B.5 (Theorem 5.8). Let Assumptions 5.1, 5.2, 5.3, 5.7 hold. Assume that the stepsize ğ›¾ of the method from (12)â€“

{ï¸

}ï¸

(13) satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = max

âˆšï¸€ ğ¿âˆ’ + ğ¿+ 2ğµ/ğ´, ğ´/2ğœ‡

. Then, for any ğ‘‡ â‰¥ 0 and âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ (ğ‘¥*)

we have

E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€

â‰¤

(1 âˆ’ ğ›¾ğœ‡)ğ‘‡

(ï¸ âˆ†0

+

ğ›¾

)ï¸ E [ï¸€ğº0]ï¸€ .

(29)

ğ´

Proof. First of all, we notice that (28) holds in this case as well. Therefore, using PÅ condition we derive

(28)
E [ï¸€ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ inf ]ï¸€ â‰¤

E [ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ 2

âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ )ï¸‚ E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€

2ğ›¾ 2

2

(â‰¤19) (1 âˆ’ ğ›¾ğœ‡)E [ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ )ï¸‚ E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ . (30)

2ğ›¾ 2

2

15

Next, we add (28) to a ğ´ğ›¾ multiple of (16) and derive

E [ï¸ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ (ğ‘¥*) + ğ›¾ ğºğ‘¡+1]ï¸ â‰¤ (1 âˆ’ ğ›¾ğœ‡)E [ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ )ï¸‚ E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

ğ´

2ğ›¾ 2

+ ğ›¾2 E [ï¸€ğºğ‘¡]ï¸€ + ğ´ğ›¾ (ï¸(1 âˆ’ ğ´)E [ï¸€ğºğ‘¡]ï¸€ + ğµğ¿2+E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸)ï¸

=

(1

âˆ’

ğ›¾ğœ‡)E [ï¸€ğ‘“ (ğ‘¥ğ‘¡)

âˆ’

ğ‘“ (ğ‘¥*)]ï¸€

+

(ï¸‚ 1

âˆ’

ğ´ )ï¸‚

ğ›¾

E [ï¸€ğºğ‘¡]ï¸€

2ğ´

âˆ’ (ï¸‚ 1 âˆ’ ğ¿âˆ’ âˆ’ ğ›¾ğµğ¿2+ )ï¸‚ E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

2ğ›¾ 2

ğ´

â‰¤

[ï¸ (1 âˆ’ ğ›¾ğœ‡)E ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥*) +

ğ›¾

]ï¸ ğºğ‘¡ ,

ğ´

where the last inequality follows from the bound ğ›¾2 2ğµğ´ğ¿2+ + ğ¿âˆ’ğ›¾ â‰¤ 1 and 1 âˆ’ ğ´/2 â‰¤ 1 âˆ’ ğ›¾ğœ‡, which holds because of our assumption on the stepsize and Lemma B.1. Unrolling the recurrence, we obtain

E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€

â‰¤

[ï¸ E ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*) +

ğ›¾

]ï¸ ğºğ‘‡

â‰¤

(1 âˆ’ ğ›¾ğœ‡)ğ‘‡

(ï¸ âˆ†0

[ï¸ ğ›¾ +E

]ï¸)ï¸ ğº0 .

ğ´

ğ´

16

C. Three Point Compressor: Special Cases
In this section, we show that several known approaches to compressed communication can be viewed as special cases of our framework (12)â€“(13). Moreover, we design several new methods ï¬tting our scheme. Please refer to Table 1 for an overview.

C.1. Error Feedback 2021: EF21

Algorithm 2 Error Feedback 2021 (EF21)

1: Input: starting point ğ‘¥0, stepsize ğ›¾, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›]

2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Broadcast ğ‘”ğ‘¡ to all workers

4: for ğ‘– = 1, . . . ,ğ‘› in parallel do

5:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

6:

Set ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡)

7: end for

8:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

=

ğ‘”ğ‘¡

+

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ’

(âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

ğ‘”ğ‘–ğ‘¡

)

9: end for

10: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

The next lemma shows that EF21 uses a special three point compressor. Lemma C.1. The compressor

ğ’â„,ğ‘¦(ğ‘¥) := â„ + ğ’(ğ‘¥ âˆ’ â„),

(31)

satisï¬es (6) with ğ´ := 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ) and ğµ := (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€, where ğ‘  > 0 is such that (1 âˆ’ ğ›¼) (1 + ğ‘ ) < 1.

Proof. By deï¬nition of ğ’â„,ğ‘¦(ğ‘¥) and ğ’ we have

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

E

[ï¸ â€–ğ’

(ğ‘¥

âˆ’

â„)

âˆ’

(ğ‘¥

âˆ’

â„)â€–2

]ï¸

â‰¤ (1 âˆ’ ğ›¼) â€–ğ‘¥ âˆ’ â„â€–2 = (1 âˆ’ ğ›¼) â€–(ğ‘¥ âˆ’ ğ‘¦) + (ğ‘¦ âˆ’ â„)â€–2 â‰¤ (1 âˆ’ ğ›¼) (1 + ğ‘ ) â€–â„ âˆ’ ğ‘¦â€–2 + (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ â€–ğ‘¥ âˆ’ ğ‘¦â€–2 .

Therefore, EF21 ï¬ts our framework. Using our general analysis (Theorems 5.5 and 5.8) we derive the following result.
Theorem C.2. EF21 is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (31) and ğ´ = ğ›¼ âˆ’ ğ‘ (1 âˆ’ ğ›¼) and ğµ = (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€, where ğ‘  > 0 is such that (1 âˆ’ ğ›¼)(1 + ğ‘ ) < 1.

1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + âˆšï¸
ğ¿+ (1âˆ’ğ›¼)(1+ğ‘ âˆ’1)/(ğ›¼âˆ’ğ‘ (1âˆ’ğ›¼)), then for any ğ‘‡ â‰¥ 0 we have

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (32) ğ›¾ğ‘‡ (ğ›¼ âˆ’ ğ‘ (1 âˆ’ ğ›¼))ğ‘‡
where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by EF21, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

2. If additionaly Assumption 5.7 hold and 0

â‰¤

ğ›¾

â‰¤

1/ğ‘€ for ğ‘€

=

{ï¸‚

âˆšï¸

}ï¸‚

max ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ›¼)(1+ğ‘ âˆ’1)/(ğ›¼âˆ’ğ‘ (1âˆ’ğ›¼)), (ğ›¼âˆ’ğ‘ (1âˆ’ğ›¼))/2ğœ‡ , then for any ğ‘‡ â‰¥ 0 we have

(ï¸‚ E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ âˆ†0 +

ğ›¾

)ï¸‚ E [ï¸€ğº0]ï¸€ .

(33)

ğ›¼ âˆ’ ğ‘ (1 âˆ’ ğ›¼)

17

Guided by exactly the same arguments as we use in the anlysis pf 3PCv5, we consider ğµ/ğ´ as a function of ğ‘  and optimizing this function in ğ‘  and ï¬nd the optimal value of this ratio.

Lemma C.3. The optimal value of

ğµ

(1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€

(ğ‘ ) =

ğ´

(ğ›¼ âˆ’ ğ‘ (1 âˆ’ ğ›¼))

under the constraint 0 < ğ‘  < ğ›¼/(1âˆ’ğ›¼) equals

ğµ

(1 âˆ’ ğ›¼)

4(1 âˆ’ ğ›¼)

ğ´ (ğ‘ *) = (1 âˆ’ âˆš1 âˆ’ ğ›¼)2 â‰¤ ğ›¼2

and

it

is

achieved

at

ğ‘ *

=

âˆ’1

+

âˆšï¸€ 1/(1âˆ’ğ›¼).

Proof. First of all, we ï¬nd the derivative of the considered function:

(ï¸‚ ğµ )ï¸‚â€²

(1 âˆ’ ğ›¼)ğ‘ 2 + 2(1 âˆ’ ğ›¼)ğ‘  âˆ’ ğ›¼

ğ´ (ğ‘ ) = (1 âˆ’ ğ›¼) (ğ›¼ğ‘  âˆ’ ğ‘ 2(1 âˆ’ ğ›¼))2 .

âˆšï¸€

âˆšï¸€

The function has 2 critical points: âˆ’1 Â± 1/(1âˆ’ğ›¼). Moreover, the derivative is non-positive for ğ‘  âˆˆ (0, âˆ’1 + 1/(1âˆ’ğ›¼)] and

âˆšï¸€ negative for ğ‘  âˆˆ (âˆ’1 + 1/(1âˆ’ğ›¼), +âˆ). This implies that the optimal value on the interval ğ‘  âˆˆ (0, ğ›¼/(1âˆ’ğ›¼)) is achieved at

âˆšï¸€ ğ‘ * = âˆ’1 + 1/(1âˆ’ğ›¼). Via simple computations one can verify that

âˆš Finally, since 1 âˆ’ 1 âˆ’ ğ›¼ â‰¥ ğ›¼/2, we have

ğµ (ğ‘ *) =

(1 âˆ’ ğ›¼)

âˆš

.

ğ´

(1 âˆ’ 1 âˆ’ ğ›¼)2

ğµ

4(1 âˆ’ ğ›¼)

ğ´ (ğ‘ *) â‰¤ ğ›¼2 .

Using this and Corollaries 5.6, 5.9, we get the following complexity results. âˆšï¸€
Corollary C.4. 1. Let the assumptions from the ï¬rst part of Theorem C.2 hold, ğ‘  = ğ‘ * = âˆ’1 + 1/(1âˆ’ğ›¼), and

1 ğ›¾ = ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ›¼)/(1âˆ’âˆš1âˆ’ğ›¼)2 .

Then for any ğ‘‡ we have

[ï¸

2]ï¸ 2âˆ†0 (ï¸ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ›¼)/(1âˆ’âˆš1âˆ’ğ›¼)2)ï¸

E [ï¸€ğº0]ï¸€

E

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

âƒ¦ )âƒ¦

â‰¤

+âˆš

,

ğ‘‡

(1 âˆ’ 1 âˆ’ ğ›¼)ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

â›

âˆ†0

(ï¸ ğ¿âˆ’

+

)ï¸ âˆšï¸€ ğ¿+ (1âˆ’ğ›¼)/ğ›¼2

â E [ï¸€ğº0]ï¸€

ğ‘‡ = ğ’ª â ğœ€2

+ ğ›¼ğœ€2 â 

(34)

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem C.2 hold and

{ï¸ƒ

âˆš }ï¸ƒ

1

1âˆ’ 1âˆ’ğ›¼

ğ›¾ = min ğ¿âˆ’ + ğ¿+âˆšï¸€2(1âˆ’ğ›¼)/(1âˆ’âˆš1âˆ’ğ›¼)2 , 2ğœ‡ .

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

(ï¸ƒ

{ï¸ƒ

âˆšï¸€

}ï¸ƒ

ğ¿âˆ’ + ğ¿+ (1âˆ’ğ›¼)/ğ›¼2

âˆ†0 + E [ï¸€ğº0]ï¸€ ğ›¾/ğ›¼ )ï¸ƒ

ğ’ª max

, ğ›¼ log

(35)

ğœ‡

ğœ€

iterations/communication rounds.

18

C.2. LAG: Lazily Aggregated Gradient

Algorithm 3 LAG: Lazily Aggregated Gradient

1: Input: starting point ğ‘¥0, stepsize ğ›¾, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›], trigger parameter ğœ > 0

2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Broadcast ğ‘”ğ‘¡ to all workers

4: for ğ‘– = 1, . . . ,ğ‘› in parallel do

5:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

{ï¸ƒ

ğ‘¡+1 âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1), if â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡â€–2 > ğœâ€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2,

6: Set ğ‘”ğ‘– = ğ‘”ğ‘–ğ‘¡,

otherwise

7: end for

8:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

9: end for

10: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

The next lemma shows that LAG is a special three point compressor. Lemma C.5. The compressor

{ï¸ƒ

ğ‘¥, if â€–ğ‘¥ âˆ’ â„â€–2 > ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2,

ğ’â„,ğ‘¦(ğ‘¥) :=

(36)

â„, otherwise,

satisï¬es (6) with ğ´ := 1 and ğµ := ğœ.

Proof. If â€–ğ‘¥ âˆ’ â„â€–2 â‰¤ ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2, then we have

â€–ğ’â„,ğ‘¦(ğ‘¥) âˆ’ ğ‘¥â€–2 = â€–â„ âˆ’ ğ‘¥â€–2 â‰¤ ğœ â€–ğ‘¥ âˆ’ ğ‘¦â€–2 .

Otherwise,

â€–ğ’â„,ğ‘¦(ğ‘¥) âˆ’ ğ‘¥â€–2 = â€–ğ‘¥ âˆ’ ğ‘¥â€–2 = 0 â‰¤ ğœ â€–ğ‘¥ âˆ’ ğ‘¦â€–2 .

Therefore, LAG ï¬ts our framework. Using our general analysis (Theorems 5.5 and 5.8) we derive the following result. Theorem C.6. LAG is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (36) and ğ´ = 1 and ğµ = ğœ.

âˆš 1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + ğ¿+ ğœ, then for any

ğ‘‡ â‰¥ 0 we have

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (37)

ğ›¾ğ‘‡

ğ‘‡

where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by LAG, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

2. If additionaly Assumption 5.7 hold and 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€ for ğ‘€ = max {ï¸€ğ¿âˆ’ + ğ¿+âˆš2ğœ, 1/2ğœ‡}ï¸€, then for any ğ‘‡ â‰¥ 0 we

have

E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ (ï¸€âˆ†0 + ğ›¾E [ï¸€ğº0]ï¸€)ï¸€ .

(38)

Using this and Corollaries 5.6, 5.9, we get the following complexity results. Corollary C.7. 1. Let the assumptions from the ï¬rst part of Theorem C.6 hold, and
ğ›¾= 1 âˆš . ğ¿âˆ’ + ğ¿+ ğœ

19

Then for any ğ‘‡ > 1 we have

[ï¸

2]ï¸ 2âˆ†0(ğ¿ + ğ¿ âˆšğœ) E [ï¸€ğº0]ï¸€

E

âƒ¦ âƒ¦âˆ‡

ğ‘“

(

ğ‘¥Ë†

ğ‘‡

âƒ¦ )âƒ¦

â‰¤

âˆ’

+

+

,

ğ‘‡

ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

ğ‘‡ = ğ’ª (ï¸ƒ âˆ†0(ğ¿âˆ’ + ğ¿+âˆšğœ) + E [ï¸€ğº0]ï¸€ )ï¸ƒ (39)

ğœ€2

ğœ€2

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem C.6 hold and

{ï¸‚

1

1 }ï¸‚

ğ›¾ = min

âˆš, .

ğ¿âˆ’ + ğ¿+ ğœ 2ğœ‡

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

ğ’ª (ï¸ƒ ğ¿âˆ’ + ğ¿+âˆšğœ log âˆ†0 + E [ï¸€ğº0]ï¸€ ğ›¾ )ï¸ƒ (40)

ğœ‡

ğœ€

iterations/communication rounds.

Proof. Both claims are straight-forward applications of Corollaries 5.6, 5.9. In the second claim, we used that

âˆš

ğ¿âˆ’ + ğ¿+ ğœ â‰¥ ğ¿âˆ’ â‰¥ 1,

ğœ‡

ğœ‡

where the second inequality holds since ğ¿âˆ’ â‰¥ ğœ‡ (Nesterov et al., 2018).

20

C.3. CLAG: Compressed Lazily Aggregated Gradient (NEW)

Algorithm 4 CLAG: Compressed Lazily Aggregated Gradient

1: Input: starting point ğ‘¥0, stepsize ğ›¾, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›], trigger parameter ğœ > 0

2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Broadcast ğ‘”ğ‘¡ to all workers

4: for ğ‘– = 1, . . . ,ğ‘› in parallel do

5:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

{ï¸ƒ

ğ‘¡+1 ğ‘”ğ‘–ğ‘¡ + ğ’ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡)ï¸€ , if â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡â€–2 > ğœâ€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2,

6: Set ğ‘”ğ‘– = ğ‘”ğ‘–ğ‘¡,

otherwise

7: end for

8:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

9: end for

10: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

The next lemma shows that CLAG uses a special three points compressor. Lemma C.8. The compressor

{ï¸ƒ

â„ + ğ’(ğ‘¥ âˆ’ â„), if â€–ğ‘¥ âˆ’ â„â€–2 > ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2,

ğ’â„,ğ‘¦(ğ‘¥) :=

(41)

â„,

otherwise,

satisï¬es (6) with ğ´ := 1âˆ’(1âˆ’ğ›¼)(1+ğ‘ ) and ğµ := max {ï¸€(1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ , ğœ}ï¸€, where ğ‘  > 0 is such that (1âˆ’ğ›¼) (1 + ğ‘ ) < 1.

Proof. First of all, if â€–ğ‘¥ âˆ’ â„â€–2 â‰¤ ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2, then we have

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

â€–â„ âˆ’ ğ‘¥â€–2 â‰¤ ğœ â€–ğ‘¥ âˆ’ ğ‘¦â€–2 .

Next, if â€–ğ‘¥ âˆ’ â„â€–2 > ğœâ€–ğ‘¥ âˆ’ ğ‘¦â€–2, then using the deï¬nition of ğ’â„,ğ‘¦(ğ‘¥) and ğ’, we derive

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

E

[ï¸ â€–ğ’

(ğ‘¥

âˆ’

â„)

âˆ’

(ğ‘¥

âˆ’

â„)â€–2

]ï¸

â‰¤ (1 âˆ’ ğ›¼) â€–ğ‘¥ âˆ’ â„â€–2 = (1 âˆ’ ğ›¼) â€–(ğ‘¥ âˆ’ ğ‘¦) + (ğ‘¦ âˆ’ â„)â€–2 â‰¤ (1 âˆ’ ğ›¼) (1 + ğ‘ ) â€–â„ âˆ’ ğ‘¦â€–2 + (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ â€–ğ‘¥ âˆ’ ğ‘¦â€–2 .

Therefore, CLAG ï¬ts our framework. Using our general analysis (Theorems 5.5 and 5.8) we derive the following result. Theorem C.9. CLAG is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (41) and ğ´ = ğ›¼ âˆ’ ğ‘ (1 âˆ’ ğ›¼) and ğµ = max {ï¸€(1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ , ğœ}ï¸€, where ğ‘  > 0 is such that (1 âˆ’ ğ›¼)(1 + ğ‘ ) < 1.
1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + âˆšï¸
ğ¿+ max{(1âˆ’ğ›¼)(1+ğ‘ âˆ’1),ğœ}/(ğ›¼âˆ’ğ‘ (1âˆ’ğ›¼)), then for any ğ‘‡ â‰¥ 0 we have
E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (42) ğ›¾ğ‘‡ (ğ›¼ âˆ’ ğ‘ (1 âˆ’ ğ›¼))ğ‘‡
where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by CLAG, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).
21

2. If additionaly Assumption 5.7 hold and 0

â‰¤

ğ›¾

â‰¤

1/ğ‘€ for ğ‘€

=

{ï¸‚

âˆšï¸

}ï¸‚

max ğ¿âˆ’ + ğ¿+ 2 max{(1âˆ’ğ›¼)(1+ğ‘ âˆ’1),ğœ}/(ğ›¼âˆ’ğ‘ (1âˆ’ğ›¼)), (ğ›¼âˆ’ğ‘ (1âˆ’ğ›¼))/2ğœ‡ , then for any ğ‘‡ â‰¥ 0 we have

(ï¸‚ E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ âˆ†0 +

ğ›¾

)ï¸‚ E [ï¸€ğº0]ï¸€ .

(43)

ğ›¼ âˆ’ ğ‘ (1 âˆ’ ğ›¼)

Using this and Corollaries 5.6, 5.9, we get the following complexity results. âˆšï¸€
Corollary C.10. 1. Let the assumptions from the ï¬rst part of Theorem C.9 hold, ğ‘  = ğ‘ * = âˆ’1 + 1/(1âˆ’ğ›¼), and
1 ğ›¾ = âˆšï¸‚ .
ğ¿âˆ’ + ğ¿+ max {ï¸ (1âˆ’(âˆš1âˆ’1ğ›¼âˆ’)ğ›¼)2 , 1âˆ’âˆšğœ1âˆ’ğ›¼ }ï¸

Then for any ğ‘‡ we have

(ï¸‚

âˆšï¸‚

)ï¸‚

{ï¸ 2âˆ†0 ğ¿âˆ’ + ğ¿+ max

(âˆš1âˆ’ğ›¼)

,

âˆšğœ

}ï¸

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

(1âˆ’ 1âˆ’ğ›¼)2 1âˆ’ 1âˆ’ğ›¼
+

E [ï¸€ğº0]ï¸€

âˆš

,

ğ‘‡

(1 âˆ’ 1 âˆ’ ğ›¼)ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

â› (ï¸‚

âˆšï¸‚

)ï¸‚

â

âˆ†0 ğ¿âˆ’ + ğ¿+ max {ï¸ (1ğ›¼âˆ’2ğ›¼) , ğ›¼ğœ }ï¸

E [ï¸€ğº0]ï¸€

âœ

âŸ

ğ‘‡ = ğ’ª âœâ ğœ€2

+ ğ›¼ğœ€2 âŸâ 

(44)

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem C.9 hold and

â§

â«

âª

âˆšâª

âª â¨

1

1âˆ’

1

âˆ’

ğ›¼

âª â¬

ğ›¾ = min

,

.

âª

âˆšï¸‚ {ï¸ 2(1âˆ’ğ›¼)

}ï¸
ğœ

2ğœ‡ âª

âª â©

ğ¿âˆ’

+

ğ¿+

max

âˆš (1âˆ’ 1âˆ’ğ›¼)2

,

âˆš 1âˆ’ 1âˆ’ğ›¼

âª â­

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

â›â§

âˆšï¸‚

â«

â

âª âª

ğ¿âˆ’

+

ğ¿+

max {ï¸ (1âˆ’2ğ›¼) , ğœ }ï¸

âª âª

0 [ï¸€ 0]ï¸€

â¨ âœ

ğ›¼ ğ›¼ â¬ âˆ† + E ğº ğ›¾/ğ›¼ âŸ

ğ’ª âœmax

, ğ›¼ log

âŸ

(45)

ââª ğœ‡

âª ğœ€â 

âª

âª

â©

â­

iterations/communication rounds.

22

C.4. 3PCv1 (NEW) Out of theoretical curiosity, we consider the following theoretical method.

Algorithm 5 Error Feedback 2021 â€“ gradient shift version (3PCv1)

1: Input: starting point ğ‘¥0, stepsize ğ›¾, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›]

2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Broadcast ğ‘”ğ‘¡ to all workers

4: for ğ‘– = 1, . . . ,ğ‘› in parallel do

5:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

6:

Set ğ‘”ğ‘–ğ‘¡+1 = âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) + ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡))

7: end for

8:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

9: end for

10: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

3PCv1 is impractical since the the compression does not help to reduce the cost of one iteration. Indeed, the server does not know the shifts âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) and the workers have to send them as well at each iteration.
Nevertheless, one can consider 3PCv1 as an ideal version of EF21. To illustrate that we derive the following lemma. Lemma C.11. The compressor

ğ’â„,ğ‘¦(ğ‘¥) := ğ‘¦ + ğ’(ğ‘¥ âˆ’ ğ‘¦),

(46)

satisï¬es (6) with ğ´ := 1 and ğµ := 1 âˆ’ ğ›¼.

Proof. By deï¬nition of ğ’â„,ğ‘¦(ğ‘¥) and ğ’ we have

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

E

[ï¸ â€–ğ’

(ğ‘¥

âˆ’

ğ‘¦

)

âˆ’

(ğ‘¥

âˆ’

ğ‘¦

)â€–2

]ï¸

â‰¤ (1 âˆ’ ğ›¼) â€–ğ‘¥ âˆ’ ğ‘¦â€–2 .

Therefore, 3PCv1 ï¬ts our framework. Using our general analysis (Theorems 5.5 and 5.8) we derive the following result.
Theorem C.12. 3PCv1 is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (46) and ğ´ = 1 and ğµ = 1 âˆ’ ğ›¼.

âˆš 1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + ğ¿+ 1 âˆ’ ğ›¼, then for any

ğ‘‡ â‰¥ 0 we have

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (47)

ğ›¾ğ‘‡

ğ‘‡

where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by 3PCv1, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

{ï¸

}ï¸

âˆšï¸€

2. If additionaly Assumption 5.7 hold and 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€ for ğ‘€ = max ğ¿âˆ’ + ğ¿+ 2(1 âˆ’ ğ›¼), 1/2ğœ‡ , then for any ğ‘‡ â‰¥ 0

we have

E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ (ï¸€âˆ†0 + ğ›¾E [ï¸€ğº0]ï¸€)ï¸€ .

(48)

Using this and Corollaries 5.6, 5.9, we get the following complexity results. Corollary C.13. 1. Let the assumptions from the ï¬rst part of Theorem C.12 hold and
ğ›¾ = 1âˆš . ğ¿âˆ’ + ğ¿+ 1 âˆ’ ğ›¼

23

Then for any ğ‘‡ we have

âˆš

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 (ï¸€ğ¿âˆ’ + ğ¿+ 1 âˆ’ ğ›¼)ï¸€ + E [ï¸€ğº0]ï¸€ ,

ğ‘‡

ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

(ï¸ƒ

âˆš

)ï¸ƒ

âˆ†0 (ï¸€ğ¿âˆ’ + ğ¿+ 1 âˆ’ ğ›¼)ï¸€ E [ï¸€ğº0]ï¸€

ğ‘‡ =ğ’ª

ğœ€2 + ğœ€2

(49)

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem C.12 hold and

{ï¸ƒ

}ï¸ƒ

1

1

ğ›¾ = min

,.

âˆšï¸€ ğ¿âˆ’ + ğ¿+ 2(1 âˆ’ ğ›¼)

2ğœ‡

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

ğ’ª (ï¸ƒ ğ¿âˆ’ + ğ¿+âˆš1 âˆ’ ğ›¼ log âˆ†0 + ğ›¾E [ï¸€ğº0]ï¸€ )ï¸ƒ (50)

ğœ‡

ğœ€

iterations/communication rounds.

24

C.5. 3PCv2 (NEW)

Algorithm 6 3PCv2

1: Input: starting point ğ‘¥0, stepsize ğ›¾, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›]

2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Broadcast ğ‘”ğ‘¡ to all workers

4: for ğ‘– = 1, . . . ,ğ‘› in parallel do

5:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

6:

Compute ğ‘ğ‘¡ğ‘– = ğ‘”ğ‘–ğ‘¡ + ğ’¬(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡))

7:

Set ğ‘”ğ‘–ğ‘¡+1 = ğ‘ğ‘¡ğ‘– + ğ’ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘ğ‘¡ğ‘–)ï¸€

8: end for

9:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

10: end for

11: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘˜=âˆ’01

Lemma C.14. The compressor

ğ’â„,ğ‘¦(ğ‘¥) := ğ‘ + ğ’ (ğ‘¥ âˆ’ ğ‘) , where ğ‘ = â„ + ğ’¬(ğ‘¥ âˆ’ ğ‘¦),

(51)

satisï¬es (6) with ğ´ := ğ›¼ and ğµ := (1 âˆ’ ğ›¼)ğœ”.

Proof. By deï¬nition of ğ’â„,ğ‘¦(ğ‘¥), ğ’, and ğ’¬ we have

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

E

[ï¸ E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸]ï¸ |ğ‘

=

E

[ï¸ E

[ï¸ â€–ğ‘

+

ğ’

(ğ‘¥

âˆ’

ğ‘)

âˆ’

ğ‘¥â€–2

]ï¸]ï¸ |ğ‘

â‰¤

E

[ï¸ (1

âˆ’

ğ›¼)

â€–ğ‘¥

âˆ’

ğ‘â€–2

]ï¸

=

(1

âˆ’

ğ›¼)E

[ï¸ â€–ğ’¬(ğ‘¥

âˆ’

ğ‘¦

)

âˆ’

(ğ‘¥

âˆ’

â„)â€–2

]ï¸

=

(1

âˆ’

ğ›¼)

[ï¸ E

[ï¸ â€–ğ’¬(ğ‘¥

âˆ’

ğ‘¦

)

âˆ’

(ğ‘¥

âˆ’

ğ‘¦

)â€–2

]ï¸

+

â€–â„

âˆ’

ğ‘¦

â€–2

]ï¸

â‰¤ (1 âˆ’ ğ›¼) â€–â„ âˆ’ ğ‘¦â€–2 + (1 âˆ’ ğ›¼)ğœ” â€–ğ‘¥ âˆ’ ğ‘¦â€–2 .

Therefore, 3PCv2 ï¬ts our framework. Before we formulate the main results for 3PCv2, we make several remarks on the proposed method. First of all, with 3PCv2, we need to communicate two compressed vectors: ğ’¬(ğ‘¥ âˆ’ ğ‘¦) and ğ’ (ğ‘¥ âˆ’ (â„ + ğ’¬(ğ‘¥ âˆ’ ğ‘¦))). This is similar to how the induced compressor works (HorvaÂ´th & RichtaÂ´rik, 2020), but 3PCv2 compressor is not unbiased. If we set ğ’¬ â‰¡ 0 (this compressor is not unbiased, so the above formulas for ğ´ and ğµ do not apply) and allow ğ’ to be arbitrary, we obtain EF21 (RichtaÂ´rik et al., 2021). Next, if we set

{ï¸ƒ

ğ‘¥ with probability ğ‘

ğ’(ğ‘¥) =

,

(52)

0 with probability 1 âˆ’ ğ‘

and allow ğ’¬ to be arbitrary, we obtain MARINA (Gorbunov et al., 2021). Note that ğ’ deï¬ned above is biased since

E [ğ’(ğ‘¥)]

=

ğ‘ğ‘¥,

and

the

variance

inequality

is

satisï¬ed

as

an

identity:

E

[ï¸ â€–ğ’

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

(1 âˆ’ ğ‘) â€–ğ‘¥â€–2.

By

choosing

a different biased compressor ğ’, e.g., Top-ğ¾, we obtain a new variant of MARINA. In particular, unlike MARINA, this

compressor never needs to communicate full gradients, which can be important in some cases.

Using our general analysis (Theorems 5.5 and 5.8) we derive the following result.
Theorem C.15. 3PCv2 is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (51) and ğ´ = ğ›¼ and ğµ = (1 âˆ’ ğ›¼)ğœ”.

25

âˆšï¸€ 1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + ğ¿+ (1âˆ’ğ›¼)ğœ”/ğ›¼, then for

any ğ‘‡ â‰¥ 0 we have

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (53)

ğ›¾ğ‘‡

ğ›¼ğ‘‡

where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by 3PCv2, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

{ï¸

}ï¸

âˆšï¸€

2. If additionaly Assumption 5.7 hold and 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€ for ğ‘€ = max ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ›¼)ğœ”/ğ›¼, ğ›¼/2ğœ‡ , then for any

ğ‘‡ â‰¥ 0 we have

E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ )

âˆ’

ğ‘“ (ğ‘¥*)]ï¸€

â‰¤

(1

âˆ’

ğ›¾ğœ‡)ğ‘‡

(ï¸ âˆ†0

+

ğ›¾

)ï¸ E [ï¸€ğº0]ï¸€ .

(54)

ğ›¼

Using this and Corollaries 5.6, 5.9, we get the following complexity results. Corollary C.16. 1. Let the assumptions from the ï¬rst part of Theorem C.15 hold and

1 ğ›¾ = âˆšï¸€ .
ğ¿âˆ’ + ğ¿+ (1âˆ’ğ›¼)ğœ”/ğ›¼

Then for any ğ‘‡ we have

(ï¸

)ï¸

[ï¸

2]ï¸ 2âˆ†0 ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ›¼)ğœ”/ğ›¼ E [ï¸€ğº0]ï¸€

E

âƒ¦ âƒ¦âˆ‡

ğ‘“

(

ğ‘¥Ë†

ğ‘‡

âƒ¦ )âƒ¦

â‰¤

+

,

ğ‘‡

ğ›¼ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

â›

âˆ†0

(ï¸ ğ¿âˆ’

+

)ï¸ âˆšï¸€ ğ¿+ (1âˆ’ğ›¼)ğœ”/ğ›¼

â E [ï¸€ğº0]ï¸€

ğ‘‡ = ğ’ª â ğœ€2

+ ğ›¼ğœ€2 â 

(55)

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem C.15 hold and

{ï¸ƒ

}ï¸ƒ

1

ğ›¼

ğ›¾ = min

,.

âˆšï¸€ ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ›¼)ğœ”/ğ›¼

2ğœ‡

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

(ï¸ƒ

{ï¸ƒ

âˆšï¸€

}ï¸ƒ

ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ›¼)ğœ”/ğ›¼

âˆ†0 + E [ï¸€ğº0]ï¸€ ğ›¾/ğ›¼ )ï¸ƒ

ğ’ª max

, ğ›¼ log

(56)

ğœ‡

ğœ€

iterations/communication rounds.

26

C.6. 3PCv3 (NEW)
In this section, we introduce a new method called 3PCv3. It can be seen as a combination of any 3PC compressor with some biased compressor. We also notice that 3PCv2 cannot be obtained as a special case of 3PCv3 as â„ + ğ’¬(ğ‘¥ âˆ’ ğ‘¦) does not satisfy (6).

Algorithm 7 3PCv3

1: Input: starting point ğ‘¥0, stepsize ğ›¾, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›]

2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Broadcast ğ‘”ğ‘¡ to all workers

4: for ğ‘– = 1, . . . ,ğ‘› in parallel do

5:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

6: Compute ğ‘ğ‘¡ğ‘– = ğ’ğ‘”1ğ‘–ğ‘¡,âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1))

7:

Set ğ‘”ğ‘–ğ‘¡+1 = ğ‘ğ‘¡ğ‘– + ğ’ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘ğ‘¡ğ‘–)ï¸€

8: end for

9:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

10: end for

11: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

Lemma C.17. Consider the compressor deï¬ned as

ğ’â„,ğ‘¦(ğ‘¥) := ğ‘ + ğ’(ğ‘¥ âˆ’ ğ‘), where ğ‘ = ğ’â„1,ğ‘¦(ğ‘¥)

(57)

and ğ’â„1,ğ‘¦(ğ‘¥) satisï¬es (6) with some ğ´1 and ğµ1. Then ğ’â„,ğ‘¦(ğ‘¥) satisï¬es (6) with ğ´ := 1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1) and ğµ := (1âˆ’ğ›¼)ğµ1.

Proof. By deï¬nition of ğ’â„1,ğ‘¦(ğ‘¥) and ğ’ we have

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

E

[ï¸ E

[ï¸ â€–ğ‘

+

ğ’(ğ‘¥

âˆ’

ğ‘)

âˆ’

ğ‘¥â€–2

|

]ï¸]ï¸ ğ‘

â‰¤

(1

âˆ’

ğ›¼)E

[ï¸ â€–ğ‘¥

âˆ’

ğ‘â€–2

]ï¸

=

(1

âˆ’

ğ›¼)E

[ï¸ âƒ¦âƒ¦ğ’â„1,ğ‘¦

(ğ‘¥)

âˆ’

âƒ¦2 ğ‘¥âƒ¦

]ï¸

â‰¤ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1)â€–â„ âˆ’ ğ‘¦â€–2 + (1 âˆ’ ğ›¼)ğµ1â€–ğ‘¥ âˆ’ ğ‘¦â€–2.

Therefore, 3PCv3 ï¬ts our framework. Using our general analysis (Theorems 5.5 and 5.8) we derive the following result.
Theorem C.18. 3PCv3 is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (57) and ğ´ := 1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1) and ğµ := (1 âˆ’ ğ›¼)ğµ1.

1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + âˆšï¸€
ğ¿+ (1âˆ’ğ›¼)ğµ1/(1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1)), then for any ğ‘‡ â‰¥ 0 we have

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (58) ğ›¾ğ‘‡ (1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1))ğ‘‡

where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by 3PCv3, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

2. If additionaly Assumption 5.7 hold and 0

â‰¤

ğ›¾

â‰¤

1/ğ‘€ for ğ‘€

=

{ï¸

}ï¸

âˆšï¸€

max ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ›¼)ğµ1/(1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1)), 1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1)/2ğœ‡ , then for any ğ‘‡ â‰¥ 0 we have

(ï¸‚ E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ âˆ†0 +

ğ›¾

)ï¸‚ E [ï¸€ğº0]ï¸€ .

(59)

1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1)

27

Using this and Corollaries 5.6, 5.9, we get the following complexity results. Corollary C.19. 1. Let the assumptions from the ï¬rst part of Theorem C.18 hold and

1

ğ›¾ = âˆšï¸ .

ğ¿âˆ’ + ğ¿+

(1âˆ’ğ›¼)ğµ1 1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1 )

Then for any ğ‘‡ we have

(ï¸

âˆšï¸

)ï¸

[ï¸

2]ï¸

2âˆ†0 ğ¿âˆ’ + ğ¿+

(1âˆ’ğ›¼)ğµ1 1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´ )

E [ï¸€ğº0]ï¸€

E

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

âƒ¦ )âƒ¦

â‰¤

1+

,

ğ‘‡

(1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1))ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

â›

âˆ†0

(ï¸ ğ¿âˆ’

+

)ï¸ âˆšï¸€ ğ¿+ (1âˆ’ğ›¼)ğœ”/ğ›¼

â E [ï¸€ğº0]ï¸€

ğ‘‡ = ğ’ª â ğœ€2

+ (1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1))ğœ€2 â 

(60)

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem C.18 hold and

â§

â«

ğ›¾ = min â¨ 1 , 1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1) â¬ .

âˆšï¸

â© ğ¿âˆ’ + ğ¿+

2(1âˆ’ğ›¼)ğµ1 1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1 )

2ğœ‡ â­

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

â› â§â¨ ğ¿âˆ’ + ğ¿+âˆšï¸ (1âˆ’ğ›¼)ğµ1

â«

â

â¬ âˆ†0 + E [ï¸€ğº0]ï¸€

ğ›¾

ğ’ª âmax

1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1) , 1 âˆ’ (1 âˆ’ ğ›¼)(1 âˆ’ ğ´1) log

1âˆ’(1âˆ’ğ›¼)(1âˆ’ğ´1 )
â 

(61)

â©ğœ‡

â­ğœ€

iterations/communication rounds.

28

C.7. 3PCv4 (NEW)
We now present another special case of 3PC compressor â€“ 3PCv4. This compressor can be seen as modiï¬cation of 3PCv2 that uses only biased compression operators.

Algorithm 8 3PCv4

1: Input: starting point ğ‘¥0, stepsize ğ›¾, number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›]

2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Broadcast ğ‘”ğ‘¡ to all workers

4: for ğ‘– = 1, . . . ,ğ‘› in parallel do

5:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

6:

Compute ğ‘ğ‘¡ğ‘– = ğ‘”ğ‘–ğ‘¡ + ğ’2(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡)

7:

Set ğ‘”ğ‘–ğ‘¡+1 = ğ‘ğ‘¡ğ‘– + ğ’1 (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘ğ‘¡ğ‘–)ï¸€

8: end for

9:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

10: end for

11: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

Lemma C.20. Let ğ’1 and ğ’2 are the contracive compressors with constants ğ›¼1 and ğ›¼2 respectively. Then the compressor deï¬ned as

ğ’â„,ğ‘¦(ğ‘¥) := â„ + ğ’2(ğ‘¥ âˆ’ â„) + ğ’1 (ğ‘¥ âˆ’ (â„ + ğ’2(ğ‘¥ âˆ’ â„)))) ,

(62)

âˆš which satisï¬es (6) with ğ´ := 1 âˆ’ 1 âˆ’ ğ›¼Â¯ and ğµ := 1âˆšâˆ’ğ›¼Â¯ , where ğ›¼Â¯ := 1 âˆ’ (1 âˆ’ ğ›¼1)(1 âˆ’ ğ›¼2).
1âˆ’ 1âˆ’ğ›¼Â¯

Proof. Let ğ‘ := â„ + ğ’2(ğ‘¥ âˆ’ â„). Then

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

=

E

[ï¸ E

[ï¸ â€–ğ’â„,ğ‘¦ (ğ‘¥)

âˆ’

ğ‘¥â€–2

|

]ï¸]ï¸ ğ‘

=

E

[ï¸ E

[ï¸ â€–ğ‘

+

ğ’1(ğ‘¥

âˆ’

ğ‘)

âˆ’

ğ‘¥â€–2

|

]ï¸]ï¸ ğ‘

â‰¤

(1

âˆ’

ğ›¼1

)E

[ï¸ â€–ğ‘¥

âˆ’

ğ‘â€–2

]ï¸

=

(1

âˆ’

ğ›¼1

)E

[ï¸ â€–ğ‘¥

âˆ’

(â„

+

ğ’2

(ğ‘¥

âˆ’

â„))â€–2

]ï¸

â‰¤

(1

âˆ’

ğ›¼1

)(1

âˆ’

ğ›¼2

)E

[ï¸ â€–ğ‘¥

âˆ’

â„â€–2

]ï¸

â‰¤ (1 âˆ’ ğ›¼1)(1 âˆ’ ğ›¼2)(1 + ğ‘ ) â€–â„ âˆ’ ğ‘¦â€–2 + (1 âˆ’ ğ›¼1)(1 âˆ’ ğ›¼2) (ï¸€1 + ğ‘ âˆ’1)ï¸€ â€–ğ‘¥ âˆ’ ğ‘¦â€–2 (63)

Optimal ğ‘  parameter can be found by direct minimization of the fraction (see Lemma C.3)

ğµ(ğ‘ ) (1 âˆ’ ğ›¼Â¯) (ï¸€1 + ğ‘ âˆ’1)ï¸€

=

,

ğ´(ğ‘ ) 1 âˆ’ (1 âˆ’ ğ›¼Â¯)(1 + ğ‘ )

âˆš where ğ›¼Â¯ := 1 âˆ’ (1 âˆ’ ğ›¼1)(1 âˆ’ ğ›¼2). Using Lemma C.3 we ï¬nally obtain ğ´(ğ‘ *) := 1 âˆ’ 1 âˆ’ ğ›¼Â¯ and ğµ(ğ‘ *) := 1âˆšâˆ’ğ›¼Â¯
1âˆ’ 1âˆ’ğ›¼Â¯

Therefore, 3PCv4 ï¬ts our framework. Using our general analysis (Theorems 5.5 and 5.8) we derive the following result. âˆš
Theorem C.21. 3PCv4 is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (51) and ğ´ := 1 âˆ’ 1 âˆ’ ğ›¼Â¯ and ğµ := 1âˆ’1âˆšâˆ’1ğ›¼Â¯âˆ’ğ›¼Â¯ , where ğ›¼Â¯ := 1 âˆ’ (1 âˆ’ ğ›¼1)(1 âˆ’ ğ›¼2).

1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ›¼Â¯)/(1âˆ’âˆš1âˆ’ğ›¼Â¯)2,
then for any ğ‘‡ â‰¥ 0 we have E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + Eâˆš[ï¸€ğº0]ï¸€ , (64) ğ›¾ğ‘‡ (1 âˆ’ 1 âˆ’ ğ›¼Â¯)ğ‘‡

29

where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by 3PCv4, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

2.

If additionaly Assumption 5.7 hold and 0

â‰¤

ğ›¾

â‰¤

1/ğ‘€

for ğ‘€

=

max

{ï¸ ğ¿âˆ’

+

ğ¿+âˆšï¸€2(1âˆ’ğ›¼Â¯)/(1âˆ’âˆš1âˆ’ğ›¼Â¯)2,

âˆš

}ï¸

1âˆ’ 1âˆ’ğ›¼Â¯/2ğœ‡ ,

then for any ğ‘‡ â‰¥ 0 we have

(ï¸‚ E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ âˆ†0 +

ğ›¾ âˆš

)ï¸‚ E [ï¸€ğº0]ï¸€ .

(65)

1 âˆ’ 1 âˆ’ ğ›¼Â¯

Using this and Corollaries 5.6, 5.9, we get the following complexity results. Corollary C.22. 1. Let the assumptions from the ï¬rst part of Theorem C.21 hold and

1 ğ›¾ = ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ›¼Â¯)/(1âˆ’âˆš1âˆ’ğ›¼Â¯)2 .

Then for any ğ‘‡ we have

[ï¸

2]ï¸ 2âˆ†0 (ï¸ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ›¼Â¯)/(1âˆ’âˆš1âˆ’ğ›¼Â¯)2)ï¸

E [ï¸€ğº0]ï¸€

E

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

âƒ¦ )âƒ¦

â‰¤

+âˆš

,

ğ‘‡

(1 âˆ’ 1 âˆ’ ğ›¼Â¯)ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

â›

âˆ†0

(ï¸ ğ¿âˆ’

+

)ï¸ âˆšï¸€ ğ¿+ (1âˆ’ğ›¼Â¯)/ğ›¼Â¯2

â E [ï¸€ğº0]ï¸€

ğ‘‡ = ğ’ª â ğœ€2

+ ğ›¼Â¯ğœ€2 â 

(66)

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem C.21 hold and

{ï¸ƒ

âˆš }ï¸ƒ

1

1 âˆ’ 1 âˆ’ ğ›¼Â¯

ğ›¾ = min ğ¿âˆ’ + ğ¿+âˆšï¸€2(1âˆ’ğ›¼Â¯)/(1âˆ’âˆš1âˆ’ğ›¼Â¯)2 , 2ğœ‡ .

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

(ï¸ƒ

{ï¸ƒ

âˆšï¸€

}ï¸ƒ

ğ¿âˆ’ + ğ¿+ (1âˆ’ğ›¼Â¯)/ğ›¼Â¯2

âˆ†0 + E [ï¸€ğº0]ï¸€ ğ›¾/ğ›¼Â¯ )ï¸ƒ

ğ’ª max

, ğ›¼Â¯ log

(67)

ğœ‡

ğœ€

iterations/communication rounds.

30

C.8. 3PCv5 (NEW) In this section, we consider a version of MARINA that uses biased compression instead of unbiased one.

Algorithm 9 Biased MARINA (3PCv5)

1: Input: starting point ğ‘¥0, stepsize ğ›¾, probability ğ‘ âˆˆ (0,1], number of iterations ğ‘‡ , starting vectors ğ‘”ğ‘–0, ğ‘– âˆˆ [ğ‘›] 2: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

3: Sample ğ‘ğ‘¡ âˆ¼ Be(ğ‘) 4: Broadcast ğ‘”ğ‘¡ to all workers

5: for ğ‘– = 1, . . . ,ğ‘› in parallel do

6:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

{ï¸ƒ

ğ‘¡+1

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1),

if ğ‘ğ‘¡ = 1,

7: Set ğ‘”ğ‘– = ğ‘”ğ‘–ğ‘¡ + ğ’ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)))ï¸€ , if ğ‘ğ‘¡ = 0

8: end for

9:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

10: end for

11: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

The next lemma shows that 3PCv5 uses a special three points compressor. Lemma C.23. The compressor

{ï¸ƒ

ğ‘¥,

w.p. ğ‘

ğ’â„,ğ‘¦(ğ‘¥) =

(68)

â„ + ğ’(ğ‘¥ âˆ’ ğ‘¦), w.p. 1 âˆ’ ğ‘

satisï¬es (6) with ğ´ = ğ‘ âˆ’ ğ‘ (1 âˆ’ ğ‘) and ğµ = (1 âˆ’ ğ‘) (ï¸€1 + ğ‘ âˆ’1)ï¸€ (1 âˆ’ ğ›¼), where ğ‘  > 0 is such that (1 âˆ’ ğ‘)(1 + ğ‘ ) < 1.

Proof. By deï¬nition of ğ’â„,ğ‘¦(ğ‘¥) and ğ’ we have

E

[ï¸ â€–ğ’â„,ğ‘¦

(ğ‘¥)

âˆ’

ğ‘¥â€–2

]ï¸

(=68)

(1

âˆ’

ğ‘)E

[ï¸ â€–â„

+

ğ’

(ğ‘¥

âˆ’

ğ‘¦

)

âˆ’

ğ‘¥â€–2

]ï¸

=

(1

âˆ’

ğ‘)E

[ï¸ â€–â„

âˆ’

ğ‘¦

+

ğ’(ğ‘¥

âˆ’

ğ‘¦)

âˆ’

(ğ‘¥

âˆ’

ğ‘¦)â€–2]ï¸

â‰¤

(1

âˆ’

ğ‘)(1

+

ğ‘ )

â€–â„

âˆ’

ğ‘¦â€–2

+

(1

âˆ’

ğ‘)

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

E

[ï¸ â€–ğ’(ğ‘¥

âˆ’

ğ‘¦)

âˆ’

(ğ‘¥

âˆ’

ğ‘¦)â€–2]ï¸

â‰¤ (1 âˆ’ ğ‘)(1 + ğ‘ ) â€–â„ âˆ’ ğ‘¦â€–2 + (1 âˆ’ ğ‘) (ï¸€1 + ğ‘ âˆ’1)ï¸€ (1 âˆ’ ğ›¼) â€–ğ‘¥ âˆ’ ğ‘¦â€–2 ,

where in the third row we use that â€–ğ‘ + ğ‘â€–2 â‰¤ (1 + ğ‘ )â€–ğ‘â€–2 + (1 + ğ‘ âˆ’1)â€–ğ‘â€–2 for all ğ‘  > 0, ğ‘,ğ‘ âˆˆ Rğ‘‘. Assuming (1 âˆ’ ğ‘)(1 + ğ‘ ) < 1, we get the result.

Therefore, 3PCv5 ï¬ts our framework. Using our general analysis (Theorems 5.5 and 5.8) we derive the following result. Theorem C.24. 3PCv5 is a special case of the method from (12)â€“(13) with ğ’â„,ğ‘¦(ğ‘¥) deï¬ned in (68) and ğ´ = ğ‘ âˆ’ ğ‘ (1 âˆ’ ğ‘) and ğµ = (1 âˆ’ ğ‘) (ï¸€1 + ğ‘ âˆ’1)ï¸€ (1 âˆ’ ğ›¼), where ğ‘  > 0 is such that (1 âˆ’ ğ‘)(1 + ğ‘ ) < 1.
1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + âˆšï¸
ğ¿+ (1âˆ’ğ‘)(1+ğ‘ âˆ’1)(1âˆ’ğ›¼)/(ğ‘âˆ’ğ‘ (1âˆ’ğ‘)), then for any ğ‘‡ â‰¥ 0 we have
E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (69) ğ›¾ğ‘‡ (ğ‘ âˆ’ ğ‘ (1 âˆ’ ğ‘))ğ‘‡
where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by 3PCv5, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

31

2. If additionaly Assumption 5.7 hold and 0

â‰¤

ğ›¾

â‰¤

1/ğ‘€ for ğ‘€

=

{ï¸‚

âˆšï¸

}ï¸‚

max ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ‘)(1+ğ‘ âˆ’1)(1âˆ’ğ›¼)/(ğ‘âˆ’ğ‘ (1âˆ’ğ‘)), (ğ‘âˆ’ğ‘ (1âˆ’ğ‘))/2ğœ‡ , then for any ğ‘‡ â‰¥ 0 we have

(ï¸‚ E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ âˆ†0 +

ğ›¾

)ï¸‚ E [ï¸€ğº0]ï¸€ .

(70)

ğ‘ âˆ’ ğ‘ (1 âˆ’ ğ‘)

Neglecting the term that depends on ğº0 (for simplicity, one can assume that ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0) for ğ‘– âˆˆ [ğ‘›]), one can notice that the smaller ğµ/ğ´, the better the rate. Considering ğµ/ğ´ as a function of ğ‘  and optimizing this function in ğ‘ , we ï¬nd the optimal
value of this ratio.

Lemma C.25. The optimal value of

ğµ

(1 âˆ’ ğ‘) (ï¸€1 + ğ‘ âˆ’1)ï¸€ (1 âˆ’ ğ›¼)

(ğ‘ ) =

ğ´

(ğ‘ âˆ’ ğ‘ (1 âˆ’ ğ‘))

under the constraint 0 < ğ‘  < ğ‘/(1âˆ’ğ‘) equals

ğµ

(1 âˆ’ ğ‘)(1 âˆ’ ğ›¼) 4(1 âˆ’ ğ‘)(1 âˆ’ ğ›¼)

ğ´ (ğ‘ *) = (1 âˆ’ âˆš1 âˆ’ ğ‘)2 â‰¤

ğ‘2

and

it

is

achieved

at

ğ‘ *

=

âˆ’1

+

âˆšï¸€ 1/(1âˆ’ğ‘).

Proof. First of all, we ï¬nd the derivative of the considered function:

(ï¸‚ ğµ )ï¸‚â€²

(1 âˆ’ ğ‘)ğ‘ 2 + 2(1 âˆ’ ğ‘)ğ‘  âˆ’ ğ‘

ğ´ (ğ‘ ) = (1 âˆ’ ğ‘)(1 âˆ’ ğ›¼) (ğ‘ğ‘  âˆ’ ğ‘ 2(1 âˆ’ ğ‘))2 .

âˆšï¸€

âˆšï¸€

The function has 2 critical points: âˆ’1 Â± 1/(1âˆ’ğ‘). Moreover, the derivative is non-positive for ğ‘  âˆˆ (0, âˆ’1 + 1/(1âˆ’ğ‘)] and

âˆšï¸€ negative for ğ‘  âˆˆ (âˆ’1 + 1/(1âˆ’ğ‘), +âˆ). This implies that the optimal value on the interval ğ‘  âˆˆ (0, ğ‘/(1âˆ’ğ‘)) is achieved at

âˆšï¸€ ğ‘ * = âˆ’1 + 1/(1âˆ’ğ‘). Via simple computations one can verify that

âˆš Finally, since 1 âˆ’ 1 âˆ’ ğ‘ â‰¥ ğ‘/2, we have

ğµ

(1 âˆ’ ğ‘)(1 âˆ’ ğ›¼)

ğ´ (ğ‘ *) = (1 âˆ’ âˆš1 âˆ’ ğ‘)2 .

ğµ

4(1 âˆ’ ğ‘)(1 âˆ’ ğ›¼)

ğ´ (ğ‘ *) â‰¤

ğ‘2 .

Using this and Corollaries 5.6, 5.9, we get the following complexity results. âˆšï¸€
Corollary C.26. 1. Let the assumptions from the ï¬rst part of Theorem C.24 hold, ğ‘  = ğ‘ * = âˆ’1 + 1/(1âˆ’ğ‘), and

1 ğ›¾ = ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ‘)(1âˆ’ğ›¼)/(1âˆ’âˆš1âˆ’ğ‘)2 .

Then for any ğ‘‡ we have

[ï¸

2]ï¸ 2âˆ†0 (ï¸ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ‘)(1âˆ’ğ›¼)/(1âˆ’âˆš1âˆ’ğ‘)2)ï¸

E [ï¸€ğº0]ï¸€

E

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦ âƒ¦

â‰¤

+âˆš

,

ğ‘‡

(1 âˆ’ 1 âˆ’ ğ‘)ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

â›

âˆ†0

(ï¸ ğ¿âˆ’

+

)ï¸ âˆšï¸€ ğ¿+ (1âˆ’ğ‘)(1âˆ’ğ›¼)/ğ‘2

â E [ï¸€ğº0]ï¸€

ğ‘‡ = ğ’ª â ğœ€2

+ ğ‘ğœ€2 â 

(71)

iterations/communication rounds.

32

2. Let the assumptions from the second part of Theorem C.24 hold and

{ï¸ƒ

âˆš }ï¸ƒ

1

1âˆ’ 1âˆ’ğ‘

ğ›¾ = min ğ¿âˆ’ + ğ¿+âˆšï¸€2(1âˆ’ğ‘)(1âˆ’ğ›¼)/(1âˆ’âˆš1âˆ’ğ‘)2 , 2ğœ‡ .

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

(ï¸ƒ

{ï¸ƒ

âˆšï¸€

}ï¸ƒ

ğ¿âˆ’ + ğ¿+ (1âˆ’ğ‘)(1âˆ’ğ›¼)/ğ‘2

âˆ†0 + E [ï¸€ğº0]ï¸€ ğ›¾/ğ‘ )ï¸ƒ

ğ’ª max

, ğ‘ log

(72)

ğœ‡

ğœ€

iterations/communication rounds.

33

D. MARINA
In this section, we show that MARINA (Gorbunov et al., 2021) can be analyzed using a similar proof technique that we use for the methods based on three points compressors.

Algorithm 10 MARINA (Gorbunov et al., 2021)

1: Input: starting point ğ‘¥0, stepsize ğ›¾, probability ğ‘ âˆˆ (0,1], number of iterations ğ‘‡

2: Initialize ğ‘”0 = âˆ‡ğ‘“ (ğ‘¥0)

3: for ğ‘¡ = 0,1, . . . ,ğ‘‡ âˆ’ 1 do

4: Sample ğ‘ğ‘¡ âˆ¼ Be(ğ‘) 5: Broadcast ğ‘”ğ‘¡ to all workers

6: for ğ‘– = 1, . . . ,ğ‘› in parallel do

7:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

{ï¸ƒ

ğ‘¡+1

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1),

if ğ‘ğ‘¡ = 1,

8: Set ğ‘”ğ‘– = ğ‘”ğ‘–ğ‘¡ + ğ’¬ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)))ï¸€ , if ğ‘ğ‘¡ = 0

9: end for

10:

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

11: end for

12: Return: ğ‘¥Ë†ğ‘‡ chosen uniformly at random from {ğ‘¥ğ‘¡}ğ‘‡ğ‘¡=âˆ’01

The next lemma casts MARINA to our theoretical framework.
Lemma D.1. Let Assumption 5.3 hold. Then, MARINA satisï¬es inequality (16) with ğºğ‘¡ = â€–ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2, ğ´ = ğ‘, and ğµ = (1âˆ’ğ‘)ğœ”/ğ‘›.

Proof. The formula for ğ‘”ğ‘–ğ‘¡+1 implies that

â§âˆ‡ğ‘“ (ğ‘¥ğ‘¡+1), â¨ ğ‘”ğ‘¡+1 = â©ğ‘”ğ‘¡ + ğ‘›1 âˆ‘ğ‘›ï¸€ ğ’¬ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)))ï¸€ ,
ğ‘–=1

if ğ‘ğ‘¡ = 1, if ğ‘ğ‘¡ = 0.

Using this and independence of ğ’¬ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)))ï¸€ for ğ‘– âˆˆ [ğ‘›] and ï¬xed ğ‘¥ğ‘¡,ğ‘¥ğ‘¡+1, we derive

E [ï¸€ğºğ‘¡+1]ï¸€

=

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

âˆ‡ğ‘“

(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

â¡

âƒ¦

ğ‘›

âƒ¦2â¤

= (1 âˆ’ ğ‘)E â£âƒ¦âƒ¦ğ‘”ğ‘¡ + 1 âˆ‘ï¸ ğ’¬ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)))ï¸€ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡+1)âƒ¦âƒ¦ â¦

âƒ¦âƒ¦ ğ‘› ğ‘–=1 âƒ¦âƒ¦

â¡

âƒ¦

ğ‘›

âƒ¦2â¤

= (1 âˆ’ ğ‘)E â£âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡) + 1 âˆ‘ï¸ ğ’¬ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)))ï¸€ âˆ’ (ï¸€âˆ‡ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡))ï¸€âƒ¦âƒ¦ â¦

âƒ¦âƒ¦ ğ‘› ğ‘–=1 âƒ¦âƒ¦

(=22)

(1

âˆ’

ğ‘)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

âˆ‡ğ‘“

(ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

â¡ âƒ¦ğ‘›

âƒ¦2â¤

+(1 âˆ’ ğ‘)E â£âƒ¦âƒ¦ 1 âˆ‘ï¸ (ï¸€ğ’¬ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)))ï¸€ âˆ’ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡))ï¸€)ï¸€âƒ¦âƒ¦ â¦

âƒ¦âƒ¦ ğ‘› ğ‘–=1 âƒ¦âƒ¦

1

âˆ’

ğ‘

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

= (1 âˆ’ ğ‘)E [ï¸€ğºğ‘¡]ï¸€ +

E

âƒ¦ âƒ¦ğ’¬

(ï¸€âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡

)))ï¸€

âˆ’

(ï¸€âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡

))ï¸€âƒ¦âƒ¦

ğ‘›2

ğ‘–=1

(22)

[ï¸€ ğ‘¡]ï¸€ (1 âˆ’ ğ‘)ğœ” âˆ‘ğ‘›ï¸ [ï¸âƒ¦ ğ‘¡+1

ğ‘¡ âƒ¦2]ï¸

= (1 âˆ’ ğ‘)E ğº + ğ‘›2

E âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ ) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦ .

ğ‘–=1

It remains to apply Assumption 5.3 to get the result.

34

We notice that the proofs of Theorems 5.5 and 5.8 rely only on the inequality (16), the update rule ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡, and the fact that ğºğ‘¡ â‰¥ â€–ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2. Therefore, using Lemma D.1 and our general results (Theorems 5.5 and 5.8), we recover the rates for MARINA from Gorbunov et al. (2021).
Theorem D.2. 1. If Assumptions 5.1, 5.2, 5.3 hold and the stepsize ğ›¾ satisï¬es 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€, where ğ‘€ = ğ¿âˆ’ + âˆšï¸€
ğ¿+ (1âˆ’ğ‘)ğœ”/ğ‘›ğ‘, then for any ğ‘‡ â‰¥ 0 we have

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2âˆ†0 + E [ï¸€ğº0]ï¸€ , (73)

ğ›¾ğ‘‡

ğ‘ğ‘‡

where ğ‘¥Ë†ğ‘‡ is sampled uniformly at random from the points {ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1} produced by MARINA, âˆ†0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , and ğº0 is deï¬ned in (15).

{ï¸

}ï¸

âˆšï¸€

2. If additionaly Assumption 5.7 hold and 0 â‰¤ ğ›¾ â‰¤ 1/ğ‘€ for ğ‘€ = max ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ‘)ğœ”/ğ‘›ğ‘, ğ‘/2ğœ‡ , then for any

ğ‘‡ â‰¥ 0 we have

E

[ï¸€ğ‘“ (ğ‘¥ğ‘‡

)

âˆ’

ğ‘“ (ğ‘¥*)]ï¸€

â‰¤

(1

âˆ’

ğ›¾ğœ‡)ğ‘‡

(ï¸‚ âˆ†0

+

ğ›¾

E

)ï¸‚ [ï¸€ğº0]ï¸€

.

(74)

ğ‘

Next, this theorem and Corollaries 5.6 and 5.9 imply the following complexity results. Corollary D.3. 1. Let the assumptions from the ï¬rst part of Theorem D.2 hold and

1 ğ›¾ = âˆšï¸€ .
ğ¿âˆ’ + ğ¿+ (1âˆ’ğ‘)ğœ”/ğ‘›ğ‘

Then for any ğ‘‡ we have

(ï¸

)ï¸

[ï¸

2]ï¸ 2âˆ†0 ğ¿âˆ’ + ğ¿+âˆšï¸€(1âˆ’ğ‘)ğœ”/ğ‘›ğ‘ E [ï¸€ğº0]ï¸€

E

âƒ¦ âƒ¦âˆ‡

ğ‘“

(

ğ‘¥Ë†

ğ‘‡

âƒ¦ )âƒ¦

â‰¤

+

,

ğ‘‡

ğ‘ğ‘‡

i.e.,

to

achieve

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2

for

some

ğœ€

>

0

the

method

requires

â›

âˆ†0

(ï¸ ğ¿âˆ’

+

)ï¸ âˆšï¸€ ğ¿+ (1âˆ’ğ‘)ğœ”/ğ‘›ğ‘

â E [ï¸€ğº0]ï¸€

ğ‘‡ = ğ’ª â ğœ€2

+ ğ‘ğœ€2 â 

(75)

iterations/communication rounds.

2. Let the assumptions from the second part of Theorem D.2 hold and

{ï¸ƒ

}ï¸ƒ

1

ğ‘

ğ›¾ = min

,.

âˆšï¸€ ğ¿âˆ’ + ğ¿+ 2(1âˆ’ğ‘)ğœ”/ğ‘›ğ‘

2ğœ‡

Then to achieve E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥*)]ï¸€ â‰¤ ğœ€ for some ğœ€ > 0 the method requires

(ï¸ƒ

{ï¸ƒ

âˆšï¸€

}ï¸ƒ

ğ¿âˆ’ + ğ¿+ (1âˆ’ğ‘)ğœ”/ğ‘›ğ‘

âˆ†0 + E [ï¸€ğº0]ï¸€ ğ›¾/ğ‘ )ï¸ƒ

ğ’ª max

, ğ‘ log

(76)

ğœ‡

ğœ€

iterations/communication rounds.

35

E. More Experiments
This section is organized as follows. We report more details on the experiment with autoencoder in Appendix E.1. In Appendix E.2, we validate the new methods 3PCv1, . . . , 3PCv5 on a synthetic quadratic problem with a careful control of heterogenity level. Finally, in Appendix E.3, we provide additional experiments with compressed lazy aggregation CLAG. We refer the reader to Appendices A and C for a formal deï¬nition of the algorithms and compressors. All methods are implemented in Python 3.8 and run on 3 different CPU cluster nodes
â€¢ AMD EPYC 7702 64-Core; â€¢ Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz; â€¢ Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz.
Communication between server and clients is emulated in one computing node.

E.1. Learning autoencoder model
In this set of experiments, we test the proposed optimization methods on the task of learning a representation of MNIST dataset (LeCun et al., 2010). We recall that we consider the following optimization problem

[ï¸ƒ

1

ğ‘
âˆ‘ï¸

]ï¸ƒ
2

min

ğ‘“ (ğ·, ğ¸) :=

â€–ğ·ğ¸ğ‘ğ‘– âˆ’ ğ‘ğ‘–â€– ,

ğ·âˆˆRğ‘‘ğ‘“ Ã—ğ‘‘ğ‘’ ,ğ¸âˆˆRğ‘‘ğ‘’Ã—ğ‘‘ğ‘“

ğ‘ ğ‘–=1

(77)

where ğ‘ğ‘– are ï¬‚attened represenations of images with ğ‘‘ğ‘“ = 784, ğ· and ğ¸ are learned parameters of the autoencoder model. We ï¬x the encoding dimensions as ğ‘‘ğ‘’ = 16 and distribute the data samples across ğ‘› = 10, 100, or 1000 clients. In order to control the heterogenity of this distribution, we use the following randomized procedure. First, split the dataset randomly into ğ‘› + 1 equal parts ğ·0, ğ·1, . . . , ğ·ğ‘› and ï¬x the homogenity level parameter 0 â‰¤ ğ‘Ë† â‰¤ 1. Then let the ğ‘–-th client take ğ·0 with probability ğ‘Ë† or ğ·ğ‘– otherwise. If ğ‘Ë† = 1, we are in homogeneous regime. If ğ‘Ë† = 0, all clients have different randomly shufï¬‚ed data samples. Additionally, we study even more heterogeneous setting where we perform the split by labels. This means that the clients from 1 to ğ‘›/10 own the images corresponding to the ï¬rst class, nodes from ğ‘›/10 + 1 to 2ğ‘›/10 own the images corresponding to the second class and so on (MNIST dataset has 10 different classes).

In this section, we choose ğ¾ = ğ‘‘/ğ‘›, where ğ‘‘ = 2 Â· ğ‘‘ğ‘“ Â· ğ‘‘ğ‘’ = 25088 is the total dimension of learning parameters ğ· and ğ¸. It is argued by (Szlendak et al., 2021) that this is a suitable choice for MARINA method with Rand-ğ¾ or Perm-ğ¾ sparsiï¬ers. Methods involving two compressor such as 3PCv2, require to communicate two sparse sequences at every communication round. To account for this, we select ğ¾1, ğ¾2 from the set {ğ¾/2, ğ¾}, that is there are four possible choices for compression levels ğ¾1, ğ¾2 of two sparsiï¬ers in 3PCv2. Then we select the pair which works best.
We ï¬ne-tune every method with the step-sizes from the set {2âˆ’12, 2âˆ’11, . . . , 25} and select the best run based on the value of â€–âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2 at the last iterate. The step-size for each method is indicated in the legend of each plot.

EF21 embraces different sparsiï¬ers. Since Seide et al. (2014) proposed the error feedback style scheme, it has been successfully used in distributed training combined with some contractive compressor. A popular choice is Top-ğ¾, which preserves the â€most importantâ€ coordinates and shows empirical superiority. However, a natural question arises:

Is the success of EF21 with Top-ğ¾ attributed to a careful algorithm design or to a greedy sparsiï¬er in use?

We compare EF21 with three different compressors: Top-ğ¾, cPerm-ğ¾, cRand-ğ¾ in Figure 3. MARINA with Perm-ğ¾ is added for the reference. In all cases, Top-ğ¾ demonstrates fast improvement in the ï¬rst communication rounds. When ğ‘› = 10, the randomized compressors (cPerm-ğ¾ and cRand-ğ¾) work best for EF21. When ğ‘› = 100 the picture is similar, but cPerm-ğ¾ shows better performance than cRand-ğ¾ when homogenity level is high (1 or 0.5). Finally, Top-ğ¾ wins in the competition for ğ‘› = 1000.
Takeaway 1: EF21 is well designed and works well with different contractive compressors, including the randomized ones.
Takeaway 2: EF21 combined with Top-ğ¾ is particularly useful if

36

|| f(xt)||2

|| f(xt)||2

Number of clients ğ‘› = 10, compression level ğ¾ = 2509.

Homogenity level: 1

Homogenity level: 0.5

Homogenity level: 0

100

EF21-TopK: 20 EF21-cPermK: 23

100

EF21-TopK: 20 EF21-cPermK: 23

100

EF21-TopK: 20 EF21-cPermK: 23

100

EF21-cRandK: 23

EF21-cRandK: 23

EF21-cRandK: 23

10 1

MARINA-PermK: 2 2 10 1

MARINA-PermK: 2 3 10 1

MARINA-PermK: 2 3 10 1

Split by labels EF21-TopK: 20 EF21-cPermK: 21 EF21-cRandK: 21 MARINA-PermK: 2 3

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 2

10 2

10 2

10 2

10 3 0
100 10 1 10 2 10 3
0
101 100 10 1 10 2 10 3 10 4
0

10 #20Mbits 3/ 0n 40 50
Homogenity level: 1 EF21-TopK: 2 3 EF21-cPermK: 22 EF21-cRandK: 22 MARINA-PermK: 2 2

10 3

10 3

0 10 #20Mbits 3/ 0n 40 50 0 10 #20Mbits 3/ 0n 40 50

Number of clients ğ‘› = 100, compression level ğ¾ = 251.

Homogenity level: 0.5

Homogenity level: 0

EF21-TopK: 2 3

EF21-TopK: 2 3

100

EF21-cPermK: 22 EF21-cRandK: 22

100

EF21-cPermK: 22 EF21-cRandK: 22

10 1

MARINA-PermK: 2 2 10 1

MARINA-PermK: 2 2

10 3 0
100 10 1

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 2

10 2

10 2

10 #20Mbits3/0n 40 50 Homogenity level: 1 EF21-TopK: 21 EF21-cPermK: 21 EF21-cRandK: 20 MARINA-PermK: 20
10 #M2b0its / n 30 40

|| f(xt)||2

10 3

10 3

0 10 #20Mbits3/0n 40 50

0 10 #20Mbits3/0n 40 50

Number of clients ğ‘› = 1000, compression level ğ¾ = 25.

Homogenity level: 0.5

101

EF21-TopK: 21

101

100

EF21-cPermK: 20 EF21-cRandK: 2 1

100

10 1

MARINA-PermK: 2 1 10 1

Homogenity level: 0 EF21-TopK: 21 EF21-cPermK: 2 1 EF21-cRandK: 2 1 MARINA-PermK: 2 1

|| f(xt)||2

10 2

10 2

10 3

10 3

10 4 0

10 #Mb2i0ts / n 30

10 4

40

0 10 #2M0bits / n30 40

|| f(xt)||2

10 3 0
101 100 10 1 10 2 10 3 10 4
0

10 #20Mbits 3/ 0n 40 50 Split by labels EF21-TopK: 2 3 EF21-cPermK: 21 EF21-cRandK: 21 MARINA-PermK: 2 2
10 #20Mbits3/0n 40 50 Split by labels EF21-TopK: 21 EF21-cPermK: 2 1 EF21-cRandK: 2 1 MARINA-PermK: 2 2
10 #2M0bits / n30 40

Figure 3: Comparison of EF21 with Top-ğ¾, cPerm-ğ¾ and cRand-ğ¾ compressors. MARINA with Perm-ğ¾ is provided for the reference.

|| f(xt)||2

â€¢ we are interested in the progress during the initial phase of training; â€¢ agressive sparsiï¬cation is applied (ğ‘˜/ğ‘‘ â‰ª 1%) and ğ‘› is large; or â€¢ nodes own very different parts of dataset, i.e., we are in heterogeneous regime.

MARINA and greedy sparsiï¬cation (3PCv5) We now draw our attention to one of the newly proposed methods: MARINA combined with biased compression operators (named as 3PCv5 in Algorithm 9 and Table 1). According to our theory, see Table 1, 3PCv5 has the same compexity as EF21. In this experiment, we aim to validate the proposed method with greedy Top-ğ¾ sparsiï¬er. We compare it to MARINA with Perm-ğ¾ and Rand-ğ¾ and include EF21 as a reference method. Interestingly, Top-ğ¾ improves over Perm-ğ¾ and Rand-ğ¾ when ğ‘› = 10; in homogeneus case, the behavior of Top-ğ¾ and Perm-ğ¾ is similar, see Figure 4. However, this improvement vanishes when ğ‘› is increased (ğ‘› = 100, 1000) and sparsiï¬cation is more agressive; MARINA with Top-ğ¾ requires much smaller step-sizes to converge. In all cases, EF21 with Top-ğ¾ is faster.
Other 3PC variants Motivated by the success of greedy sparsiï¬cation and favorable properties of randomized sparsiï¬ers, we aim to investigate if one can combine the two in a nontrivial way and obtain even faster method. One possible way to do so is to look more closely to one of the special cases of 3PC named 3PCv2. With 3PCv2 (Algorithm 6), we have
37

|| f(xt)||2

|| f(xt)||2

Number of clients ğ‘› = 10, compression level ğ¾ = 2509.

Homogenity level: 1

Homogenity level: 0.5

Homogenity level: 0

10 1

3PCv5-TopK: 2 2 EF21-TopK: 20

10 1

3PCv5-TopK: 2 2 EF21-TopK: 20

10 1

3PCv5-TopK: 2 2 EF21-TopK: 20

10 1

MARINA-PermK: 2 2

MARINA-PermK: 2 3

MARINA-PermK: 2 3

MARINA-RandK: 2 3

MARINA-RandK: 2 3

MARINA-RandK: 2 3

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 2

10 2

10 2

10 2

Split by labels 3PCv5-TopK: 2 2 EF21-TopK: 20 MARINA-PermK: 2 3 MARINA-RandK: 2 3

0 10 1 10 2

10 #20Mbits 3/ 0n 40 50
Homogenity level: 1 3PCv5-TopK: 2 6 EF21-TopK: 2 3 MARINA-PermK: 2 2 MARINA-RandK: 2 2

0 10 #20Mbits 3/ 0n 40 50 0 10 #20Mbits 3/ 0n 40 50

Number of clients ğ‘› = 100, compression level ğ¾ = 251.

Homogenity level: 0.5

Homogenity level: 0

3PCv5-TopK: 2 6

3PCv5-TopK: 2 6

10 1

EF21-TopK: 2 3

10 1

EF21-TopK: 2 3

MARINA-PermK: 2 2

MARINA-PermK: 2 2

MARINA-RandK: 2 2

MARINA-RandK: 2 2

0 10 1

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 2

10 2

10 2

10 #20Mbits 3/ 0n 40 50
Split by labels 3PCv5-TopK: 2 6 EF21-TopK: 2 3 MARINA-PermK: 2 2 MARINA-RandK: 2 2

0
100 10 1 10 2 10 3 10 4
0

10 2#0Mbits3/0n 40 50 Homogenity level: 1 3PCv5-TopK: 2 10 EF21-TopK: 21 MARINA-PermK: 20 MARINA-RandK: 2 2
10 #M2b0its / n 30 40

|| f(xt)||2

0 10 2#0Mbits3/0n 40 50

0 10 #20Mbits3/0n 40 50

Number of clients ğ‘› = 1000, compression level ğ¾ = 25.

Homogenity level: 0.5

Homogenity level: 0

100

3PCv5-TopK: 2 10

100

3PCv5-TopK: 2 11

EF21-TopK: 21

EF21-TopK: 21

10 1

MARINA-PermK: 2 1 MARINA-RandK: 2 2

10 1

MARINA-PermK: 2 1 MARINA-RandK: 2 2

10 2

10 2

|| f(xt)||2

10 3

10 3

10 4 0

10 #M2b0its / n 30

10 4

40

0

10 #2M0bits / n30 40

|| f(xt)||2

0
100 10 1 10 2 10 3 10 4
0

10 #20Mbits3/0n 40 50 Split by labels 3PCv5-TopK: 2 11 EF21-TopK: 21 MARINA-PermK: 2 2 MARINA-RandK: 2 2
10 #2M0bits / n30 40

Figure 4: Comparison of MARINA with Perm-ğ¾, Rand-ğ¾ and 3PCv5 with Top-ğ¾.

|| f(xt)||2

more freedom because it has two compressors. In our experiments, we consider three different sparsiï¬ers (Top-ğ¾, Rand-ğ¾, Perm-ğ¾) as for the ï¬rst compressor and ï¬x the second one as Top-ğ¾, see Figure 5. For ğ‘› = 10, the performance of 3PCv2 with Rand-ğ¾-Top-ğ¾ and Top-ğ¾-Top-ğ¾ is very similar to the one of EF21 with Topğ¾. Interestingly, 3PCv2-Rand-ğ¾-Top-ğ¾ becomes superior for ğ‘› = 100 converging even faster than EF21. The difference is especially prominent in heterogeneous setting. Finally, EF21 shows slightly beter performance in the experiments with 1000 nodes. We can conclude that:
â€¢ 3PCv2 can outperform EF21 in some cases, for example, Appendix E.1,

â€¢ EF21 is still superior when ğ‘› is large.
However, more emprirical evidence is needed to investigate the behavior of 3PCv2 and other new methods ï¬tting 3PC framework.
38

Number of clients ğ‘› = 10, compression level ğ¾ = 2509.

Homogenity level: 1

Homogenity level: 0

Split by labels

10 1

3PCv2-PermK-TopK: 2 2 3PCv2-RandK-TopK: 20

10 1

3PCv2-PermK-TopK: 2 3 3PCv2-RandK-TopK: 20

10 1

3PCv2-PermK-TopK: 2 3 3PCv2-RandK-TopK: 20

3PCv2-TopK-TopK: 20

3PCv2-TopK-TopK: 20

3PCv2-TopK-TopK: 20

EF21-TopK: 20

EF21-TopK: 20

EF21-TopK: 20

|| f(xt)||2

|| f(xt)||2

10 2

10 2

10 2

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

0
101 100 10 1 10 2 10 3
0
100 10 1 10 2 10 3 10 4
0

10 #20Mbits /30n 40 50 0 10 #20Mbits /30n 40 50 0

Number of clients ğ‘› = 100, compression level ğ¾ = 251.

Homogenity level: 1 3PCv2-PermK-TopK: 21

101

Hom3oPgCevn2it-PyelermveKl-:T0opK: 2 2 101

3PCv2-RandK-TopK: 21 3PCv2-TopK-TopK: 2 2

100

3PCv2-RandK-TopK: 21 3PCv2-TopK-TopK: 2 2

100

EF21-TopK: 2 3

EF21-TopK: 2 3

10 1

10 1

|| f(xt)||2

|| f(xt)||2

10 2

10 2

10 #20Mbits /30n 40

10 3 50 0

10 #20Mbits /30n 40

10 3 50 0

Number of clients ğ‘› = 100, compression level ğ¾ = 25.

Homogenity level: 1

Homogenity level: 0

3PCv2-PermK-TopK: 2 1 100 3PCv2-RandK-TopK: 20 3PCv2-TopK-TopK: 21 10 1 EF21-TopK: 21
10 2

3PCv2-PermK-TopK: 20 100 3PCv2-RandK-TopK: 20 3PCv2-TopK-TopK: 21 10 1 EF21-TopK: 21
10 2

|| f(xt)||2

|| f(xt)||2

10 3

10 3

10 #20Mbits /30n 40

10 4

50

0

10 #20Mbits /30n 40

10 4

50

0

10 #20Mbits /30n 40 50 Split by labels 3PCv2-PermK-TopK: 2 2 3PCv2-RandK-TopK: 21 3PCv2-TopK-TopK: 2 2 EF21-TopK: 2 3
10 #20Mbits /30n 40 50 Split by labels 3PCv2-PermK-TopK: 20 3PCv2-RandK-TopK: 21 3PCv2-TopK-TopK: 21 EF21-TopK: 21
10 #20Mbits /30n 40 50

Figure 5: Comparison of 3PCv2 with Perm-ğ¾, Rand-ğ¾ and Top-ğ¾ as the ï¬rst compressor. Top-ğ¾ is used as the second compressor. EF21 with Top-ğ¾ is provided for the reference.

E.2. Solving synthetic quadratic problem

In this experimental section we compare practical performance of the proposed methods 3PCv1, 3PCv2, 3PCv4, 3PCv5

against existing state-of-the-art methods for compressed distributed optimization MARINA and EF21. For this comparison

we set up the similar setting that was introduced in (Szlendak et al., 2021). Firstly, let us describe the experimental setup in

detail.

We

consider

the

ï¬nite

sum

function

ğ‘“ (ğ‘¥)

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘“ğ‘–(ğ‘¥),

consisting

of

synthetic

quadratic

functions

ğ‘“ğ‘–(ğ‘¥) = 1 ğ‘¥âŠ¤Ağ‘–ğ‘¥ âˆ’ ğ‘¥âŠ¤ğ‘ğ‘–,

(78)

2

where Ağ‘– âˆˆ Rğ‘‘Ã—ğ‘‘, ğ‘ğ‘– âˆˆ Rğ‘‘, and Ağ‘– = AâŠ¤ğ‘– is the training data that belongs to the device/worker ğ‘–. In all experiments of

this

section,

we

have

ğ‘‘

=

1000

and

generated

Ağ‘–

in

a

such

way

that

ğ‘“

is

ğœ†â€“strongly

convex

(

i.e.,

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

Ağ‘–

ğœ†I for

39

ğœ† > 0) with ğœ† = 1eâˆ’6. We now present Algorithm 11 which is used to generate these synthetic matrices (training data).

Algorithm 11 Quadratic optimization task generation (Szlendak et al., 2021)

1: Parameters: number nodes ğ‘›, dimension ğ‘‘, regularizer ğœ†, and noise scale ğ‘ .

2: for ğ‘– = 1, . . . , ğ‘› do

3: Generate random noises ğœˆğ‘–ğ‘  = 1 + ğ‘ ğœ‰ğ‘–ğ‘  and ğœˆğ‘–ğ‘ = ğ‘ ğœ‰ğ‘–ğ‘, i.i.d. ğœ‰ğ‘–ğ‘ , ğœ‰ğ‘–ğ‘ âˆ¼ ğ’© (0, 1)

4:

Take

vector

ğ‘ğ‘–

=

ğœˆğ‘–ğ‘  (âˆ’1
4

+

ğœˆğ‘–ğ‘, 0, Â· Â· Â·

, 0)

âˆˆ

Rğ‘‘

5: Take the initial tridiagonal matrix

â› 2 âˆ’1

0â

ğœˆğ‘  âœ âˆ’1 . . . . . .

âŸ

Ağ‘– =

ğ‘–âœ âœ

âŸ âŸ

âˆˆ

Rğ‘‘Ã—ğ‘‘

4 âœâ . . . . . . âˆ’1 âŸâ 

0

âˆ’1 2

6: end for

7:

Take the mean of matrices A =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

Ağ‘–

8: Find the minimum eigenvalue ğœ†min(A)

9: for ğ‘– = 1, . . . , ğ‘› do

10: Update matrix Ağ‘– = Ağ‘– + (ğœ† âˆ’ ğœ†min(A))I

11: end for

âˆš

12: Take starting point ğ‘¥0 = ( ğ‘‘, 0, Â· Â· Â· , 0)

13: Output: matrices A1, Â· Â· Â· , Ağ‘›, vectors ğ‘1, Â· Â· Â· , ğ‘ğ‘›, starting point ğ‘¥0

We generated optimization tasks having different number of nodes ğ‘› = {10, 100, 1000} and capturing various dataheterogeneity regimes that are controlled by so-called Hessian variance6 term:
Deï¬nition E.1 (Hessian variance (Szlendak et al., 2021)). Let ğ¿Â± â‰¥ 0 be the smallest quantity such that

ğ‘›

1 ğ‘›

âˆ‘ï¸€

â€–âˆ‡ğ‘“ğ‘–(ğ‘¥)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¦)â€–2

âˆ’

â€–âˆ‡ğ‘“ (ğ‘¥)

âˆ’

âˆ‡ğ‘“ (ğ‘¦)â€–2

â‰¤

ğ¿2Â± â€–ğ‘¥

âˆ’

ğ‘¦â€–2 ,

âˆ€ğ‘¥,ğ‘¦ âˆˆ Rğ‘‘.

(79)

ğ‘–=1

We refer to the quantity ğ¿2Â± by the name Hessian variance.

From the deï¬nition , it follows that the case of similar (or even identical) functions ğ‘“ğ‘– relates to the small (or even 0) Hessian variance, whereas in the case of completely different ğ‘“ğ‘– (which relate to heterogeneous data regime) ğ¿Â± can be large.

In our experiments, homogeneity of each optimizations task is controlled by noise scale ğ‘  introduced in the Algorithm

11. Indeed, for the noise scale ğ‘  = 0, all matrices Ağ‘– are equal, whereas with the increase of the noise scale, functions

become less â€œsimilarâ€ and ğ¿Â± rises. We take noise scales ğ‘  âˆˆ {0.0, 0.05, 0.8, 1.6, 6.4}. A summary of the ğ¿Â± and ğ¿âˆ’

values corresponding to these noise scales is given in the Tables 3 and 4. For the considered quadratic problem ğ¿Â± can be

âˆšï¸‚

analytically expressed as ğ¿Â± =

ğœ†max

(ï¸

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

A2ğ‘–

âˆ’

(ï¸€

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

)ï¸€2)ï¸ Ağ‘– .

Table 3 Summary of the Hessian variance terms ğ¿Â± for different number of nodes ğ‘› various noise scales ğ‘ .

ğ‘› ğ‘  0 0.05 0.8 1.6 6.4

10 100 1000

0 0.06 0.9 1.79 7.17 0 0.05 0.82 1.65 6.58 0 0.05 0.81 1.62 6.48

For all algorithms, at each iteration we compute the squared norm of the exact/full gradient for comparison of the methods performance. We terminate our algorithms either if they reach the certain number of iterations or the following stopping criterion is satisï¬ed: â€–âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2 â‰¤ 10âˆ’7.
6For more details, see the original paper Szlendak et al. (2021) introducing this concept.

40

n = 10 || f(xt)||2

Table 4 Summary of the Hessian variance terms ğ¿âˆ’ for different number of nodes ğ‘› various noise scales ğ‘ .

ğ‘› ğ‘  0 0.05 0.8 1.6 6.4

10 100 1000

1.0 1.02 1.35 1.7 3.82 1.0 1.0 0.97 0.94 0.77 1.0 1.0 0.97 0.95 0.78

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.0; Lavg = 1.0

EF21-Top100; 32x

103

EF21-cRand100; 32x

EF21-cPerm100; 32x
MARINA-Perm100; 1x 101

10 1

10 3

10 5

10 7

10

20

30

0

LÂ±avg = 0.05; Lavg = 1.01
EEFF2211--TcRopan1d0100; 03;23x2x 103
EF21-cPerm100; 32x
MARINA-Perm100; 2x 101

10 1

10 3

10 5

10 7

10

20

30

0

LÂ±avg = 0.84; Lavg = 1.1

EF21-Top100; 32x EF21-cRand100; 16x

103

EF21-cPerm100; 32x

MARINA-Perm100; 4x 101

10 1

10 3

10 5

10 7

10

20

30

0

LÂ±avg = 1.69; Lavg = 1.2
EF21-Top100; 32x EF21-cRand100; 16x EF21-cPerm100; 32x MARINA-Perm100; 4x

20

40

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 6.74; Lavg = 1.79
EF21-Top100; 32x EF21-cRand100; 16x EF21-cPerm100; 16x MARINA-Perm100; 4x

5000

10000

103 101 10 1 10 3 10 5 10 7
0

EF21-Top10; 64x

EF21-cRand10; 256x EF21-cPerm10; 256x

103

MARINA-Perm10; 1x

101

10 1

10 3

10 5

10 7

5

10

0

EF21-Top10; 256x

EF21-cRand10; 256x EF21-cPerm10; 256x

103

MARINA-Perm10; 2x
101

10 1

10 3

10 5

10 7

2

4

6

0

EF21-Top10; 256x EF21-cRand10; 128x EF21-cPerm10; 256x MARINA-Perm10; 8x
5

103 101 10 1 10 3 10 5 10 7
0

EF21-Top10; 256x

103

EF21-cRand10; 128x

EF21-cPerm10; 64x MARINA-Perm10; 8x

101

10 1

10 3

10 5

10 7

5

10

0

EF21-Top10; 256x EF21-cRand10; 32x EF21-cPerm10; 64x MARINA-Perm10; 4x

50

100

150

103 101 10 1 10 3 10 5 10 7
0.0

EF21-Top1; 64x EF21-cRand1; 128x EF21-cPerm1; 256x MARINA-Perm1; 1x
2.5 5.0 7.5 #Mbits/n

103 101 10 1 10 3 10 5 10 7
0

EF21-Top1; 256x

EF21-cRand1; 128x

EF21-cPerm1; 256x

103

MARINA-Perm1; 4x

101

10 1

10 3

10 5

2 #Mbits/n

10 7 4 0.0

EF21-Top1; 512x EF21-cRand1; 512x EF21-cPerm1; 1024x MARINA-Perm1; 4x
2.5 5.0 7.5 #Mbits/n

103 101 10 1 10 3 10 5 10 7
0

EF21-Top1; 512x EF21-cRand1; 512x EF21-cPerm1; 512x MARINA-Perm1; 4x

2

4

#Mbits/n

103 101 10 1 10 3 10 5 10 7
0

EF21-Top1; 512x EF21-cRand1; 256x EF21-cPerm1; 256x MARINA-Perm1; 4x

10

20

#Mbits/n

Figure 6: Comparison of MARINA with Perm-ğ¾, EF21 with Top-ğ¾, cPerm-ğ¾ and cRand-ğ¾ with ğ¾ = ğ‘‘/ğ‘› and tuned
stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 100 || f(xt)||2

n = 1000 || f(xt)||2

In all experiments, the stepsize of each method is set to the largest theoreticaly possible stepsize multiplied by some constant multiplier which was individually tuned in all cases within powers of 2 : {2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768} .
EF21 and different compressors Following the same order as in the section E.1 we start by comparing existing SOTA methods (MARINA with Perm-ğ¾ and EF21 with Top-ğ¾) against EF21 with cPerm-ğ¾ and cRand-ğ¾. In Figure 6, paramater ğ¾ = ğ‘‘/ğ‘› is ï¬xed for each row. Each column corresponds to a heterogeneity levels deï¬ned by the averaged ğ¿Â± and ğ¿âˆ’ per values ğ‘› (averaged per column in the Tables 3 and 4).
These experiments shows that, in low Hessian variance regime EF21 with cPerm-ğ¾ and cRand-ğ¾ in some cases improves MARINA with Perm-ğ¾ for ğ‘› = 10, 100, whereas for ğ‘› = 1000 MARINA with cPerm-ğ¾ still dominates. Moreover, even in big Hessian variance regime EF21 methods converges faster than MARINA with cPerm-ğ¾ but not as fast as EF21 with Top-ğ¾. We are not aware of any prior empirical study for EF21 combined with cPerm-ğ¾ or cRand-ğ¾.
MARINA and different compressors In this section, we keep the same setting and compare a new method 3PCv5 with Top-ğ¾ against MARINA with Perm-ğ¾, Rand-ğ¾ and EF21 with Top-ğ¾. In Figure 7, one can see that 3PCv5 with Top-ğ¾ outperforms MARINA methods only in a couple of cases for ğ‘› = 10, whereas for the most of the regimes it converges slower
41

n = 10 || f(xt)||2

n = 100 || f(xt)||2

103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0.0

LÂ±avg = 0.0; Lavg = 1.0
EF21-Top100; 32x MARINA-Perm100; 1x 3PCv5-Top100; 4x MARINA-Rand100; 4x

20

40

EF21-Top10; 64x MARINA-Perm10; 1x 3PCv5-Top10; 4x MARINA-Rand10; 8x

10

20

EF21-Top1; 64x MARINA-Perm1; 1x 3PCv5-Top1; 4x MARINA-Rand1; 4x

2.5 5.0 7.5 #Mbits/n

103 LÂ±avg = 0.05; Lavg = 1.01 EF21-Top100; 32x

MARINA-Perm100; 2x

101

3PCv5-Top100; 4x

MARINA-Rand100; 4x

10 1

10 3

10 5

10 7 0
103 101 10 1

20

40

EF21-Top10; 256x MARINA-Perm10; 2x 3PCv5-Top10; 4x MARINA-Rand10; 8x

10 3

10 5

10 7 0

5

10

103 101 10 1 10 3 10 5 10 7
0.0

EF21-Top1; 256x MARINA-Perm1; 4x 3PCv5-Top1; 4x MARINA-Rand1; 32x
2.5 5.0 7.5 #Mbits/n

LÂ±avg = 0.84; Lavg = 1.1

103

EF21-Top100; 32x

103

MARINA-Perm100; 4x

101

3PCv5-Top100; 32x MARINA-Rand100; 4x

101

10 1

10 1

10 3

10 3

10 5

10 5

10 7 0
103 101 10 1

20

40

EF21-Top10; 256x MARINA-Perm10; 8x 3PCv5-Top10; 4x MARINA-Rand10; 8x

10 7 0
103 101 10 1

10 3

10 3

10 5

10 5

10 7 0
103 101 10 1

10 7

5

10

0

103
EF21-Top1; 512x

MARINA-Perm1; 4x

3PCv5-Top1; 4x

101

MARINA-Rand1; 16x

10 1

10 3

10 3

10 5

10 5

10 7 0.0

2.5 5.0 #Mbits/n

10 7

7.5

0

LÂ±avg = 1.69; Lavg = 1.2
EF21-Top100; 32x MARINA-Perm100; 4x 3PCv5-Top100; 32x MARINA-Rand100; 4x

20

40

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 6.74; Lavg = 1.79
EF21-Top100; 32x MARINA-Perm100; 4x 3PCv5-Top100; 8x MARINA-Rand100; 4x

5000

10000

EF21-Top10; 256x

MARINA-Perm10; 8x

3PCv5-Top10; 8x

103

MARINA-Rand10; 8x

101

10 1

10 3

10 5

10 7

5

10

0

103
EF21-Top1; 512x

MARINA-Perm1; 4x

3PCv5-Top1; 8x

101

MARINA-Rand1; 16x

10 1

10 3

10 5

2

4

#Mbits/n

10 7 0

EF21-Top10; 256x MARINA-Perm10; 4x 3PCv5-Top10; 32x MARINA-Rand10; 4x

50

100

150

EF21-Top1; 512x MARINA-Perm1; 4x 3PCv5-Top1; 32x MARINA-Rand1; 8x

10

20

30

#Mbits/n

Figure 7: Comparison of MARINA with Perm-ğ¾, Rand-ğ¾, EF21 with Top-ğ¾ and 3PCv5 with ğ¾ = ğ‘‘/ğ‘› and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 1000 || f(xt)||2

than other methods.

3PCv2 beat SOTA methods in the most cases! In this series of experiments, we stick to the previous setting and append the results of the new method 3PCv2 with 2 different combinataion of compressors: Randğ¾1-Topğ¾2 and Randğ¾1 *Permğ¾ -Topğ¾2, where Randğ¾1 *Perm-ğ¾ is the composition of Rand-ğ¾1 and Perm-ğ¾. For both methods, constants ğ¾1 and ğ¾2 were extensively tuned over the set of 9 different pairs (see Figures 10 and 12 for details). In the Figure 8 it is shown that both variants 3PCv2 methods converge quickly for ğ‘› = 100 in all heterogeneity regimes, outperforming MARINA and EF21. In the big Hessian variance regime and ğ‘› = 10, 3PCv2 also converges faster than EF21 with Top-ğ¾, however, for even more homogeneous cases 3PCv2 slightly looses to EF21 with cPerm-ğ¾ or cRand-ğ¾. We also would like to note that we excluded 3PCv4 with Topğ¾1-Topğ¾2 from our comparison here since in practice for ğ¾ = ğ‘‘/ğ‘› it coincides with EF21 with Top-ğ¾ (see Figure 14 for more details)7
7We believe that this behaviors of 3PCv4 with Topğ¾1-Topğ¾2 takes place is due to the problem sparsity.
42

n = 10 || f(xt)||2

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.0; Lavg = 1.0
3PCv2-Rand10-Top90; 64x 3PCv2-Rand60*Perm100-Top40; 128x EF21-cPerm100; 32x EF21-Top100; 32x EF21-cRand100; 32x MARINA-Perm100; 1x 3PCv5-Top100; 4x MARINA-Rand100; 4x

20

40

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.05; Lavg = 1.01
3PCv2-Rand10-Top90; 64x 3PCv2-Rand60*Perm100-Top40; 128x EF21-cPerm100; 32x EF21-Top100; 32x EF21-cRand100; 32x MARINA-Perm100; 2x 3PCv5-Top100; 4x MARINA-Rand100; 4x

20

40

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.86; Lavg = 1.16

33PPCCvv22--RRaanndd1900-*TPoeprm901;0604-xTop10; 256x 103

EF21-cPerm100; 32x

EF21-Top100; 32x

101

EF21-cRand100; 16x

MARINA-Perm100; 4x 3PCv5-Top100; 32x

10 1

MARINA-Rand100; 4x
10 3

10 5

10 7

20

40

0

LÂ±avg = 1.72; Lavg = 1.32
3PCv2-Rand20-Top80; 64x 3PCv2-Rand10*Perm100-Top90; 256x EF21-cPerm100; 32x EF21-Top100; 32x EF21-cRand100; 16x MARINA-Perm100; 4x 3PCv5-Top100; 32x MARINA-Rand100; 4x

10

20

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 6.88; Lavg = 2.29
3PCv2-Rand20-Top80; 64x 3PCv2-Rand10*Perm100-Top90; 256x EF21-cPerm100; 16x EF21-Top100; 32x EF21-cRand100; 16x MARINA-Perm100; 4x 3PCv5-Top100; 8x MARINA-Rand100; 4x

5000

10000

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand2-Top8; 512x 3PCv2-Rand4*Perm10-Top6; 4096x EF21-cPerm10; 256x EF21-Top10; 64x EF21-cRand10; 256x MARINA-Perm10; 1x 3PCv5-Top10; 4x MARINA-Rand10; 8x

2

4

#Mbits/n

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand2-Top8; 512x 3PCv2-Rand4*Perm10-Top6; 4096x EF21-cPerm10; 256x EF21-Top10; 256x EF21-cRand10; 256x MARINA-Perm10; 2x 3PCv5-Top10; 4x MARINA-Rand10; 8x

2

4

#Mbits/n

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand5-Top5; 512x 3PCv2-Rand4*Perm10-Top6; 4096x EF21-cPerm10; 256x EF21-Top10; 256x EF21-cRand10; 128x MARINA-Perm10; 8x 3PCv5-Top10; 4x MARINA-Rand10; 8x

2

4

#Mbits/n

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand1-Top9; 1024x 3PCv2-Rand3*Perm10-Top7; 8192x EF21-cPerm10; 64x EF21-Top10; 256x EF21-cRand10; 128x MARINA-Perm10; 8x 3PCv5-Top10; 8x MARINA-Rand10; 8x

2

4

#Mbits/n

103 101 10 1 10 3 10 5 10 7 0

3PCv2-Rand2-Top8; 512x 3PCv2-Rand1*Perm10-Top9; 8192x EF21-cPerm10; 64x EF21-Top10; 256x EF21-cRand10; 32x MARINA-Perm10; 4x 3PCv5-Top10; 32x MARINA-Rand10; 4x

50

100

#Mbits/n

Figure 8: Comparison of MARINA, EF21, 3PCv2 and 3PCv5 with various compressors , ğ¾ = ğ‘‘/ğ‘› and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 100 || f(xt)||2

n = 10 || f(xt)||2

n = 100 || f(xt)||2

We further continue with the setting where ğ¾/ğ‘‘ = 0.02 is ï¬xed for each ğ‘›. In the Figure 9 illustrates that 3PCv2 remaines the best choice for ğ‘› = 10 and ğ‘› = 100, whereas in the homogeneous regime and ğ‘› = 1000 EF21 with cRand-ğ¾ can reach the desired tolerance a bit faster. However, for big Hessian varince regime 3PCv2 as again preferable.

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.0; Lavg = 1.0
3PCv2-Rand2-Top18; 256x 3PCv2-Rand2*Perm100-Top18; 1024x EF21-Top20; 32x EF21-cRand20; 32x 3PCv5-Top20; 4x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 32x
5 10 15

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.05; Lavg = 1.01
3PCv2-Rand2-Top18; 256x 3PCv2-Rand2*Perm100-Top18; 1024x EF21-Top20; 128x EF21-cRand20; 32x 3PCv5-Top20; 4x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 128x
5 10 15

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.84; Lavg = 1.1
3PCv2-Rand6-Top14; 256x 3PCv2-Rand2*Perm100-Top18; 1024x EF21-Top20; 128x EF21-cRand20; 32x 3PCv5-Top20; 4x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 128x
5 10 15

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 1.69; Lavg = 1.2
3PCv2-Rand10-Top10; 256x 3PCv2-Rand16*Perm100-Top4; 1024x EF21-Top20; 128x EF21-cRand20; 8x 3PCv5-Top20; 8x MARINA-Rand20; 4x 3PCv4-Top8-Top12; 256x

2

4

6

103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 6.74; Lavg = 1.79
3PCv2-Rand4-Top16; 128x 3PCv2-Rand4*Perm100-Top16; 512x EF21-Top20; 128x EF21-cRand20; 16x 3PCv5-Top20; 8x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 128x

2000

4000

103 101 10 1 10 3 10 5 10 7
0.0
103 101 10 1 10 3 10 5 10 7
0.0

3PCv2-Rand16-Top4; 256x 3PCv2-Rand12*Perm10-Top8; 2048x EF21-Top20; 32x EF21-cRand20; 128x 3PCv5-Top20; 4x MARINA-Rand20; 8x 3PCv4-Top2-Top18; 32x
2.5 5.0 7.5

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand16-Top4; 256x 3PCv2-Rand2*Perm1-Top18; 8192x EF21-Top20; 32x EF21-cRand20; 512x 3PCv5-Top20; 4x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 32x
2.5 5.0 7.5 #Mbits/n

103 101 10 1 10 3 10 5 10 7
0.0

3PCv2-Rand16-Top4; 256x 3PCv2-Rand12*Perm10-Top8; 2048x

103

EF21-Top20; 128x

E3PFC21v-5c-RTaonpd2200; ;41x28x 101

MARINA-Rand20; 8x

3PCv4-Top2-Top18; 128x

10 1

10 3

10 5

10 7

5

10

0.0

3PCv2-Rand16-Top4; 256x 3PCv2-Rand2*Perm1-Top18; 8192x EF21-Top20; 128x EF21-cRand20; 512x 3PCv5-Top20; 4x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 128x
2.5 5.0 7.5 #Mbits/n

103 101 10 1 10 3 10 5 10 7
0.0

3PCv2-Rand10-Top10; 256x 3PCv2-Rand2*Perm10-Top18; 4096x EF21-Top20; 128x EF21-cRand20; 64x 3PCv5-Top20; 4x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 256x
2.5 5.0 7.5

103 101 10 1 10 3 10 5 10 7
0.0

3PCv2-Rand18-Top2; 512x 3PCv2-Rand6*Perm1-Top14; 8192x EF21-Top20; 256x EF21-cRand20; 512x 3PCv5-Top20; 4x MARINA-Rand20; 4x 3PCv4-Top8-Top12; 256x
2.5 5.0 7.5 #Mbits/n

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand2-Top18; 512x 3PCv2-Rand14*Perm10-Top6; 4096x EF21-Top20; 256x EF21-cRand20; 64x 3PCv5-Top20; 8x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 256x
2.5 5.0 7.5

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand16-Top4; 512x 3PCv2-Rand10*Perm1-Top10; 8192x EF21-Top20; 256x EF21-cRand20; 1024x 3PCv5-Top20; 8x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 256x

2

4

#Mbits/n

103 101 10 1 10 3 10 5 10 7
0

3PCv2-Rand6-Top14; 256x 3PCv2-Rand2*Perm10-Top18; 4096x EF21-Top20; 256x EF21-cRand20; 64x 3PCv5-Top20; 32x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 256x

50

100

3PCv2-Rand8-Top12; 1024x 3PCv2-Rand14*Perm1-Top6; 8192x EF21-Top20; 256x EF21-cRand20; 256x 3PCv5-Top20; 32x MARINA-Rand20; 4x 3PCv4-Top2-Top18; 256x

10

20

30

#Mbits/n

Figure 9: Comparison of MARINA, EF21, 3PCv2, 3PCv5 and 3PCv5 with various compressors , ğ¾ = 0.02ğ‘‘ and tuned
stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 1000 || f(xt)||2

43

Fine-tuning of (ğ¾1, ğ¾2) pairs for 3PCv2 and 3PCv4 In this section we provide with some auxillary results on the demonstration of the tuning (ğ¾1, ğ¾2) pairs for 3PCv2 and 3PCv4 on different compressors. In the ğ¾ = ğ‘‘/ğ‘› scenario (see Figure 10), in all cases the best performance of 3PCv2 with Randğ¾1-Topğ¾2 is achieved when ğ¾2 > ğ¾1, whereas for the case when ğ¾/ğ‘‘ = 0.02 (see Figure 11) there is a dependence on ğ‘›; for ğ‘› = 10, the choice when ğ¾2 > ğ¾1 is preferable in all cases, whereas for ğ‘› = 100 and ğ‘› = 1000 it is the case only in big Hessian variance regime. At the same time, for optimal pairs (ğ¾1,ğ¾2) of the method 3PCv2 with Randğ¾1*Permğ¾-Topğ¾2 we observe that the choice is ğ¾2 > ğ¾1 (see Figures 12, 13).

n = 10 || f(xt)||2

n = 100 || f(xt)||2

10 3

LÂ±avg = 0.0; Lavg = 1.0

10 3 LÂ±avg = 0.05; Lavg = 1.01

10 3 LÂ±avg = 0.86; Lavg = 1.16

10 3 LÂ±avg = 1.72; Lavg = 1.32

10 3 LÂ±avg = 6.88; Lavg = 2.29

3PCv2-Rand90-Top10; 32x

3PCv2-Rand90-Top10; 32x

3PCv2-Rand90-Top10; 32x

3PCv2-Rand90-Top10; 32x

3PCv2-Rand90-Top10; 32x

3PCv2-Rand80-Top20; 32x

3PCv2-Rand80-Top20; 32x

3PCv2-Rand80-Top20; 32x

3PCv2-Rand80-Top20; 32x

3PCv2-Rand80-Top20; 32x

3PCv2-Rand70-Top30; 32x

3PCv2-Rand70-Top30; 32x

3PCv2-Rand70-Top30; 32x

3PCv2-Rand70-Top30; 32x

3PCv2-Rand70-Top30; 16x

3PCv2-Rand60-Top40; 32x

3PCv2-Rand60-Top40; 32x

3PCv2-Rand60-Top40; 32x

3PCv2-Rand60-Top40; 32x

3PCv2-Rand60-Top40; 16x

3PCv2-Rand50-Top50; 32x

3PCv2-Rand50-Top50; 32x

3PCv2-Rand50-Top50; 32x

3PCv2-Rand50-Top50; 32x

3PCv2-Rand50-Top50; 32x

10 5

3PCv2-Rand40-Top60; 32x 3PCv2-Rand30-Top70; 32x

10 5

3PCv2-Rand40-Top60; 32x 3PCv2-Rand30-Top70; 32x

10 5

3PCv2-Rand40-Top60; 32x 3PCv2-Rand30-Top70; 32x

10 5

3PCv2-Rand40-Top60; 32x 3PCv2-Rand30-Top70; 32x

10 5

3PCv2-Rand40-Top60; 32x 3PCv2-Rand30-Top70; 32x

3PCv2-Rand20-Top80; 32x

3PCv2-Rand20-Top80; 32x

3PCv2-Rand20-Top80; 32x

3PCv2-Rand20-Top80; 64x

3PCv2-Rand20-Top80; 64x

3PCv2-Rand10-Top90; 64x

3PCv2-Rand10-Top90; 64x

3PCv2-Rand10-Top90; 64x

3PCv2-Rand10-Top90; 64x

3PCv2-Rand10-Top90; 64x

10 7 0
10 3
10 5

20

40

3PCv2-Rand9-Top1; 512x 3PCv2-Rand8-Top2; 256x 3PCv2-Rand7-Top3; 256x 3PCv2-Rand6-Top4; 256x 3PCv2-Rand5-Top5; 256x 3PCv2-Rand4-Top6; 256x 3PCv2-Rand3-Top7; 256x 3PCv2-Rand2-Top8; 512x 3PCv2-Rand1-Top9; 512x

10 7 0
10 3
10 5

20

40

3PCv2-Rand9-Top1; 512x 3PCv2-Rand8-Top2; 256x 3PCv2-Rand7-Top3; 256x 3PCv2-Rand6-Top4; 256x 3PCv2-Rand5-Top5; 256x 3PCv2-Rand4-Top6; 256x 3PCv2-Rand3-Top7; 256x 3PCv2-Rand2-Top8; 512x 3PCv2-Rand1-Top9; 512x

10 7 0
10 3
10 5

20

40

3PCv2-Rand9-Top1; 512x 3PCv2-Rand8-Top2; 256x 3PCv2-Rand7-Top3; 256x 3PCv2-Rand6-Top4; 256x 3PCv2-Rand5-Top5; 512x 3PCv2-Rand4-Top6; 512x 3PCv2-Rand3-Top7; 512x 3PCv2-Rand2-Top8; 512x 3PCv2-Rand1-Top9; 512x

10 7 0
10 3
10 5

10

20

3PCv2-Rand9-Top1; 256x 3PCv2-Rand8-Top2; 256x 3PCv2-Rand7-Top3; 256x 3PCv2-Rand6-Top4; 256x 3PCv2-Rand5-Top5; 512x 3PCv2-Rand4-Top6; 512x 3PCv2-Rand3-Top7; 512x 3PCv2-Rand2-Top8; 512x 3PCv2-Rand1-Top9; 1024x

10 7 0
10 3
10 5

5000

10000

3PCv2-Rand9-Top1; 128x 3PCv2-Rand8-Top2; 64x 3PCv2-Rand7-Top3; 64x 3PCv2-Rand6-Top4; 64x 3PCv2-Rand5-Top5; 128x 3PCv2-Rand4-Top6; 256x 3PCv2-Rand3-Top7; 256x 3PCv2-Rand2-Top8; 512x 3PCv2-Rand1-Top9; 512x

10 7 0

2 #Mbits/n

10 7

4

0

2 #Mbits/n

10 7

4

0

10 7

2

4

0

#Mbits/n

2 #Mbits/n

10 7

4

0

20 40 60 #Mbits/n

Figure 10: Comparison of 3PCv2 with methods with Randğ¾1-Topğ¾2 with ğ¾1 + ğ¾2 = ğ‘‘/ğ‘› and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

44

n = 10 || f(xt)||2

n = 100 || f(xt)||2

10 3

LÂ±avg = 0.0; Lavg = 1.0

10 3 LÂ±avg = 0.05; Lavg = 1.01

10 3 LÂ±avg = 0.84; Lavg = 1.1

10 3 LÂ±avg = 1.69; Lavg = 1.2

10 3 LÂ±avg = 6.74; Lavg = 1.79

3PCv2-Rand18-Top2; 64x

3PCv2-Rand18-Top2; 64x

3PCv2-Rand18-Top2; 64x

3PCv2-Rand18-Top2; 64x

3PCv2-Rand18-Top2; 32x

3PCv2-Rand16-Top4; 64x

3PCv2-Rand16-Top4; 64x

3PCv2-Rand16-Top4; 32x

3PCv2-Rand16-Top4; 32x

3PCv2-Rand16-Top4; 32x

3PCv2-Rand14-Top6; 32x

3PCv2-Rand14-Top6; 32x

3PCv2-Rand14-Top6; 32x

3PCv2-Rand14-Top6; 256x

3PCv2-Rand14-Top6; 32x

3PCv2-Rand12-Top8; 128x

3PCv2-Rand12-Top8; 128x

3PCv2-Rand12-Top8; 32x

3PCv2-Rand12-Top8; 256x

3PCv2-Rand12-Top8; 32x

3PCv2-Rand10-Top10; 128x

3PCv2-Rand10-Top10; 128x

3PCv2-Rand10-Top10; 128x

3PCv2-Rand10-Top10; 256x

3PCv2-Rand10-Top10; 32x

3PCv2-Rand8-Top12; 128x

3PCv2-Rand8-Top12; 128x

3PCv2-Rand8-Top12; 128x

3PCv2-Rand8-Top12; 256x

10 5

3PCv2-Rand6-Top14; 128x 10 5

3PCv2-Rand6-Top14; 128x 10 5

3PCv2-Rand6-Top14; 256x 10 5

3PCv2-Rand6-Top14; 256x 10 5

3PCv2-Rand4-Top16; 128x

3PCv2-Rand4-Top16; 128x

3PCv2-Rand4-Top16; 256x

3PCv2-Rand4-Top16; 256x

3PCv2-Rand8-Top12; 64x 3PCv2-Rand6-Top14; 64x 3PCv2-Rand4-Top16; 128x

3PCv2-Rand2-Top18; 256x

3PCv2-Rand2-Top18; 256x

3PCv2-Rand2-Top18; 256x

3PCv2-Rand2-Top18; 256x

3PCv2-Rand2-Top18; 128x

10 7 0
10 3
10 5

10 7

10

20

0

10 3
3PCv2-Rand18-Top2; 256x 3PCv2-Rand16-Top4; 256x 3PCv2-Rand14-Top6; 128x 3PCv2-Rand12-Top8; 128x 3PCv2-Rand10-Top10; 128x 3PCv2-Rand8-Top12; 128x
3PCv2-Rand6-Top14; 128x 10 5
3PCv2-Rand4-Top16; 128x 3PCv2-Rand2-Top18; 256x

10 7

10

20

0

10 3
3PCv2-Rand18-Top2; 256x 3PCv2-Rand16-Top4; 256x 3PCv2-Rand14-Top6; 128x 3PCv2-Rand12-Top8; 128x 3PCv2-Rand10-Top10; 128x 3PCv2-Rand8-Top12; 128x
3PCv2-Rand6-Top14; 128x 10 5
3PCv2-Rand4-Top16; 128x 3PCv2-Rand2-Top18; 256x

10 7

5

10

15

0

10 3
3PCv2-Rand18-Top2; 256x 3PCv2-Rand16-Top4; 256x 3PCv2-Rand14-Top6; 256x 3PCv2-Rand12-Top8; 256x 3PCv2-Rand10-Top10; 256x 3PCv2-Rand8-Top12; 256x
3PCv2-Rand6-Top14; 256x 10 5
3PCv2-Rand4-Top16; 256x 3PCv2-Rand2-Top18; 256x

2

4

3PCv2-Rand18-Top2; 256x 3PCv2-Rand16-Top4; 256x 3PCv2-Rand14-Top6; 128x 3PCv2-Rand12-Top8; 128x 3PCv2-Rand10-Top10; 256x 3PCv2-Rand8-Top12; 256x 3PCv2-Rand6-Top14; 256x 3PCv2-Rand4-Top16; 256x 3PCv2-Rand2-Top18; 512x

10 7 0
10 3
10 5

2000 4000 6000
3PCv2-Rand18-Top2; 128x 3PCv2-Rand16-Top4; 64x 3PCv2-Rand14-Top6; 64x 3PCv2-Rand12-Top8; 64x 3PCv2-Rand10-Top10; 128x 3PCv2-Rand8-Top12; 128x 3PCv2-Rand6-Top14; 256x 3PCv2-Rand4-Top16; 256x 3PCv2-Rand2-Top18; 256x

10 7 0
10 3
10 5

2

4

6

3PCv2-Rand18-Top2; 256x 3PCv2-Rand16-Top4; 256x 3PCv2-Rand14-Top6; 128x 3PCv2-Rand12-Top8; 128x 3PCv2-Rand10-Top10; 128x 3PCv2-Rand8-Top12; 128x 3PCv2-Rand6-Top14; 128x 3PCv2-Rand4-Top16; 128x 3PCv2-Rand2-Top18; 256x

10 7 0
10 3
10 5

2

4

6

3PCv2-Rand18-Top2; 256x 3PCv2-Rand16-Top4; 256x 3PCv2-Rand14-Top6; 128x 3PCv2-Rand12-Top8; 128x 3PCv2-Rand10-Top10; 128x 3PCv2-Rand8-Top12; 128x 3PCv2-Rand6-Top14; 128x 3PCv2-Rand4-Top16; 128x 3PCv2-Rand2-Top18; 256x

10 7 0
10 3
10 5

2

4

6

3PCv2-Rand18-Top2; 512x 3PCv2-Rand16-Top4; 256x 3PCv2-Rand14-Top6; 256x 3PCv2-Rand12-Top8; 256x 3PCv2-Rand10-Top10; 256x 3PCv2-Rand8-Top12; 256x 3PCv2-Rand6-Top14; 256x 3PCv2-Rand4-Top16; 256x 3PCv2-Rand2-Top18; 256x

10 7 0
10 3
10 5

2

4

6

3PCv2-Rand18-Top2; 512x 3PCv2-Rand16-Top4; 512x 3PCv2-Rand14-Top6; 256x 3PCv2-Rand12-Top8; 256x 3PCv2-Rand10-Top10; 256x 3PCv2-Rand8-Top12; 256x 3PCv2-Rand6-Top14; 256x 3PCv2-Rand4-Top16; 256x 3PCv2-Rand2-Top18; 512x

10 7 0
10 3
10 5

25 50 75
3PCv2-Rand18-Top2; 512x 3PCv2-Rand16-Top4; 512x 3PCv2-Rand14-Top6; 512x 3PCv2-Rand12-Top8; 512x 3PCv2-Rand10-Top10; 512x 3PCv2-Rand8-Top12; 1024x 3PCv2-Rand6-Top14; 1024x 3PCv2-Rand4-Top16; 1024x 3PCv2-Rand2-Top18; 512x

10 7 0

2

4

6

#Mbits/n

10 7 0

2

4

6

#Mbits/n

10 7 0

2

4

#Mbits/n

10 7

6

0

2 #Mbits/n

10 7

4

0

2

4

6

#Mbits/n

Figure 11: Comparison of 3PCv2 with methods with Randğ¾1-Topğ¾2 with ğ¾1 + ğ¾2 = 0.02ğ‘‘ and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 1000 || f(xt)||2

n = 10 || f(xt)||2

n = 100 || f(xt)||2

10 3

LÂ±avg = 0.0; Lavg = 1.0

10 3 LÂ±avg = 0.05; Lavg = 1.01

10 3 LÂ±avg = 0.86; Lavg = 1.16

10 3 LÂ±avg = 1.72; Lavg = 1.32

10 3 LÂ±avg = 6.88; Lavg = 2.29

3PCv2-Rand90*Perm100-Top10; 128x

3PCv2-Rand90*Perm100-Top10; 128x

3PCv2-Rand90*Perm100-Top10; 256x

3PCv2-Rand90*Perm100-Top10; 256x

3PCv2-Rand90*Perm100-Top10; 32x

3PCv2-Rand80*Perm100-Top20; 128x

3PCv2-Rand80*Perm100-Top20; 128x

3PCv2-Rand80*Perm100-Top20; 128x

3PCv2-Rand80*Perm100-Top20; 128x

3PCv2-Rand80*Perm100-Top20; 64x

3PCv2-Rand70*Perm100-Top30; 128x

3PCv2-Rand70*Perm100-Top30; 128x

3PCv2-Rand70*Perm100-Top30; 128x

3PCv2-Rand70*Perm100-Top30; 128x

3PCv2-Rand70*Perm100-Top30; 64x

3PCv2-Rand60*Perm100-Top40; 128x

3PCv2-Rand60*Perm100-Top40; 128x

3PCv2-Rand60*Perm100-Top40; 128x

3PCv2-Rand60*Perm100-Top40; 128x

3PCv2-Rand60*Perm100-Top40; 128x

3PCv2-Rand50*Perm100-Top50; 64x

3PCv2-Rand50*Perm100-Top50; 64x

3PCv2-Rand50*Perm100-Top50; 128x

3PCv2-Rand50*Perm100-Top50; 128x

3PCv2-Rand50*Perm100-Top50; 128x

10 5

3PCv2-Rand40*Perm100-Top60; 64x 3PCv2-Rand30*Perm100-Top70; 128x

10 5

3PCv2-Rand40*Perm100-Top60; 64x 3PCv2-Rand30*Perm100-Top70; 128x

10 5

3PCv2-Rand40*Perm100-Top60; 128x 3PCv2-Rand30*Perm100-Top70; 128x

10 5

3PCv2-Rand40*Perm100-Top60; 128x 3PCv2-Rand30*Perm100-Top70; 128x

10 5

3PCv2-Rand40*Perm100-Top60; 128x 3PCv2-Rand30*Perm100-Top70; 128x

3PCv2-Rand20*Perm100-Top80; 128x

3PCv2-Rand20*Perm100-Top80; 128x

3PCv2-Rand20*Perm100-Top80; 128x

3PCv2-Rand20*Perm100-Top80; 128x

3PCv2-Rand20*Perm100-Top80; 128x

3PCv2-Rand10*Perm100-Top90; 128x

3PCv2-Rand10*Perm100-Top90; 128x

3PCv2-Rand10*Perm100-Top90; 128x

3PCv2-Rand10*Perm100-Top90; 256x

3PCv2-Rand10*Perm100-Top90; 256x

10 7 0
10 3
10 5

20

40

3PCv2-Rand9*Perm10-Top1; 4096x 3PCv2-Rand8*Perm10-Top2; 2048x 3PCv2-Rand7*Perm10-Top3; 4096x 3PCv2-Rand6*Perm10-Top4; 4096x 3PCv2-Rand5*Perm10-Top5; 4096x 3PCv2-Rand4*Perm10-Top6; 4096x 3PCv2-Rand3*Perm10-Top7; 4096x 3PCv2-Rand2*Perm10-Top8; 4096x 3PCv2-Rand1*Perm10-Top9; 4096x

10 7 0
10 3
10 5

20

40

3PCv2-Rand9*Perm10-Top1; 2048x 3PCv2-Rand8*Perm10-Top2; 2048x 3PCv2-Rand7*Perm10-Top3; 4096x 3PCv2-Rand6*Perm10-Top4; 4096x 3PCv2-Rand5*Perm10-Top5; 4096x 3PCv2-Rand4*Perm10-Top6; 4096x 3PCv2-Rand3*Perm10-Top7; 4096x 3PCv2-Rand2*Perm10-Top8; 4096x 3PCv2-Rand1*Perm10-Top9; 4096x

10 7 0
10 3
10 5

20
3PCv2-Rand9*Perm10-Top1; 4096x 3PCv2-Rand8*Perm10-Top2; 2048x 3PCv2-Rand7*Perm10-Top3; 4096x 3PCv2-Rand6*Perm10-Top4; 4096x 3PCv2-Rand5*Perm10-Top5; 4096x 3PCv2-Rand4*Perm10-Top6; 4096x 3PCv2-Rand3*Perm10-Top7; 4096x 3PCv2-Rand2*Perm10-Top8; 4096x 3PCv2-Rand1*Perm10-Top9; 8192x

10 7 0
10 3
10 5

5

10

3PCv2-Rand9*Perm10-Top1; 4096x 3PCv2-Rand8*Perm10-Top2; 4096x 3PCv2-Rand7*Perm10-Top3; 8192x 3PCv2-Rand6*Perm10-Top4; 8192x 3PCv2-Rand5*Perm10-Top5; 4096x 3PCv2-Rand4*Perm10-Top6; 4096x 3PCv2-Rand3*Perm10-Top7; 8192x 3PCv2-Rand2*Perm10-Top8; 8192x 3PCv2-Rand1*Perm10-Top9; 8192x

10 7 0
10 3
10 5

5000 10000 15000
3PCv2-Rand9*Perm10-Top1; 2048x 3PCv2-Rand8*Perm10-Top2; 4096x 3PCv2-Rand7*Perm10-Top3; 4096x 3PCv2-Rand6*Perm10-Top4; 4096x 3PCv2-Rand5*Perm10-Top5; 4096x 3PCv2-Rand4*Perm10-Top6; 4096x 3PCv2-Rand3*Perm10-Top7; 4096x 3PCv2-Rand2*Perm10-Top8; 4096x 3PCv2-Rand1*Perm10-Top9; 8192x

10 7 0

2

4

#Mbits/n

10 7 0

2

4

#Mbits/n

10 7 0

2

4

#Mbits/n

10 7 0

2 #Mbits/n

10 7

4

0

20 40 60 #Mbits/n

Figure 12: Comparison of 3PCv2 with methods with Randğ¾1*Permğ¾-Topğ¾2 with ğ¾1 + ğ¾2 = ğ‘‘/ğ‘› and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

45

n = 10 || f(xt)||2

n = 100 || f(xt)||2

10 3

LÂ±avg = 0.0; Lavg = 1.0

10 3 LÂ±avg = 0.05; Lavg = 1.01

10 3 LÂ±avg = 0.84; Lavg = 1.1

10 3 LÂ±avg = 1.69; Lavg = 1.2

10 3 LÂ±avg = 6.74; Lavg = 1.79

3PCv2-Rand18*Perm100-Top2; 1024x

3PCv2-Rand18*Perm100-Top2; 1024x

3PCv2-Rand18*Perm100-Top2; 1024x

3PCv2-Rand18*Perm100-Top2; 1024x

3PCv2-Rand18*Perm100-Top2; 32x

3PCv2-Rand16*Perm100-Top4; 512x

3PCv2-Rand16*Perm100-Top4; 512x

3PCv2-Rand16*Perm100-Top4; 512x

3PCv2-Rand16*Perm100-Top4; 1024x

3PCv2-Rand16*Perm100-Top4; 128x

3PCv2-Rand14*Perm100-Top6; 512x

3PCv2-Rand14*Perm100-Top6; 512x

3PCv2-Rand14*Perm100-Top6; 512x

3PCv2-Rand14*Perm100-Top6; 512x

3PCv2-Rand14*Perm100-Top6; 128x

3PCv2-Rand12*Perm100-Top8; 512x

3PCv2-Rand12*Perm100-Top8; 512x

3PCv2-Rand12*Perm100-Top8; 512x

3PCv2-Rand12*Perm100-Top8; 512x

3PCv2-Rand12*Perm100-Top8; 256x

3PCv2-Rand10*Perm100-Top10; 512x

3PCv2-Rand10*Perm100-Top10; 512x

3PCv2-Rand10*Perm100-Top10; 512x

3PCv2-Rand10*Perm100-Top10; 512x

3PCv2-Rand10*Perm100-Top10; 256x

3PCv2-Rand8*Perm100-Top12; 512x

3PCv2-Rand8*Perm100-Top12; 512x

3PCv2-Rand8*Perm100-Top12; 512x

3PCv2-Rand8*Perm100-Top12; 512x

3PCv2-Rand8*Perm100-Top12; 256x

10 5

3PCv2-Rand6*Perm100-Top14; 512x 10 5

3PCv2-Rand6*Perm100-Top14; 512x 10 5

3PCv2-Rand6*Perm100-Top14; 512x 10 5

3PCv2-Rand6*Perm100-Top14; 512x 10 5

3PCv2-Rand6*Perm100-Top14; 256x

3PCv2-Rand4*Perm100-Top16; 512x

3PCv2-Rand4*Perm100-Top16; 512x

3PCv2-Rand4*Perm100-Top16; 512x

3PCv2-Rand4*Perm100-Top16; 1024x

3PCv2-Rand4*Perm100-Top16; 512x

3PCv2-Rand2*Perm100-Top18; 1024x

3PCv2-Rand2*Perm100-Top18; 1024x

3PCv2-Rand2*Perm100-Top18; 1024x

3PCv2-Rand2*Perm100-Top18; 1024x

3PCv2-Rand2*Perm100-Top18; 512x

10 7 0
10 3
10 5

2

4

6

3PCv2-Rand18*Perm10-Top2; 2048x 3PCv2-Rand16*Perm10-Top4; 2048x 3PCv2-Rand14*Perm10-Top6; 2048x 3PCv2-Rand12*Perm10-Top8; 2048x 3PCv2-Rand10*Perm10-Top10; 1024x 3PCv2-Rand8*Perm10-Top12; 2048x 3PCv2-Rand6*Perm10-Top14; 2048x 3PCv2-Rand4*Perm10-Top16; 2048x 3PCv2-Rand2*Perm10-Top18; 2048x

10 7 0
10 3
10 5

2

4

6

3PCv2-Rand18*Perm10-Top2; 2048x 3PCv2-Rand16*Perm10-Top4; 2048x 3PCv2-Rand14*Perm10-Top6; 2048x 3PCv2-Rand12*Perm10-Top8; 2048x 3PCv2-Rand10*Perm10-Top10; 1024x 3PCv2-Rand8*Perm10-Top12; 2048x 3PCv2-Rand6*Perm10-Top14; 2048x 3PCv2-Rand4*Perm10-Top16; 2048x 3PCv2-Rand2*Perm10-Top18; 2048x

10 7 0
10 3
10 5

2

4

6

3PCv2-Rand18*Perm10-Top2; 2048x 3PCv2-Rand16*Perm10-Top4; 2048x 3PCv2-Rand14*Perm10-Top6; 2048x 3PCv2-Rand12*Perm10-Top8; 2048x 3PCv2-Rand10*Perm10-Top10; 2048x 3PCv2-Rand8*Perm10-Top12; 2048x 3PCv2-Rand6*Perm10-Top14; 2048x 3PCv2-Rand4*Perm10-Top16; 2048x 3PCv2-Rand2*Perm10-Top18; 4096x

10 7 0
10 3
10 5

10 7

1

2

3

0

10 3
3PCv2-Rand18*Perm10-Top2; 4096x 3PCv2-Rand16*Perm10-Top4; 4096x 3PCv2-Rand14*Perm10-Top6; 4096x 3PCv2-Rand12*Perm10-Top8; 2048x 3PCv2-Rand10*Perm10-Top10; 2048x 3PCv2-Rand8*Perm10-Top12; 2048x
3PCv2-Rand6*Perm10-Top14; 4096x 10 5
3PCv2-Rand4*Perm10-Top16; 4096x 3PCv2-Rand2*Perm10-Top18; 4096x

2000 4000 6000
3PCv2-Rand18*Perm10-Top2; 2048x 3PCv2-Rand16*Perm10-Top4; 2048x 3PCv2-Rand14*Perm10-Top6; 2048x 3PCv2-Rand12*Perm10-Top8; 2048x 3PCv2-Rand10*Perm10-Top10; 2048x 3PCv2-Rand8*Perm10-Top12; 2048x 3PCv2-Rand6*Perm10-Top14; 2048x 3PCv2-Rand4*Perm10-Top16; 2048x 3PCv2-Rand2*Perm10-Top18; 4096x

10 7 0.0
10 3
10 5

2.5 5.0 7.5
3PCv2-Rand18*Perm1-Top2; 4096x 3PCv2-Rand16*Perm1-Top4; 4096x 3PCv2-Rand14*Perm1-Top6; 4096x 3PCv2-Rand12*Perm1-Top8; 4096x 3PCv2-Rand10*Perm1-Top10; 4096x 3PCv2-Rand8*Perm1-Top12; 4096x 3PCv2-Rand6*Perm1-Top14; 4096x 3PCv2-Rand4*Perm1-Top16; 4096x 3PCv2-Rand2*Perm1-Top18; 8192x

10 7 0.0
10 3
10 5

2.5 5.0 7.5
3PCv2-Rand18*Perm1-Top2; 4096x 3PCv2-Rand16*Perm1-Top4; 4096x 3PCv2-Rand14*Perm1-Top6; 4096x 3PCv2-Rand12*Perm1-Top8; 4096x 3PCv2-Rand10*Perm1-Top10; 4096x 3PCv2-Rand8*Perm1-Top12; 4096x 3PCv2-Rand6*Perm1-Top14; 4096x 3PCv2-Rand4*Perm1-Top16; 4096x 3PCv2-Rand2*Perm1-Top18; 8192x

10 7 0.0
10 3
10 5

2.5

5.0

7.5

3PCv2-Rand18*Perm1-Top2; 8192x 3PCv2-Rand16*Perm1-Top4; 8192x 3PCv2-Rand14*Perm1-Top6; 8192x 3PCv2-Rand12*Perm1-Top8; 8192x 3PCv2-Rand10*Perm1-Top10; 8192x 3PCv2-Rand8*Perm1-Top12; 8192x 3PCv2-Rand6*Perm1-Top14; 8192x 3PCv2-Rand4*Perm1-Top16; 8192x 3PCv2-Rand2*Perm1-Top18; 8192x

10 7 0
10 3
10 5

2

4

3PCv2-Rand18*Perm1-Top2; 8192x 3PCv2-Rand16*Perm1-Top4; 8192x 3PCv2-Rand14*Perm1-Top6; 8192x 3PCv2-Rand12*Perm1-Top8; 8192x 3PCv2-Rand10*Perm1-Top10; 8192x 3PCv2-Rand8*Perm1-Top12; 8192x 3PCv2-Rand6*Perm1-Top14; 8192x 3PCv2-Rand4*Perm1-Top16; 8192x 3PCv2-Rand2*Perm1-Top18; 8192x

10 7 0
10 3
10 5

50

100

3PCv2-Rand18*Perm1-Top2; 16384x 3PCv2-Rand16*Perm1-Top4; 8192x 3PCv2-Rand14*Perm1-Top6; 8192x 3PCv2-Rand12*Perm1-Top8; 8192x 3PCv2-Rand10*Perm1-Top10; 8192x 3PCv2-Rand8*Perm1-Top12; 8192x 3PCv2-Rand6*Perm1-Top14; 16384x 3PCv2-Rand4*Perm1-Top16; 16384x 3PCv2-Rand2*Perm1-Top18; 16384x

10 7 0

5 #Mbits/n

10 7

10

0

5 #Mbits/n

10 7

10

0.0

2.5

5.0

#Mbits/n

10 7

7.5

0.0

2.5 5.0 7.5 #Mbits/n

10 7 0

10 #Mbits/n

Figure 13: Comparison of 3PCv2 with methods with Randğ¾1*Permğ¾-Topğ¾2 with ğ¾1 + ğ¾2 = 0.02ğ‘‘ and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 1000 || f(xt)||2

n = 10 || f(xt)||2

Figures 14 and 15 show that for the considered sparse quadratic problem in most cases the method 3PCv4 with Topğ¾1-Topğ¾2 compressors behaves as a EF21 with Top-ğ¾. Only in a few cases 3PCv4 shows an improvement over EF21.

10 3

LÂ±avg = 0.0; Lavg = 1.0

10 3 LÂ±avg = 0.05; Lavg = 1.01

10 3 LÂ±avg = 0.86; Lavg = 1.16

10 3 LÂ±avg = 1.72; Lavg = 1.32

10 3 LÂ±avg = 6.88; Lavg = 2.29

3PCv4-Top10-Top90; 32x

3PCv4-Top10-Top90; 32x

3PCv4-Top10-Top90; 32x

3PCv4-Top10-Top90; 32x

3PCv4-Top10-Top90; 32x

3PCv4-Top20-Top80; 32x

3PCv4-Top20-Top80; 32x

3PCv4-Top20-Top80; 32x

3PCv4-Top20-Top80; 32x

3PCv4-Top20-Top80; 32x

3PCv4-Top30-Top70; 32x

3PCv4-Top30-Top70; 32x

3PCv4-Top30-Top70; 32x

3PCv4-Top30-Top70; 32x

3PCv4-Top30-Top70; 32x

3PCv4-Top40-Top60; 32x

3PCv4-Top40-Top60; 32x

3PCv4-Top40-Top60; 32x

3PCv4-Top40-Top60; 32x

3PCv4-Top40-Top60; 32x

3PCv4-Top50-Top50; 32x

3PCv4-Top50-Top50; 32x

3PCv4-Top50-Top50; 32x

3PCv4-Top50-Top50; 32x

3PCv4-Top50-Top50; 32x

10 5

3PCv4-Top60-Top40; 32x 3PCv4-Top70-Top30; 32x

10 5

3PCv4-Top60-Top40; 32x 3PCv4-Top70-Top30; 32x

10 5

3PCv4-Top60-Top40; 32x 3PCv4-Top70-Top30; 32x

10 5

3PCv4-Top60-Top40; 32x 3PCv4-Top70-Top30; 32x

10 5

3PCv4-Top60-Top40; 32x 3PCv4-Top70-Top30; 32x

3PCv4-Top80-Top20; 32x

3PCv4-Top80-Top20; 32x

3PCv4-Top80-Top20; 32x

3PCv4-Top80-Top20; 32x

3PCv4-Top80-Top20; 32x

3PCv4-Top90-Top10; 32x

3PCv4-Top90-Top10; 32x

3PCv4-Top90-Top10; 32x

3PCv4-Top90-Top10; 32x

3PCv4-Top90-Top10; 32x

EF21-Top100; 32x

EF21-Top100; 32x

EF21-Top100; 32x

EF21-Top100; 32x

EF21-Top100; 32x

10 7 0
10 3
10 5

10

20

3PCv4-Top1-Top9; 64x 3PCv4-Top2-Top8; 64x 3PCv4-Top3-Top7; 64x 3PCv4-Top4-Top6; 64x 3PCv4-Top5-Top5; 64x 3PCv4-Top6-Top4; 64x 3PCv4-Top7-Top3; 64x 3PCv4-Top8-Top2; 64x 3PCv4-Top9-Top1; 64x EF21-Top10; 64x

10 7 0
10 3
10 5

10

20

3PCv4-Top1-Top9; 256x 3PCv4-Top2-Top8; 256x 3PCv4-Top3-Top7; 256x 3PCv4-Top4-Top6; 256x 3PCv4-Top5-Top5; 256x 3PCv4-Top6-Top4; 256x 3PCv4-Top7-Top3; 256x 3PCv4-Top8-Top2; 256x 3PCv4-Top9-Top1; 256x EF21-Top10; 256x

10 7 0
10 3
10 5

10 7

10

20

30

0

10 3
3PCv4-Top1-Top9; 256x

3PCv4-Top2-Top8; 256x

3PCv4-Top3-Top7; 256x

3PCv4-Top4-Top6; 256x

3PCv4-Top5-Top5; 256x

3PCv4-Top6-Top4; 256x 3PCv4-Top7-Top3; 256x

10 5

3PCv4-Top8-Top2; 256x

3PCv4-Top9-Top1; 256x

EF21-Top10; 256x

5

10

3PCv4-Top1-Top9; 256x 3PCv4-Top2-Top8; 256x 3PCv4-Top3-Top7; 256x 3PCv4-Top4-Top6; 256x 3PCv4-Top5-Top5; 256x 3PCv4-Top6-Top4; 256x 3PCv4-Top7-Top3; 256x 3PCv4-Top8-Top2; 256x 3PCv4-Top9-Top1; 256x EF21-Top10; 256x

10 7 0
10 3
10 5

5000

10000

3PCv4-Top1-Top9; 256x 3PCv4-Top2-Top8; 256x 3PCv4-Top3-Top7; 256x 3PCv4-Top4-Top6; 256x 3PCv4-Top5-Top5; 256x 3PCv4-Top6-Top4; 256x 3PCv4-Top7-Top3; 256x 3PCv4-Top8-Top2; 256x 3PCv4-Top9-Top1; 256x EF21-Top10; 256x

10 7 0

5

10

#Mbits/n

10 7 0

1

2

#Mbits/n

10 7

3

0

2 #Mbits/n

10 7 0

2 #Mbits/n

10 7

4

0

20

40

60

#Mbits/n

Figure 14: Comparison of 3PCv4 with methods with Topğ¾1-Topğ¾2 with ğ¾1 + ğ¾2 = ğ‘‘/ğ‘› and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

46

n = 100 || f(xt)||2

n = 10 || f(xt)||2

n = 100 || f(xt)||2

10 3

LÂ±avg = 0.0; Lavg = 1.0

10 3 LÂ±avg = 0.05; Lavg = 1.01

10 3 LÂ±avg = 0.84; Lavg = 1.1

10 3 LÂ±avg = 1.69; Lavg = 1.2

10 3 LÂ±avg = 6.74; Lavg = 1.79

3PCv4-Top2-Top18; 32x

3PCv4-Top2-Top18; 128x

3PCv4-Top2-Top18; 128x

3PCv4-Top2-Top18; 256x

3PCv4-Top2-Top18; 128x

3PCv4-Top4-Top16; 32x

3PCv4-Top4-Top16; 128x

3PCv4-Top4-Top16; 128x

3PCv4-Top4-Top16; 128x

3PCv4-Top4-Top16; 128x

3PCv4-Top6-Top14; 32x

3PCv4-Top6-Top14; 128x

3PCv4-Top6-Top14; 128x

3PCv4-Top6-Top14; 128x

3PCv4-Top6-Top14; 128x

3PCv4-Top8-Top12; 32x

3PCv4-Top8-Top12; 128x

3PCv4-Top8-Top12; 128x

3PCv4-Top8-Top12; 256x

3PCv4-Top8-Top12; 128x

3PCv4-Top10-Top10; 32x

3PCv4-Top10-Top10; 128x

3PCv4-Top10-Top10; 128x

3PCv4-Top10-Top10; 256x

3PCv4-Top10-Top10; 128x

3PCv4-Top12-Top8; 32x

3PCv4-Top12-Top8; 128x

3PCv4-Top12-Top8; 128x

3PCv4-Top12-Top8; 128x

3PCv4-Top12-Top8; 128x

10 5

3PCv4-Top14-Top6; 32x 10 5

3PCv4-Top14-Top6; 128x 10 5

3PCv4-Top14-Top6; 128x 10 5

3PCv4-Top14-Top6; 128x 10 5

3PCv4-Top14-Top6; 128x

3PCv4-Top16-Top4; 32x

3PCv4-Top16-Top4; 128x

3PCv4-Top16-Top4; 128x

3PCv4-Top16-Top4; 128x

3PCv4-Top16-Top4; 128x

3PCv4-Top18-Top2; 32x

3PCv4-Top18-Top2; 128x

3PCv4-Top18-Top2; 128x

3PCv4-Top18-Top2; 256x

3PCv4-Top18-Top2; 128x

EF21-Top20; 32x

EF21-Top20; 128x

EF21-Top20; 128x

EF21-Top20; 128x

EF21-Top20; 128x

10 7 0
10 3
10 5

10

20

3PCv4-Top2-Top18; 32x 3PCv4-Top4-Top16; 32x 3PCv4-Top6-Top14; 32x 3PCv4-Top8-Top12; 32x 3PCv4-Top10-Top10; 32x 3PCv4-Top12-Top8; 32x 3PCv4-Top14-Top6; 32x 3PCv4-Top16-Top4; 32x 3PCv4-Top18-Top2; 32x EF21-Top20; 32x

10 7 0
10 3
10 5

2

4

6

3PCv4-Top2-Top18; 128x 3PCv4-Top4-Top16; 128x 3PCv4-Top6-Top14; 128x 3PCv4-Top8-Top12; 128x 3PCv4-Top10-Top10; 128x 3PCv4-Top12-Top8; 128x 3PCv4-Top14-Top6; 128x 3PCv4-Top16-Top4; 128x 3PCv4-Top18-Top2; 128x EF21-Top20; 128x

10 7 0.0
10 3
10 5

10 7

2.5

5.0

7.5

0

10 3
3PCv4-Top2-Top18; 256x 3PCv4-Top4-Top16; 128x 3PCv4-Top6-Top14; 256x 3PCv4-Top8-Top12; 256x 3PCv4-Top10-Top10; 128x 3PCv4-Top12-Top8; 128x
3PCv4-Top14-Top6; 128x 10 5
3PCv4-Top16-Top4; 128x 3PCv4-Top18-Top2; 128x EF21-Top20; 128x

1

2

3

3PCv4-Top2-Top18; 256x 3PCv4-Top4-Top16; 256x 3PCv4-Top6-Top14; 256x 3PCv4-Top8-Top12; 256x 3PCv4-Top10-Top10; 256x 3PCv4-Top12-Top8; 256x 3PCv4-Top14-Top6; 256x 3PCv4-Top16-Top4; 256x 3PCv4-Top18-Top2; 128x EF21-Top20; 256x

10 7 0
10 3
10 5

1000 2000
3PCv4-Top2-Top18; 256x 3PCv4-Top4-Top16; 256x 3PCv4-Top6-Top14; 256x 3PCv4-Top8-Top12; 256x 3PCv4-Top10-Top10; 256x 3PCv4-Top12-Top8; 256x 3PCv4-Top14-Top6; 256x 3PCv4-Top16-Top4; 256x 3PCv4-Top18-Top2; 256x EF21-Top20; 256x

10 7 0
10 3
10 5

10

20

3PCv4-Top2-Top18; 32x 3PCv4-Top4-Top16; 32x 3PCv4-Top6-Top14; 32x 3PCv4-Top8-Top12; 32x 3PCv4-Top10-Top10; 32x 3PCv4-Top12-Top8; 32x 3PCv4-Top14-Top6; 32x 3PCv4-Top16-Top4; 32x 3PCv4-Top18-Top2; 32x EF21-Top20; 32x

10 7 0
10 3
10 5

2

4

6

3PCv4-Top2-Top18; 128x 3PCv4-Top4-Top16; 128x 3PCv4-Top6-Top14; 128x 3PCv4-Top8-Top12; 128x 3PCv4-Top10-Top10; 128x 3PCv4-Top12-Top8; 128x 3PCv4-Top14-Top6; 128x 3PCv4-Top16-Top4; 128x 3PCv4-Top18-Top2; 128x EF21-Top20; 128x

10 7 0
10 3
10 5

10 7

2

4

6

0

10 3
3PCv4-Top2-Top18; 128x 3PCv4-Top4-Top16; 256x 3PCv4-Top6-Top14; 128x 3PCv4-Top8-Top12; 256x 3PCv4-Top10-Top10; 256x 3PCv4-Top12-Top8; 256x
3PCv4-Top14-Top6; 128x 10 5
3PCv4-Top16-Top4; 256x 3PCv4-Top18-Top2; 256x EF21-Top20; 256x

2

4

3PCv4-Top2-Top18; 256x 3PCv4-Top4-Top16; 256x 3PCv4-Top6-Top14; 256x 3PCv4-Top8-Top12; 256x 3PCv4-Top10-Top10; 256x 3PCv4-Top12-Top8; 256x 3PCv4-Top14-Top6; 256x 3PCv4-Top16-Top4; 256x 3PCv4-Top18-Top2; 256x EF21-Top20; 256x

10 7 0
10 3
10 5

20

40

60

3PCv4-Top2-Top18; 256x 3PCv4-Top4-Top16; 256x 3PCv4-Top6-Top14; 256x 3PCv4-Top8-Top12; 256x 3PCv4-Top10-Top10; 256x 3PCv4-Top12-Top8; 256x 3PCv4-Top14-Top6; 256x 3PCv4-Top16-Top4; 256x 3PCv4-Top18-Top2; 256x EF21-Top20; 256x

10 7 0

10

20

#Mbits/n

10 7 0

2

4

#Mbits/n

10 7

6

0

2

4

6

#Mbits/n

10 7 0

2

4

#Mbits/n

10 7 0

5

10

15

#Mbits/n

Figure 15: Comparison of 3PCv4 with methods with Topğ¾1-Topğ¾2 with ğ¾1 + ğ¾2 = 0.02ğ‘‘ and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 1000 || f(xt)||2

3PCv1 The next sequence of plots compares EF21 with Top-ğ¾, 3PCv1 with Top-ğ¾ and classical GD. Since all methods communicates different amount of ï¬‚oats8 on each iteration they are compared in terms of the # communication rounds. Yet being unpractical, 3PCv1 can provide an intuition of how the intermediate method between GD and EF21 could work and what performance can be achieved in 3PCv1 by additional sending ğ‘‘ dimensional vector from each node to the server. Figure 16 illustrates that in low Hessian variance regime 3PCv1 with Top-ğ¾ behaves as a classical GD, whereas in a more heterogeneous regime, it can loose GD in terms of the number of communication rounds.
8Each node in EF21 with Top-ğ¾ send exactly ğ¾ ï¬‚oats on server, whereas for 3PCv1 with Top-ğ¾ and GD server receives ğ‘‘ + ğ¾ and ğ‘‘ ï¬‚oats from each node respectively.
47

n = 10 || f(xt)||2

n = 100 || f(xt)||2

103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.0; Lavg = 1.0

103

EF21-Top20; 32x

3PCv1-Top20; 2x

GD; 1x

101

LÂ±avg = 0.05; Lavg = 1.01
EF21-Top20; 128x 3PCv1-Top20; 2x GD; 1x

10 1

10 3

10 5

10 7

5000

10000 0

103
EF21-Top20; 32x

3PCv1-Top20; 2x

GD; 1x

101

2000 4000 6000
EF21-Top20; 128x 3PCv1-Top20; 2x GD; 1x

10 1

10 3

10 5

10 7

5000

10000 0

103
EF21-Top20; 32x

3PCv1-Top20; 2x

GD; 1x

101

2000 4000 6000
EF21-Top20; 128x 3PCv1-Top20; 2x GD; 1x

10 1

10 3

10 5

10 7

5000

10000 0

communication rounds

2000 4000 6000 communication rounds

103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 0.84; Lavg = 1.1
EF21-Top20; 128x 3PCv1-Top20; 2x GD; 1x
2000 4000 6000
EF21-Top20; 128x 3PCv1-Top20; 2x GD; 1x
2000 4000 6000
EF21-Top20; 256x 3PCv1-Top20; 2x GD; 1x
2000 4000 6000 communication rounds

103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0 103 101 10 1 10 3 10 5 10 7
0

LÂ±avg = 1.69; Lavg = 1.2

EF21-Top20; 128x 3PCv1-Top20; 2x

103

GD; 1x

101

LÂ±avg = 6.74; Lavg = 1.79
EF21-Top20; 128x 3PCv1-Top20; 2x GD; 1x

10 1

10 3

10 5

10 7 1000 2000 3000 0.0

103
EF21-Top20; 256x

3PCv1-Top20; 2x

GD; 1x

101

0.5

1.0

1e6

EF21-Top20; 256x 3PCv1-Top20; 4x GD; 1x

10 1

10 3

10 5

10 7

2000 4000 6000

0

103
EF21-Top20; 256x

3PCv1-Top20; 2x

GD; 1x

101

20000

40000

EF21-Top20; 256x 3PCv1-Top20; 8x GD; 1x

10 1

10 3

10 5

2500 5000 7500 communication rounds

10 7 0

5000

10000

communication rounds

Figure 16: Comparison of GD , 3PCv1 with Top-ğ¾ and EF21 with Top-ğ¾ for ğ¾ = 0.02ğ‘‘ and tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize is set to a multiple of the largest stepsize predicted by theory. ğ¿ğ‘Â±ğ‘£ğ‘” and ğ¿ğ‘âˆ’ğ‘£ğ‘” are the averaged constants ğ¿Â± and ğ¿âˆ’ per column.

n = 1000 || f(xt)||2

E.3. Testing compressed lazy aggregation (CLAG)
Following RichtaÂ´rik et al. (2021), we show the performance advantages of CLAG. We recall that we are interested in solving the non-convex logistic regression problem,

1

ğ‘
âˆ‘ï¸

âŠ¤

ğ‘‘
âˆ‘ï¸

ğ‘¥2ğ‘—

min ğ‘“ (ğ‘¥) =

ğ‘‘

ğ‘

log(1 + exp(âˆ’ğ‘¦ğ‘–ğ‘ğ‘– ğ‘¥)) + ğœ†

1 + ğ‘¥2 ,

(80)

ğ‘¥âˆˆR ğ‘–=1

ğ‘—=1

ğ‘—

where ğ‘ğ‘– âˆˆ Rğ‘‘, ğ‘¦ğ‘– âˆˆ {âˆ’1, 1} are the training data, and ğœ† > 0 is a regularization parameter. Parameter ğœ† is always set to 0.1 in the experiments. We use four LIBSVM (Chang & Lin, 2011) datasets phishing, w6a, a9a, ijcnn1 as training data. A dataset has been evenly split into ğ‘› = 20 equal parts where each part represents a separate client dataset (the remainder of partition between clients has been withdrawn).
Heatmap of communication complexities of CLAG for different combinations of parameters. In our ï¬rst group of experiments (see Figures 17, 18, 19, 20), we run CLAG with Top-ğ¾ compressor. The compression level ğ¾ varies evenly between 1 and ğ‘‘, where ğ‘‘ is the number of features of a chosen dataset. Trigger ğœ passes zero and subsequent powers of two from zero to eleven. For each combination of ğ¾ and ğœ, we compute empirically the minimum number of bits per worker sent from clients to the server. Minimum is taken among 12 launches of CLAG with different scalings of the theoretical stepsize, scales are powers of two from zero to eleven. The stopping criterion for each launch is based on the condition: â€–âˆ‡ğ‘“ (ğ‘¥)â€– < ğ›¿, where ğ›¿ equals to 10âˆ’4 for phishing and to 10âˆ’2 for a9a, ijcnn1 and w6a datasets. Since the algorithm may not converge with too large stepsizes, the time limit of ï¬ve minutes has been set for one launch. We stress that CLAG reduces to LAG when ğ‘˜ = ğ‘‘ and to EF21 when ğœ = 0. The experiment shows that for the most of datasets (excluding phishing) the minimum communication complexity is attained at a combination of (ğ¾, ğœ), which does not reduce CLAG to EF21 or LAG. Thus CLAG can be consistently faster than EF21 and LAG.

48

Plots for limited communication cost. In our second group of experiments (see Figures 21, 22, 23, 24), we are in the same setup as in the previous one but this time the stopping criterion bounds the communication cost of algorithms; CLAG, LAG and EF21 stop when they ï¬rst hit the communication cost of 32 Mbits per client. Compression levels ğ¾ for each dataset at each plot correspond to 1, 25% and 50% of features. Stepsizes for each algorithm is ï¬ne-tuned over the same grid as in the previous experiment. The best ğœ are chosen for CLAG and LAG from the same grid as in the previous experiment. The experiment exhibits again but from the different perspective the advantages of CLAG over its counterparts.

68 58 48 comp3r9ession l2e9vel K 20 10 1

EF21

phishing

12448 12704 12704 12704 12160 12736 13088 13408 13760 11136 13088 13603 12262

13696 13056 13056 13056 12416 12688 12560 12528 12512 12624 12768 12720 12960

28000 22000

13056 11776 11776 12224 12416 12224 11776 11104 11776 11136 11168 11136 11136

12384 14240 14240 15168 12476 13312 13776 13312 14332 14147 14240 13915 14240

15000

14656 15904 14656 15904 14656 13782 14656 14656 13532 13408 13408 13408 13408

19072 19072 14464 16000 14387 14464 13004 12928 12928 13081 12928 12928 12928 15168 16745 13312 17024 13312 13312 13312 13312 15168 13312 15168 15168 15168

10000

23936 13817 23936 15232 13056 28288 23936 26112 21760 23936 6528 21760 21760 LAG 7000 0 1 2 4 8 16 trig3g2er 64 128 256 512 1024 2048

Figure 17: Heatmap of communication complexities of CLAG for different combination of compression levels and triggers with ï¬ne-tuned stepsizes on phishing dataset. We contour cells corresponding to EF21 and LAG, as special cases of CLAG, by black rectangles. The red-countered cell indicates the experiment with the smallest communication cost.

49

300 266 233 2c0o0mp1r6es7sio1n3l3evel10K0 67 34 1

EF21

w6a

27520 27456 27456 27456 27360 27392 27296 27232 27060 26950 26651 26139 26067 25059

27008 25920 25920 25920 27443 25593 27008 26137 25376 24451 22819 22656 22003 21622

24608 28896 28896 27073 26323 24393 23750 24179 22785 22249 20534 21070 20856 22142

25600 32000 31040 28320 22400 22080 25600 23360 22560 22080 22400 22400 22400 22400

26624 38966 29603 30241 28539 29390 30241 27900 26411 25985 26411 22580 22580 22580

25632 47008 25632 30976 25899 28571 27769 27235 25632 25632 25632 20020 20288 20288

35200 48000 28800 23040 23680 24640 24320 24320 16000 16000 16000 16000 16000 16000

31968 54336 31968 26748 24512 24512 24884 25630 24512 17056 24512 17056 17056 17056

35136 60672 35136 35136 27049 19388 20665 23644 22368 19388 21516 23219 18112 18112

38400 41280 38400 38400 34080 19200 28800 19200 19200 19200 19200 19200 28800 28800 LAG 0 1 2 4 8 16 3t2rigger64 128 256 512 1024 2048 4096

60000 45000 32000 22000 16000

Figure 18: Heatmap of communication complexities of CLAG for different combination of compression levels and triggers with ï¬ne-tuned stepsizes on w6a dataset. We contour cells corresponding to EF21 and LAG, as special cases of CLAG, by black rectangles. The red-countered cell indicates the experiment with the smallest communication cost.

22 19 17 1c5ompr1e2ssion1l0evel 8K 5 3 1

EF21

ijcnn1.bz2

3008 3872 3872 3872 3883 3054 2867 4808 4880 4811 4872 3288

4544 3363 3070 2835 2691 2628 2926 4884 2216 2681 3497 3929

5984 4160 2464 2952 2568 3352 5216 2352 5128 5224 3512 3568

9152 4505 2918 3379 5888 5696 7667 8230 3724 8652 3968 8652

5184 4496 9088 3520 3568 4736 7344 5520 4768 5296 7296 6144

6080 7462 9267 9651 7136 6464 6368 6003 4678 4064 8038 9324

14624 7040 9824 8792 7688 3560 7448 10304 4592 4256 4160 10088

13216 7041 6905 12128 11176 4267 6742 3369 9979 5980 6089 9326

24416 8668 12712 4595 5142 7027 5324 6601 5537 4504 6176 4291

16896 9328 7497 4998 10313 14995 9046 4963 4892 8060 5632 10032 LAG 0 1 2 4 8 16trigger 32 64 128 256 512 1024

24000 15000 8500 4500 2500

Figure 19: Heatmap of communication complexities of CLAG for different combination of compression levels and triggers with ï¬ne-tuned stepsizes on ijcnn1 dataset. We contour cells corresponding to EF21 and LAG, as special cases of CLAG, by black rectangles. The red-countered cell indicates the experiment with the smallest communication cost.

50

123 105 88 comp7r0ession l5e3vel K 35 18 1

EF21

a9a

11360 11296 11296 11296 11104 11360 11392 10880 11328 11620 11699 13502

45000

12000 13152 13670 17184 19488 21792 19459 20035 19516 19171 18912 18739

18496 18496 25328 19616 19784 19616 19672 19616 19616 19616 19616 19616 17504 17504 17504 19200 19200 18860 17504 17504 17504 17504 17504 17504

30000

30816 26336 21856 24096 19616 24096 24096 19616 19616 17376 17376 17376 26464 22944 37728 48992 40544 40544 37728 37728 28716 18016 20832 17734

20000

47616 22416 40896 47616 44256 40896 37536 37536 34344 37536 34176 34176

35424 33652 39360 47232 39360 35424 35424 33259 31488 31488 31488 31488 LAG 0 1 2 4 8 16trigger 32 64 128 256 512 1024

11000

Figure 20: Heatmap of communication complexities of CLAG for different combination of compression levels and triggers with ï¬ne-tuned stepsizes on a9a dataset. We contour cells corresponding to EF21 and LAG, as special cases of CLAG, by black rectangles. The red-countered cell indicates the experiment with the smallest communication cost.We contour cells corresponding to EF21 and LAG, as special cases of CLAG, by black rectangles. The red-countered cell indicates the experiment with the smallest communication cost.

|| f(xt)||2

k = 1

k = 17

k = 34

10 2

10 4

10 6

10 8

10 10

10 12

10 14

10 16 10 18
10 20

EF21 64x
LAG 1x, = 0.0039 CLAG 64x, = 16.0

10 2

10 4

10 6

10 8

10 10

10 12

10 14

10 16 10 18
10 20

EF21 4x
LAG 1x, = 0.0039 CLAG 4x, = 16.0

10 2

10 4

10 6

10 8

10 10

10 12

10 14

10 16 10 18
10 20

EF21 4x
LAG 1x, = 0.0039 CLAG 4x, = 0.0039

0

10

20

30

#Mbits/n

0

10

20

30

#Mbits/n

0

10

20

30

#Mbits/n

Figure 21: Comparison of CLAG, LAG and EF21 with Top-ğ¾ with ï¬ne-tuned stepsizes on phishing dataset

51

|| f(xt)||2

|| f(xt)||2

10 1 10 2 10 3 10 4 10 5 10 6 0

k = 1
EF21 16x
LAG 4x, = 0.0039 CLAG 16x, = 256.0
20 #Mbits/n

10 1 10 2 10 3 10 4 10 5 40 10 6 0

k = 75
EF21 16x
LAG 4x, = 0.0039 10 1 CLAG 16x, = 0.0039
10 2

k = 150

10 3

20 #Mbits/n

10 4

EF21 16x

10 5

LAG 4x, = 0.0039

CLAG 16x, = 0.0039

40 10 6 0

20

40

#Mbits/n

Figure 22: Comparison of CLAG, LAG and EF21 with Top-ğ¾ with ï¬ne-tuned stepsizes on w6a dataset

k = 1 10 2

k = 5 10 2

k = 11 10 2

10 4

10 4

10 4

10 6

10 6

10 6

10 8

10 8

10 8

10 10

10 10

10 10

10 12 10 14

EF21 16x
LAG 1x, = 0.0039 CLAG 16x, = 256.0

10 12 10 14

EF21 16x
LAG 1x, = 0.0039 CLAG 16x, = 1.0

10 12 10 14

EF21 4x
LAG 1x, = 0.0039 CLAG 4x, = 256.0

0

10

20

30

#Mbits/n

0

10

20

30

#Mbits/n

0

10

20

30

#Mbits/n

Figure 23: Comparison of CLAG, LAG and EF21 with Top-ğ¾ with ï¬ne-tuned stepsizes on ijcnn1 dataset

k = 1 10 1

k = 30 10 1

k = 61 10 1

10 3

10 3

10 3

10 5

10 5

10 5

10 7

10 7

10 7

10 9

EF21 256x
LAG 1x, = 0.0039

10 9

EF21 16x
LAG 1x, = 0.0039

10 9

EF21 4x
LAG 1x, = 0.0039

10 11

CLAG 256x, = 1.0

10 11

CLAG 16x, = 1.0

10 11

CLAG 4x, = 256.0

0 10 20 30

0 10 20 30

0 10 20 30

#Mbits/n

#Mbits/n

#Mbits/n

Figure 24: Comparison of CLAG, LAG and EF21 with Top-ğ¾ with ï¬ne-tuned stepsizes on a9a dataset

|| f(xt)||2

52

