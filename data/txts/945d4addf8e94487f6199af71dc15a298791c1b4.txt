Age-of-information minimization via opportunistic sampling by an energy harvesting source

Akanksha Jaiswal, Arpan Chattopadhyay and Amokh Varma

arXiv:2201.02787v1 [eess.SY] 8 Jan 2022

Abstractâ€”Herein, minimization of time-averaged age-ofinformation (AoI) in an energy harvesting (EH) source equipped remote sensing setting is considered. The EH source opportunistically samples one or multiple processes over discrete time instants, and sends the status updates to a sink node over a time-varying wireless link. At any discrete time instant, the EH node decides whether to ascertain the link quality using its stored energy (called probing) and then, decides whether to sample a process and communicate the data based on the channel probe outcome. The trade-off is between the freshness of information available at the sink node and the available energy at the energy buffer of the source node. To this end, inï¬nite horizon Markov decision process (MDP) theory is used to formulate the problem of minimization of time-averaged expected AoI for a single energy harvesting source node. The following two scenarios are considered: (i) energy arrival process and channel fading process are independent and identically distributed (i.i.d.) across time, (ii) energy arrival process and channel fading process are Markovian. In the i.i.d. setting, after probing a channel, the optimal source node sampling policy is shown to be a threshold policy involving the instantaneous age of the process, the available energy in the buffer and the instantaneous channel quality as the decision variables. Also, for unknown channel state and energy harvesting characteristics, a variant of the Q-learning algorithm is proposed for the two-stage action taken by the source, that seeks to learn the optimal status update policy over time. For Markovian channel and Markovian energy arrival processes, the problem is again formulated as an MDP, and a learning algorithm is provided to handle unknown dynamics. Finally, numerical results are provided to demonstrate the policy structures and performance trade-offs.
Index Termsâ€”Age-of-information, remote sensing, energy harvesting, Markov decision process (MDP), Reinforcement learning.
I. INTRODUCTION
In recent years, the need for combining the physical systems with the cyber-world has attracted signiï¬cant research interest. These cyber-physical systems (CPS) are supported by ultralow power, low latency Internet of Things (IoT) networks, and encompass a large number of applications such as vehicle tracking, environment monitoring, intelligent transportation, industrial process monitoring, smart home systems etc. Such systems often require deployment of sensor nodes to monitor a physical process and send the real time status updates to a remote estimator over a wireless network. However, for such critical CPS applications, minimizing mean packet delay
Akanksha Jaiswal is with the Department of Electrical Engineering, Indian Institute of Technology, Delhi. Email: akanksha.jaiswal@ee.iitd.ac.in. Arpan Chattopadhyay is with the Department of Electrical Engineering and the Bharti School of Telecom Technology and Management, Indian Institute of Technology, Delhi. Email: arpanc@ee.iitd.ac.in. Amokh Varma is with the Department of Mathematics, Indian Institute of Technology, Delhi. Email: mt6180527@maths.iitd.ac.in.
This work was supported by the faculty seed grant and professional development allowance (PDA) of A.C. at IIT Delhi.
The preliminary version of this paper appeared in [1].

Fig. 1: Pictorial representation of a remote sensing system where an EH source samples one of ğ‘ number of processes at a time and sends the observation packet to a sink node.
without accounting for delay jitter can often be detrimental for the system performance. Also, mean delay minimization does not guarantee delivery of the observation packets to the sink node in the same order in which they were generated, thereby often resulting in unnecessarily dedicating network resources towards delivering outdated observation packets despite the availability of a freshly generated observation packet in the system. Hence, it is necessary to take into account the freshness of information of the data packets, apart from the mean packet delay.
Recently, a metric named Age of Information (AoI) has been proposed [2] as a measure of the freshness of the information update. In this setting, a sensor monitoring a system generates time stamped status update and sends it to the sink over a network. At time ğ‘¡, if the latest monitoring information available to the sink node comes from a packet whose timestamped generation instant was ğ‘¡ , then the AoI at the sink node is computed as (ğ‘¡ âˆ’ ğ‘¡ ). Thus, AoI has emerged as an alternative metric to mean delay [3].
However, timely delivery of the status updates is often limited by energy and bandwidth constraints in the network. Recent efforts towards designing EH source nodes (e.g., source nodes equipped with solar panels) have opened a new research paradigm for IoT network operations. The energy generation process in such nodes are very uncertain and typically modeled as a stochastic process. The harvested energy is stored in an energy buffer as energy packets, and used for sensing and communication as and when needed. This EH capability signiï¬cantly improves network lifetime and eliminates the need for frequent manual battery replacement, but poses a new challenge towards network operations due to uncertainty in the available energy at the source nodes at any given time.
Motivated by the above challenges, we consider the problem of minimizing the time-averaged expected AoI in a remote sensing setting, where a single EH source probes the channel state, samples one or multiple processes and sends the observation packets to the sink node over a fading channel. Energy generation process is modeled as a discrete-time

i.i.d. process, and a ï¬nite energy buffer is considered. Two variants of the problem are considered: (i) single process with CSIT, (ii) multiple processes with CSIT. Channel state probing and process sampling for time-averaged expected AoI minimization is formulated as an MDP, and the threshold nature of the optimal policy is established analytically for each case. Next, we propose reinforcement learning (RL) algorithms to ï¬nd the optimal policy when the channel statistics and energy harvesting characteristics are unknown. We also extend these works to the setting where the system has Markovian channel and Markovian energy harvesting process. Numerical results validate the theoretical results and intuitions.
A. Related work
Initial efforts towards optimizing AoI mostly involved the analysis of various queueing models; e.g., [2] for analysing a single source single server queueing system with FCFS service discipline, [4] for LCFS service discipline for M/M/1 queue, [5] and [6] for multi-source single sink system with M/M/1 queueing at each source, [7] for AoI performance analysis for multi-source single-sink inï¬nite-buffer queueing system where unserved packets are substituted by available newer ones, etc.
On the other hand, a number of papers have considered AoI minimization problem under EH setting: [8] for derivation of average AoI for a single source having ï¬nite battery capacity, [9] for derivation of the minimal age policy for EH two hop network, [10] for average AoI expression for single source EH server, [11] for AoI minimization for wirelessly powered user, [12] for sampling, transmission scheduling and transmit power selection for a single source single sink system over inï¬nite time horizon where delay is dependent on the packet transmission energy. The authors in [13] considered two source nodes (power grid node and EH sensor node) sending different data packets to a common destination by using multiple access channel, and derived the delay and AoI of two source nodes respectively.
There have also been several other works on developing optimal scheduling policy for minimizing AoI for EH sensor networks [14]â€“[24]. For example, [14] has investigated optimal online policy for single sensor single sink system with a noiseless channel for inï¬nite, ï¬nite and unit size battery capacity; for ï¬nite battery size, it has provided energy aware status update policy. The paper [15] has considered a multisensor single sink system with inï¬nite battery size, and proposed a randomized myopic scheduling scheme. For an EH source with ï¬nite battery and Poisson energy arrival process, the authors in [16] have provided age-energy tradeoff and optimal threshold policy for AoI minimization, but unlike our work, they do not consider probing/estimation of a fading channel while making a sampling decision. In [17], the optimal online status update policy to minimize the long run average AoI for EH source with updating erasures have been proposed. It has been shown that the best effort uniform updating policy is optimal when there is no feedback and besteffort uniform updating with re-transmission (BUR) policy is optimal when feedback is available to the source. The authors of [18] examined the problem of minimizing AoI under a constraint on the count of status updates. The authors of

[19] addressed AoI minimization problem for cognitive radio communication system with EH capability; they formulated optimal sensing and updating for perfect and imperfect sensing as a partially observable Markov decision process (POMDP). Information source diversity, i.e., multiple sources tracking the same process but with dissimilar energy cost, and sending status updates to the EH monitoring node with ï¬nite battery size, has been considered in [20] with an MDP formulation, but no structure was provided for the optimal policy. In [21], reinforcement learning has been used to minimize AoI for a single EH sensor with HARQ protocol, but no clear intuition on the policy structure was provided. In [23], the authors have formulated optimal transmission schemes for an EH sensor which sends update packets to the BS by considering different transmission modes (controlled by power and error probability) and battery recovery setting as an MDP. The authors of [24] have developed a threshold policy for minimizing AoI for a single sensor single sink system with erasure channel and no channel feedback. For a system with Poisson energy arrival, unit battery size and error-free channel, it has shown that a threshold policy achieves average age lower than that of zero-wait policy; based on this, lower bound on average age for general battery size and erasure channel has been derived. The papers [25] and [26] have proposed threshold based sampling policies for AoI minimization in a multi-source system, and also use deep reinforcement learning to learn the optimal policy. However, these paper do not consider channel probing as done in our paper. A very similar claim applies to [27] which also proposes threshold-based scheduling policy.
B. Our contributions and organization
1) We formulate the problem of minimizing the timeaveraged expected AoI in an EH remote sensing system with a single source monitoring one or multiple processes, as an MDP with two stage action model, which is different from standard MDP in the literature. Under the assumptions of i.i.d. time-varying channel with CSIT, channel state probing capability at the source, and ï¬nite battery size, we derive the optimal policy structures which turn out to be simple threshold policies. The source node, depending on the current age of a process, decides whether to probe the channel or not. Afterwards, based on the channel probe outcome, the source node decides whether to sample the process and send an observation packet, or to remain idle. Thus, the MDP involves taking action in two stages at each time instant.
2) We prove convergence of an analogue of value iteration for this two-stage MDP.
3) The threshold for multiple processes turns out to be a function of the relative age of the processes.
4) We prove certain interesting properties of various cost functions and some properties of the thresholds as a function of energy and age of the process, either analytically or numerically.
5) We also formulate the AoI minimization problem for a Markovian system as an MDP; the channel dynamics

and energy harvesting characteristics are modeled as Markov chains in this case. We provide optimality equations for the MDP. Certain conjectures regarding the threshold structure of the optimal policy for this MDP are validated numerically. 6) For unknown channel statistics and energy harvesting characteristics, we propose reinforcement learning algorithms that yield the optimal policies for the formulated MDPs. However, unlike standard Q-learning algorithm involving a single action at each time instant, we propose a variant of the Q-learning algorithm, that involves taking the action in two stages; ï¬rst the source decides whether to probe the channel quality, and if the channel quality turns out to be good, then it further decides whether to sample and communicate based on its available energy and age in sampling. 7) We numerically compare the AoI performance of our algorithm with the case where channel probing is not allowed. Surprisingly, it turns out that probing is sometimes better and sometimes worse than no probing; we provide intuitive justiï¬cation for this, and also assert that channel probing leads to smaller AoI in many practical scenarios.
The rest of the paper is organized as follows. The system model has been explained in Section II. AoI minimization for both single and multiple process case with i.i.d. system model is addressed in Section III, whereas AoI minimization for Markovian system model is considered in Section IV. Further, reinforcement learning for i.i.d. and Markovian system model is proposed in Section V. Lastly, numerical results are provided in Section VI, followed by the conclusions in Section VII. All proofs are provided in the appendices.
II. SYSTEM MODEL
We consider an EH source capable of sensing one out of ğ‘ different processes at a time, and reporting the observation packet to a sink node over a fading channel; see Figure 1. Time is discretized with the discrete time index ğ‘¡ âˆˆ {0, 1, 2, 3, Â· Â· Â·}. At each time, the source node can decide whether to estimate the quality of the channel from the source to the sink, or not. If the source node decides to probe the channel state, it can further decide whether to sample a process and communicate the data packet to the sink or not, depending on the instantaneous channel quality. The source has a ï¬nite energy buffer of size ğµ units, where ğ¸ ğ‘ units of buffer energy is used to probe the channel once and ğ¸ğ‘  units of buffer energy is used in sensing and communication of one packet.
A. Channel model
We consider a fading channel between the source and the sink, where a particular fading state is characterized by the probability of success of a transmitted packet. We denote by ğ‘(ğ‘¡) âˆˆ {ğ‘1, ğ‘2, Â· Â· Â· , ğ‘ğ‘š} the probability of packet transmission success from the source to the sink node at time ğ‘¡, and by ğ¶ (ğ‘¡) âˆˆ {ğ¶1, ğ¶2, Â· Â· Â· , ğ¶ğ‘š} the corresponding channel state. The packet success probability corresponding to channel state ğ¶ ğ‘— is given by ğ‘(ğ¶ ğ‘— ) = ğ‘ ğ‘— . Let us also denote by ğ‘Ÿ (ğ‘¡) âˆˆ {0, 1}

the indicator that the packet transmission from the source to the sink at time ğ‘¡ is successful. Obviously, P(ğ‘Ÿ (ğ‘¡) = 1|ğ¶ (ğ‘¡) = ğ¶ ğ‘— ) = ğ‘ ğ‘— for all ğ‘— âˆˆ {1, 2, Â· Â· Â· , ğ‘š}. It is assumed that the channel state ğ¶ (ğ‘¡) is learnt perfectly via a channel probe. I.I.D. channel model: In this case, we assume that {ğ‘(ğ‘¡)}ğ‘¡ â‰¥0 is i.i.d. across ğ‘¡, with P( ğ‘(ğ‘¡) = ğ‘ ğ‘— ) = ğ‘ ğ‘— for all ğ‘— âˆˆ {1, 2, Â· Â· Â· , ğ‘š}. Markovian channel model: Here the fading channel is modelled as ï¬nite state Markov chain [28], where the onestep state transition probability from state ğ¶ğ‘– to state ğ¶ ğ‘— is denoted by ğ‘ğ‘–, ğ‘— and the ğ‘¡-step transition probability is given by ğ‘ğ‘–(,ğ‘¡ğ‘—) = ğ‘ƒ(ğ¶ (ğ‘¡) = ğ¶ ğ‘— |ğ¶ (0) = ğ¶ğ‘–) for all ğ‘–, ğ‘— âˆˆ {1, 2, Â· Â· Â· , ğ‘š}.
B. Energy harvesting process
Let ğ´(ğ‘¡) denote the number of energy packet arrivals to the energy buffer at time ğ‘¡, and ğ¸ (ğ‘¡) denote the energy available to the source at time ğ‘¡, for all ğ‘¡ â‰¥ 0. We consider the following two models for energy harvesting. I.I.D. model: The energy packet generation process { ğ´(ğ‘¡)}ğ‘¡ â‰¥0 in the energy buffer is assumed to be an i.i.d. process with known mean ğœ† > 0. Markov model: Here the EH state of the source is modelled as a two state Markov chain with states {ğ»1, ğ»2}, where ğ»1 is called the harvesting state and ğ»2 is called the non harvesting state. When the source node is in harvesting state, the energy packet generation process is considered to be an i.i.d. process with known mean ğœ† > 0. When the source node is in nonharvesting state, then the energy packet generation rate is zero. The transition probability from state ğ»1 to state ğ»2 is â„1,2 and from state ğ»2 to state ğ»1 is â„2,1. Markov process for energy arrival has previously been used in [29], [30].
We assume that, in case the energy buffer at the source node is full, the newly generated energy packets will not be accommodated unless ğ¸ ğ‘ unit of energy is spent in probing.
C. Decision process and policy
At time ğ‘¡, let ğ‘(ğ‘¡) âˆˆ {0, 1} denote the indicator of deciding to probe the channel, and ğ‘(ğ‘¡) âˆˆ {0, 1, Â· Â· Â· , ğ‘ } denote the identity of the process being sampled, with ğ‘(ğ‘¡) = 0 meaning that no process is sampled, and ğ‘(ğ‘¡) = 0 meaning that the channel is not probed. Also, ğ‘(ğ‘¡) = 0 implies ğ‘(ğ‘¡) = 0. The set of possible actions or decisions is denoted by A = {{0, 0} âˆª {1 Ã— {0, 1, 2, Â· Â· Â· , ğ‘ }}}, where a generic action at time ğ‘¡ is denoted by (ğ‘(ğ‘¡), ğ‘(ğ‘¡)).
Let us denote by ğœğ‘˜ (ğ‘¡) sup{0 â‰¤ ğœ < ğ‘¡ : ğ‘(ğœ) = ğ‘˜, ğ‘Ÿ (ğœ) = 1} the last time instant before time ğ‘¡, when process ğ‘˜ was sampled and the observation packet was successfully delivered to the sink. The age of information (AoI) for the ğ‘˜-th process at time ğ‘¡ is given by ğ‘‡ğ‘˜ (ğ‘¡) = (ğ‘¡ âˆ’ğœğ‘˜ (ğ‘¡)). However, if ğ‘(ğ‘¡) = ğ‘˜ and ğ‘Ÿ (ğ‘¡) = 1, then ğ‘‡ğ‘˜ (ğ‘¡) = 0 since the current observation of the ğ‘˜-th process is available to the sink node. A generic Markov scheduling policy is a collection of mappings {ğœ‡ğ‘¡ }ğ‘¡ â‰¥0. where ğœ‡ğ‘¡ maps (ğ¸ (ğ‘¡), ğ¶ (ğ‘¡), {ğ‘‡ğ‘˜ (ğ‘¡)}1â‰¤ğ‘˜ â‰¤ğ‘ ) to the action space A. If ğœ‡ğ‘¡ = ğœ‡ for all ğ‘¡ â‰¥ 0, the policy is called stationary, else non-stationary.

We seek to ï¬nd a stationary scheduling policy ğœ‡ that minimizes the expected sum AoI which is averaged over time:

1

ğ‘‡
âˆ‘ï¸

ğ‘
âˆ‘ï¸

min

Eğœ‡ (ğ‘‡ğ‘˜ (ğ‘¡))

(1)

ğœ‡ğ‘‡

ğ‘¡=0 ğ‘˜=1

III. I.I.D. SYSTEM
In this section, we derive the optimal channel probing, source activation and data transmission policy for a single EH source sampling a single process (ğ‘ = 1) or multiple processes, when the channel and energy harvesting processes follow the i.i.d. model as described in Section II.

A. Single process (ğ‘ = 1)
Here, we formulate (1) as a long-run average cost MDP with state space S {0, 1, Â· Â· Â· , ğµ} Ã— Z+ and an intermediate state space V = {0, 1, . . . .., ğµ} Ã— Z+ Ã— {ğ¶1, ğ¶2, Â· Â· Â· , ğ¶ğ‘š}. A generic state ğ‘  = (ğ¸, ğ‘‡) means that the energy buffer has ğ¸ energy packets, and the source was last activated ğ‘‡ slots ago. A generic intermediate state ğ‘£ = (ğ¸, ğ‘‡, ğ¶) additionally means that the current channel state obtained via probing is ğ¶. The action space is A = {{0, 0} âˆª {1 Ã— {0, 1}}} with ğ‘(ğ‘¡), ğ‘(ğ‘¡) âˆˆ {0, 1}. At each time, if the source node decides not to probe the channel (ğ‘(ğ‘¡) = 0) and consequently does not sample (ğ‘(ğ‘¡) = 0), the expected single-stage AoI cost is ğ‘‡. However, if ğ‘(ğ‘¡) = 1, then the expected single-stage AoI cost is ğ‘‡ for ğ‘(ğ‘¡) = 0 and ğ‘‡ (1 âˆ’ ğ‘(ğ¶)) for ğ‘(ğ‘¡) = 1, where the expectation of the single stage cost is taken over packet success probability ğ‘(ğ¶). We ï¬rst formulate the average-cost MDP problem as an ğ›¼discounted cost MDP problem with ğ›¼ âˆˆ (0, 1), and derive the optimal policy, from which the solution of the average cost minimization problem can be obtained by taking ğ›¼ â†’ 1 [31, Section 4.1].
1) Optimality equation: Let ğ½âˆ— (ğ¸, ğ‘‡) be the optimal value function for state (ğ¸, ğ‘‡) in the discounted cost problem, and let ğ‘Šâˆ— (ğ¸, ğ‘‡, ğ¶) be the cost-to-go from an intermediate state (ğ¸, ğ‘‡, ğ¶). The Bellman equations are given by:

ğ½ âˆ— (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) = ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1) ,

ğ‘‰ âˆ—(ğ¸,ğ‘‡ )

ğ‘š
ğ‘‰ âˆ— (ğ¸ , ğ‘‡ ) = âˆ‘ï¸ ğ‘ğ‘— ğ‘Š âˆ— (ğ¸ , ğ‘‡ , ğ¶ ğ‘— )

ğ‘—=1
ğ‘Š âˆ— (ğ¸ , ğ‘‡ , ğ¶) = ğ‘šğ‘–ğ‘›{ğ‘‡ + ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ + ğ´, ğµ }, ğ‘‡ + 1) , ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) + ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  +

ğ´, ğµ }, ğ‘‡ + 1) }

ğ½ âˆ— (ğ¸ < ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) = ğ‘‡ + ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1)

(2)

The ï¬rst expression in the minimization in the R.H.S. of the ï¬rst equation in (2) is the cost of not probing channel state (ğ‘(ğ‘¡) = 0), which includes single-stage AoI cost ğ‘‡ and an ğ›¼ discounted future cost for a random next state (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1), averaged over the distribution of the number of energy packet generation ğ´. The quantity ğ‘‰âˆ— (ğ¸, ğ‘‡) is the expected cost of probing the channel state, which explains the second equation in (2). At an intermediate state (ğ¸, ğ‘‡, ğ¶), if ğ‘(ğ‘¡) = 0, a single stage AoI cost ğ‘‡ is incurred and the next

state becomes (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ + ğ´, ğµ}, ğ‘‡ + 1); if ğ‘(ğ‘¡) = 1, the expected AoI cost is ğ‘‡ (1 âˆ’ ğ‘(ğ¶)) (expectation taken over the packet success probability ğ‘(ğ¶)), and the next random state becomes (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, 1) and (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡ + 1) if ğ‘Ÿ (ğ‘¡) = 1 and ğ‘Ÿ (ğ‘¡) = 0, respectively. The last equation in (2) follows similarly since ğ‘(ğ‘¡) = 0, ğ‘(ğ‘¡) = 0 is the only possible action when ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ .
Substituting the value of ğ‘‰âˆ— (ğ¸, ğ‘‡) in the ï¬rst equation of
(2), we obtain the following Bellman equations:

ğ½âˆ—(ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘ ,ğ‘‡)
= ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1), Eğ¶ ğ‘šğ‘–ğ‘›{ğ‘‡ + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ + ğ´, ğµ}, ğ‘‡ + 1), ğ‘‡ (1 âˆ’ ğ‘(ğ¶)) + ğ›¼ğ‘(ğ¶)Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, 1) + ğ›¼(1 âˆ’ ğ‘(ğ¶))Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡ + 1)}

ğ½âˆ—(ğ¸ < ğ¸ğ‘ + ğ¸ğ‘ ,ğ‘‡)

= ğ‘‡ + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1)

(3)

2) Policy structure: We ï¬rst provide the convergence proof of value iteration since we have a two-stage decision process as opposed to traditional MDP where a single action is taken.
Proposition 1. The value function ğ½ (ğ‘¡) (ğ‘ ) converges to ğ½âˆ— (ğ‘ ) as ğ‘¡ â†’ âˆ.

Proof. See Appendix A.

Next we provide some properties of the value function, which will help in proving the structure of the optimal policy.
Lemma 1. For ğ‘ = 1, ğ½âˆ— (ğ¸, ğ‘‡) is increasing in ğ‘‡ and ğ‘Šâˆ— (ğ¸, ğ‘‡, ğ¶) is decreasing in ğ‘(ğ¶).

Proof. See Appendix B.

Conjecture 1. For ğ‘ = 1, the optimal probing policy for the ğ›¼-discounted AoI cost minimization problem is a threshold policy on ğ‘‡. For any ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , the optimal action is to probe the channel state if and only if ğ‘‡ â‰¥ ğ‘‡ğ‘¡â„ (ğ¸) for a threshold function ğ‘‡ğ‘¡â„ (ğ¸) of ğ¸.

This conjecture and many other conjectures later have been validated numerically in Section VI.

Theorem 1. For ğ‘ = 1, at any time, if the source decides to probe the channel, then the optimal sampling policy is a threshold policy on ğ‘(ğ¶). For any ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘  and probed channel state, the optimal action is to sample the source node if and only if ğ‘(ğ¶) â‰¥ ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) for a threshold function ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) of ğ¸ and ğ‘‡.

Proof. See Appendix C.

The policy structure supports the following two intuitions. Firstly, given ğ¸ and ğ‘‡, the source decides to probe the channel state if AoI is greater than some threshold value. Secondly, given ğ¸ and ğ‘‡ and probed channel state, if the channel quality is better than a threshold, then the optimal action is to sample the process and communicate the observation to the sink node. We will later numerically observe in Section VI

some intuitive properties of ğ‘‡ğ‘¡â„ (ğ¸) as a function of ğ¸ and ğœ†, and ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) as a function of ğ¸, ğ‘‡ and ğœ†.
3) No probing case: If we consider the case where the source samples the process directly without channel probing, then the Bellman equations for this case is given by:

ğ½ âˆ— (ğ¸ â‰¥ ğ¸ğ‘  , ğ‘‡ ) = ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1) ,

Eğ¶ ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘  + ğ´, ğµ },

1) + ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1)

ğ½ âˆ— (ğ¸ < ğ¸ğ‘  , ğ‘‡ ) = ğ‘‡ + ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1)

(4)

Here also we numerically observe an optimal threshold policy on ğ‘‡; see Section VI.

B. Multiple processes(ğ‘ > 1)

Here we formulate the ğ›¼-discounted cost version of (1)

as an MDP with a generic state ğ‘  = (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

which means that the energy buffer has ğ¸ energy packets,

and the k-th process was last activated ğ‘‡ğ‘˜ slots ago. Also, a

generic intermediate state is ğ‘£ = (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ , ğ¶) which

additionally means that the current channel state ğ¶ is learnt

by probing, has packet success probability ğ‘(ğ¶). The action

space A = {{0, 0} âˆª {1 Ã— {0, 1, 2, Â· Â· Â· , ğ‘ }}} with ğ‘(ğ‘¡) âˆˆ {0, 1}

and ğ‘(ğ‘¡) âˆˆ {0, 1, Â· Â· Â· , ğ‘ }. At each time, if the source node

decides not to probe the channel state then it will not sample

any process, thus ğ‘(ğ‘¡) = 0, ğ‘(ğ‘¡) = 0 and the expected single

stage AoI cost is

ğ‘ ğ‘–=1

ğ‘‡ğ‘– .

However,

if

the

source

node

decides

to probe the channel state (ğ‘(ğ‘¡) = 1), the expected single stage

AoI cost is

ğ‘
ğ‘–=1 ğ‘‡ğ‘–

for

ğ‘(ğ‘¡)

=

0

and

ğ‘–â‰ ğ‘˜ ğ‘‡ğ‘– +ğ‘‡ğ‘˜ (1 âˆ’ ğ‘(ğ¶)) for

ğ‘(ğ‘¡) = ğ‘˜ where the expectation is taken over packet success

probability ğ‘(ğ¶).

1) Optimality equation: In this case, the Bellman equations

are given by:

ğ½ âˆ— (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

ğ‘

=

ğ‘šğ‘–ğ‘›

âˆ‘ï¸ ğ‘‡ğ‘–

+

ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸

+

ğ´,

ğµ }, ğ‘‡1

+

1, ğ‘‡2

+

1,

Â·

Â·

Â· , ğ‘‡ğ‘

+

1) ,

ğ‘–=1

ğ‘‰ âˆ— (ğ¸ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

ğ‘‰ âˆ— (ğ¸ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )
ğ‘š
= âˆ‘ï¸ ğ‘ğ‘— ğ‘Š âˆ— (ğ¸ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ , ğ¶ ğ‘— )
ğ‘—=1

ğ‘Š âˆ— (ğ¸ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ , ğ¶)

ğ‘

=

âˆ‘ï¸ ğ‘šğ‘–ğ‘›{ ğ‘‡ğ‘–

+

ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ }, ğ‘‡1

+

1, ğ‘‡2

+

1,

Â·

Â·

Â· , ğ‘‡ğ‘

+

1) ,

ğ‘–=1

min

âˆ‘ï¸ ğ‘‡ + ğ‘‡ (1 âˆ’ ğ‘(ğ¶)) + ğ›¼ğ‘(ğ¶)E

ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸

âˆ’ğ¸

+ ğ´,

ğ‘–ğ‘˜

ğ´

ğ‘

ğ‘ 

1â‰¤ğ‘˜â‰¤ğ‘ ğ‘–â‰ ğ‘˜

ğµ }, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ = 1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) + ğ›¼(1 âˆ’ ğ‘ (ğ¶))
ğ‘˜

Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) }

ğ½ âˆ— (ğ¸ < ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

ğ‘

=

âˆ‘ï¸ ğ‘‡ğ‘–

+

ğ›¼Eğ´ ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸

+

ğ´,

ğµ }, ğ‘‡1

+

1, ğ‘‡2

+

1,

Â·

Â·

Â· , ğ‘‡ğ‘

+

1)

(5)

ğ‘–=1

The ï¬rst expression in the minimization in the R.H.S. of the

ï¬rst equation in (5) is the cost of not probing channel state

(ğ‘(ğ‘¡) = 0), which includes single-stage AoI cost

ğ‘
ğ‘–=1 ğ‘‡ğ‘–

and

an ğ›¼ discounted future cost with a random next state (min{ğ¸ +

ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1), averaged over the distribution

of the number of energy packet generation ğ´. The quantity ğ‘‰âˆ— (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is the optimal expected cost of probing

the channel state, which explains the second equation in (5).

At an intermediate state (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ , ğ¶), if ğ‘(ğ‘¡) = 0,

a single stage AoI cost

ğ‘
ğ‘–=1 ğ‘‡ğ‘–

is

incurred

and

the

next

state

becomes (min{ğ¸âˆ’ğ¸ ğ‘+ğ´, ğµ}, ğ‘‡1+1, ğ‘‡2+1, Â· Â· Â· , ğ‘‡ğ‘ +1); if ğ‘(ğ‘¡) =

ğ‘˜, the expected AoI cost is ğ‘–â‰ ğ‘˜ ğ‘‡ğ‘– +ğ‘‡ğ‘˜ (1âˆ’ ğ‘(ğ¶)) (expectation

taken over the packet success probability ğ‘(ğ¶)), and the next

random state becomes (min{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 +

1, Â· Â· Â· , ğ‘‡ğ‘˜ = 1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) and (min{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) if ğ‘Ÿ (ğ‘¡) = 1 and ğ‘Ÿ (ğ‘¡) = 0,

respectively. The last equation in (5) follows similarly since

ğ‘(ğ‘¡) = 0, ğ‘(ğ‘¡) = 0 is the only possible action when ğ¸ <

ğ¸ğ‘ + ğ¸ğ‘ . Substituting the value of ğ‘‰âˆ— (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) in the ï¬rst

equation of (5), we obtain the Bellman equations (6).

2) Policy structure:

Lemma 2. For ğ‘ > 1, the value function ğ½âˆ— (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is increasing in each of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ and ğ‘Šâˆ— (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ , ğ¶) is decreasing in ğ‘(ğ¶).
Proof. See Appendix D.

Let us deï¬ne T = [ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ] and Tâˆ’ğ‘˜ = [ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘˜âˆ’1, ğ‘‡ğ‘˜+1, Â· Â· Â· , ğ‘‡ğ‘ ].
Conjecture 2. For ğ‘ > 1, the optimal probing policy for the ğ›¼-discounted AoI cost minimization problem is a threshold policy on max1â‰¤ğ‘˜ â‰¤ğ‘ ğ‘‡ğ‘˜ ğ‘‡ğ‘˜âˆ— . For any ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , the optimal action is to probe the channel state if and only if max1â‰¤ğ‘˜ â‰¤ğ‘ ğ‘‡ğ‘˜ â‰¥ ğ‘‡ğ‘¡â„ (ğ¸, Tâˆ’ğ‘˜âˆ— ) for a threshold function ğ‘‡ğ‘¡â„ (ğ¸, Tâˆ’ğ‘˜âˆ— ) of ğ¸ and Tâˆ’ğ‘˜âˆ— .

We next show that sampling the process with largest AoI is optimal.

Theorem 2. For ğ‘ > 1, after probing the channel state, the optimal source activation policy for the ğ›¼-discounted cost problem is a threshold policy on p(C). For any ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘  and probed channel state, the optimal action is to sample the process arg max1â‰¤ğ‘˜ â‰¤ğ‘ ğ‘‡ğ‘˜ if and only if ğ‘(ğ¶) â‰¥ ğ‘ğ‘¡â„ (ğ¸, T ) for a threshold function ğ‘ğ‘¡â„ (ğ¸, T ) of (ğ¸, T ).

Proof. See Appendix E.

We will later numerically demonstrate some intuitive properties of ğ‘‡ğ‘¡â„ (ğ¸, Tâˆ’ğ‘˜ ) as a function of ğ¸, Tâˆ’ğ‘˜ and ğœ† and ğ‘ğ‘¡â„ (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) as a function of ğ¸, (ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) and ğœ† in Section VI.

IV. MARKOVIAN SYSTEM MODEL: SINGLE PROCESS
Here, we derive the optimal channel probing, source activation and data transmission policy for an EH source sampling a single process with Markovian channel and Markovian energy arrival process. We formulate the ğ›¼discounted cost version of (1) as an MDP with state space

ğ‘

ğ½âˆ— (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

=

ğ‘šğ‘–ğ‘›

âˆ‘ï¸ ğ‘‡ğ‘–

+

ğ›¼E ğ´ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸

+

ğ´,

ğµ}, ğ‘‡1

+

1, ğ‘‡2

+

1, Â· Â· Â· , ğ‘‡ğ‘

+

1),

ğ‘–=1

ğ‘

Eğ¶

âˆ‘ï¸ ğ‘šğ‘–ğ‘›{ ğ‘‡ğ‘–

+

ğ›¼E ğ´ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ}, ğ‘‡1

+

1, ğ‘‡2

+

1, Â· Â· Â· , ğ‘‡ğ‘

+

1),

ğ‘–=1

min âˆ‘ï¸ ğ‘‡ğ‘– + ğ‘‡ğ‘˜ (1 âˆ’ ğ‘(ğ¶)) + ğ›¼ ğ‘(ğ¶)Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 + 1,
1â‰¤ğ‘˜ â‰¤ğ‘ ğ‘–â‰ ğ‘˜
ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘˜ = 1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) + ğ›¼(1 âˆ’ ğ‘(ğ¶))Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ},

ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) }

ğ‘

ğ½âˆ— (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

=

âˆ‘ï¸ ğ‘‡ğ‘–

+

ğ›¼E ğ´ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸

+

ğ´,

ğµ}, ğ‘‡1

+

1, ğ‘‡2

+

1, Â· Â· Â· , ğ‘‡ğ‘

+

1)

(6)

ğ‘–=1

S {0, 1, Â· Â· Â· , ğµ} Ã— Z+ Ã— Z+ Ã— {ğ¶1, ğ¶2, Â· Â· Â· , ğ¶ğ‘š} Ã— {ğ»1, ğ»2} where a generic state ğ‘  = (ğ¸, ğ‘‡, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) means that the energy buffer has ğ¸ energy packets, the source was last activated ğ‘‡ slots ago, the channel state was last probed ğœ slots ago where ğœ â‰¤ ğ‘‡, the previous channel state obtained via probing is ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ âˆˆ {ğ¶1, ğ¶2, Â· Â· Â· , ğ¶ğ‘š}, and the EH source is in harvesting state ğ»1. A generic state ğ‘  = (ğ¸, ğ‘‡, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»2) similarly means that the EH source is in non-harvesting state ğ»2. Also, V denotes intermediate state space where a generic state ğ‘£ = (ğ¸, ğ‘‡, ğ¶, ğ»1) or ğ‘£ = (ğ¸, ğ‘‡, ğ¶, ğ»2) is used for current channel state ğ¶ (obtained after probing). The action space A = {{0, 0} âˆª {1 Ã— {0, 1}}} with ğ‘(ğ‘¡), ğ‘(ğ‘¡) âˆˆ {0, 1}. At each time, if ğ‘(ğ‘¡) = 0, ğ‘(ğ‘¡) = 0, then the expected single-stage AoI cost is ğ‘‡. However, if ğ‘(ğ‘¡) = 1, the expected single-stage AoI cost is ğ‘‡ and ğ‘‡ (1 âˆ’ ğ‘(ğ¶)) for ğ‘(ğ‘¡) = 0 and ğ‘(ğ‘¡) = 1, respectively.
A. Optimality equation
In this case, for state (ğ¸, ğ‘‡, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1), the Bellman equations are given by:
ğ½ âˆ— (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1)
= ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´|ğ»1,ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ |ğ»1 ğ½ âˆ— (ğ‘šğ‘–ğ‘› {ğ¸ + ğ´, ğµ }, ğ‘‡ + 1, ğœ + 1, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ ,
ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ ) , ğ‘‰ âˆ— (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1)
ğ‘‰ âˆ— (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) = Eğ¶âˆ¼ğ‘ ( ğœ) (Â·|ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ ) ğ‘Š âˆ— (ğ¸ , ğ‘‡ , ğ¶ , ğ»1)
ğ‘Š âˆ— (ğ¸ , ğ‘‡ , ğ¶, ğ»1) = ğ‘šğ‘–ğ‘› {ğ‘‡ + ğ›¼Eğ´|ğ»1,ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ |ğ»1 ğ½ âˆ— (ğ‘šğ‘–ğ‘› {ğ¸ âˆ’ ğ¸ğ‘ + ğ´, ğµ }, ğ‘‡ + 1, 1, ğ¶,
ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ ) , ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´|ğ»1,ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ |ğ»1 ğ½ âˆ— (ğ‘šğ‘–ğ‘› {ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1, 1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ ) + ğ›¼ (1 âˆ’ ğ‘ (ğ¶))Eğ´|ğ»1,ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ |ğ»1 ğ½ âˆ— (ğ‘šğ‘–ğ‘› {ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1, 1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ ) }
ğ½ âˆ— (ğ¸ < ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) = ğ‘‡ + ğ›¼Eğ´|ğ»1,ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ |ğ»1 ğ½ âˆ— (ğ‘šğ‘–ğ‘› {ğ¸ + ğ´, ğµ }, ğ‘‡ + 1, ğœ + 1, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ )
(7)
The ï¬rst expression in the minimization in the R.H.S. of the ï¬rst equation in (7) is the cost when ğ‘(ğ‘¡) = 0. The quantity ğ‘‰âˆ— (ğ¸, ğ‘‡, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) is the expected cost when ğ‘(ğ‘¡) = 1,

which explains the second equation in (7). Here ğ‘ (ğœ) denote the ğœ-step transition probability of the channel dynamics, starting from ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ .
Similarly, in (7), if we consider ğ»2 for state (ğ¸, ğ‘‡, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»2) then the same Bellman equations hold with ğ´ = 0. B. Policy structure
We conjecture the following properties regarding the value function and the optimal policy. Numerical validation of these conjectures is given in VI-C.
Conjecture 3. For Markovian system model and ğ‘ = 1, the value function ğ½âˆ— (ğ¸, ğ‘‡, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ») is increasing in ğ‘‡ and ğ‘Šâˆ— (ğ¸, ğ‘‡, ğ¶, ğ») is decreasing in ğ‘(ğ¶).
Conjecture 4. For Markovian system model and ğ‘ = 1, the optimal probing policy for the ğ›¼-discounted AoI cost minimization problem is a threshold policy on ğ‘‡. For any ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , the optimal action is to probe the channel state if and only if ğ‘‡ â‰¥ ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ») for a threshold function ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ») of ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ».
Conjecture 5. For Markovian system model and ğ‘ = 1, at any time, if the source decides to probe the channel, then the optimal sampling policy is a threshold policy on ğ‘(ğ¶). For any ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘  and probed channel state, the optimal action is to sample the source node if and only if ğ‘(ğ¶) â‰¥ ğ‘ğ‘¡â„ (ğ¸, ğ‘‡, ğ») for a threshold function ğ‘ğ‘¡â„ (ğ¸, ğ‘‡, ğ») of ğ¸, ğ‘‡ and ğ».
V. REINFORCEMENT LEARNING FOR AOI OPTIMIZATION: SINGLE PROCESS
If the channel statistics and the energy harvesting characteristics are not known at the source, then the source has to learn them with time. We adapt the Q-learning technique [32, Section 11.4.2] to ï¬nd the optimal policy for our twostage action model. A. I.I.D. system
Here we assume that the energy generation rate ğœ† and the channel state probabilities {ğ‘ ğ‘— }1â‰¤ ğ‘— â‰¤ğ‘š are not known to the source. The packet success probabilities { ğ‘ ğ‘— }1â‰¤ ğ‘— â‰¤ğ‘š are either known or unknown; the packet success indicator ğ‘Ÿ (ğ‘¡) is used to estimate ğ‘ ğ‘— if it is unknown.

1) Optimality equation in terms of Q-function: The optimal Q function is denoted by ğ‘„âˆ— (ğ¸, ğ‘‡, ğ‘) for a generic state-action pair (ğ¸, ğ‘‡, ğ‘), where the state is (ğ¸, ğ‘‡) and the action is ğ‘ âˆˆ {0, 1}. The quantity ğ‘„âˆ— (ğ¸, ğ‘‡, ğ‘) denotes the expected cost obtained if the current state is (ğ¸, ğ‘‡), current action chosen is ğ‘, and an optimal policy is employed from the next time instant. Also, ğ‘„âˆ— (ğ¸, ğ‘‡, ğ¶, ğ‘) corresponds to the intermediate state (ğ¸, ğ‘‡, ğ¶) and a corresponding action ğ‘ âˆˆ {0, 1}. It is important to note that, we have to maintain two different types of Q functions to handle states and intermediate states; this is not done in standard Q-learning. The optimal Q-value functions are given by:
ğ‘„âˆ— (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡, ğ‘ = 0) = ğ‘‡ + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1) = ğ‘‡ + ğ›¼Eğ´ min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1, ğ‘) (8)
ğ‘ âˆˆ {0,1 }
Here the terms in the R.H.S of the ï¬rst equality operator in (8) is the immediate cost of not probing channel state action (ğ‘ = 0) when state is (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡) and an ğ›¼ discounted future cost Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1) which is later substituted by Eğ´ minğ‘âˆˆ{0,1} ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1, ğ‘) (averaged over the distribution of A). On the other hand,
ğ‘š
ğ‘„âˆ— (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğ‘ = 1) = âˆ‘ï¸ ğ‘ ğ‘— ğ‘Š âˆ— (ğ¸ , ğ‘‡ , ğ¶ ğ‘— )
ğ‘—=1 ğ‘š
= âˆ‘ï¸ ğ‘ğ‘— min ğ‘„âˆ— (ğ¸ , ğ‘‡ , ğ¶ ğ‘— , ğ‘) (9)
ğ‘—=1 ğ‘âˆˆ{0,1}
Now,
ğ‘„âˆ— (ğ¸, ğ‘‡, ğ¶, ğ‘ = 0) = ğ‘‡ + ğ›¼Eğ´ min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ + ğ´, ğµ}, ğ‘‡ + 1, ğ‘) (10)
ğ‘ âˆˆ {0,1 }
=ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸âˆ’ğ¸ğ‘+ğ´,ğµ },ğ‘‡ +1)
and
ğ‘„âˆ— (ğ¸ , ğ‘‡ , ğ¶, ğ‘ = 1) = ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1, ğ‘)
ğ‘âˆˆ{0,1}
+ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1, ğ‘)
ğ‘âˆˆ{0,1}
(11)
Similarly, the optimal Q-function for state (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡) is given by (12) in which only not probing (ğ‘ = 0) is a feasible action:
ğ‘„âˆ— (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡, ğ‘ = 0) = ğ‘‡ + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1) = ğ‘‡ + ğ›¼Eğ´ min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1, ğ‘) (12)
ğ‘ âˆˆ {0,1 }
2) Q-Learning algorithm: The optimal Q function is given by the zeros of (8)-(12). Here, we employ asynchronous stochastic approximation [33] to iteratively converge to the optimal Q function, starting from any arbitrary Q-function. Online learning is facilitated by sequentially observing ğ¶ (ğ‘¡) (via channel probing), ğ‘Ÿ (ğ‘¡) (via transmission ACK/NACK) and the number ğ´(ğ‘¡) of energy packets harvested at time ğ‘¡.

In asynchronous stochastic approximation, we need step

size sequence ğ‘‘ (ğ‘¡) (ğ‘¡ â‰¥ 0) which satisï¬es the following

assumptions [34]:

Assumption 1. (i) 0 < ğ‘‘ (ğ‘¡) â‰¤ ğ‘‘Â¯ where ğ‘‘Â¯ > 0,

(ii) ğ‘‘ (ğ‘¡ + 1) â‰¤ ğ‘‘ (ğ‘¡) for all ğ‘¡ â‰¥ 0,

(iii)

âˆ ğ‘¡ =0

ğ‘‘(ğ‘¡)

=

âˆ

and

âˆğ‘¡=0 (ğ‘‘ (ğ‘¡))2 < âˆ.

Let us denote by ğœˆğ‘¡ (ğ‘ , ğ‘) the number of occurrences of the state-action pair (ğ‘ , ğ‘) up to iteration ğ‘¡ where ğ‘  âˆˆ S and ğ‘ âˆˆ {0, 1}. A similar notation is assumed for any generic intermediate state ğ‘£ and the corresponding action ğ‘. The following assumption is required for the convergence of asynchronous stochastic approximation [34]. Assumption 2. lim infğ‘¡â†’âˆ ğœˆğ‘¡ (ğ‘ ,ğ‘) > 0 almost surely âˆ€(ğ‘ , ğ‘),
ğ‘¡
and lim infğ‘¡â†’âˆ ğœˆğ‘¡ (ğ‘£,ğ‘) > 0 almost surely âˆ€(ğ‘£, ğ‘).
ğ‘¡
The proposed algorithm maintains a look-up table ğ‘„ğ‘¡ (Â·, Â·) for various state-action pairs, and iteratively updates its entries depending on the current state, current action taken and the observed next state. However, Assumption 2 requires that all state-action pairs should be visited inï¬nitely and comparatively often. This is ensured by taking a random action (uniformly chosen) with probability ğœ– at each decision instant, and taking the action arg ğ‘šğ‘–ğ‘›ğ‘ğ‘„(ğ‘ , ğ‘) or arg ğ‘šğ‘–ğ‘›ğ‘ğ‘„(ğ‘£, ğ‘) with probability (1 âˆ’ ğœ–).
One example of Q-value update is the following, aimed at convergence to the solution of (8):

ğ‘„ğ‘¡+1 (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğ‘ = 0) = ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ‘ = 0) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğ‘ = 0))1{ğ‘  (ğ‘¡) = (ğ¸ , ğ‘‡ ) , ğ‘ (ğ‘¡) = 0} [ğ‘‡
+ğ›¼ min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´(ğ‘¡) , ğµ }, ğ‘‡ + 1, ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ‘ = 0) ] (13)
ğ‘âˆˆ{0,1}
Here 1{Â·} is the indicator function. It is important to note that, this ğ‘„ update can be performed only when the random next state (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´(ğ‘¡), ğµ}, ğ‘‡ + 1) is is observed.
In a similar fashion, equations (9)-(12) lead to the following Q-updates for various states and intermediate states:

ğ‘„ğ‘¡+1 (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡ , ğ‘ = 1)

= ğ‘„ğ‘¡ (ğ¸, ğ‘‡, ğ‘ = 1) + ğ‘‘ (ğœˆğ‘¡ (ğ¸, ğ‘‡, ğ‘ = 1)1{ğ‘ (ğ‘¡) = (ğ¸, ğ‘‡), ğ‘(ğ‘¡) = 1}

[ min ğ‘„ğ‘¡ (ğ¸, ğ‘‡, ğ¶, ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸, ğ‘‡, ğ‘ = 1)]

(14)

ğ‘ âˆˆ {0,1 }

ğ‘„ğ‘¡+1 (ğ¸, ğ‘‡, ğ¶, ğ‘ = 0)

= ğ‘„ğ‘¡ (ğ¸, ğ‘‡, ğ¶, ğ‘ = 0) + ğ‘‘ (ğœˆğ‘¡ (ğ¸, ğ‘‡, ğ¶, ğ‘ = 0))1{ğ‘£(ğ‘¡) = (ğ¸, ğ‘‡, ğ¶),

ğ‘(ğ‘¡) = 0}[ğ‘‡ + ğ›¼ min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ + ğ´(ğ‘¡), ğµ}, ğ‘‡ + 1, ğ‘)
ğ‘ âˆˆ {0,1 }

âˆ’ğ‘„ğ‘¡ (ğ¸, ğ‘‡, ğ¶, ğ‘ = 0)]

(15)

ğ‘„ğ‘¡+1 (ğ¸ , ğ‘‡ , ğ¶, ğ‘ = 1)

= ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ‘ = 1) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ‘ = 1))1{ğ‘£ (ğ‘¡) = (ğ¸ , ğ‘‡ , ğ¶) ,

ğ‘ (ğ‘¡) = 1} [ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶) min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´(ğ‘¡) ,
ğ‘âˆˆ{0,1}

ğµ }, 1, ğ‘) + ğ›¼(1 âˆ’ ğ‘ (ğ¶)) min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´(ğ‘¡) , ğµ }, ğ‘‡ + 1,
ğ‘âˆˆ{0,1}

ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ‘ = 1) ]

(16)

ğ‘„ğ‘¡+1 (ğ¸ < ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğ‘ = 0)

= ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ‘ = 0) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğ‘ = 0))1{ğ‘  (ğ‘¡) = (ğ¸ , ğ‘‡ ) , ğ‘ (ğ‘¡) = 0} [ğ‘‡ +

ğ›¼ min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´(ğ‘¡) , ğµ }, ğ‘‡ + 1, ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ‘ = 0) ]

(17)

ğ‘âˆˆ{0,1}

In (16), we consider the case where packet success probability ğ‘(ğ¶) for channel state ğ¶ is known to the source. However, if these probabilities are unknown, then one can replace ğ‘(ğ¶) in (16) by ğ‘Ÿ (ğ‘¡) to obtain a valid Q-update.
Q learning type algorithm for ğ‘ > 1 can also be formulated similarly.

ğ‘„ğ‘¡+1 (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 1)

= ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 1) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 1))

1{ğ‘  (ğ‘¡) = (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) , ğ‘ (ğ‘¡) = 1} [ min ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘) âˆ’
ğ‘âˆˆ{0,1}

ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 1) ]

(24)

B. Markovian system
Here we adapt Q-learning for the Markovian model with unknown channel statistics and unknown energy harvesting characteristics to obtain the optimal solutions for (7).
1) Optimality equation in terms of Q-function: The optimal Q-value functions are given by:

ğ‘„âˆ— (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) = ğ‘‡ + ğ›¼Eğ´|ğ»1,ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ |ğ»1 ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1, ğœ + 1, ğ¶ğ‘ğ‘Ÿ ğ‘’ğ‘£ ,

ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ )

= ğ‘‡ + ğ›¼Eğ´|ğ» ,ğ» |ğ» min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1, ğœ + 1,
1 ğ‘›ğ‘’ğ‘¥ğ‘¡ 1 ğ‘ âˆˆ {0,1}

ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘)

(18)

On the other hand,

ğ‘„âˆ— (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 1) = Eğ¶âˆ¼ğ‘(ğœ) ( Â· |ğ¶ ) ğ‘Š âˆ— (ğ¸, ğ‘‡ , ğ¶, ğ»1)
ğ‘ğ‘Ÿ ğ‘’ ğ‘£
= Eğ¶âˆ¼ğ‘(ğœ) ( Â·|ğ¶ ) min ğ‘„âˆ— (ğ¸, ğ‘‡ , ğ¶, ğ»1, ğ‘) (19)
ğ‘ğ‘Ÿğ‘’ğ‘£ ğ‘âˆˆ{0,1}
When ğ‘(ğ‘¡) = 1, for a current measured channel state ğ¶, the optimal ğ‘„âˆ— functions for action ğ‘ = 0 and ğ‘ = 1 are given by:

ğ‘„ğ‘¡+1 (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 0)

= ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 0) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 0))1{ğ‘£ (ğ‘¡) =

(ğ¸ , ğ‘‡ , ğ¶, ğ»1) , ğ‘ (ğ‘¡) = 0} [ğ‘‡ + ğ›¼ min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ + ğ´(ğ‘¡) , ğµ },
ğ‘âˆˆ{0,1}

ğ‘‡ + 1, 1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 0) ]

(25)

ğ‘„ğ‘¡+1 (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 1) = ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 1) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 1))1{ğ‘£ (ğ‘¡) =
(ğ¸ , ğ‘‡ , ğ¶, ğ»1) , ğ‘ (ğ‘¡) = 1} [ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶) min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’
ğ‘âˆˆ{0,1}
ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´(ğ‘¡) , ğµ }, 1, 1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘) + ğ›¼(1 âˆ’ ğ‘ (ğ¶)) min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸
ğ‘âˆˆ{0,1}
âˆ’ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´(ğ‘¡) , ğµ }, ğ‘‡ + 1, 1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 1) ] (26)

ğ‘„ğ‘¡+1 (ğ¸ < ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) = ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0))
1{ğ‘  (ğ‘¡) = (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) , ğ‘ (ğ‘¡) = 0} [ğ‘‡ + ğ›¼ min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´(ğ‘¡) ,
ğ‘âˆˆ{0,1}
ğµ }, ğ‘‡ + 1, ğœ + 1, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) ] (27)
Similarly, the Q-value function iteration updates are given for state (ğ¸, ğ‘‡, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»2) by taking ğ´ = 0. Also, when ğ‘(ğ¶) is not known, it can be replaced by ğ‘Ÿ (ğ‘¡) in (26).

ğ‘„âˆ— (ğ¸, ğ‘‡, ğ¶, ğ»1, ğ‘ = 0) = ğ‘‡ + ğ›¼Eğ´|ğ» ,ğ» |ğ» min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ + ğ´, ğµ}, ğ‘‡ + 1,
1 ğ‘›ğ‘’ğ‘¥ğ‘¡ 1 ğ‘ âˆˆ {0,1}

1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘)

(20)

ğ‘„âˆ— (ğ¸ , ğ‘‡ , ğ¶, ğ»1, ğ‘ = 1) = ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´|ğ» ,ğ» |ğ» min ğ‘„âˆ— (ğ‘šğ‘–ğ‘› {ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘ 
1 ğ‘›ğ‘’ğ‘¥ğ‘¡ 1 ğ‘âˆˆ{0,1}

+ğ´, ğµ }, 1, 1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘) + ğ›¼ (1 âˆ’ ğ‘ (ğ¶))Eğ´|ğ»1,ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ |ğ»1 min
ğ‘âˆˆ{0,1}

ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1, 1, ğ¶, ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘) }

(21)

Also, the optimal Q-function for state (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) is given by:

ğ‘„âˆ— (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) = ğ‘‡ + ğ›¼Eğ´|ğ» ,ğ» |ğ» min ğ‘„âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ}, ğ‘‡ + 1, ğœ + 1,
1 ğ‘›ğ‘’ğ‘¥ğ‘¡ 1 ğ‘ âˆˆ {0,1}

ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘)

(22)

Similarly, the Q functions are given for state (ğ¸, ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»2) by putting ğ´ = 0.
2) Q-Learning algorithm: Here also we employ asynchronous stochastic approximation to ï¬nd the solution of (18)-(22) as in V-A. The Q-updates for various states and intermediate states are given by following:

ğ‘„ğ‘¡+1 (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) = ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) + ğ‘‘ (ğœˆğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0))
1{ğ‘  (ğ‘¡) = (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) , ğ‘ (ğ‘¡) = 0} [ğ‘‡ + ğ›¼ min ğ‘„ğ‘¡ (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´(ğ‘¡) ,
ğ‘âˆˆ{0,1}
ğµ }, ğ‘‡ + 1, ğœ + 1, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»ğ‘›ğ‘’ğ‘¥ğ‘¡ , ğ‘) âˆ’ ğ‘„ğ‘¡ (ğ¸ , ğ‘‡ , ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1, ğ‘ = 0) ] (23)

VI. NUMERICAL RESULTS
A. Single process, I.I.D. system, known dynamics
We consider ï¬ve channel states (ğ‘š = 5) with channel state occurrence probabilities q = [0.2, 0.2, 0.2, 0.2, 0.2] and the corresponding packet success probabilities p = [0.9, 0.7, 0.5, 0.3, 0.1]. Energy arrival process is i.i.d. ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–(ğœ†) with energy buffer size ğµ = 12, ğ¸ ğ‘ = 1 unit, ğ¸ğ‘  = 1 unit. Numerical exploration reveals that there exists a threshold policy on ğ‘‡ in decision making for channel state probing; see Figure 2(a) which substantiates our conjecture in III-A2. It is observed that this ğ‘‡ğ‘¡â„ (ğ¸) decreases with ğ¸ since higher available energy in the energy buffer allows the EH node to probe the channel state more aggressively. Similar reasoning explains the observation that ğ‘‡ğ‘¡â„ (ğ¸) decreases with ğœ†. For probed channel state, Figure 2(b) shows the variation of ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) with ğ¸, ğ‘‡, ğœ†. It is observed that ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) decreases with ğ¸, since the EH node tries to sample the process more aggressively if more energy is available in the buffer. Similarly, higher value of ğ‘‡ results in aggressive sampling, and hence ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) decreases with ğ‘‡. By similar arguments as before, we can explain the observation that this ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) decreases with ğœ†.
B. Multiple processes (ğ‘ > 1), I.I.D. model, known dynamics
We choose ğ‘ = 3, ğµ = 12, ğ¸ ğ‘ = 1, ğ¸ğ‘  = 1 and the same channel model and parameters as in Section VI-A. Figure 3(a) provides validation to our conjecture for optimal probing policy in III-B2 by demonstrating the variation on the threshold on ğ‘‡1 for given ğ‘‡2, ğ‘‡3, for channel state

(a)

(a)

(b)
Fig. 2: Single process (ğ‘ = 1), I.I.D. model, known dynamics: (a) Variation of ğ‘‡ğ‘¡â„ (ğ¸) with ğ¸, ğœ† and (b) Variation of ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) with ğ¸, ğ‘‡, ğœ†.

(b)
Fig. 3: ğ‘ = 3, I.I.D. model, known dynamics: (a) Variation of ğ‘‡ğ‘¡â„ (ğ¸, ğ‘‡2, ğ‘‡3) with ğ¸, ğ‘‡2, ğ‘‡3, ğœ† and (b) Variation of ğ‘ğ‘¡â„ (ğ¸, ğ‘‡1, ğ‘‡2, ğ‘‡3) with ğ¸, ğ‘‡1, ğ‘‡2, ğ‘‡3, ğœ†.

probing. It is observed that ğ‘‡ğ‘¡â„ (ğ¸, ğ‘‡2, ğ‘‡3) decreases with ğ¸ and ğœ†. Extensive numerical work also demonstrated that this
threshold decreases with each of ğ‘‡2, ğ‘‡3. For probed channel state, Figure 3(b) shows that ğ‘ğ‘¡â„ (ğ¸, ğ‘‡1, ğ‘‡2, ğ‘‡3) decreases with ğ¸ and ğœ†. Further numerical analysis also demonstrated that
this threshold decreases with each of ğ‘‡1, ğ‘‡2, ğ‘‡3 .

C. Markovian model, ğ‘ = 1, known dynamics
We consider a two state Markovian fading channel (ğ‘š = 2), where ğ¶1 is the good channel state and ğ¶2 is the bad channel state. The channel state transition probabilities are ğ‘1,2 = 0.1 and ğ‘2,1 = 0.1, and the corresponding packet transmission success probabilities are ğ‘1 = 0.9, ğ‘2 = 0.4. Also, for the Markovian energy harvesting process, we consider the transition probabilities â„1,2 = 0.3 and â„2,1 = 0.3. When the source node is in harvesting state, the energy packet generation process is i.i.d. ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–(ğœ†) across time, and otherwise zero. We consider ğµ = 9, ğ¸ ğ‘ = 1 unit, ğ¸ğ‘  = 1 unit, and ğœ† = 0.4.
Our numerical exploration demonstrated the threshold nature of the optimal policy which veriï¬ed conjectures predicted in IV-B. Figure 4 shows that, for a ï¬xed ğœ, ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ») decreases with ğ¸, ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»1) â‰¤ ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ»2) and ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶1, ğ») â‰¤ ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶2, ğ») (because probability of packet success in channel state ğ¶1 is higher than that in ğ¶2). Also, from Figure 5(a) it is observed that for probed channel state, ğ‘ğ‘¡â„ (ğ¸, ğ‘‡, ğ») decreases with ğ¸ and ğ‘‡ , and ğ‘ğ‘¡â„ (ğ¸, ğ‘‡ , ğ»1) â‰¤ ğ‘ğ‘¡â„ (ğ¸, ğ‘‡ , ğ»2).
Also, numerical exploration revealed that, after probing is done, the optimal sampling policy can also be represented as a threshold policy on ğ‘‡ instead of ğ‘(ğ¶); in this case, sampling

(a)
(b)
Fig. 4: ğ‘ = 1, Markov model: (a) Variation of ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ») with ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ = ğ¶1 and ğ» and (b) Variation of ğ‘‡ğ‘¡â„ (ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ , ğ») with ğ¸, ğœ, ğ¶ğ‘ğ‘Ÿğ‘’ğ‘£ = ğ¶2 and ğ».
is done when ğ‘‡ â‰¥ ğ‘‡ğ‘¡â„ (ğ¸, ğ¶, ğ»). The variation of ğ‘‡ğ‘¡â„ (ğ¸, ğ¶, ğ») with ğ¸, ğ¶, ğ» is shown in Figure 5(b), which supports many

(a) (a)

(b)
Fig. 5: ğ‘ = 1, Markov model: (a) Variation of ğ‘ğ‘¡â„ (ğ¸, ğ‘‡, ğ») with ğ¸, ğ‘‡, and ğ» (b) Variation of ğ‘‡ğ‘¡â„ (ğ¸, ğ¶, ğ») with ğ¸, ğ¶, and ğ».
intuitive justiï¬cations provided earlier in this section.
D. Comparison between probing and not probing
1) I.I.D. System: We consider ğ‘ = 1 and the same model as in Section VI-A, and compare the performance of our probing based threshold policy against the optimal policy that involves sampling mandatorily without probing (as in Section III-A3) based on ğ‘‡ and ğ¸. The results are summarised in Figure 6 (a); we note that AoI increases with ğ¸ ğ‘ and ğ¸ğ‘  since each action becomes more costly for the source. One interesting observation is that when the sensing energy is close to probing energy (ğ¸ğ‘  = 1, ğ¸ ğ‘ = 1), we see slightly higher time averaged AoI values for probing case compared to the case with ğ¸ğ‘  = 1 and no probing. This happens because the advantage offered by probing is being overshadowed by the extra amount of energy expended for it. However, when ğ¸ğ‘  = 5, ğ¸ ğ‘ = 1, we notice that probing yields a lower time averaged AoI. Obviously, the importance of probing is dependent on the trade off between the advantage offered by probing and the extra energy required for it. Typically, ğ¸ ğ‘ is much smaller than ğ¸ğ‘  since probing requires less energy as compared to sampling and transmitting a data packet. We have also observed numerically that, when there is sufï¬cient variation in the channel quality, probing improves the AoI performance. On the other hand, when channel variation is not predominant, spending energy in probing tends to hit the AoI performance and can sometimes render it worse than no probing.

(b)
Fig. 6: For ğ‘ = 1, Variation of time averaged AoI and comparison of a probing and non-probing system with change in ğœ†, ğ¸ ğ‘ and ğ¸ğ‘  for (a) I.I.D. model, (b) Markovian model.
2) Markovian System: We consider the same system parameters as in Section VI-C. Here also Figure 6 (b) exhibits similar trade-off between the improvement offered by probing and the energy spent in it, as seen for the i.i.d. model.
E. Q-Learning We consider the same settings as Section VI-A and
Section VI-C, except that we set ğµ = 5 and an upper bound on the age as ğ‘‡ğ‘šğ‘ğ‘¥ = 7, in order to reduce the number of possible states. We then run an online Q-learning scheme across two different sample paths. For the sake of comparison, we also plot the time averaged AoI of a uniform random strategy as well as that of the optimal policy derived through value iteration. Convergence of the time-averaged AoI under Q-learning to the optimal AoI for the i.i.d. channel system and the Markovian model can be seen in Figure 7 (a) and Figure 7 (b), respectively. We also observe signiï¬cant improvement from the initial policy which always picks an action uniformly at random; this demonstrates the necessity of the MDP based formulation for probing and sampling.
VII. CONCLUSIONS In this paper, we have derived optimal policy structures for minimizing the time-averaged expected AoI under an energy-harvesting source. We considered single and multiple processes, i.i.d. and Markovian time varying channels, time varying characteristics for energy harvesting, and channel probing capability at the source. The optimal source sampling policy often turned out to be a threshold policy. We have also proposed RL algorithms for unknown channel statistics and

Similarly, using ğ½ (ğ‘¡) (ğ¸, ğ‘‡) â‰¥ ğ½âˆ— (ğ¸, ğ‘‡) âˆ’ ğ‘’ğ‘¡ in (28), we obtain:

ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) â‰¥ ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1) âˆ’ ğ‘’ğ‘¡ ) ,

Eğ¶ ğ‘šğ‘–ğ‘›{ğ‘‡ + ğ›¼Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ + ğ´, ğµ }, ğ‘‡ + 1) âˆ’ ğ‘’ğ‘¡ ) ,

ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) âˆ’ ğ‘’ğ‘¡ )

+ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1) âˆ’ ğ‘’ğ‘¡ ) } (30)

(a)

ğ½ âˆ— (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) âˆ’ ğ›¼ğ‘’ğ‘¡

â‰¤ ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) â‰¤ ğ½ âˆ— (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) + ğ›¼ğ‘’ğ‘¡

=â‡’ | ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) âˆ’ ğ½ âˆ— (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) | â‰¤ ğ›¼ğ‘’ğ‘¡

=â‡’ max | ğ½ (ğ‘¡+1) (ğ‘ ) âˆ’ ğ½ âˆ— (ğ‘ ) | â‰¤ ğ›¼ğ‘’ğ‘¡
ğ‘ âˆˆS
=â‡’ ğ‘’ğ‘¡+1 â‰¤ ğ›¼ğ‘’ğ‘¡

=â‡’ lim ğ‘’ğ‘¡ = 0

(31)

ğ‘¡ â†’âˆ

Hence, ğ½ (ğ‘¡) (ğ‘ ) converges to ğ½âˆ— (ğ‘ ).

(b)
Fig. 7: Improvement in time averaged AoI exhibited by Qlearning, ğ‘ = 1, (a) I.I.D. model, (b) Markovian model.

energy harvesting characteristics. However, there are a number of issues that remain untouched, such as multi-source star topology and multi-hop network setting with multiple source and sink nodes. We plan to address these issues in our future research endeavours.

APPENDIX A PROOF OF PROPOSITION 1 We prove that ğ‘šğ‘ğ‘¥ğ‘ âˆˆS |ğ½ (ğ‘¡) (ğ‘ ) âˆ’ ğ½âˆ— (ğ‘ )| â†‘ 0 as ğ‘¡ â†‘ âˆ. Let us deï¬ne the error ğ‘’ğ‘¡ = maxğ‘ âˆˆS |ğ½ (ğ‘¡) (ğ‘ ) âˆ’ ğ½âˆ— (ğ‘ )| and ğ½ (0) (ğ‘ ) as initial estimate for ğ½âˆ— (ğ‘ ). For any state ğ‘ , we seek to establish relation between the error at time ğ‘¡ + 1 to the error at time ğ‘¡.
ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ )
= ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1) ,

Eğ¶

ğ‘šğ‘– ğ‘› {ğ‘‡

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ}, ğ‘‡

+

1) ,

ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) +

ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1) }

(28)

We assume there exists an optimal value function ğ½âˆ— (ğ¸, ğ‘‡)
for the discounted cost problem. By using the fact that ğ½ (ğ‘¡) (ğ¸, ğ‘‡) â‰¤ ğ½âˆ— (ğ¸, ğ‘‡) + ğ‘’ğ‘¡ in (28), we obtain:

ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) â‰¤ ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1) + ğ‘’ğ‘¡ ) ,
Eğ¶ ğ‘šğ‘–ğ‘›{ğ‘‡ + ğ›¼Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ + ğ´, ğµ }, ğ‘‡ + 1) + ğ‘’ğ‘¡ ) , ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) + ğ‘’ğ‘¡ ) +ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ( ğ½ âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1) + ğ‘’ğ‘¡ ) } (29)

APPENDIX B PROOF OF LEMMA 1 We prove this result by value iteration:
ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ )
= ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1) ,

Eğ¶

ğ‘šğ‘– ğ‘› {ğ‘‡

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ}, ğ‘‡

+

1) ,

ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) +

ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1) }

ğ½ (ğ‘¡+1) (ğ¸ < ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ )

=

ğ‘‡

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

+

ğ´,

ğµ}, ğ‘‡

+

1)

(32)

Let us start with ğ½ (0) (ğ‘ ) = 0 for all ğ‘  âˆˆ ğ‘†. Clearly, ğ½ (1) (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡) = ğ‘šğ‘–ğ‘›{ğ‘‡, Eğ¶ (ğ‘šğ‘–ğ‘›{ğ‘‡, ğ‘‡ (1 âˆ’ ğ‘(ğ¶))})} = ğ‘šğ‘–ğ‘›{ğ‘‡, Eğ¶ (ğ‘‡ (1 âˆ’ ğ‘(ğ¶))} and ğ½ (1) (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡) = ğ‘‡. Hence, for any given ğ¸, the value function ğ½ (1) (ğ¸, ğ‘‡) is an
increasing function of ğ‘‡. As induction hypothesis, we assume that ğ½ (ğ‘¡) (ğ¸, ğ‘‡) is also increasing function of ğ‘‡. Now,

ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡ ) = ğ‘šğ‘–ğ‘› ğ‘‡ + ğ›¼Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ + ğ´, ğµ }, ğ‘‡ + 1) ,

Eğ¶

ğ‘šğ‘– ğ‘› {ğ‘‡

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ}, ğ‘‡

+

1) ,

ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) +

ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1) }

(33)

We need to show that ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡ ) is also increasing in ğ‘‡. The ï¬rst term inside the minimization operation in (33) is increasing in ğ‘‡, by the induction hypothesis and from the fact that expectation is a linear operation. On the other hand, the second term has linear expectation over channel state and another minimization operator. Also, the ï¬rst and second terms inside the second minimization operation in (33) are increasing in ğ‘‡ by the induction hypothesis and the linearity of expectation operation. Thus, ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡) is also increasing in ğ‘‡ . By

similar arguments, we can claim that ğ½ (ğ‘¡+1) (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡) is increasing in ğ‘‡. Now, since ğ½ (ğ‘¡) (Â·) â†‘ ğ½âˆ— (Â·) as ğ‘¡ â†‘ âˆ by proof of Proposition 1, ğ½âˆ— (ğ¸, ğ‘‡) is also increasing in ğ‘‡. Hence, the
ï¬rst part of the lemma is proved.
For currrent probed channel state, the value function ğ‘Š (ğ‘¡+1) (ğ¸, ğ‘‡ , ğ¶) is given by:

ğ‘Š (ğ‘¡+1) (ğ¸ , ğ‘‡ , ğ¶)

=

ğ‘šğ‘– ğ‘› {ğ‘‡

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ}, ğ‘‡

+

1) ,

ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) +

ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1) }

(34)

The ï¬rst term (cost of not sampling a source) inside the minimization operation in (34) is independent of ğ‘(ğ¶), whereas the second term (cost of sampling a source) inside the minimization operation in (34) is given by:

ğ‘‡ (1 âˆ’ ğ‘ (ğ¶)) + ğ›¼ ğ‘ (ğ¶)Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1) +

ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1)

=

ğ‘‡

(1

âˆ’

ğ‘ (ğ¶) )

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

âˆ’

ğ¸ğ‘

âˆ’

ğ¸ğ‘ 

+

ğ´,

ğµ}, ğ‘‡

+

1)

âˆ’

ğ‘ (ğ¶) ğ›¼ Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡ + 1) âˆ’

Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘› {ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, 1)

(35)

By the induction hypothesis and the linearity of expectation operation, Eğ´ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡ + 1) âˆ’ Eğ´ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, 1) is non-negative. Thus,
the second term inside the minimization operation in (34) is decreasing in ğ‘(ğ¶). Now, since ğ‘Š (ğ‘¡) (Â·) â†‘ ğ‘Šâˆ— (Â·) as ğ‘¡ â†‘ âˆ, ğ‘Šâˆ— (ğ¸, ğ‘‡, ğ¶) is also decreasing in ğ‘(ğ¶).

âˆ’ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ = 1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) +
ğ‘˜
ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· ,
ğ‘‡ğ‘ + 1) }

ğ½ (ğ‘¡+1) (ğ¸ < ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

ğ‘

=

âˆ‘ï¸ ğ‘‡ğ‘–

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

+

ğ´,

ğµ }, ğ‘‡1

+

1, ğ‘‡2

+

1,

Â·

Â·

Â· , ğ‘‡ğ‘

+

1)

ğ‘–=1

(36)

Let us start with ğ½ (0) (ğ‘ ) = 0 for all ğ‘  âˆˆ ğ‘†.

Clearly, ğ½ (1) (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) =

min{

ğ‘ ğ‘–=1

ğ‘‡ğ‘–

,

Eğ¶

(

ğ‘š

ğ‘–

ğ‘›

{

ğ‘ ğ‘–=1

ğ‘‡ğ‘–

,

min1

â‰¤

ğ‘˜

â‰¤

ğ‘

(

ğ‘–â‰ ğ‘˜ ğ‘‡ğ‘–

+ ğ‘‡ğ‘˜ (1

âˆ’

ğ‘(ğ¶)))}} and ğ½ (1) (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) =

ğ‘ ğ‘–=1

ğ‘‡ğ‘–

.

Hence, for any given ğ¸, the value function

ğ½ (1) (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is an increasing function of

ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ . As induction hypothesis, we assume

that ğ½ (ğ‘¡) (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is also increasing function

of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ . Now,

ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

ğ‘

=

ğ‘šğ‘–ğ‘›

âˆ‘ï¸ ğ‘‡ğ‘–

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

+

ğ´,

ğµ }, ğ‘‡1

+

1, ğ‘‡2

+

1,

Â·

Â·

Â· , ğ‘‡ğ‘

+

1) ,

ğ‘–=1

ğ‘

Eğ¶

âˆ‘ï¸ ğ‘šğ‘–ğ‘›{ ğ‘‡ğ‘–

+

ğ›¼E ğ´ ğ½

(ğ‘¡)

( ğ‘šğ‘– ğ‘› { ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ },

ğ‘‡1

+

1,

ğ‘‡2

+

1,

Â·

Â·

Â·

,

ğ‘–=1

ğ‘‡ + 1) , min

âˆ‘ï¸ ğ‘‡ +ğ‘‡

(1 âˆ’ ğ‘(ğ¶)) + ğ›¼ğ‘(ğ¶)E

ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘› {ğ¸ âˆ’ ğ¸

ğ‘

ğ‘–ğ‘˜

ğ´

ğ‘

1â‰¤ğ‘˜â‰¤ğ‘ ğ‘–â‰ ğ‘˜

âˆ’ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ = 1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) +
ğ‘˜

ğ›¼(1 âˆ’ ğ‘ (ğ¶))Eğ´ ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ }, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· ,

ğ‘‡ğ‘ + 1) }

(37)

APPENDIX C PROOF OF THEOREM 1
From (3), it is obvious that for probed channel state the optimal decision for ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘  is to sample the source if and only if the cost of sampling is lower than the cost of not sampling the source, i.e., ğ‘‡ + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ + ğ´, ğµ}, ğ‘‡ + 1) â‰¥ ğ‘‡ (1 âˆ’ ğ‘(ğ¶)) + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡ + 1) âˆ’
ğ›¼ ğ‘(ğ¶) Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡ + 1) âˆ’ Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’
ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, 1) . Now, by Lemma 1, Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’
ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡ + 1) âˆ’ Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, 1) is nonnegative. Thus the R.H.S. decreases with ğ‘(ğ¶), whereas the L.H.S. is independent of ğ‘(ğ¶). Hence, for probed channel state the optimal action is to sample if and only if ğ‘(ğ¶) â‰¥ ğ‘ğ‘¡â„ (ğ¸, ğ‘‡) for some suitable threshold function ğ‘ğ‘¡â„ (ğ¸, ğ‘‡).

APPENDIX D
PROOF OF LEMMA 2 The proof is similar to the proof of Lemma 1 and it follows from the convergence of value iteration as given below:

ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ğ‘ + ğ¸ğ‘  , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ )

ğ‘

=

ğ‘šğ‘–ğ‘›

âˆ‘ï¸ ğ‘‡ğ‘–

+

(ğ‘¡)
ğ›¼E ğ´ ğ½

( ğ‘šğ‘– ğ‘› { ğ¸

+

ğ´,

ğµ }, ğ‘‡1

+

1, ğ‘‡2

+

1,

Â·

Â·

Â· , ğ‘‡ğ‘

+

1) ,

ğ‘–=1

ğ‘

Eğ¶

âˆ‘ï¸ ğ‘šğ‘–ğ‘›{ ğ‘‡ğ‘–

+

ğ›¼E ğ´ ğ½

(ğ‘¡)

( ğ‘šğ‘– ğ‘› { ğ¸

âˆ’

ğ¸ğ‘

+

ğ´,

ğµ },

ğ‘‡1

+

1,

ğ‘‡2

+

1,

Â·

Â·

Â·

,

ğ‘–=1

ğ‘‡ + 1), min

âˆ‘ï¸ ğ‘‡ +ğ‘‡

(1 âˆ’ ğ‘(ğ¶)) + ğ›¼ğ‘(ğ¶)E

ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘› {ğ¸ âˆ’ ğ¸

ğ‘

ğ‘–ğ‘˜

ğ´

ğ‘

1â‰¤ğ‘˜â‰¤ğ‘ ğ‘–â‰ ğ‘˜

We seek to show that ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is also increasing in each of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ . The ï¬rst term inner to the minimization operation in (37) is increasing in each of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ , utilizing the induction hypothesis and linear property of expectation operation. On the other hand, the second term has expectation over channel state and another minimization operator. Also, the ï¬st and second term of second minimization operator is increasing in each of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ by using induction hypothesis and the linearity of expectation operation. Thus, ğ½ (ğ‘¡+1) (ğ¸ â‰¥ ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is also increasing in each of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ . Similarly, we can assert that ğ½ (ğ‘¡+1) (ğ¸ < ğ¸ ğ‘ + ğ¸ğ‘ , ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is increasing in each of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ . Now, since ğ½ (ğ‘¡) (Â·) â†‘ ğ½âˆ— (Â·) as ğ‘¡ â†‘ âˆ, ğ½âˆ— (ğ¸, ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ) is also increasing in each of ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ . Hence, the ï¬rst part of the lemma is proved. Proof of second part of Lemma 2 is similar to the proof of second part of Lemma 1
APPENDIX E PROOF OF THEOREM 2
It is obvious that ğ½âˆ— (Â·) is invariant to any permutation of (ğ‘‡1, ğ‘‡2, Â· Â· Â· , ğ‘‡ğ‘ ). Hence, by Lemma 2,
arg min1â‰¤ğ‘˜ â‰¤ğ‘ ğ‘–â‰ ğ‘˜ ğ‘‡ğ‘– +ğ‘‡ğ‘˜ (1âˆ’ ğ‘(ğ¶))+ğ›¼ ğ‘(ğ¶)Eğ´ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’
ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘˜ = 1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) + ğ›¼(1 âˆ’ ğ‘(ğ¶))Eğ´ğ½ (ğ‘¡) (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 +
1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) = arg max1â‰¤ğ‘˜ â‰¤ğ‘ ğ‘‡ğ‘˜ , i.e., the best

process to activate is ğ‘˜âˆ— arg max1â‰¤ğ‘˜ â‰¤ğ‘ ğ‘‡ğ‘˜ in case one process has to be activated. For probed channel state, it
is optimal to sample a process if and only if the cost
of sampling this process is less than or equal to the
cost of not sampling this process, which translates into ğ‘‡ğ‘˜âˆ— + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ + ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) â‰¥ ğ‘‡ğ‘˜âˆ— (1 âˆ’ ğ‘(ğ¶)) + ğ›¼Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 +
1, Â· Â· Â· , ğ‘‡ğ‘ + 1) âˆ’ ğ›¼ ğ‘(ğ¶) Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 +
1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) âˆ’ Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 +
1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘˜ = 1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) . Now, by Lemma 2,
Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) âˆ’ Eğ´ğ½âˆ— (ğ‘šğ‘–ğ‘›{ğ¸ âˆ’ ğ¸ ğ‘ âˆ’ ğ¸ğ‘  + ğ´, ğµ}, ğ‘‡1 + 1, ğ‘‡2 + 1, Â· Â· Â· , ğ‘‡ =
ğ‘˜
1, ğ‘‡ğ‘˜+1 + 1, Â· Â· Â· , ğ‘‡ğ‘ + 1) is non negative. Thus, the R.H.S. is decreasing in ğ‘(ğ¶) and the L.H.S. is independent of ğ‘(ğ¶).
Hence, the threshold structure of the optimal sampling policy
is proved.
REFERENCES
[1] A. Jaiswal and A. Chattopadhyay, â€œMinimization of age-of-information in remote sensing with energy harvesting,â€ in 2021 IEEE International Symposium on Information Theory (ISIT). IEEE, 2021, pp. 3249â€“3254.
[2] S. Kaul, R. Yates, and M. Gruteser, â€œReal-time status: How often should one update?â€ in 2012 Proceedings IEEE INFOCOM. IEEE, 2012, pp. 2731â€“2735.
[3] R. Talak, S. Karaman, and E. Modiano, â€œCan determinacy minimize age of information?â€ arXiv preprint arXiv:1810.04371, 2018.
[4] S. K. Kaul, R. D. Yates, and M. Gruteser, â€œStatus updates through queues,â€ in 2012 46th Annual Conference on Information Sciences and Systems (CISS). IEEE, 2012, pp. 1â€“6.
[5] R. D. Yates and S. K. Kaul, â€œThe age of information: Real-time status updating by multiple sources,â€ IEEE Transactions on Information Theory, vol. 65, no. 3, pp. 1807â€“1827, 2018.
[6] S. K. Kaul and R. D. Yates, â€œTimely updates by multiple sources: The m/m/1 queue revisited,â€ in 2020 54th Annual Conference on Information Sciences and Systems (CISS). IEEE, 2020, pp. 1â€“6.
[7] A. Kosta, N. Pappas, A. Ephremides, and V. Angelakis, â€œAge of information performance of multiaccess strategies with packet management,â€ Journal of Communications and Networks, vol. 21, no. 3, pp. 244â€“255, 2019.
[8] S. Farazi, A. G. Klein, and D. R. Brown, â€œAge of information in energy harvesting status update systems: When to preempt in service?â€ in 2018 IEEE International Symposium on Information Theory (ISIT). IEEE, 2018, pp. 2436â€“2440.
[9] A. Arafa and S. Ulukus, â€œAge-minimal transmission in energy harvesting two-hop networks,â€ in GLOBECOM 2017-2017 IEEE Global Communications Conference. IEEE, 2017, pp. 1â€“6.
[10] S. Farazi, A. G. Klein, and D. R. Brown, â€œAverage age of information for status update systems with an energy harvesting server,â€ in IEEE INFOCOM 2018-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 2018, pp. 112â€“117.
[11] H. Hu, K. Xiong, Y. Zhang, P. Fan, T. Liu, and S. Kang, â€œAge of information in wireless powered networks in low snr region for future 5g,â€ Entropy, vol. 20, no. 12, p. 948, 2018.
[12] A. Arafa and S. Ulukus, â€œAge minimization in energy harvesting communications: Energy-controlled delays,â€ in 2017 51st Asilomar Conference on Signals, Systems, and Computers. IEEE, 2017, pp. 1801â€“1805.
[13] Z. Chen, N. Pappas, E. BjÃ¶rnson, and E. G. Larsson, â€œAge of information in a multiple access channel with heterogeneous trafï¬c and an energy harvesting node,â€ in IEEE INFOCOM 2019-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 2019, pp. 662â€“667.
[14] X. Wu, J. Yang, and J. Wu, â€œOptimal status update for age of information minimization with an energy harvesting source,â€ IEEE Transactions on Green Communications and Networking, vol. 2, no. 1, pp. 193â€“204, 2017.
[15] J. Yang, X. Wu, and J. Wu, â€œOptimal scheduling of collaborative sensing in energy harvesting sensor networks,â€ IEEE Journal on Selected Areas in Communications, vol. 33, no. 3, pp. 512â€“523, 2015.

[16] B. T. Bacinoglu, Y. Sun, E. Uysal-Bivikoglu, and V. Mutlu, â€œAchieving the age-energy tradeoff with a ï¬nite-battery energy harvesting source,â€ in 2018 IEEE International Symposium on Information Theory (ISIT). IEEE, 2018, pp. 876â€“880.
[17] S. Feng and J. Yang, â€œAge of information minimization for an energy harvesting source with updating erasures: With and without feedback,â€ arXiv preprint arXiv:1808.05141, 2018.
[18] B. T. Bacinoglu, E. T. Ceran, and E. Uysal-Biyikoglu, â€œAge of information under energy replenishment constraints,â€ in 2015 Information Theory and Applications Workshop (ITA). IEEE, 2015, pp. 25â€“31.
[19] S. Leng and A. Yener, â€œAge of information minimization for an energy harvesting cognitive radio,â€ IEEE Transactions on Cognitive Communications and Networking, vol. 5, no. 2, pp. 427â€“439, 2019.
[20] E. Gindullina, L. Badia, and D. GÃ¼ndÃ¼z, â€œAge-of-information with information source diversity in an energy harvesting system,â€ arXiv preprint arXiv:2004.11135, 2020.
[21] E. T. Ceran, D. GÃ¼ndÃ¼z, and A. GyÃ¶rgy, â€œReinforcement learning to minimize age of information with an energy harvesting sensor with harq and sensing cost,â€ in IEEE INFOCOM 2019-IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 2019, pp. 656â€“661.
[22] A. Arafa, J. Yang, and S. Ulukus, â€œAge-minimal online policies for energy harvesting sensors with random battery recharges,â€ in 2018 IEEE International Conference on Communications (ICC). IEEE, 2018, pp. 1â€“6.
[23] C. Tunc and S. Panwar, â€œOptimal transmission policies for energy harvesting age of information systems with battery recovery,â€ in 2019 53rd Asilomar Conference on Signals, Systems, and Computers. IEEE, 2019, pp. 2012â€“2016.
[24] B. T. Bacinoglu and E. Uysal-Biyikoglu, â€œScheduling status updates to minimize age of information with an energy harvesting sensor,â€ in 2017 IEEE International Symposium on Information Theory (ISIT). IEEE, 2017, pp. 1122â€“1126.
[25] M. A. Abd-Elmagid, H. S. Dhillon, and N. Pappas, â€œA reinforcement learning framework for optimizing age of information in rf-powered communication systems,â€ IEEE Transactions on Communications, vol. 68, no. 8, pp. 4747â€“4760, 2020.
[26] M. Hatami, M. Leinonen, and M. Codreanu, â€œAoi minimization in status update control with energy harvesting sensors,â€ arXiv preprint arXiv:2009.04224, 2020.
[27] A. Arafa, J. Yang, S. Ulukus, and H. V. Poor, â€œTimely status updating over erasure channels using an energy harvesting sensor: Single and multiple sources,â€ IEEE Transactions on Green Communications and Networking, 2021.
[28] G. Yao, A. M. Bedewy, and N. B. Shroff, â€œAge-optimal low-power status update over time-correlated fading channel,â€ in 2021 IEEE International Symposium on Information Theory (ISIT). IEEE, 2021, pp. 2972â€“2977.
[29] N. Michelusi, K. Stamatiou, and M. Zorzi, â€œTransmission policies for energy harvesting sensors with time-correlated energy supply,â€ IEEE Transactions on Communications, vol. 61, no. 7, pp. 2988â€“3001, 2013.
[30] B. Sombabu and S. Moharir, â€œAge-of-information aware scheduling under markovian energy arrivals,â€ in 2020 International Conference on Signal Processing and Communications (SPCOM). IEEE, 2020, pp. 1â€“5.
[31] D. P. Bertsekas, â€œDynamic programming and optimal control 3rd edition, volume ii,â€ Belmont, MA: Athena Scientiï¬c, 2011.
[32] S. Bhatnagar, H. Prasad, and L. Prashanth, Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods. Springer, 2013.
[33] V. S. Borkar, â€œAsynchronous stochastic approximations,â€ SIAM Journal on Control and Optimization, vol. 36, no. 3, pp. 840â€“851, 1998.
[34] S. Bhatnagar, â€œThe borkarâ€“meyn theorem for asynchronous stochastic approximations,â€ Systems & control letters, vol. 60, no. 7, pp. 472â€“478, 2011.

