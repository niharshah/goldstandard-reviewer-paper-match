Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning

Weiwen Liu
wwliu@cse.cuhk.edu.hk The Chinese University of Hong Kong
Hong Kong, China

Feng Liu
fengliu@stu.hit.edu.cn Harbin Institute of Technology
Shenzhen, China

Ruiming Tang
tangruiming2015@163.com Shenzhen, China

arXiv:2106.13386v1 [cs.IR] 25 Jun 2021

Ben Liao
liao@hotmail.co.uk Shenzhen, China

Guangyong Chen
gychen@link.cuhk.edu.hk Shenzhen Institutes of Advanced Technology, Chinese Academy of
Sciences Shenzhen, China

Pheng Ann Heng
pheng@cse.cuhk.edu.hk The Chinese University of Hong Kong
Hong Kong, China

ABSTRACT
Fairness in recommendation has attracted increasing attention due to bias and discrimination possibly caused by traditional recommenders. In Interactive Recommender Systems (IRS), user preferences and the systemâ€™s fairness status are constantly changing over time. Existing fairness-aware recommenders mainly consider fairness in static settings. Directly applying existing methods to IRS will result in poor recommendation. To resolve this problem, we propose a reinforcement learning based framework, FairRec, to dynamically maintain a long-term balance between accuracy and fairness in IRS. User preferences and the systemâ€™s fairness status are jointly compressed into the state representation to generate recommendations. FairRec aims at maximizing our designed cumulative reward that combines accuracy and fairness. Extensive experiments validate that FairRec can improve fairness, while preserving good recommendation quality.
KEYWORDS
fairness-aware recommendation, reinforcement learning
1 INTRODUCTION
Interactive Recommender Systems (IRS) have been widely implemented in various fields, e.g., news, movies, and finance [24]. Different from the conventional recommendation settings [12], IRS consecutively recommend items to individual users and receive their feedback in interactive processes. IRS gradually refine the recommendation policy according to the obtained user feedback in an online manner. The goal of such a system is to maximize the total utility over the whole interaction period. A typical utility of IRS is user acceptance of recommendations. Conversion Rate (CVR) is one of the most commonly used measures of recommendation acceptance, computing the ratio of users performing a systemâ€™s desired activity to users having viewed recommended items. A desired
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). DRL4IR â€™21, July 15, 2021, Virtual Event Â© 2021 Copyright held by the owner/author(s).

activity could be downloading from App stores, or making loans for microlending.
However, optimizing CVR solely may result in fairness issues, one of which is the unfair allocation of desired activities, like clicks or downloads, over different demographic groups. Under such unfair circumstances, majority (over-representing) groups may dominate recommendations, thereby holding a higher proportion of opportunities and resources, while minority groups are largely under-represented or even totally ignored. A fair allocation is a critical objective in recommendation due to the following benefits:
Legal. Recommendation in particular settings are explicitly mandated to guarantee fairness. In the setting of employment, education, housing, or public accommodation, a fair treatment with respect to race, color, religion, etc., is required by the anti-discrimination laws [8]. For job recommendation, it is expected that jobs at minorityowned businesses are being recommended and applied at the same rate as jobs at white-owned businesses. In microlending, loan recommender systems must ensure borrowers of different races or regions have an equal chance of being recommended and funded.
Financial. Under-representing for some groups leads to the abandonment of the system. For instance, video sharing platforms like YouTube involve viewers and creators. It is desirable to ensure each creator has a fair chance of being recommended and promoted. Otherwise, if the new creators do not get adequate exposure and appreciation, they tend to leave the platform, resulting in less user-generated content. Consequently, usersâ€™ satisfaction from both viewers and creators, as well as the platformâ€™s total income are affected in the long run.
The fairness concern in recommender systems is quite challenging, as accuracy and fairness are usually conflicting goals to be achieved to some extent. On the one hand, to obtain the ideal fairness, one could simply divide the recommendation opportunities equally to each item group, but usersâ€™ satisfaction will be affected by being persistently presented with unattractive items. On the other hand, existing recommender systems have been demonstrated to favor popular items [5], resulting in extremely unbalanced recommendation results. Thus, our work aims to answer this question: Can we achieve a fairer recommendation while preserving or just sacrificing a little recommendation accuracy?

DRL4IR â€™21, July 15, 2021, Virtual Event

Weiwen Liu, Feng Liu, Ruiming Tang, Ben Liao, Guangyong Chen, and Pheng Ann Heng

Most prior works consider fairness for the conventional recommender systems [1, 2], where the recommendation is regarded as a static process at a certain time instant. A general framework that formulates fairness constraints on rankings in terms of exposure allocation is proposed in [23]. Individual attention fairness is discussed in [3]. [25] models re-ranking with fairness constraints in Multi-sided Recommender Systems (MRS) as an integer linear programming. The balanced neighborhoods method [4] balances protected and unprotected groups by reformulating the Sparse LInear Method (SLIM) with a new regularizer.
However, it is hard to directly apply those methods to IRS due to:
(i) It is infeasible to impose fairness constraints at every time instant. Forcing the system to be fair at any time and increasing fairness uniformly for all users will result in poor recommendations. In fact, IRS focus on the long-term cumulative utility over the whole interaction session, where the system could focus on improving accuracy for users with particular favor, and the lack of fairness at the time can later be compensated when recommending items to users with diversified interests. As such, we can achieve longterm systemâ€™s fairness while preserving satisfying recommendation quality.
(ii) Existing work only considers the distribution of the number of recommendations (exposure) an item group received. Actually, the distribution of the desired activities that take place after an exposure like clicks or downloads has much larger commercial value and can be directly converted to revenue.
To resolve the above-mentioned problem, we design a novel Fairness-aware Recommendation framework with reinforcement learning (FairRec) for IRS. FairRec jointly compresses the user preferences and the systemâ€™s fairness status into the current state representation. A two-fold reward is designed to measure the system gain regarding accuracy and fairness. FairRec is trained to maximize the long-term cumulative reward to maintain an accuracy-fairness balance. The major contributions of this paper are as follows:
â€¢ We formulate a fairness objective for IRS. To the best of our knowledge, this is the first work that balances accuracy and fairness in IRS.
â€¢ We propose a reinforcement learning based framework, FairRec, to dynamically maintain a balance between accuracy and fairness in IRS. In FairRec, user preferences and the systemâ€™s fairness status are jointly compressed into the state representation to generate recommendations. We also design a two-fold reward to combine accuracy and fairness.
â€¢ We evaluate our proposed FairRec algorithm on both synthetic and real-world data. We show that FairRec can achieve a better balance between accuracy and fairness, compared to the state-of-the-art methods.
2 PROBLEM FORMULATION
2.1 Markov Decision Process for IRS
In this paper, we model the fairness-aware recommendation for IRS as a finite time Markov Decision Process (MDP), with an action space A, a state space S, and a reward function ğ‘Ÿ : S Ã— A â†’ R. When a user ğ‘¢ arrives at time step ğ‘¡ = 1, . . . ,ğ‘‡ , the system observes

the current state ğ‘ ğ‘¡ âˆˆ S of the user ğ‘¢ and takes an action ğ‘ğ‘¡ âˆˆ A (e.g., recommending an item to the user).
The user views the item and provides feedback ğ‘¦ğ‘ğ‘¡ , e.g., clicking or downloading on the recommended item, if she feels interested. Let ğ‘¦ğ‘ğ‘¡ âˆˆ {0, 1} denote the userâ€™s feedback, with ğ‘¦ğ‘ğ‘¡ = 1 meaning the user performs desired activities, and 0 otherwise. The system then receives a reward ğ‘Ÿğ‘¡ (a function of ğ‘¦ğ‘ğ‘¡ ), and updates the model. The problem formulation is formally presented as follows:
States S: The state ğ‘ ğ‘¡ is described by user preferences and the systemâ€™s fairness status. We jointly embed them into the current state representation. The detailed design of the state representation is given in Section 3.2.
Transitions P: The transition of states models the dynamic change of user preferences and the systemâ€™s fairness. The successor state ğ‘ ğ‘¡+1 is obtained once the userâ€™s feedback at time ğ‘¡ is collected.
Action A: An action ğ‘ğ‘¡ is recommending an item chosen from the available candidate item set A. Our framework can be easily extended to the case of recommending a list of items. To simplify our presentations, we focus on recommending an item at a time in this paper.
Reward R: The reward ğ‘Ÿğ‘¡ is a scalar measuring the systemâ€™s gain regarding accuracy and fairness after taking action ğ‘ğ‘¡ , elaborated in Section 3.3.
We aim to learn a policy ğœ‹, mapping from states to actions ğ‘ğ‘¡ = ğœ‹ (ğ‘ ğ‘¡ ), to generate recommendations that are both accurate and fair. The goal is to maximize the sum of discounted rewards (return) from time ğ‘¡ onward, which is defined by ğ‘…ğ‘¡ğ›¾ = ğ‘‡ğ‘˜=ğ‘¡ ğ›¾ğ‘˜âˆ’ğ‘¡ ğ‘Ÿğ‘˜ , and ğ›¾ is the discount factor.

2.2 Weighted Proportional Fairness for IRS

Each item is associated with a categorical protected attribute ğ¶ âˆˆ

{ğ‘1, . . . , ğ‘ğ‘™ }. Let Ağ‘ = {ğ‘|ğ¶ = ğ‘, ğ‘ âˆˆ A} denote the group of items

with an attribute value ğ‘. Take loan recommendation for instance,

if the protected attribute is the geographical region, then Ağ‘ with

ğ‘ = â€œOceania" contains all the loans applied from Oceania. Denote

by ğ‘¥ğ‘¡

âˆˆ

ğ‘™
R+

the

allocation

vector,

where ğ‘¥ğ‘¡ğ‘–

represents

the

allocation

proportion of group ğ‘– up to time ğ‘¡,

ğ‘¥ğ‘– = ğ‘˜ğ‘¡ =1 ğ‘¦ğ‘ğ‘˜ 1Ağ‘ğ‘– (ğ‘ğ‘˜ ) , (1)

ğ‘¡

ğ‘™ ğ‘–â€² =1

ğ‘¡ ğ‘˜ =1

ğ‘¦ğ‘ ğ‘˜

1 Ağ‘

â€²

(ğ‘ğ‘˜ )

ğ‘–

where 1ğ´ (ğ‘¥) equals to 1 if ğ‘¥ âˆˆ ğ´, and 0 otherwise. Recall that ğ‘¦ğ‘ ğ‘˜
is the userâ€™s feedback on recommended item ğ‘ğ‘˜ . In loan recommendation, ğ‘¥ğ‘¡ğ‘– denotes the rate of funded loans from the region ğ‘– over all funded ones up to time ğ‘¡.
In this work, we focus on a well-accepted and axiomatically justified metric of fairness, the weighted proportional fairness [10]. Weighted proportional fairness is a generalized Nash solution for multiple groups.

Definition 2.1 (Weighted Proportional Fairness). An allocation of desired activities ğ‘¥ğ‘¡ is weighted proportionally fair if it is the solution of the following optimization problem,

ğ‘™

ğ‘™

âˆ‘ï¸

âˆ‘ï¸

max ğ‘¤ğ‘– log(ğ‘¥ğ‘¡ğ‘– ), s.t. ğ‘¥ğ‘¡ğ‘– = 1, ğ‘¥ğ‘¡ğ‘– â‰¥ 0, ğ‘– = 1, . . . , ğ‘™ . (2)

ğ‘¥ğ‘¡

ğ‘– =1

ğ‘– =1

Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning

DRL4IR â€™21, July 15, 2021, Virtual Event

Actor
ranking score
state representation ğ’‰

item embeddings ğ’† â¨€ Tanh ReLU ReLU
user preference state ğ’

ğœ·ğŸ

â€¦

ğœ·ğ‘µ

Attention Net

ranking strategy vector ğ’›
concat fairness state ğ’ ReLU

user ğ’–

+

â€¦

+

item1

group1

itemN

groupN

fairness allocation vector

Critic
ranking strategy vector ğ’›

Figure 1: The architecture of FairRec.

ğ‘¸(ğ’”, ğ’›) ReLU ReLU
ReLU
state representation ğ’‰

The coefficient ğ‘¤ğ‘– âˆˆ R+ is a pre-defined parameter weighing the importance of each group. The optimal solution can be easily solved by standard Lagrangian multiplier methods, namely

ğ‘–
ğ‘¥âˆ—

=

ğ‘¤ğ‘–

.

(3)

ğ‘™
â€²

ğ‘¤â€²

ğ‘– =1 ğ‘–

As such, we aim to improve the weighted proportional fairness

ğ‘™ ğ‘– =1

ğ‘¤ğ‘–

log(ğ‘¥ğ‘–
ğ‘‡

)

while

preserving

high

conversions

ğ‘‡
ğ‘¡ =1 ğ‘¦ğ‘ğ‘¡

up to

time ğ‘‡ .

3 PROPOSED MODEL
This section begins with a brief overview of our proposed FairRec. After that, we introduce the components of FairRec and the learning algorithm in detail.

3.1 Overview
To balance accuracy and fairness in the long run, we formulate IRS recommendation as an MDP, which is then solved by reinforcement learning.
The previously studied reinforcement learning models can be categorized as follows: Value-based methods approximate the value function, then the action with the largest value is selected [31, 32]. Value-based methods are more sample-efficient and steady, but the computational cost is high when the action space is large. Policybased methods directly learn a policy that takes as input of the current user state and outputs an action [6, 29], which generally have a faster convergence. Actor-critic architectures take advantage of both value-based and policy-based methods [17, 30]. Therefore, we design our model following the actor-critic framework.
The overall architecture of FairRec is illustrated in Figure 1, which consists of an actor network and a critic network. The actor network performs time-varying recommendations according to the dynamic user preferences and the fairness status. The critic network estimates the value of the outputs associated with the actor network to encourage or discourage the recommended items.

3.2 Personalized Fairness-aware State Representation
We propose a personalized fairness-aware state representation to jointly consider accuracy and fairness, which is composed of the the User Preference State (UPS) and the Fairness State (FS). State representation learns a non-linear transformation â„ğ‘¡ = ğ‘“ğ‘  (ğ‘ ğ‘¡ ) that maps the current state ğ‘ ğ‘¡ to a continuous vector â„ğ‘¡ .
User Preference State (UPS). UPS represents personalized user preferences. We propose a two-level granularity representation: the item-level and the group-level. The item-level representation indicates the userâ€™s fine-grained preferences to each item, while the group-level representation shows the userâ€™s coarse-grained interests in each item group. Such two-level granularity representation provides more information on the propensity of different users towards diverse recommendation. Therefore, the agent could focus on accuracy for the users with particular favor, and the lack of fairness at a point in time can later be compensated when recommending items to users with diverse interests.
The input of UPS is the sequence of the user ğ‘¢â€™s ğ‘ most recent positively interacted items, as well as the corresponding group IDs that the items belong to at ğ‘¡. Items belonging to the same group share the same protected attribute value ğ‘. Each item ğ‘ is mapped to a continuous embedding vector ğ‘’ğ‘ âˆˆ Rğ‘‘ . The embedding vector of each group ID ğ‘’ğ‘” is the average of the embedding vectors of all items belonging to the group ğ‘”. Then each item is represented by

ğœ–ğ‘ = ğ‘’ğ‘ + ğ‘’ğ‘”,

(4)

where ğœ–ğ‘

âˆˆ

ğ‘‘
R

,

and

item

ğ‘

belongs to group ğ‘”. The group embedding

ğ‘’ğ‘” is added to serve as a global bias (or a regularizer), allowing items

belonging to the same group to share the same group information.

As for a specific user ğ‘¢, the affects of different historical inter-

actions on her future interest may vary significantly. To capture

this sequential dependencies among the historical interacted items,

we apply an attention mechanism [27] to weigh each item in the

interacted item sequence. The attention net learns a weight vector ğ›½ of size ğ‘ , ğ›½ = Softmax(ğœ”1ğœ (ğœ”2 [ğœ–ğ‘1, . . . , ğœ–ğ‘ğ‘ ] + ğ‘2) + ğ‘1), where ğœ”1, ğ‘1, ğœ”2, ğ‘2 are the network parameters and ğœ (Â·) is the ReLU ac-

tivation function. The user preference state representation ğ‘šğ‘¡ is

DRL4IR â€™21, July 15, 2021, Virtual Event

Weiwen Liu, Feng Liu, Ruiming Tang, Ben Liao, Guangyong Chen, and Pheng Ann Heng

obtained by multiplying the attention weights with the corresponding item representations as ğ‘šğ‘¡ = [ğ›½1ğœ–ğ‘1, . . . , ğ›½ğ‘ ğœ–ğ‘ğ‘ ], where ğ‘šğ‘¡ is of dimension ğ‘ Ã— ğ‘‘ and ğ›½ğ‘– denotes the ğ‘–-th entry in the weight vector ğ›½. Therefore, the items currently contributing more to the outcome are assigned with higher weights.
Fairness State (FS) The input of FS is the current allocation distribution of the desired activities at time ğ‘¡. As a complementary for UPS, FS provides evidence of the current fairness status and helps the agent to promote items belonging to under-represented groups. In particular, we deploy a Multi-Layer Perceptron (MLP) to map the allocation vector ğ‘¥ğ‘¡ to a latent space, ğ‘›ğ‘¡ = MLP(ğ‘¥ğ‘¡ ). Then we concatenate ğ‘šğ‘¡ and ğ‘›ğ‘¡ to obtain the final state representation,

â„ğ‘¡ = [ğ‘šğ‘¡ ||ğ‘›ğ‘¡ ],

(5)

with || denotes concatenation operation.

3.3 Reward Function Design

The reward is designed to measure the systemâ€™s gain regarding

accuracy and fairness. Existing reinforcement learning frameworks

for recommendation only consider the recommendation accuracy,

and one commonly used definition of reward is ğ‘Ÿ = 1 if the user

performs desired activities and âˆ’1 otherwise [17, 30]. To incorporate

the fairness measure into IRS, we propose a two-fold reward by

first examining whether the user performs the desired activities on

the recommended item, and then evaluating the fairness gain of

performing such a desired activity.

As discussed in Section 2, to achieve the weighted proportional

fairness, the optimal allocation vector is ğ‘¥âˆ—ğ‘– = ğ‘™â€²ğ‘¤ğ‘– ğ‘¤ â€² , with ğ‘¤ğ‘– the ğ‘– =1 ğ‘–
pre-defined target allocation proportion of group ğ‘–. Therefore, we

incorporate

the

deviation

from

the

optimal

solution

ğ‘–
ğ‘¥âˆ—

âˆ’

ğ‘–
ğ‘¥
ğ‘¡

into

the reward as the fairness indicator:

ğ‘Ÿğ‘¡ = ğ‘™ğ‘–=1 1Ağ‘ğ‘– (ğ‘ğ‘¡ ) ğ‘¥âˆ—ğ‘– âˆ’ ğ‘¥ğ‘¡ğ‘– + 1 , if ğ‘¦ğ‘ğ‘¡ = 1 , (6)

âˆ’ğœ†,

if ğ‘¦ğ‘ = 0

ğ‘¡

where 1ğ´ (ğ‘¥) is the indicator function and is 1 when ğ‘¥ âˆˆ ğ´, 0

otherwise, ğ‘¥ğ‘¡ğ‘– is the allocation proportion of group ğ‘– at time ğ‘¡. The

constant ğœ† > 1 is the penalty value for inaccurate recommendations

and manages the accuracy-fairness tradeoff. A larger ğœ† means that

the agent focuses more on accuracy.

Since the fairness metric (Eq. (1) and Eq. (2)) is computed ac-

cording to the number of the desired activities, only positive ğ‘¦ğ‘ğ‘¡

influences fairness. Therefore, we simply give a negative reward

âˆ’ğœ† for ğ‘¦ğ‘ğ‘¡ = 0 to punish the undesired activities. When ğ‘¦ğ‘ğ‘¡ = 1, we

compute

the

fairness

score

ğ‘¥

ğ‘– âˆ—

âˆ’

ğ‘–
ğ‘¥
ğ‘¡

,

which

is

the

difference

between

the optimal distribution and current allocation. Suppose the user

performs a desired activity on the item ğ‘ğ‘¡ âˆˆ Ağ‘ğ‘– . Then the fair-

ness

score

ğ‘–
ğ‘¥âˆ—

âˆ’

ğ‘–
ğ‘¥
ğ‘¡

is

negative

if

the

ğ‘–-th

group

is

over-representing

(ğ‘¥ğ‘–
ğ‘¡

>

ğ‘¥âˆ—ğ‘– ),

and

is

more

negative

if

Ağ‘ğ‘–

already

has

a

higher

rate

of

the desired activity, indicating that the system should focus more

on

other

groups.

Similarly,

the

fairness

score

ğ‘–
ğ‘¥âˆ—

âˆ’

ğ‘–
ğ‘¥
ğ‘¡

is

positive

if

the ğ‘–-th group is currently under-representing

(ğ‘¥ğ‘–
ğ‘¡

<

ğ‘¥âˆ—ğ‘– ), and is

more positive if Ağ‘ğ‘– is more lacking in the desired activity. We add

1 to the fairness score to ensure the reward is positive if ğ‘¦ğ‘ğ‘¡ = 1.

To sum up, the agent receives a large positive reward if the user

performs a desired activity on the item and the item belongs to an

under-representing group. Whereas the reward is a smaller positive

number if the activity is desired, but the item belongs to an overrepresenting (majority) group. We punish the most severely with ğ‘¦ğ‘ğ‘¡ = 0, as it neither contributes to accuracy nor fairness.

3.4 Model Update

Actor Network. The actor network extracts latent features from

ğ‘ ğ‘¡ and outputs a ranking strategy vector ğ‘§ğ‘¡ . The recommendation is performed according to the ranking vector by ğ‘ğ‘¡ = arg maxğ‘âˆˆA ğ‘’ğ‘âŠ¤ğ‘§ğ‘¡ .
In particular, we first embed ğ‘ ğ‘¡ to â„ğ‘¡ following the architecture de-

scribed in Section 3.2, then we stack fully-connected layers on top

of â„ğ‘¡ to learn the nonlinear transformation and generate ğ‘§ğ‘¡ , as

presented in Figure 1. Suppose the policy ğœ‹ğœƒ (ğ‘ ) learned by the actor is parameterized
by ğœƒ . The actor is trained according to ğ‘„ğœ‚ (ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ) from the critic,

and updated by the sampled policy gradient [22] with ğ›¼ğœƒ as the

learning rate, ğµ as the batch size,

1 âˆ‘ï¸

ğœƒ â† ğœƒ + ğ›¼ğœƒ ğµ

âˆ‡ğ‘§ğ‘„ğœ‚ (ğ‘ , ğ‘§) |ğ‘ =ğ‘ ğ‘¡ ,ğ‘§=ğœ‹ğœƒ (ğ‘ ğ‘¡ ) âˆ‡ğœƒ ğœ‹ğœƒ (ğ‘ ) |ğ‘ =ğ‘ ğ‘¡ , (7)

ğ‘¡

Critic Network. The critic adopts a deep neural network ğ‘„ğœ‚ (ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ),

parameterized by ğœ‚, to estimate the expected total discounted re-

ward

E

[

ğ›¾
ğ‘…

|ğ‘ 

ğ‘¡

,

ğ‘§

ğ‘¡

;

ğœ‹

]

,

given

the

state

ğ‘ ğ‘¡

and

the

ranking

strategy

ğ‘¡

vector ğ‘§ğ‘¡ under the policy ğœ‹. Specifically for this problem, the net-

work structure is designed as follows

ğ‘„ğœ‚ (ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ) = MLP( [ğœ (ğ‘Šâ„â„ğ‘¡ + ğ‘â„)||ğ‘§ğ‘¡ ]),

(8)

by first mapping â„ğ‘¡ to the same space as ğ‘§ğ‘¡ with a fully-connected layer and then concatenating it with ğ‘§ğ‘¡ , while MLP(Â·) denotes a mutli-layer perceptron, and â„ğ‘¡ = ğ‘“ğ‘  (ğ‘ ğ‘¡ ) is the state representation as presented in Section 3.2.
We use the temporal-difference (TD) learning [26] to update the critic. The loss function is the mean square error ğ¿ = ğ‘¡ (ğœˆğ‘¡ âˆ’ ğ‘„ğœ‚ (ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ))2, where ğœˆğ‘¡ = ğ‘Ÿğ‘¡ + ğ›¾ğ‘„ğœ‚â€² (ğ‘ ğ‘¡+1, ğœ‹ğœƒâ€² (ğ‘ ğ‘¡+1)). The term ğœˆğ‘¡ âˆ’ ğ‘„ğœ‚ (ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ) is called time difference (TD), ğœ‚â€² and ğœƒ â€² are the parameters of the target critic and actor network that are periodically copied from ğœ‚, ğœƒ and kept constant for a number of iterations to ensure the stability of the training [16]. The parameter ğœƒ is updated by gradient descent, with ğ›¼ğœ‚ the learning rate and ğµ the batch size:

1 âˆ‘ï¸

ğœ‚ â† ğœ‚ + ğ›¼ğœ‚

(ğœˆğ‘¡ âˆ’ ğ‘„ğœ‚ (ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ))âˆ‡ğœ‚ğ‘„ğœ‚ (ğ‘ ğ‘¡ , ğ‘§ğ‘¡ ).

(9)

ğµ

ğ‘¡

4 EXPERIMENTS
4.1 Experimental Settings
We evaluate the proposed FairRec algorithm on both synthetic and real-world data, comparing with the state-of-the-art recommendation methods in terms of fairness and accuracy.
4.1.1 Datasets. We use MovieLens1 and Kiva.org datasets for evaluation.
MovieLens is a public benchmark dataset for recommender systems, with 943 users, 1,602 items and 100,000 user-item interactions. Since the MovieLens data do not have protected attributes, we created 10 groups to represent differences among group inventories, and randomly assigned movies to each of such groups following a geometric distribution. An interaction with the rating (ranging from
1 https://grouplens.org/datasets/movielens

Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning
Table 1: Experimental results on MovieLens and Kiva.

DRL4IR â€™21, July 15, 2021, Virtual Event

NMF SVD DeepFM LinUCB DRR MRPC FairRec

MovieLens

CVR PropFair
UFG

0.7972 0.8592 4.2362

0.8478 0.8337 5.4795

0.8612 0.8098 5.8323

0.8577 0.8464 5.9476

0.8592 0.8470 6.0177

0.8361 0.8608 5.2508

0.8702* 0.8666* 6.6776*

CVR 0.4211 0.4870 0.6349 0.6517 0.6567 0.4286 0.6905*

Kiva

PropFair 0.8473 0.8686 0.8671 0.8697 0.8645 0.8761 0.8838*

UFG 1.4635 1.6931 2.3752 2.4970 2.5183 1.5332 2.8555*

We conduct a two-sided significant test [21] between FairRec and the strongest baseline DRR, where * means the p-value is smaller than 0.05.

1 to 5) larger than 3 is defined as a desired activity in calculating CVR.
Kiva.org is a proprietary dataset obtained from Kiva.org, consisting of lending transactions over a 6-month period. We followed the pre-processing technique used in [18] to densify the dataset. The retained dataset has 1,589 loans, 589 lenders and 43,976 ratings. The geographical region of loans is selected as the protected attribute, as Kiva.org has a stated mission of equalizing access to capital across different regions so that loans from each region have a fair chance to be funded. We define a transaction amount greater than USD25 as the desired activity for Kiva.

4.1.2 Evaluation Metrics. We evaluate the recommendation accuracy by the Conversion Rate (CVR):

ğ‘‡

CVR =

ğ‘˜=1 ğ‘¦ğ‘ğ‘˜ ,

(10)

ğ‘‡

and measure the fairness by Weighted Proportional Fairness (PropFair)2:

ğ‘™

âˆ‘ï¸

PropFair =

ğ‘¤ğ‘–

log(1

+

ğ‘–
ğ‘¥

).

(11)

ğ‘‡

ğ‘– =1

Moreover, we propose a Unit Fairness Gain (UFG) to jointly

consider accuracy and fairness,

PropFair

PropFair

UFG =

=

.

(12)

CVRmax âˆ’ CVR 1 âˆ’ CVR

UFG indicates the fairness of the system under unit accuracy budget.

For any recommendation system, the ideal maximum CVR, namely

CVRmax, equals to 1. Thus UFG can be interpreted as the slope of fairness versus accuracy â€” the fairness gain if we decrease a

unit accuracy from CVRmax. A larger UFG means a higher value of PropFair can be achieved with unit deviation from CVRmax, namely, the larger, the better.

4.1.3 Reproducibility. We randomly sample 80% of the user with associated rating sequences for training, and 10% for validation, 10% for testing, so that the item dependencies within each session can be learned. We use grid search to select the hyper-parameters for all the methods to maximize the hybrid metric UFG: the embedding dimension in {10, 30, 50, 100}, the learning rate in {0.0001, 0.001, 0.01}. Embedding vectors are pre-trained using standard matrix factorization [12] following the traditional processing as in [17, 30]. For the proposed FairRec, we set the number of recent interacted items ğ‘ = 5, discount factor ğ›¾ = 0.9, the width of each hidden layer

2The input of PropFair is shifted by one to avoid infinity results.

of the actor-critic network is 1000. The batch size is set to 1024, and the optimization method is Adam. Without loss of generality, we set ğ‘¤ğ‘– = 1, ğ‘– = 1, . . . , ğ‘™. All results are averaged from multiple independent runs.
4.2 Results and Analysis
4.2.1 Comparison with Existing Methods. We compare our proposed FairRec with six representative recommendation algorithms:
â€¢ NMF. Non-negative Matrix Factorization (NMF) [13] estimates the rating matrix with positive user and item factors.
â€¢ SVD. Singular Value Decomposition (SVD) [11] is the classic matrix factorization based method that decomposes the rating matrix via a singular value decomposition.
â€¢ DeepFM. DeepFM [7] is the state-of-the-art deep learning model in recommendation that combines the factorization machines and deep neural networks.
â€¢ LinUCB. LinUCB [15] is the state-of-the-art contextual bandits algorithm that sequentially selects items and balances between exploitation and exploration in IRS.
â€¢ DRR. DRR [17] is a deep reinforcement learning framework designed for IRS to maximize the long-term reward.
â€¢ MRPC. Multi-sided Recommendation with Provider Constraints (MRPC) [25] is the state-of-the-art fairness-aware method by formulating the fairness problem as an integer programming.
Table 1 shows the results. Bold numbers are the best results and underlined numbers are the strongest baselines. We have the following observations:
First, the deep learning based method (DeepFM) outperforms matrix factorization based methods (NMF and SVD) in CVR, while PropFair of DeepFM is lower. This is consistent with our expectation that DeepFM combines low-order and high-order feature interactions and has great fitting capability, yet it solely maximizes the accuracy, with fairness issues overlooked.
Second, LinUCB and DRR generally achieve better CVR than matrix factorization and deep learning methods. It is because LinUCB and DRR consider the IRS setting, and aims to maximize the longterm reward. Compared LinUCB to DRR, LinUCB underperforms DRR since LinUCB assumes states of the system remain unchanged and fails to tailor the recommendation to match the dynamic user preferences. DRR is the strongest baseline as it achieves the best tradeoff between accuracy and fairness, with UFG = 6.0177 on MovieLens and UFG = 2.5183 on Kiva, respectively.

DRL4IR â€™21, July 15, 2021, Virtual Event

Weiwen Liu, Feng Liu, Ruiming Tang, Ben Liao, Guangyong Chen, and Pheng Ann Heng

Table 2: Ablation study on MovieLens and Kiva.

FairRec(reward-) FairRec(state-)
FairRec

CVR
0.8561 0.8194
0.8702

MovieLens PropFair
0.8053 0.8758
0.8666

UFG
5.5957 4.8494
6.6776

CVR
0.6935 0.6723
0.6905

Kiva PropFair
0.8670 0.8746
0.8838

UFG
2.8290 2.6688
2.8555

Figure 2: Experimental results with embedding dimension ğ‘‘ on MovieLens: cumulative reward (left) and CVR, PropFair, and UFG (right).
Third, MRPC considers fairness by adding fairness constraints for static recommendation. Therefore, MRPC generates the fairest recommendation on both datasets, but the CVR significantly decreases as MRPC ignores the dynamic change of user preferences and the fairness status.
Fourth, FairRec consistently yields the best performance in terms of CVR, PropFair, and UFG on both datasets, demonstrating FairRec is effective in maintaining the accuracy-fairness tradeoff over time. FairRec outperforms the strongest baselines, DRR, by 1.3%, 2.3%, and 11% in CVR, PropFair, and UFG on MovieLens, and 5.1%, 2.2%, and 13.4% on Kiva. Considering UFG, with unit accuracy loss, FairRec achieves the most fairness gain. FairRec observes the current user preferences and the fairness status, and estimates the long-term discounted cumulative reward. Therefore, FairRec is capable of long-term planning to manage the balance between accuracy and fairness.
4.2.2 Influence of Embedding Dimension. Embedding dimension ğ‘‘ is an important factor for FairRec. We study how the embedding dimension ğ‘‘ influences the performance of FairRec. We vary ğ‘‘ in {10, 30, 50}, and run 2500 epochs. The cumulative reward and the test performance are plotted in Figure 2.
We observe that when ğ‘‘ is large (ğ‘‘ = 30 and ğ‘‘ = 50), the algorithm benefits from sufficient expressive power and the reward converges at a high level. As for ğ‘‘ = 10, the cumulative reward converges fast at a relatively low value, indicating that the model suffers from the limited fitting capability. In terms of UFG value, UFG = 6.68 when ğ‘‘ = 50, which is slightly better than 6.6 as ğ‘‘ = 30. Similar results can be found on Kiva, which is omitted for limited space. Therefore, we select ğ‘‘ = 50 in FairRec for all the experiments.
4.2.3 Ablation Study. To evaluate the effectiveness of different components (i.e., the state representation and the reward function) in FairRec, we replace a component of FairRec with the standard setting in RL at each time, and compare the performance with the full-fledged FairRec. Experimental results are presented in Table 2. We design two variants: FairRec(reward-) with standard reward as in [17, 30]; and FairRec(state-) with simple concatenation of item embeddings as the state representation as in [17].
Results show that FairRec(reward-) generally has high CVR, as no punishment on unfair recommendation. Moreover, the model simply optimizes accuracy, failing to balance accuracy and fairness. As for FairRec(state-), CVR is downgraded significantly, validating the importance of our designed state representation. Overall, UFG

of FairRec is the largest, confirming that all the components of FairRec work together yield the best results.
5 RELATED WORK
Our work is closely related to recommendation with deep reinforcement learning and fairness-aware recommendation.
5.1 Recommendation with Reinforcement Learning
Reinforcement Learning (RL) recommender systems recommend items by maximizing the long-term reward, where the reward can be the number of user repetitive clicks or purchases. Existing RLbased recommenders can be categorized into value-based methods [31, 32] and policy-based methods [9, 17, 30].
Value-based methods compute Q-values of each item (or item subset) given a user state, and the one with the maximum Q-value is selected and recommended. Zheng et al. incorporate user return pattern as a supplement to click / no click feedback and adopt a Deep Q-learning framework for news recommendation [32]. A DQN framework with Gated Recurrent Units (GRU) is used to capture usersâ€™ positive and negative preferences simultaneously in [31]. However, as value-based methods need to evaluate the Q-values of all the items under a specific user state [28], the computation becomes intractable when the number of items is large.
Policy-based methods directly learn a policy that takes as input of the current user state and outputs an action â€” the item to be recommended. Zhao et al. utilized the Deep Deterministic Policy Gradient (DDPG) framework for page-wise recommendation [30]. Various user state embeddings are studied in [17]. A deterministic policy gradient with full backup estimation (DPG-FBE) is proposed for learning the ranking function in [9]. The ranking score of each item is further computed by the inner product of the item embeddings vectors and the learned ranking parameters.
5.2 Fairness-aware Recommendation
Fairness and related concerns have become of increasing importance in recommender systems [19]. Fairness is defined by the inverse of JS-Divergence between the actual exposure distribution and the desired exposure distribution and a post-processing method is proposed in [20]. Rather than group fairness as we discussed in this chapter, individual fairness of each item is discussed in [14]. The balanced neighborhoods method [4] formulates the fairness problem into balancing protected and unprotected groups by imposing a regularizer on the Sparse Linear Method (SLIM). The fairness constraint is formulated as an integer programming optimization in [25]. However, all the existing methods (i) only consider the distribution of the number of recommendations (exposure) an item

Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning

DRL4IR â€™21, July 15, 2021, Virtual Event

group received. Actually, the distribution of the desired actions like
clicks or downloads is the major concern; (ii) perform the fairness-
aware recommendation at every instant in time, leading to inferior
recommendation results.
6 CONCLUSION
In this work, we propose a fairness-aware recommendation frame-
work in IRS to dynamically balance accuracy and fairness in the
long run with reinforcement learning. In the proposed state rep-
resentation component, the user preference state (UPS) models
both personalized preference and propensity to diversity; the fair-
ness state (FS) is utilized to describe the current fairness status
of IRS. A two-fold reward is designed to combine accuracy and
fairness. Experimental results demonstrate the effectiveness in the
balance of accuracy and fairness of our proposed framework over
the state-of-the-art models.
REFERENCES
[1] Himan Abdollahpouri, Gediminas Adomavicius, Robin Burke, Ido Guy, Dietmar Jannach, Toshihiro Kamishima, Jan Krasnodebski, and Luiz Pizzato. 2020. Beyond personalization: Research directions in multistakeholder recommendation. User Modeling and User-Adapted Interaction (2020).
[2] Himan Abdollahpouri and Robin Burke. 2019. Multi-stakeholder Recommendation and its Connection to Multi-sided Fairness. arXiv preprint arXiv:1907.13158 (2019).
[3] Asia J Biega, Krishna P Gummadi, and Gerhard Weikum. 2018. Equity of attention: Amortizing individual fairness in rankings. In The 41st international acm sigir conference on research & development in information retrieval. 405â€“414.
[4] Robin Burke, Nasim Sonboli, and Aldo Ordonez-Gauger. 2018. Balanced neighborhoods for multi-sided fairness in recommendation. In Conference on Fairness, Accountability and Transparency. PMLR, 202â€“214.
[5] Ã’scar Celma and Pedro Cano. 2008. From hits to niches?: or how popular artists can bias music recommendation and discovery. In Proceedings of the 2nd KDD Workshop on Large-Scale Recommender Systems and the Netflix Prize Competition.
[6] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 3312â€“3320.
[7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. 1725â€“1731.
[8] Elisa Holmes. 2005. Anti-Discrimination Rights Without Equality. The Modern Law Review 2 (2005).
[9] Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu. 2018. Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 368â€“377.
[10] Frank P Kelly, Aman K Maulloo, and David KH Tan. 1998. Rate control for communication networks: shadow prices, proportional fairness and stability. Journal of the Operational Research society 49, 3 (1998), 237â€“252.
[11] Yehuda Koren. 2010. Factor in the neighbors: Scalable and accurate collaborative filtering. ACM Transactions on Knowledge Discovery from Data (TKDD) 4, 1 (2010), 1â€“24.
[12] Yehuda Koren et al. 2009. Matrix factorization techniques for recommender systems. Computer 8 (2009), 30â€“37.
[13] DD Lee and HS Seung. [n.d.]. Algorithms for Non-Negative Matrix Factorization. NIPS (2000). Google Scholar ([n. d.]), 556â€“562.
[14] Eric L Lee et al. 2014. Fairness-Aware Loan Recommendation for Microfinance Services. In SocialCom. ACM, Article 3, 4 pages.
[15] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextualbandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web. 661â€“670.
[16] Timothy P Lillicrap et al. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).
[17] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, and Yuzhou Zhang. 2018. Deep reinforcement learning based recommendation with explicit user-item interactions modeling. arXiv preprint arXiv:1810.12027 (2018).
[18] Weiwen Liu and Robin Burke. 2018. Personalizing Fairness-aware Re-ranking. arXiv preprint arXiv:1809.02921 (2018).

[19] Weiwen Liu, Jun Guo, Nasim Sonboli, Robin Burke, and Shengyu Zhang. 2019. Personalized fairness-aware re-ranking for microlending. In Proceedings of the 13th ACM Conference on Recommender Systems. 467â€“471.
[20] Natwar Modani, Deepali Jain, Ujjawal Soni, Gaurav Kumar Gupta, and Palak Agarwal. 2017. Fairness aware recommendations on behance. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 144â€“155.
[21] Graeme D Ruxton. 2006. The unequal variance t-test is an underused alternative to Studentâ€™s t-test and the Mannâ€“Whitney U test. Behavioral Ecology (2006).
[22] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. Deterministic policy gradient algorithms. In International conference on machine learning. PMLR, 387â€“395.
[23] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of exposure in rankings. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2219â€“2228.
[24] Harald Steck, Roelof van Zwol, and Chris Johnson. 2015. Interactive recommender systems: Tutorial. In Proceedings of the 9th ACM Conference on Recommender Systems. 359â€“360.
[25] Ã–zge SÃ¼rer, Robin Burke, and Edward C Malthouse. 2018. Multistakeholder recommendation with provider constraints. In Proceedings of the 12th ACM Conference on Recommender Systems. 54â€“62.
[26] Richard S Sutton, Andrew G Barto, et al. 1998. Introduction to reinforcement learning. Vol. 135. MIT press Cambridge.
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762 (2017).
[28] Baoxiang Wang and Nidhi Hegde. 2019. Privacy-preserving q-learning with functional noise in continuous spaces. In Advances in Neural Information Processing Systems. 11327â€“11337.
[29] Xiting Wang, Yiru Chen, Jie Yang, Le Wu, Zhengtao Wu, and Xing Xie. 2018. A reinforcement learning framework for explainable recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 587â€“596.
[30] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems. 95â€“103.
[31] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018. Recommendations with negative feedback via pairwise deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1040â€“1048.
[32] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework for news recommendation. In Proceedings of the 2018 World Wide Web Conference. 167â€“176.

