arXiv:2007.12553v1 [cs.CV] 24 Jul 2020

Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach
Chaitanya Ahuja1[0000−0003−4396−2050], Dong Won Lee1, Yukiko I. Nakano2, and Louis-Philippe Morency1[0000−0001−6376−7696]
1 Carnegie Mellon University, Pittsburgh, PA, USA {cahuja,dongwonl,morency}@cs.cmu.edu
2 Seikei University, Musashino, Tokyo, Japan y.nakano@st.seikei.ac.jp
Abstract. How can we teach robots or virtual assistants to gesture naturally? Can we go further and adapt the gesturing style to follow a speciﬁc speaker? Gestures that are naturally timed with corresponding speech during human communication are called co-speech gestures. A key challenge, called gesture style transfer, is to learn a model that generates these gestures for a speaking agent ‘A’ in the gesturing style of a target speaker ‘B’. A secondary goal is to simultaneously learn to generate co-speech gestures for multiple speakers while remembering what is unique about each speaker. We call this challenge style preservation. In this paper, we propose a new model, named Mix-StAGE, which trains a single model for multiple speakers while learning unique style embeddings for each speaker’s gestures in an end-to-end manner. A novelty of Mix-StAGE is to learn a mixture of generative models which allows for conditioning on the unique gesture style of each speaker. As Mix-StAGE disentangles style and content of gestures, gesturing styles for the same input speech can be altered by simply switching the style embeddings. Mix-StAGE also allows for style preservation when learning simultaneously from multiple speakers. We also introduce a new dataset, PoseAudio-Transcript-Style (PATS), designed to study gesture generation and style transfer. Our proposed Mix-StAGE model signiﬁcantly outperforms the previous state-of-the-art approach for gesture generation and provides a path towards performing gesture style transfer across multiple speakers. Link to code, data and videos: http://chahuja.com/mix-stage.
Keywords: Gesture animation · Style transfer · Co-speech gestures
1 Introduction
Nonverbal behaviours such as body posture, hand gestures and head nods play a crucial role in human communication [56,42]. Pointing at different objects, moving hands updown in emphasis, and describing the outline of a shape are some of the many gestures that co-occur with the verbal and vocal content of communication. These are known as co-speech gestures [39,27]. When creating new robots or embodied virtual assistants designed to communicate with humans, it is important to generate naturalistic looking gestures that are meaningful with the speech [6]. Some recent works have proposed speaker-speciﬁc gesture generation models [18,13,11,16] that are both trained

2

C. Ahuja et al.

Fig. 1: Overview of co-speech gesture generation and gesture style transfer/preservation task. The models learns a style embedding for each speaker, which can be be mapped to a gesture space with either the same speaker’s audio to generate style preserved gestures or a different speaker’s audio to generate style transferred gestures.
and tested on the same speaker. The intuition behind this prior work is that co-speech gestures are idiosyncratic [58,39]. There is an unmet need to learn generative models that are able to learn to generate gestures simultaneously from multiple speakers ( in Figure 1) while at the same time remembering what is unique for each speaker’s gesture style. These models should not simply remember the “average” speaker. A bigger technical challenge is to be able to transfer gesturing style of speaker ‘B’ to speaker ‘A’ ( in Figure 1).
The gesturing style can deﬁned along two dimensions which is a result of (a) the speakers idiosyncrasy (or speaker-level style), and (b) due to some more general attributes such as standing versus sitting, or the body orientation such as left versus right (or attribute-level style). For both gesture style types, the generation model needs to be able to learn the diversity and expressivity [43,8] present in the gesture space, within and amongst speakers. The gesture distribution is likely to have multiple modes, some of them shared among speakers and some distinct to a speakers prototypical gestures.
In this paper, we introduce the Mixture-Model guided Style and Audio for Gesture Generation (or Mix-StAGE) approach which trains a single model for multiple speakers while learning unique style embeddings for each speaker’s gestures in an end-to-end manner (see Figure 1). We use this model to perform two tasks for gesture generation conditioned on the input audio signal, (1) style preservation which ensures that while learning from multiple speakers we are still able to preserve unique gesturing styles of each speaker, and (2) style transfer where generated gestures are from a new style that was not the same as the source of the speech. A novelty of Mix-StAGE is to learn a mixture of generative models which allows for conditioning on the unique gesture style of each speaker. Our experiments study the impact of multiple speakers on both style transfer and preservation. Our study focuses on the non-verbal components of speech asking the research question if we can predict gestures without explicitly modeling verbal signals. We also introduce a new dataset, Pose-Audio-Transcript-Style (PATS), designed to study gesture generation and style transfer.

Style Transfer for Co-Speech Gesture Animation

3

Fig. 2: t-SNE[38] representation of the Multi-mode Multimodal Gesture Space (Section 4.1). Each color represents a style, which is ﬁxed for both plots. The plot on the left visualizes the gesture space generated from the audio content and style of the same speaker. The plot on the right shows the generated gesture space where the audio content and style are not from the same speaker. It can be observed that a similar gesture space is occupied by each speaker’s style even when the audio content is not of their own.
2 Related Work
Speech driven Gesture Generation: For prosody-driven head motion generation [50] and body motion generation [33,32], Hidden Markov Models were used to predict a sequence of frames. Chiu & Marsella [12] proposed a two-step process: predicting gesture labels from speech signal using conditional random ﬁelds (CRFs) and converting the label to gesture motion using Gaussian process latent variable models (GPLVMs). More recently, an LSTM network was applied to MFCC features extracted from speech to predict a sequence of frames for gestures [22] and body motions [51,1]. Generative adversarial networks (GAN) were used to generate head motions [48] and body motions[16]. Gestures driven by an audio signal[18] is the closest approach to our task of style preservation but it uses models trained on single speakers unlike our multispeaker models.
Disentanglement and Transfer of Style : Style extraction and transfer have been studied in context of image artistic style [17,26], factorizing foreground and background in videos[15,55], disentanglement in speech [57,9,20]. These approaches were extended to translation between properties of style such as map edges and real photos using paired samples [25]. Paired data limits the variety of attributes of source and target, which encouraged unsupervised domain translation for images [59,60] and videos[7]. Style

4

C. Ahuja et al.

was disentangled from content using a shared latent space[34], a cycle consistency loss [59] and contrastive learning [40]. Cycle consistency losses were shown to limit diversity in the generated outputs as opposed to a weak consistency loss [24] and shared content space [30]. Cycle consistency in cross-domain translation assumes reversibility (i.e. domain A can be translated to domain B and vice-versa). These assumptions are violated in cross-modal translation [37] and style control [57] tasks where information in modality B (e.g. pose) is a subset of that in modality B (e.g. audio). Style transfer for pose has been studied in context of generating dance moves based on the content of the audio [31] or walking styles [53]. Generated dance moves are conditioned on both the style and content of the audio (i.e. kind of music like ballet or hip-hop), unlike cospeech gesture generation which requires only the content and not the style of the audio (i.e. speaker speciﬁc style like identity or fundamental frequency). Co-speech gesture styles have been studied in context of speaker personalities [41], but requires a long annotation process to create a proﬁle for each speaker. To our knowledge, this is the ﬁrst fully data-driven approach that learns gesture style transfer for multiple speakers in a co-speech gesture generation setting.

3 Stylized Co-Speech Gesture Animation

We deﬁne the problem of stylized co-speech gesture animation with two main goals, (1) generation of an animation which represents the gestures that would co-occur with the the spoken segment and (2) modiﬁcation of the style of these gestures. Figure 1 shows the ﬁrst goal ( ) exempliﬁed with the style preservation scenario, while the second goal ( ) exempliﬁes with the style transfer scenario.
Formally, given a sequence of T audio frames Xa ∼ Fa and ith speaker’s style S(i), the goal is to predict a sequence of T frames of 2-D poses Yp ∼ Fp. Here Fa and Fp are the marginal distributions of the content of input audio and style of output pose sequences. To control pose generation by both style and audio, we learn a joint distribution over pose, audio and style Fp,a,s which can be broken down into 3 parts

Fp,a,s = Fp|ΦFΦ|a,s · Fs · Fa

(1)

where FΦ|a,s is the distribution of the gesture space Φ conditioned on the audio and style of pose (Figure 1). We discuss the modelling of Fp|ΦFΦ|a,s, Fa, and Fs in Section 4.1, 4.2 and 4.3 respectively.

4 Mix-StAGE: Mixture-Model guided Style and Audio for Gesture Generation
Figure 3 shows an overview of our Mix-StAGE model, including the training inference pathways. A ﬁrst component of our Mix-StAGE model is the audio encoder Eac, which takes as input the spoken audio Xa. During training, we also have the pose sequence of the speaker Yp. This pose sequence is decomposed into content and style, with two specialized encoders Epc and Eps. During training, the style for the pose sequence can either be concatenated with the audio or the pose content.

Style Transfer for Co-Speech Gesture Animation

5

Fig. 3: (a) Overview of the proposed model Mix-StAGE in training mode, where audio Xa and pose Yp are fed as inputs to learn a style embedding and concurrently generate a gesture animation. S represents the style matrix, which is multiplied with a separately encoded pose. represents argmax for style ID followed by matrix multiplication. Discriminator D is used for adversarial training. All the loss functions are represented with dashed lines. (b) Mix-StAGE in inference mode, where any speaker’s style embedding can be used on an input audio Xa to generate gesture style-transferred or style-preserved animations (c) CMix-GAN generator: a visual representation of the conditional Mix-GAN model, where the represents a weighted sum of the model priors Φ with the generated outputs by the sub-generators.

The pose sequences for multiple speakers are represented as a distribution with multiple modes [21]. To decode from this multi-mode multimodal gesture space, we use a common generator G with multiple sub-generators (or CMix-GAN) conditioned on input audio and style to decode both these embeddings to output pose Yp.
Our loss function comprises of a mode-regularization loss (Section 4.1) to ensure that audio and style embedding can sample from the appropriate marginal distribution of poses, a joint loss (Section 4.2) to ensure latent distribution matching for content in a cross-modal translation task, a style consistency loss (Section 4.3) to ensure that the correct style is being generated and an adversarial loss (Section 4.4) that matches the generated pose distribution to the target pose distribution.

4.1 M2GS: Multi-mode Multimodal Gesture Space

Humans perform different styles of gestures, where each style consists of different kinds of gestures (i.e beat, metaphorical, emblematic, iconic and so on)[39]. Learning pose generators for multiple speakers, each with their own style of gestures, presents a distribution with multiple modes. These gestures have a tendency of switching from one mode to the other over time, which depends on style embeddings and content of the audio.
To prevent mode collapse [4] we propose the use of mixture-model guided subgenerators [21,5,23], each learning a different mode of M 2 gesture space FΦ|a,s.

M

Yˆ p = φmGm(Z) = G(Z)

(2)

m=1

6

C. Ahuja et al.

where Z ∈ {Za→p, Zp→p} are cross-modal and self-modal latent spaces respectively.

They are deﬁned as Za→p =

E

c a

(

X

a

)

,

Eps

(

Y

p

)

S and Zp→p = Epc(Yp), Eps(Yp)

S

where S is the style embedding matrix (See Section 4.3) and is argmax for style

ID followed by matrix multiplication. Pose sequence Yˆp ∼ Fp|ΦFΦ|a,s represents the

pose probability distribution conditioned on audio and style. Gm ∼ Fpm|a,s ∀m ∈

[1, 2, . . . M ] are sub-generator functions with corresponding mixture-model priors Φ =

{φ1, φ2, . . . φM }. These mixture model priors represent the M 2 gesture space and are

estimated at inference time conditioned on the input audio and style.

Estimating Mixture Model Priors: During training, we partition poses Yp into M

clusters using an unsupervised approach, Lloyd’s algorithm [36]. While other unsuper-

vised clustering methods [44] can also be used at this stage, we choose Lloyd’s algo-

rithm for its simplicity and speed. Each of these clusters represent samples from prob-
ability distributions {Fp1|a,s, Fp2|a,s, . . . FpM|a,s}. If a sample belongs to the mth cluster, φm = 1, otherwise φm = 0, making Φ a sequence of one-hot vectors. While training the generator G with loss function Lrec, if a sample belongs to the distribution Fpm|a,s, only parameters of sub-generator Gm are updated. Hence, each sub-generator learns

different components of the true distribution, which are combined using Equation 2 to

give the generated pose.

At inference time, we do not have the true values of mixture-model priors Φ. As

mixture model priors modulate based on the style of the speaker and audio content at

any given moment, we jointly learn a classiﬁcation network H ∼ FΦ|a,s to estimate values of Φ in form of a mode regularization loss function

Lmix = EΦ,ZCCE(Φ, H(Z))

(3)

where CCE is categorical cross-entropy.

4.2 Joint Space of Style, Audio and Pose
A set of marginal distributions Fa and Fs are learnt by our content encoders Eac and Epc, which together deﬁne the joint distribution of the generated poses: Fp,a,s. Since both cross-modal Za→p and self-modal Zp→p latent spaces are designed to represent the same underlying content distribution, they should be consistent with each other. Using the same generator G for decoding both of these embeddings[35] yields content invariant generator. We enforce a reconstruction and joint loss [2] which encourages a reduction in distance between Za→p and Zp→p. As cross-modal translation is not reversible for this task (i.e. audio signal cannot be generated with pose input), a bidirectional reconstruction loss [30] for latent distribution matching cannot be directly used. This joint loss achieves the same goal of latent distribution matching in a unimodal translation task [24,46,47] but for a cross-modal translation task.

Ljoint = EYp Yp − G(Zp→p) 1

(4)

Lrec = EYp,Xa Yp − G(Za→p) 1

(5)

Style Transfer for Co-Speech Gesture Animation

7

4.3 Style Embedding
We represent style as a collection of embeddings S(i) ∈ S ∼ Fs, where S(i) is the style of the ith speaker in the style matrix S. Style space and embeddings are conceptually similar to the GST (Global Style Token) layer [57] which decomposes the audio embedding space into a set of basis vectors or style tokens, but only one out of the two modalities in the stylized audio generation task [57,37] have both style and content. In our case, both audio and pose have style and content. To ensure that generator G is attending only to style of pose while ignoring style of the audio, a style consistency loss is enforced on input Yp and generated Yˆ p.
Lid = EY ∈{Yp,Yˆp}CCE Softmax Eps(Y ) , ID (6)
where ID is a one-hot vector denoting the speaker level style.

4.4 Total Loss with Adversarial Training

To alleviate the challenge of overly smooth generation caused by L1 reconstruction and joint losses in Equation 4,5, we use the generated pose sequence Yˆ p as a signal for
the adversarial discriminator D [18]. The discriminator tries to classify the true pose Yp from the generated pose Yˆ p, while the generator learns to fool the discriminator by
generating realistic poses. This adversarial loss[19] is written as:

Ladv = EYp log D Yp) + EXa,Yp log 1 − D(G Eac(Xa), Eps(Xp)

(7)

The model is jointly trained to optimize the overall loss function:

max min Lmix + Ljoint + Lrec + λidLid + Ladv

(8)

D Eac ,Epc,Eps,G

where λid controls the weight of the style consistency loss term.

4.5 Network Architectures
Our proposed approach can work with any temporal network, giving it the ﬂexibility of incorporating domain dependent or pre-trained temporal models.
In our experiments we use a Temporal Convolution Network (TCN) module for both content and style encoders. The style space is a matrix S ∈ RN×D where N is the number of speakers and D is the length of the style embeddings. The generator G(.) consists of a 1D version of U-Net [45,18] followed by M TCNs as sub-generator functions. The discriminator is also a TCN module with lower capacity than the generators. A more detailed architecture can be found in the supplementary.

5 Experiments
Our experiments are divided into 2 sections, (1) Style Preservation: Generating cospeech gestures for multiple speakers with their own individualistic style, (2) Style

8

C. Ahuja et al.

Single-Speaker Models

Multi-Speaker Models

No. of Speaker S2G[18] CMix-GAN MUNIT[24] StAGE Mix-StAGE

Speakers

PCK F1 PCK F1 PCK F1 PCK F1 PCK F1

Mean 0.25 0.08 0.26 0.27 0.24 0.06 0.36 0.21 0.34 0.22

2 Corden 0.30 0.05 0.32 0.21 0.25 0.06 0.36 0.21 0.34 0.24 lec cosmic 0.19 0.12 0.19 0.33 0.15 0.19 0.20 0.48 0.24 0.49

Mean 0.37 0.18 0.37 0.27 0.22 0.05 0.38 0.34 0.39 0.35

4 Corden 0.30 0.05 0.32 0.21 0.24 0.07 0.35 0.27 0.35 0.30 lec cosmic 0.19 0.12 0.19 0.33 0.19 0.16 0.18 0.23 0.20 0.19

Mean 0.36 0.14 0.37 0.26 0.31 0.21 0.38 0.32 0.40 0.33

8 Corden 0.30 0.05 0.32 0.21 0.23 0.03 0.32 0.28 0.36 0.27 lec cosmic 0.19 0.12 0.19 0.33 0.13 0.09 0.23 0.34 0.24 0.32
Table 1: Style Preservation: Objective metrics for pose generation of single-speaker and multi-speaker models as indicated in the columns. Each row refers to the number of speakers the model was trained, with the average performance indicated at the top. The scores for common individual speakers are also indicated below alongside. For detailed results on other speakers please refer to the supplementary. Bold numbers indicate p < 0.1 in a bootstrapped two sided t-test.

Transfer: Generating co-speech gestures with content (or audio) of a speaker and gesture style of another speaker. Additionally, style transfer can be speaker-level as well as attribute-level. We choose visually distinguishable attribute-level styles: (1) body orientation, (2) gesture frequency, (3) primary arm function and (4) sitting/standing posture. We start by describing the baseline models followed by the evaluation metrics, which we will use to compare our model. We end this section with the description of our proposed dataset.
5.1 Baseline Models
Single-Speaker Models : These models are not designed to perform style transfer and hence are not included for those experiments.
– Speech2Gesture [18]: The closest work to co-speech gesture generation is one that only generates individualistic styles. We use the pre-trained models available from their code-base to render the videos. For rest of the speakers in PATS, we replicate their model, hyper-parameters and train speaker speciﬁc models.
– CMix-GAN (variant of our model): As an ablation, we remove the style embedding module and style consistency losses from our model Mix-StAGE . Hence, a separate model is required to be trained for each speaker for style preservation experiments.

Style Transfer for Co-Speech Gesture Animation

9

Multi-speaker Models
– MUNIT [24]: The closest work to our style-transfer task is MUNIT which takes multiple domains of images (i.e. uni-modal). We modify the encoders and decoders to domain speciﬁc architectures (i.e. 1D convolutions for audio instead of 2D convolutions for images) while retaining the loss functions.
– StAGE (variant of our model): As an ablation, we ﬁx the number of sub-generators in our model Mix-StAGE to one. This is equivalent to setting M = 1 in equation 2.

5.2 Evaluation Metrics
Human Perceptual Study: We conduct a human perceptual study on Amazon Mechanical Turk (AMT) for co-speech gesture generation (or style preservation) and style transfer (speaker-level and attribute-level) and measure preferences in two aspects of the generated animations, (1) naturalness, and (2) style transfer correctness for animation generation with content (i.e. audio) of speaker A and style of speaker B. We show a pair of videos with skeletal animations to the annotators. One of the animations is from the ground-truth set, while the other is generated using our proposed model. The generated animation could either have the same style or a different style as the original speaker. With unlimited time, the annotator has to answer two questions, (1) Which of the videos has more natural gestures? and (2) Do these videos have the same attributelevel style (or speaker-level style)? The ﬁrst question is a real vs. fake perceptual study against the ground truth, while the second question measures how often the algorithm is able to visually preserve or transfer style (attribute or individual level). We run this study for randomly selected 100 pairs of videos from the held-out set. .

Probability of Correct Keypoints (PCK): To measure the accuracy of the gesture generation, PCK [3,52] is used to evaluate all models. PCK values are averaged over α = 0.1, 0.2 as suggested in [18].

Mode Classiﬁcation F1: Correctness of shape of a gesture can be quantiﬁed by mea-
suring the number of times the model has sampled from the correct mode of the pose distribution. Formally, we use the true (Yp) and generated (Yˆ p) pose to ﬁnd the closest cluster mˆ and m respectively. If m = mˆ , the generated pose was sampled from the correct mode. F1 score of this M -class classiﬁcation problem is deﬁned as Mode
Classiﬁcation F1, or simply F1.

Inception Score (IS): Generated pose sequences with the audio of speaker A and style of speaker B does not have a ground truth reference. To quantitatively measure the correctness and diversity of generated pose sequence we use the inception score [49]. For generative tasks such as image generation, this metric has been used with a pre-trained classiﬁcation network such as Inception Model [54]. In our case, the generated samples are not images, but a set of 2D keypoints. Hence, we train a network which classiﬁes a sequence of poses to its corresponding speaker which estimates the conditional likelihood to calculate IS scores.

10

C. Ahuja et al.

(a) Style Preservation Naturalness

(b) Style Transfer Natu- (c) Style Transfer Cor-

ralness

rectness

Fig. 4: Perceptual Study for speaker-level style preservation in (a) and speaker level style transfer in (b), (c). We have naturalness preference for both style transfer and preservation, and style transfer correctness scores for style transfer. Higher is better. Error bars calculated for p < 0.1 using a bootstrapped two sided t-test.

5.3 Pose-Audio-Transcript-Style (PATS) dataset
Gesture styles, which may be deﬁned by attributes such as type, frequency, orientation of the body, is representative of the idiosyncrasies of the speaker [14]. We create a new dataset, Pose-Audio-Transcript-Style (PATS), to study various styles of gestures for a large number of speakers in diverse settings.
PATS contains pose sequences aligned with corresponding audio signals and transcripts3 for 25 speakers (including 10 speakers from [18]) to offer a total of 251 hours of data, with a mean of 10.7 seconds and a standard deviation of 13.5 seconds per interval. The demographics of the speakers include 15 talk show hosts, 5 lecturers, 3 YouTubers, and 2 televangelists.
Each speaker’s pose is represented via skeletal keypoints collected via OpenPose [10] similar to [18]. It consists of of 52 coordinates of an individual’s major joints for each frame at 15 frames per second, which we rescale by holding the length of each individual’s shoulder constant. This prevents the model from encoding limb length in the style embeddings. Following prior work [29,18], we represent audio features as mel spetrograms, which is a rich input representation shown to be useful for gesture generation.
6 Results and Discussion
We group our results and discussions in (1) a ﬁrst set of experiments studying style preservation (when output gesture styles are the same the original speaker) and (2) a second set of experiments studying transfer of gesture styles.
3 While transcripts are a part of this dataset, they are ignored for the purposed of this work.

Style Transfer for Co-Speech Gesture Animation

11

Model

Number of Speakers

Attributes

2 Speakers 4 Speakers 8 Speakers Sitting vs Gesture Body Primary Standing Frequency Orientation Arm Func.

MUNIT [24] 1.11

1.90

2.06

1.10

2.49

1.05

3.32

StAGE

2.17

2.85

3.89

1.68

4.38

6.81

3.14

Mix-StAGE 2.61

2.85

4.48

3.08

4.50

6.69

3.32

Table 2: Style Transfer: Inception scores for style transfer on multi-speaker models (indicated in each row). Columns on the left refer to the speaker-level style transfer task while those on the right refer to the speciﬁc attribute-level style task. Bold numbers indicate p < 0.1 in a bootstrapped two sided t-test.

6.1 Gesture Animation and Style Preservation
To understand the impact of adding more speakers, we select a random sample of 8 speakers for the largest 8-speaker multi-speaker model, and train smaller 4-speaker and 2-speaker models where the speakers trained are always a subset of the speakers that were trained in a larger model. This allows to compare the performance on the same two initial speakers which are ‘Corden’ and ‘lec cosmic’ in our case4. We also compare with single-speaker models trained and tested on one speaker at a time.
Impact of training with Multiple Speakers Results from Table 1 show that multispeaker models outperform single-speaker models especially for pose accuracy (i.e. PCK), shape and timing (i.e. F1). We ﬁnd that increasing the number of speakers could sometimes reduce the performance of individual speakers but the overall performance generally shows improvement.
Comparison with previous baselines To compare with prior baselines, we focus ﬁrst on the subjective evaluation shown in Figure 4a, since it is arguably the most important metric. The results show consistent improvements on the naturalness rating for our proposed model Mix-StAGE and also our single-speaker variant CMix-GAN over the previous state of the art approach S2G [18]. We also observe that multi-speaker models perform better than single speaker-models. In Table 1, we show similar quantitative improvements of Mix-StAGE and CMix-GAN over S2G for both PCK and F1 scores.
Impact of Multiple Generators for Decoding Mix-StAGE’s gesture space models multiple modes, as seen in Figure 2. Its importance is shown in Table 1 where models with single generators as the decoder (i.e. S2G, MUNIT and StAGE) showed lower F1 scores, most likely due to mode collapse while training. Multiple generators in CMixGAN and Mix-StAGE boost F1 scores as compared to other models in the singlespeaker and multi-speaker regimes respectively. A similar trend was observed in the perceptual study in Figure 4.
4 The complete set of speakers used in our experiments are listed in the supplementary.

12

C. Ahuja et al.

(a) Naturalness Preference

(b) Style Transfer Correctness

Fig. 5: A visualization of the perceptual human study for attribute-level style transfer with (a) naturalness preference, and (b) style transfer correctness scores for the generated animations for a different style than the speaker. Higher is better. Error bars calculated for p < 0.1 using a bootstrapped two sided t-test.

We also study the impact of the number of generators (hyperparameter M) in our Mix-StAGE model. While for small number of speakers (i.e. 2 speakers) a single generator is good enough, the positive effect of multiple generators can be observed as the number of speakers increase (see Table 1). We also vary M ∈ {1, 2, 4, 8, 12} and observe that improvements seem to plateau at M = 8 with only marginal improvements for larger number of sub-generators. For the ablation study we refer the readers to the supplementary.
Attribute-level Style Preservation in Multi-Speaker Models We also study style preservation for attributes in Section 5 as a perceptual study in Figure 6. We observe that humans deem animations generated by Mix-StAGE signiﬁcantly more natural in most cases. High scores ranging 60-90% for style preservation correctness, with Mix-StAGE outperforming others, are observed for pairs of speakers in Figure 6b. This indicates that style preservation may be a relatively easy task as compared to style transfer for multi-speaker models. With this, we now shift our focus to style transfer.
6.2 Style Transfer
Speaker-level Style Transfer To study our capability to transfer style of a speciﬁc speaker to a new speaker, we will compare the gesture spaces between the original speakers and the transferred speakers. Figure 2a shows that each original speaker occupies different regions in the M 2 gesture space. Using our Mix-StAGE model to transfer style, we can see the new gesture space in Figure 2b. For the transferred speakers the 2 spaces look quite similar. For instance, ‘Corden’ style (a speaker in our dataset) is represented by the color blue in Figure 2a and occupies the lower region of the gesture space. When Mix-StAGE generates co-speech gestures using audio of ‘Oliver’ and the style of ‘Corden’, it occupies a subset of ‘Corden’s’ region in the gesture space, also represented by blue in Figure 2b. We see a similar trend for styles of ‘Oliver‘ and

Style Transfer for Co-Speech Gesture Animation

13

(a) Naturalness Preference

(b) Style Preservation Correctness

Fig. 6: A visualization of the perceptual human study for attribute-level style preservation with (a) naturalness preference, and (b) style preservation correctness scores for the generated animations for the same style as the speaker. Higher is better. Error bars calculated for p < 0.1 using a bootstrapped two sided t-test.

‘ytch prof ’. This is an indication of a successful style transfer across different speakers. We note the lack of clean separation in the gesture space among different styles as there could common gestures across multiple speakers.
For the perceptual study, we want to know if humans can distinguish the generated speaker styles. For this, we show human annotators two videos: a ground truth video in a speciﬁc style, and a generated video which is either from the style of the same speaker or a different speaker. Annotators have to decide if this is the same style or not. We use the 4-speaker model for this experiment. Figure 4b shows naturalness preference and 4c shows percentage of the time style was transferred correctly. Our model Mix-StAGE performs best in both cases. This trend is corroborated with higher inception scores in Table 2.
Impact of Number of Speakers for Style Transfer In Table 2, we observe that increasing the number of speakers used for training also increases the average inception score for the stylized gesture generations. This is a welcome effect as it indicates increases in the diversity and the accuracy of the generations.
Attribute-level Style Transfer in Multi-Speaker Models We study four common attributes of gesture style which are also visually distinguishable by humans: (1) sitting vs. standing, (2) high vs low gesture frequency, (3) left vs right body orientation and (4) left vs right primary arm. Speakers were selected carefully to represent each extremes of these four attributes. We run a perceptual study similar to the one for speaker-level styles. However, we ask the annotators to judge if the attribute is the same in both of the videos (e.g. are both the people gesturing with the same arm?). Results from Figure 5 show that Mix-StAGE generates more (or similar) number of natural gestures with the correct attribute-level style compared to the other baselines. We also observe that it is harder for humans to determine if a person is standing or sitting, which we suspect is due to the missing waistline in the animation.

14

C. Ahuja et al.

(a) Primary Arm Func. (b) Body Orientation (c) Sitting vs Standing (d) Gesture Frequency
Fig. 7: Style-Content Heatmaps for attribute-level style transfer. Each column represents the same style, while rows have input audio from different speakers. These heatmaps show that gestures are consistent across audio inputs but different between styles. Red regions correspond to the motion of the right arm, while blue corresponds to the left.
For a visual understanding of the generated gestures and stylized gestures, we plot a style-content heatmap in Figure 7, where columns represent generations for a speciﬁc style, while rows represent different speaker’s audio as input. These heatmaps show that gestures are consistent across audio inputs but different between styles. Accuracy and diversity of style transfer is corroborated by inception scores in Table 2.
7 Conclusions
In this paper, we propose a new model, named Mix-StAGE, which learns a single model for multiple speakers while learning unique style embeddings for each speaker’s gestures in an end-to-end manner. A novelty of Mix-StAGE was to learn a mixture of generative models conditioned on gesture style while the audio drives the co-speech gesture generation. We also introduced a new dataset, Pose-Audio-Transcript-Style (PATS), designed to study gesture generation and style transfer. It consists of 25 speakers (15 new speakers and 10 speakers from Ginosar et. al. [18]) for a total of 250+ hours of gestures and aligned audio signals. Our proposed Mix-StAGE model signiﬁcantly outperformed previous state-of-the-art approach for gesture generation and provided a path towards performing gesture style transfer across multiple speakers. We also demonstrated, through human perceptual studies, that the generated animations by our model are more natural whilst being able to retain or transfer style.
Acknowledgements This material is based upon work partially supported by the National Science Foundation (Awards #1750439 #1722822), National Institutes of Health and the InMind project. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of National Science Foundation or National Institutes of Health, and no ofﬁcial endorsement should be inferred.

Style Transfer for Co-Speech Gesture Animation

15

References
1. Ahuja, C., Ma, S., Morency, L.P., Sheikh, Y.: To react or not to react: End-to-end visual pose forecasting for personalized avatar during dyadic conversations. In: 2019 International Conference on Multimodal Interaction. pp. 74–84. ACM (2019) 3
2. Ahuja, C., Morency, L.P.: Language2pose: Natural language grounded pose forecasting. In: 2019 International Conference on 3D Vision (3DV). pp. 719–728. IEEE (2019) 6
3. Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.: 2d human pose estimation: New benchmark and state of the art analysis. In: Proceedings of the IEEE Conference on computer Vision and Pattern Recognition. pp. 3686–3693 (2014) 9
4. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. arXiv preprint arXiv:1701.07875 (2017) 5
5. Arora, S., Ge, R., Liang, Y., Ma, T., Zhang, Y.: Generalization and equilibrium in generative adversarial nets (gans). In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. pp. 224–232. JMLR. org (2017) 5
6. Bailenson, J.N., Yee, N., Merget, D., Schroeder, R.: The effect of behavioral realism and form realism of real-time avatar faces on verbal disclosure, nonverbal disclosure, emotion recognition, and copresence in dyadic interaction. Presence: Teleoperators and Virtual Environments 15(4), 359–372 (2006) 1
7. Bansal, A., Ma, S., Ramanan, D., Sheikh, Y.: Recycle-gan: Unsupervised video retargeting. In: Proceedings of the European conference on computer vision (ECCV). pp. 119–135 (2018) 3
8. Bergmann, K., Kopp, S.: Increasing the expressiveness of virtual agents: autonomous generation of speech and gesture for spatial description tasks. In: Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems-Volume 1. pp. 361–368 (2009) 2
9. Bian, Y., Chen, C., Kang, Y., Pan, Z.: Multi-reference tacotron by intercross training for style disentangling, transfer and control in speech synthesis. arXiv preprint arXiv:1904.02373 (2019) 3
10. Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: Openpose: realtime multi-person 2d pose estimation using part afﬁnity ﬁelds. arXiv preprint arXiv:1812.08008 (2018) 10
11. Cassell, J., Vilhja´lmsson, H.H., Bickmore, T.: Beat: the behavior expression animation toolkit. In: Life-Like Characters, pp. 163–185. Springer (2004) 1
12. Chiu, C.C., Marsella, S.: Gesture generation with low-dimensional embeddings. In: Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. pp. 781–788 (2014) 3
13. Chiu, C.C., Morency, L.P., Marsella, S.: Predicting co-verbal gestures: A deep and temporal modeling approach. In: Proceedings of the 15th international conference on Intelligent virtual agents (IVA2015). vol. 9238, pp. 152–166 (2015) 1
14. Davis, R.O., Vincent, J.: Sometimes more is better: Agent gestures, procedural knowledge and the foreign language learner. British Journal of Educational Technology 50(6), 3252– 3263 (2019) 10
15. Denton, E.L., et al.: Unsupervised learning of disentangled representations from video. In: Advances in neural information processing systems. pp. 4414–4423 (2017) 3
16. Ferstl, Y., Neff, M., McDonnell, R.: Multi-objective adversarial gesture generation. In: Motion, Interaction and Games. p. 3. ACM (2019) 1, 3
17. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576 (2015) 3
18. Ginosar, S., Bar, A., Kohavi, G., Chan, C., Owens, A., Malik, J.: Learning individual styles of conversational gesture. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3497–3506 (2019) 1, 3, 7, 8, 9, 10, 11, 14

16

C. Ahuja et al.

19. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural information processing systems. pp. 2672–2680 (2014) 7
20. Gurunath, N., Rallabandi, S.K., Black, A.: Disentangling speech and non-speech components for building robust acoustic models from found data. arXiv preprint arXiv:1909.11727 (2019) 3
21. Hao, G.Y., Yu, H.X., Zheng, W.S.: Mixgan: learning concepts from different domains for mixture generation. arXiv preprint arXiv:1807.01659 (2018) 5
22. Hasegawa, D., Kaneko, N., Shirakawa, S., Sakuta, H., Sumi, K.: Evaluation of Speech-toGesture Generation Using Bi-Directional LSTM Network. In: Proceedings of the 18th International Conference on Intelligent Virtual Agents (IVA18). pp. 79–86 (2018) 3
23. Hoang, Q., Nguyen, T.D., Le, T., Phung, D.: Mgan: Training generative adversarial nets with multiple generators (2018) 5
24. Huang, X., Liu, M.Y., Belongie, S., Kautz, J.: Multimodal unsupervised image-to-image translation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 172–189 (2018) 4, 6, 8, 9, 11
25. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1125–1134 (2017) 3
26. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and superresolution. In: European conference on computer vision. pp. 694–711. Springer (2016) 3
27. Kendon, A.: Gesture and speech: two aspects of the process of utterance. In: M. R. Key (ed.) Nonverbal Communication and Language, pp. 207–227 (1980) 1
28. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 21
29. Kucherenko, T., Hasegawa, D., Henter, G.E., Kaneko, N., Kjellstro¨m, H.: Analyzing input and output representations for speech-driven gesture generation. arXiv preprint arXiv:1903.03369 (2019) 10
30. Lee, H.Y., Tseng, H.Y., Mao, Q., Huang, J.B., Lu, Y.D., Singh, M., Yang, M.H.: Drit++: Diverse image-to-image translation via disentangled representations. arXiv preprint arXiv:1905.01270 (2019) 4, 6
31. Lee, H.Y., Yang, X., Liu, M.Y., Wang, T.C., Lu, Y.D., Yang, M.H., Kautz, J.: Dancing to music. In: Advances in Neural Information Processing Systems. pp. 3581–3591 (2019) 4
32. Levine, S., Kra¨henbu¨hl, P., Thrun, S., Koltun, V.: Gesture controllers. ACM Trans. Graph. 29(4), 124:1–124:11 (Jul 2010) 3
33. Levine, S., Theobalt, C., Koltun, V.: Real-time prosody-driven synthesis of body language. ACM Trans. Graph. 28(5), 172:1–172:10 (Dec 2009) 3
34. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation networks. In: Advances in neural information processing systems. pp. 700–708 (2017) 4
35. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: Advances in neural information processing systems. pp. 469–477 (2016) 6
36. Lloyd, S.: Least squares quantization in pcm. IEEE transactions on information theory 28(2), 129–137 (1982) 6
37. Ma, S., Mcduff, D., Song, Y.: Neural tts stylization with adversarial and collaborative games (2018) 4, 7
38. Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(Nov), 2579–2605 (2008) 3
39. McNeill, D.: Hand and mind: What gestures reveal about thought. University of Chicago Press (1992) 1, 2, 5

Style Transfer for Co-Speech Gesture Animation

17

40. Nagrani, A., Chung, J.S., Albanie, S., Zisserman, A.: Disentangled speech embeddings using cross-modal self-supervision. In: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6829–6833. IEEE (2020) 4
41. Neff, M., Kipp, M., Albrecht, I., Seidel, H.P.: Gesture modeling and animation based on a probabilistic re-creation of speaker style. ACM Transactions on Graphics (TOG) 27(1), 1–24 (2008) 4
42. Obermeier, C., Kelly, S.D., Gunter, T.C.: A speakers gesture style can affect language comprehension: Erp evidence from gesture-speech integration. Social cognitive and affective neuroscience 10(9), 1236–1243 (2015) 1
43. Pelachaud, C.: Studies on gesture expressivity for a virtual agent. Speech Communication 51(7), 630–639 (2009) 2
44. Reynolds, D.A.: Gaussian mixture models. Encyclopedia of biometrics 741 (2009) 6 45. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical im-
age segmentation. In: International Conference on Medical image computing and computerassisted intervention. pp. 234–241. Springer (2015) 7 46. Rosca, M., Lakshminarayanan, B., Warde-Farley, D., Mohamed, S.: Variational approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987 (2017) 6 47. Royer, A., Bousmalis, K., Gouws, S., Bertsch, F., Mosseri, I., Cole, F., Murphy, K.: Xgan: Unsupervised image-to-image translation for many-to-many mappings. In: Domain Adaptation for Visual Understanding, pp. 33–49. Springer (2020) 6 48. Sadoughi, N., Busso, C.: Novel realizations of speech-driven head movements with generative adversarial networks. pp. 6169–6173 (04 2018). https://doi.org/10.1109/ICASSP.2018.8461967 3 49. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training gans. In: Advances in neural information processing systems. pp. 2234–2242 (2016) 9 50. Sargin, M.E., Yemez, Y., Erzin, E., Tekalp, A.M.: Analysis of head gesture and prosody patterns for prosody-driven head-gesture animation. IEEE Trans. Pattern Anal. Mach. Intell. 30, 1330–1345 (2008). https://doi.org/10.1109/TPAMI.2007.70797 3 51. Shlizerman, E., Dery, L., Schoen, H., Kemelmacher, I.: Audio to body dynamics. Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition (06 2018) 3 52. Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single images using multiview bootstrapping. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 1145–1153 (2017) 9 53. Smith, H.J., Cao, C., Neff, M., Wang, Y.: Efﬁcient neural networks for real-time motion style transfer. Proceedings of the ACM on Computer Graphics and Interactive Techniques 2(2), 1–17 (2019) 4 54. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818–2826 (2016) 9 55. Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content for natural video sequence prediction. arXiv preprint arXiv:1706.08033 (2017) 3 56. Wagner, P., Malisz, Z., Kopp, S.: Gesture and speech in interaction: An overview (2014) 1 57. Wang, Y., Stanton, D., Zhang, Y., Skerry-Ryan, R., Battenberg, E., Shor, J., Xiao, Y., Ren, F., Jia, Y., Saurous, R.A.: Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. arXiv preprint arXiv:1803.09017 (2018) 3, 4, 7

18

C. Ahuja et al.

58. Xu, J., Gannon, P.J., Emmorey, K., Smith, J.F., Braun, A.R.: Symbolic gestures and spoken language are processed by a common neural system. Proceedings of the National Academy of Sciences 106(49), 20664–20669 (2009) 2
59. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycleconsistent adversarial networks. In: Proceedings of the IEEE international conference on computer vision. pp. 2223–2232 (2017) 3, 4
60. Zhu, J.Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.A., Wang, O., Shechtman, E.: Toward multimodal image-to-image translation. In: Advances in neural information processing systems. pp. 465–476 (2017) 3
Supplementary

A PATS dataset
A.1 Speaker List
The list of speakers in the dataset are in Figure 8 as a dendrogram. This dendogram was created using text as the discriminating features. Speakers within the same cluster have a similar vocabulary. For the purposes of our experiments we use the speakers listed in Table 4 and 5.

Fig. 8: List of speakers in the dataset as a dendrogram based on the content of the speech.

A.2 Attributes
We deﬁne 4 different attributes in Table 3 and select pairs of speakers that demonstrate visually striking differences with respect to those attributes.

Sitting/Standing Gesture Frequency Body Orientation Primary Arm Func.

Sitting: Noah Low: Seth Standing: Maher High: Oliver

Right: Chemistry Right Arm: lec cosmic

Left: Oliver

Left Arm: lec cosmic

Table 3: Selection of speakers for attribute-level style modeling

B Other Results, Discussions and Future Directions
These results complement Section 6 of the main paper with a few more observations, explorations and ablation studies. This is followed by potential future directions.

Style Transfer for Co-Speech Gesture Animation

19

Attribute-Level Style Preservation Gesture generation for pairs of speakers shows improvements in PCK and F1 scores (see Table 4) following the trend of perceptual study in Figure 6 of the main paper.

Single-Speaker Models

Multi-Speaker Models

Attributes

Speakers S2G CMix-GAN MUNIT StAGE Mix-StAGE

PCK F1 PCK F1 PCK F1 PCK F1 PCK F1

Sitting/Standing Mean

0.35 0.12 0.35 0.25 0.36 0.07 0.42 0.18 0.42 0.25

Sitting

Noah

0.45 0.11 0.45 0.28 0.34 0.09 0.44 0.14 0.44 0.26

Standing

Maher 0.25 0.13 0.24 0.22 0.22 0.07 0.28 0.26 0.26 0.25

Gesture Frequency Mean

0.55 0.41 0.56 0.44 0.34 0.14 0.58 0.51 0.58 0.53

Low

Seth

0.56 0.50 0.58 0.54 0.22 0.02 0.58 0.54 0.59 0.57

High

Oliver

0.54 0.32 0.54 0.34 0.35 0.22 0.54 0.38 0.56 0.42

Body Orientation Mean

0.39 0.14 0.43 0.25 0.14 0.05 0.40 0.42 0.40 0.40

Right

Chemistry 0.35 0.23 0.36 0.27 0.15 0.05 0.37 0.39 0.40 0.39

Left

lec evol 0.44 0.05 0.50 0.23 0.28 0.34 0.50 0.44 0.49 0.46

Primary Arm Func. Mean

0.43 0.12 0.43 0.33 0.35 0.02 0.59 0.30 0.61 0.37

Left Arm

lec cosmic 0.41 0.08 0.41 0.28 0.32 0.06 0.60 0.27 0.62 0.36

Right Arm

lec cosmic 0.45 0.18 0.45 0.38 0.44 0.12 0.58 0.31 0.60 0.35

Table 4: Objective metrics for attribute-level style preservation of single-speaker and

multi-speaker models as indicated in the columns. Each row refers to the number of

speakers the model was trained, with the average performance indicated at the top. The

scores for common individual speakers are also indicated below alongside. For detailed

results on other speakers please refer to the supplementary

Speaker-Level Style Preservation Complete numerical results for speaker-level style preservation (for Table 1 in the main paper) are listed in Table 5. The PCK and F1 scores of the individual speakers show the same trend as the average score for each model.
Impact of value of M on gesture generation We run an ablation study on the choice of M for the pose decoder. We report the average of PCK and F1 scores in Table ?? which were calculated for each speaker in single-speaker models. We ﬁnd the the scores plateau with increasing values of M for single speaker models unlike multi-speaker models like Mix-StAGE .
Exploring Style Control As a preliminary experiment, we modiﬁed the style vector to [0.5, 0.5] in order to mix the styles of two speakers with different Primary Arm Functions. The generated gesture space in Figure 9 indicates that different speaker styles could be interpolated into a completely new style.
Future Directions Our efforts were aimed at modeling, disentangling and transferring gesture style under the assumption that the emotional state of the speaker does not affect

20

C. Ahuja et al.

No. of

Single-Speaker Models

Multi-Speaker Models

Speakers Speaker S2G CMix-GAN MUNIT StAGE Mix-StAGE

PCK F1 PCK F1 PCK F1 PCK F1 PCK F1

Mean

0.25 0.08 0.26 0.27 0.24 0.06 0.36 0.21 0.34 0.22

2 Corden 0.30 0.05 0.32 0.21 0.25 0.06 0.36 0.21 0.34 0.22

lec cosmic 0.19 0.12 0.19 0.33 0.15 0.19 0.20 0.48 0.24 0.49

Mean

0.37 0.18 0.37 0.27 0.22 0.03 0.38 0.34 0.39 0.35

Corden 0.30 0.05 0.32 0.21 0.24 0.07 0.35 0.27 0.35 0.30

4 lec cosmic 0.19 0.12 0.19 0.33 0.19 0.16 0.18 0.23 0.20 0.19

ytch prof 0.43 0.22 0.43 0.22 0.15 0.02 0.42 0.34 0.40 0.32

Oliver

0.54 0.32 0.54 0.34 0.20 0.09 0.54 0.47 0.55 0.52

Mean

0.36 0.14 0.37 0.26 0.31 0.15 0.38 0.32 0.40 0.33

Corden 0.30 0.05 0.32 0.21 0.23 0.03 0.32 0.28 0.36 0.27

lec cosmic 0.19 0.12 0.19 0.33 0.13 0.09 0.23 0.34 0.24 0.32

ytch prof 0.43 0.22 0.43 0.22 0.39 0.37 0.44 0.39 0.45 0.39

8 Oliver

0.54 0.32 0.54 0.34 0.35 0.30 0.54 0.39 0.54 0.46

Ellen

0.29 0.13 0.30 0.23 0.33 0.17 0.34 0.21 0.33 0.25

Noah

0.45 0.11 0.45 0.28 0.40 0.23 0.44 0.24 0.44 0.27

lec evol 0.44 0.05 0.50 0.23 0.33 0.42 0.45 0.66 0.48 0.66

Maher 0.25 0.13 0.24 0.22 0.23 0.17 0.25 0.25 0.25 0.25

Table 5: Objective metrics for speaker-level style preservation of single-speaker and

multi-speaker models as indicated in the columns. Each row refers to the number of

speakers the model was trained, with the average performance indicated at the top. The

scores for individual speakers are also indicated below alongside. * refers to a Single-

speaker Model

Single Speaker

Metrics

Models

F1 ↑ PCK ↑

S2G

18.9 36.6

CMix-GAN (M = 1) 26.6 37.9

CMix-GAN (M = 4) 27.7 36.6

CMix-GAN (M = 8) 28.0 36.7

CMix-GAN (M = 12) 27.8 37.0

Table 6: Comparision of Mix-StAGE with different values of M over F1 and PCK.

The results are reported as a mean over all speakers in PATS. We can see that the

performance for single speaker models does not improve by increasing the number of

modes M . This is unlike multi-speaker models, where the addition of sub-generators

gives the model an edge over single-speaker models.

Style Transfer for Co-Speech Gesture Animation

21

Fig. 9: Heat map of mixture of two styles: primary arm function of left and right mixed to give motion for both hands. Red represents the left hand and blue represents the right hand.
the gestures. While this is reasonable for speakers in PATS, which are mostly scripted monologues, it may not be true in general hence motivating an interesting future direction. Another direction, that might induce diversity in the generated gestures, is the inclusion of verbal information (i.e. natural language). This may not be trivial in context of style transfer as the difference in vocabulary of different speakers could create an unwanted bias - some words might get associated with certain styles of gesturing.
C Implementation Details
This section gives more detail about the exact architectures used for our model also described in Section 4 of the main paper.
C.1 Network Architectures
Figure 10 and 11 consists of the visual representation of the architectures used for our model Mix-StAGE . For the decoder is a weighted sum as described in Equation (2) of the main paper. Every operation is a 1D-convolution followed by a Batch-Norm and ﬁnally ReLU. Each convolution uses a kernal size of 3 and hop length of 1, except for cases where temporal dimension is downsampled where the kernal size is 4 and hop length is 2.
C.2 Training Details
We use Adam [28] to optimize the model with a exponentially decaying learning rate of 0.001. We train each model for 60000 iterations while check-pointing every 3000 iterations. Finally, we choose the best model based on loss on the development set. We use λid = 0.1 to prevent the style consistency loss from stealing focus while training the pose gesture generator.

22

C. Ahuja et al.

Fig. 10: Encoder Architecture
C.3 Human study experiments
We conducted our human studies on Amazon Mechanical Turk (or AMT), for which we used 100 random videos for each speaker which gave us 2400 pairs of comparisons per model for each study (out of 5). This is a signiﬁcant number of comparisons and helps with reliability of the results. Each annotation task contained 20 videos and is performed by 3 different users; hence we had approximately (2400*3/20) = 360 participants.
To help ﬁlter unreliable annotators, we use two ground truth videos from the same speaker with the same style as control samples. If annotators tag these two videos as different styles, then we disregard this annotation set as unreliable.
Sample study for style transfer (other studies also follow similar method) Two videos are shown to the user. One video is a ground truth(Speaker A Style A) and the other is generated by a model. The generated video could be either of (a) Speaker A Style A or (b) Speaker B Style A. We ask two questions to measure correctness of style transfer and naturalness:
1. Do the animations have different styles of gestures? 2. Which of the videos 1 or 2 has more Natural gestures with respect to the audio?
D Videos: Style Transfer and Preservation
We refer the readers to http://chahuja.com/mix-stage for demo videos.

Style Transfer for Co-Speech Gesture Animation

23

Fig. 11: Decoder Architecture
E Video Frames: Style Preservation Qualitative Results
These results complement Figure 8 in the main paper. We plot some more animation ﬁgures generated by random audio samples in the test-set to provide some more samples for qualitative judgment in Figure 12.

24

C. Ahuja et al.

Fig. 12: Animation depicted as a series of frames for different speakers. The vertical axis is labeled as models and horizontal axis is time. The generated animation is superimposed over the ground truth video.

oliver

lec cosmic

corden

ytch prof

