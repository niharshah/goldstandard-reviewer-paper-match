EF21 with Bells & Whistles

Oct 6, 2021

arXiv:2110.03294v1 [cs.LG] 7 Oct 2021

EF21 WITH BELLS & WHISTLES: PRACTICAL ALGORITHMIC EXTENSIONS OF MODERN ERROR FEEDBACK

Ilyas Fatkhullin* KAUST Saudi Arabia

Igor Sokolov KAUST Saudi Arabia

Eduard Gorbunov MIPT & Yandex Russia

Zhize Liâ€  KAUST Saudi Arabia

Peter RichtÃ¡rik KAUST Saudi Arabia

ABSTRACT
First proposed by Seide et al. (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is ğ‘‚(1/ğ‘‡ 2/3), the rate of gradient descent in the same regime is ğ‘‚(1/ğ‘‡ )). Recently, RichtÃ¡rik et al. (2021) (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deï¬ciencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21, all supported by strong convergence theory: partial participation, stochastic approximation, variance reduction, proximal setting, momentum and bidirectional compression. Several of these techniques were never analyzed in conjunction with EF before, and in cases where they were (e.g., bidirectional compression), our rates are vastly superior.

1 INTRODUCTION

In this paper, we consider the nonconvex distributed/federated optimization problem of the form

{ï¸ƒ

def

1

ğ‘›
âˆ‘ï¸

}ï¸ƒ

min ğ‘“ (ğ‘¥) =

ğ‘“ğ‘–(ğ‘¥) ,

(1)

ğ‘¥âˆˆRğ‘‘

ğ‘›

ğ‘–=1

where ğ‘› denotes the number of clients/workers/devices/nodes connected with a server/master and client ğ‘– has an access to the local loss function ğ‘“ğ‘– only. The local loss of each client is allowed to have the online/expectation form

ğ‘“ğ‘–(ğ‘¥) = Eğœ‰ğ‘–âˆ¼ğ’Ÿğ‘– [ğ‘“ğœ‰ğ‘– (ğ‘¥)] ,

(2)

or the ï¬nite-sum form

1

ğ‘š
âˆ‘ï¸

ğ‘“ğ‘–(ğ‘¥) = ğ‘š ğ‘“ğ‘–ğ‘—(ğ‘¥). (3)

ğ‘—=1

Problems of this structure appear in federated learning (KonecË‡nÃ½ et al., 2016; Kairouz, 2019), where training is performed directly on the clientsâ€™ devices. In a quest for state-of-the-art performance, machine learning practitioners develop elaborate model architectures and train their models on enormous data sets. Naturally, for training at this scale to be possible, one needs to rely on distributed computing (Goyal et al., 2017; You et al., 2020). Since in recent years remarkable empirical successes were obtained with massively over-parameterized models (Arora et al., 2018), which puts an extra strain on the communication links during training, recent research activity and practice
*The work of Ilyas Fatkhullin was performed during a Summer research internship conducted in the Optimization and Machine Learning Lab led by Peter RichtÃ¡rik. At the time when this paper was ï¬rst released, Ilyas Fatkhullin was a masterâ€™s student at the Technical University of Munich, Germany.
â€ Corresponding author.

1

EF21 with Bells & Whistles

Oct 6, 2021

focuses on developing distributed optimization methods and systems capitalizing on (deterministic or randomized) lossy communication compression techniques to reduce the amount of communication trafï¬c.

A compression mechanism is typically formalized as an operator ğ’ : Rğ‘‘ â†¦â†’ Rğ‘‘ mapping hard-tocommunicate (e.g., dense) input messages into easy-to-communicate (e.g., sparse) output messages. The operator is allowed to be randomized, and typically operates on models Khaled & RichtÃ¡rik (2019) or on gradients Alistarh et al. (2017); Beznosikov et al. (2020), both of which can be described as vectors in Rğ‘‘. Besides sparsiï¬cation (Alistarh et al., 2018), typical examples of useful compression mechanisms include quantization (Alistarh et al., 2017; HorvÃ¡th et al., 2019a) and low-rank approximation (Vogels et al., 2019; Safaryan et al., 2021).

There are two large classes of compression operators often studied in the literature: i) unbiased compression operators ğ’, meaning that there exists ğœ” â‰¥ 0 such that

E [ğ’(ğ‘¥)] = ğ‘¥, E [ï¸€â€–ğ’(ğ‘¥) âˆ’ ğ‘¥â€–2]ï¸€ â‰¤ ğœ”â€–ğ‘¥â€–2, âˆ€ğ‘¥ âˆˆ Rğ‘‘;

(4)

and ii) biased compression operators ğ’, meaning that there exists 0 < ğ›¼ â‰¤ 1 such that

E [ï¸€â€–ğ’(ğ‘¥) âˆ’ ğ‘¥â€–2]ï¸€ â‰¤ (1 âˆ’ ğ›¼) â€–ğ‘¥â€–2, âˆ€ğ‘¥ âˆˆ Rğ‘‘.

(5)

Note that the latter â€œbiasedâ€ class contains the former one, i.e., if ğ’ satisï¬es (4) with ğœ”, then a scaled version (1 + ğœ”)âˆ’1ğ’ satisï¬es (5) with ğ›¼ = 1/(1+ğœ”). While distributed optimization methods with unbiased compressors (4) are well understood (Alistarh et al., 2017; Khirirat et al., 2018; Mishchenko et al., 2019; HorvÃ¡th et al., 2019b; Li et al., 2020; Li & RichtÃ¡rik, 2021a; Li & RichtÃ¡rik, 2020; Islamov et al., 2021; Gorbunov et al., 2021), biased compressors (5) are signiï¬cantly harder to analyze. One of the main reasons behind this is rooted in the observation that when deployed within distributed gradient descent in a naive way, biased compresors may lead to (even exponential) divergence (Karimireddy et al., 2019; Beznosikov et al., 2020). Error Feedback (EF) (or Error Compensation (EC))â€”a technique originally proposed by Seide et al. (2014)â€”emerged as an empirical ï¬x of this problem. However, this technique remained poorly understood until very recently.

Although several theoretical results were obtained supporting the EF framework in recent years (Stich et al., 2018; Alistarh et al., 2018; Beznosikov et al., 2020; Gorbunov et al., 2020; Qian et al., 2020; Tang et al., 2020; Koloskova et al., 2020), they use strong assumptions (e.g., convexity, bounded gradients, bounded dissimilarity), and do not get ğ’ª(1/ğ›¼ğ‘‡ ) convergence rates in the smooth nonconvex regime. Very recently, RichtÃ¡rik et al. (2021) proposed a new EF mechanism called EF21, which uses standard smoothness assumptions only, and also enjoys the desirable ğ‘‚(1/ğ›¼ğ‘‡ ) convergence rate for the nonconvex case (in terms of number of communication rounds ğ‘‡ this matches the best-known rate ğ’ª((1+ğœ”/âˆšğ‘›)/ğ‘‡ ) obtained by Gorbunov et al. (2021) using unbiased compressors), improving the previous ğ‘‚(1/(ğ›¼ğ‘‡ ) )2/3 rate of the standard EF mechanism (Koloskova et al., 2020).

2 OUR CONTRIBUTIONS
While RichtÃ¡rik et al. (2021) provide a new theoretical SOTA for error feedback based methods, the authors only study their EF21 mechanism in a pure form, without any additional â€œbells and whistlesâ€ which are of importance in practice. In this paper, we aim to push the EF21 framework beyond its pure form by extending it in several directions of high theoretical and practical importance. In particular, we further enhance the EF21 mechanism with the following six useful and practical algorithmic extensions:
1. stochastic approximation, 2. variance reduction, 3. partial participation, 4. bidirectional compression, 5. momentum, and 6. proximal setting (regularization).
We do not stop at merely proposing these algorithmic enhancements: we derive strong convergence results for all of these extensions. Several of these techniques were never analyzed in conjunction

2

EF21 with Bells & Whistles

Oct 6, 2021

Setup

Method

Citation

Compl. (NC)

Compl. (PL)

Comment

gFruadlls EF21 RichtÃ¡rik et al. (2021) ğ›¼1ğœ€2 ğ›¼1ğœ‡

Stoch. grads

Choco-SGD EF21-SGD EF21-SGD
EF21-PAGE

Koloskova et al. (2020) RichtÃ¡rik et al. (2021)
NEW NEW

ğœ€12 + ğ›¼ğºğœ€3 + ğ‘›ğœğœ€24 ğ›¼1ğœ€2 + ğ›¼ğœ32ğœ€4
ğ›¼1ğœ€2 + 1+ğ›¼3Î”ğœ€i4nf
âˆš
ğ‘šğœ€+21/ğ›¼ + ğ‘š

N/A
ğ›¼1ğœ‡ + ğœ‡2ğœğ›¼23ğœ€ ğ›¼1ğœ‡ + 1ğœ‡+2Î”ğ›¼i3nğœ€f
âˆš
ğ‘šğœ‡+1/ğ›¼ + ğ‘š

â€–âˆ‡ğ‘“ğ‘–(ğ‘¥)â€– â‰¤ ğº UBV (Ex. 1)

IS (Ex. 2)

ğ‘š

ğ‘“ğ‘–(ğ‘¥) =

1 ğ‘š

âˆ‘ï¸€

ğ‘“ğ‘–ğ‘— (ğ‘¥)

ğ‘—=1

PP

EF21-PP

NEW

ğ‘ğ›¼1ğœ€2 (1) + ğ›¼1ğœ€2

ğ‘ğ›¼1ğœ‡ (1) + ğ›¼1ğœ‡

Full grads

BC DoubleSqueeze EF21-BC

Tang et al. (2020) NEW

ğœ€12 + ğœ€Î”3 + ğ‘›ğœğœ€24
1 ğ›¼ğ‘¤ ğ›¼ğ‘€ ğœ€2

N/A
1 ğ›¼ğ‘¤ ğ›¼ğ‘€ ğœ‡

E [â€–ğ’(ğ‘¥) âˆ’ ğ‘¥â€–] â‰¤ Î” Full grads

Mom.

M-CSER

EF21-HB

Xie et al. (2020)(2) ğœ€12 + (1âˆ’ğœ‚ğº)ğ›¼ğœ€3 N/A

NEW

(ï¸

)ï¸

1 ğœ€2

1âˆ’1 ğœ‚ + ğ›¼1

N/A

â€–âˆ‡ğ‘“ğ‘–(ğ‘¥)â€– â‰¤ ğº Full grads

Prox

EF21-Prox

NEW

1 ğ›¼ğœ€2

1 (3) ğ›¼ğœ‡

Full grads

(1) Red term = number of communication rounds, blue term = expected number of gradient computations per client.

(2) Xie et al. (2020) consider Nesterovâ€™s momentum. Moreover, they analyzed the version with stochastic gradients, bidirectional compression

and local steps. However, the derived result is not better than state-of-the-art ones with either stochastic gradients or bidirectional compression.

Therefore, to maintain the table compact, we do not include the results of Xie et al. (2020) in the other parts of the table. (3) This result is obtained under the generalized PÅ-condition for composite optimization problems (see Assumption 5 from Appendix I.2).

Table 1: Summary of the state-of-the-art complexity results for ï¬nding an ğœ€-stationary point, i.e., such a point ğ‘¥^ that E [ï¸€â€–âˆ‡ğ‘“ (ğ‘¥^)â€–2]ï¸€ â‰¤ ğœ€2, for generally non-convex functions and an ğœ€-solution, i.e., such a point ğ‘¥^ that E [ğ‘“ (ğ‘¥^) âˆ’ ğ‘“ (ğ‘¥*)] â‰¤ ğœ€, for functions satisfying PÅ-condition using error-feedback type methods. By

(computation) complexity we mean the average number of (stochastic) ï¬rst-order oracle calls needed to ï¬nd an

ğœ€-stationary point (â€œCompl. (NC)â€) or ğœ€-solution (â€œCompl. (PÅ)â€). Removing the terms colored in blue from the

complexity bounds shown in the table, one can get communication complexity bounds, i.e., the total number

of communication rounds needed to ï¬nd an ğœ€-stationary point (â€œCompl. (NC)â€) or ğœ€-solution (â€œCompl. (PÅ)â€).

Dependences on the numerical constants, â€œqualityâ€ of the starting point, and smoothness constants are omitted

in the complexity bounds. Moreover, dependencies on log(1/ğœ€) are also omitted in the column â€œCompl. (PÅ)â€.

Abbreviations: â€œBCâ€ = bidirectional compression, â€œPPâ€ = partial participation; â€œMom.â€ = momentum; ğ‘‡ = the

number of communications rounds needed to ï¬nd an ğœ€-stationary point; #grads = the number of (stochastic)

ï¬rst-order oracle calls needed to ï¬nd an ğœ€-stationary point. Notation: ğ›¼ = the compression parameter, ğ›¼ğ‘¤ and

ğ›¼ğ‘€

=

the

compression

parameters

of

worker

and

master

nodes

respectively

for

EF21-BC,

ğœ2

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğœğ‘–2

(see Example 1), Î”inf = ğ‘“ inf âˆ’ ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1 ğ‘“ğ‘–iğ‘—nf (see Example 2), ğ‘ = probability of sampling the client

in EF21-PP, ğœ‚ = momentum parameter. To the best of our knowledge, combinations of error feedback with

partial participation (EF21-PP) and proximal versions of error feedback (EF21-Prox) were never analyzed in

the literature.

with the original EF mechanism before, and in cases where they were, our new results with EF21 are vastly superior. See Table 1 for an overview of our results. In summary, our results constitute the new algorithmic and theoretical state-of-the-art in the area of error feedback.
We now brieï¬‚y comment on each extension proposed in this paper:
â—‡ Stochastic approximation. The vanilla EF21 method requires all clients to compute the exact/full gradient in each round. While RichtÃ¡rik et al. (2021) do consider a stochastic extension of EF21, they do not formalize their result, and only consider the simplistic scenario of uniformly bounded variance, which does not in general hold for stochasticity coming from subsampling (Khaled & RichtÃ¡rik, 2020). However, exact gradients are not available in the stochastic/online setting (2), and in the ï¬nite-sum setting (3) it is more efï¬cient in practice to use subsampling and work with stochastic gradients instead. In our paper, we extend EF21 to a more general stochastic approximation framework than the simplistic framework considered in the original paper. Our method is called EF21-SGD (Algorithm 2); see Appendix D for more details.
â—‡ Variance reduction. As mentioned above, EF21 relies on full gradient computations at all clients. This incurs a high or unaffordable computation cost, especially when local clients hold large training sets, i.e., if ğ‘š is very large in (3). In the ï¬nite-sum setting (3), we enhance EF21 with a variance reduction technique to reduce the computational complexity. In particular, we adopt the simple and efï¬cient variance-reduced method PAGE (Li et al., 2021; Li, 2021b) (which is optimal for solving problems (3)) into EF21, and call the resulting method EF21-PAGE (Algorithm 3). See Appendix E for more details.
3

EF21 with Bells & Whistles

Oct 6, 2021

â—‡ Partial participation. The EF21 method proposed by RichtÃ¡rik et al. (2021) requires full participation of clients for solving problem (1), i.e., in each round, the server needs to communicate with all ğ‘› clients. However, full participation is usually impractical or very hard to achieve in massively distributed (e.g., federated) learning problems (KonecË‡nÃ½ et al., 2016; Cho et al., 2020; Kairouz, 2019; Li & RichtÃ¡rik, 2021b; Zhao et al., 2021). To remedy this situation, we propose a partial participation (PP) variant of EF21, which we call EF21-PP (Algorithm 4). See Appendix F for more details.

â—‡ Bidirectional compression. The vanilla EF21 method only considers upstream compression of the messages sent by the clients to the server. However, in some situations, downstream communication is also costly (HorvÃ¡th et al., 2019a; Tang et al., 2020; Philippenko & Dieuleveut, 2020). In order to cater to these situations, we modify EF21 so that the server can also optionally compresses messages before communication. Our master compression is intelligent in that it employs the Markov compressor proposed in EF21 to be used at the devices. The proposed method, based on bidirectional compression, is EF21-BC (Algorithm 5). See Appendix G for more details.

â—‡ Momentum. A very successful and popular technique for enhancing both optimization and generalization is momentum/acceleration (Polyak, 1964; Nesterov, 1983; Lan & Zhou, 2015; AllenZhu, 2017; Lan et al., 2019; Li, 2021a). For instance, momentum is a key building block behind the widely-used Adam method (Kingma & Ba, 2014). In this paper, we add the well-known (Polyak) heavy ball momentum (Polyak, 1964; Loizou & RichtÃ¡rik, 2020) to EF21, and call the resulting method EF21-HB (Algorithm 6). See Appendix H for more details.

â—‡ Proximal setting. It is common practice to solve regularized versions of empirical risk minimization problems instead of their vanilla variants (Shalev-Shwartz & Ben-David, 2014). We thus consider the composite/regularized/proximal problem

{ï¸ƒ

def

1

ğ‘›
âˆ‘ï¸

}ï¸ƒ

min Î¦(ğ‘¥) =

ğ‘“ğ‘–(ğ‘¥) + ğ‘Ÿ(ğ‘¥) ,

(6)

ğ‘¥âˆˆRğ‘‘

ğ‘›

ğ‘–=1

where ğ‘Ÿ(ğ‘¥) : Rğ‘‘ â†’ R âˆª {+âˆ} is a regularizer, e.g., â„“1 regularizer â€–ğ‘¥â€–1 or â„“2 regularizer â€–ğ‘¥â€–22. To broaden the applicability of EF21 to such problems, we propose a proximal variant of EF21 to solve
the more general composite problems (6). We call this new method EF21-Prox (Algorithm 7). See
Appendix I for more details.

Our theoretical complexity results are summarized in Table 1. In addition, we also analyze EF21SGD, EF21-PAGE, EF21-PP, EF21-BC under Polyak-Åojasiewicz (PÅ) condition (Polyak, 1963; Lojasiewicz, 1963) and EF21-Prox under the generalized PÅ-condition (Li & Li, 2018) for composite optimization problems. Due to space limitations, we defer all the details about the analysis under the PÅ-condition to the appendix and provide only simpliï¬ed rates in Table 1. We comment on some preliminary experimental results in Section 5. More experiments including deep learning experiments are presented in Appendix A.

3 METHODS

Since our methods are modiï¬cations of EF21, they share many features, and are presented in a uniï¬ed
way in Table 2. At each iteration of the proposed methods, worker ğ‘– computes the compressed vector ğ‘ğ‘¡ğ‘– and sends it to the master. The methods differ in the way of computing ğ‘ğ‘¡ğ‘– but have similar (in case of EF21-SGD, EF21-PAGE, EF21-PP â€“ exactly the same) update rules to the one of EF21:

ğ‘¡+1

ğ‘¡

ğ‘¡

ğ‘¡+1

ğ‘¡ğ‘¡

ğ‘¡+1

1

ğ‘›
âˆ‘ï¸

ğ‘¡+1

ğ‘¡ 1 âˆ‘ğ‘›ï¸ ğ‘¡

ğ‘¥ = ğ‘¥ âˆ’ ğ›¾ğ‘” ,

ğ‘”ğ‘– = ğ‘”ğ‘– + ğ‘ğ‘–,

ğ‘”= ğ‘›

ğ‘”ğ‘–

=ğ‘” + ğ‘›

ğ‘ğ‘– .

(7)

ğ‘–=1

ğ‘–=1

The pseudocodes of the methods are given in the appendix. Below we brieï¬‚y describe each method.

â—‡ EF21-SGD: Error feedback and SGD. EF21-SGD is essentially EF21 but instead of the full gradients âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1), workers compute the stochastic gradients ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1), and use them to compute

ğ‘ğ‘¡ğ‘– = ğ’(ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡).

Despite the seeming simplicity of this extension, it is highly important for various applications of machine learning and statistics where exact gradients are either unavailable or prohibitively expensive to compute.

4

EF21 with Bells & Whistles

Oct 6, 2021

Update

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡,

ğ‘›

ğ‘”ğ‘¡

=

1 ğ‘›

âˆ‘ï¸€

ğ‘”ğ‘–ğ‘¡ ,

ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡,

ğ‘”ğ‘¡+1 = ğ‘”ğ‘¡ + ğ‘ğ‘¡+1,

ğ‘ğ‘¡+1 = ğ’ğ‘€ (ğ‘”ğ‘¡+1 âˆ’ ğ‘”ğ‘¡), Ìƒï¸€

ğ‘”ğ‘¡+1 = 1 âˆ‘ï¸€ğ‘› ğ‘”ğ‘¡+1,

Ìƒï¸€

ğ‘› ğ‘–=1 Ìƒï¸€ğ‘–

ğ‘”ğ‘¡+1 Ìƒï¸€ğ‘–

=

ğ‘”Ìƒï¸€ğ‘–ğ‘¡

+

ğ‘ğ‘¡ğ‘–

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘£ğ‘¡,

ğ‘£ğ‘¡+1 = ğœ‚ğ‘£ğ‘¡ + ğ‘”ğ‘¡+1,

ğ‘”ğ‘¡+1 = ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğ‘”ğ‘–ğ‘¡+1, ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

ğ‘¥ğ‘¡+1 = proxğ›¾ğ‘Ÿ (ï¸€ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡)ï¸€, ğ‘”ğ‘¡+1 = ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğ‘”ğ‘–ğ‘¡+1, ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

Method
EF21 EF21-SGD
EF21-PAGE
EF21-PP EF21-BC
EF21-HB EF21-Prox

Alg. # Alg. 1 Alg. 2 Alg. 3
Alg. 4 Alg. 5
Alg. 6 Alg. 7

ğ‘ğ‘¡ğ‘– ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡) ğ’(ğ‘”^ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡)
ğ’(ğ‘£ğ‘–ğ‘¡+1 âˆ’ ğ‘”ğ‘–ğ‘¡)
ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡) 0
ğ’ğ‘¤ (âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡)
ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡)
ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡)

Comment

ğ‘”^ğ‘–(ğ‘¥ğ‘¡+1) satisï¬es As. 2

ğ‘ğ‘¡ğ‘– âˆ¼ Be(ğ‘),

ğ‘£ğ‘–ğ‘¡+1 = âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1), if ğ‘ğ‘¡ğ‘– = 1,

ğ‘£ğ‘–ğ‘¡+1

=

ğ‘£ğ‘–ğ‘¡

+

1 ğœ

âˆ‘ï¸€ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡+1)

ğ‘– ğ‘—âˆˆğ¼ğ‘¡

ğ‘–

âˆ’ ğœ1 âˆ‘ï¸€ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡), if ğ‘ğ‘¡ğ‘– = 0,
ğ‘– ğ‘—âˆˆğ¼ğ‘¡

ğ‘–

ğ¼ğ‘–ğ‘¡ is a minibatch, |ğ¼ğ‘–ğ‘¡| = ğœğ‘–

if ğ‘– âˆˆ ğ‘†ğ‘¡

if ğ‘– Ì¸âˆˆ ğ‘†ğ‘¡

Master broadcasts ğ‘ğ‘¡+1; ğ’ğ‘¤ is used on the workersâ€™ side, ğ’ğ‘€ is used on the masterâ€™s side

ğœ‚ âˆˆ [0,1) â€“ momentum parameter
For problem (6); proxğ›¾ğ‘Ÿ(ğ‘¥) is deï¬ned in (91)

Table 2: Description of the methods developed and analyzed in the paper. For the ease of comparison, we also provide a description of EF21. In all methods only compressed vectors ğ‘ğ‘¡ğ‘– are transmitted from workers to the master and the master broadcasts non-compressed iterates ğ‘¥ğ‘¡+1 (except EF21-BC, where the master broadcasts compressed vector ğ‘ğ‘¡+1). Initialization of ğ‘”ğ‘–0, ğ‘– = 1, . . . , ğ‘› can be arbitrary (possibly randomized). One possible choice is ğ‘”ğ‘–0 = ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥0)). The pseudocodes for each method are given in the appendix.

â—‡ EF21-PAGE: Error feedback and variance reduction. In the ï¬nite-sum regime (3), variance
reduced methods usually perform better than vanilla SGD in many situations (Gower et al., 2020).
Therefore, for this setup we modify EF21 and combine it with variance reduction. In particular, this time we replace âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) in the formula for ğ‘ğ‘¡ğ‘– with the PAGE estimator (Li et al., 2021) ğ‘£ğ‘–ğ‘¡+1. With (typically small) probability ğ‘ this estimator equals the full gradient ğ‘£ğ‘–ğ‘¡+1 = âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1), and with probability 1 âˆ’ ğ‘ it is set to

ğ‘£ğ‘¡+1 = ğ‘£ğ‘¡ + 1 âˆ‘ï¸ (ï¸€âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡))ï¸€ ,

ğ‘–

ğ‘– ğœğ‘–

ğ‘—âˆˆğ¼ğ‘–ğ‘¡

where ğ¼ğ‘–ğ‘¡ is a minibatch of size ğœğ‘–. Typically, the number of data points ğ‘š owned by each client is large, and ğ‘ â‰¤ 1/ğ‘š when ğœğ‘– â‰¡ 1. As a result, computation of full gradients rarely happens during the optimization procedure: on average, once in every ğ‘š iterations only. Although it is possible to
use other variance-reduced estimators like in SVRG or SAGA, we use the PAGE-estimator: unlike
SVRG or SAGA, PAGE is optimal for smooth nonconvex optimization, and therefore gives the best
theoretical guarantees (we have obtained results for both SVRG and SAGA and indeed, they are worse,
and hence we do not include them).

Notice that unlike VR-MARINA (Gorbunov et al., 2021), which is a state-of-the-art distributed optimization method designed speciï¬cally for unbiased compressors and which also uses the PAGEestimator, EF21-PAGE does not require the communication of full (non-compressed) vectors at all. This is an important property of the algorithm since, in some distributed networks, and especially when ğ‘‘ is very large, as is the case in modern over-parameterized deep learning, full vector communication is prohibitive. However, unlike the rate of VR-MARINA, the rate of EF21-PAGE does not improve with increasing ğ‘›. This is not a ï¬‚aw of our method, but rather an inevitable drawback of distributed methods that rely on biased compressors such as Top-ğ‘˜.

â—‡ EF21-PP: Error feedback and partial participation. The extension of EF21 to the case of
partial participation of the clients is mathematically identical to EF21 up to the following change: ğ‘ğ‘¡ğ‘– = 0 for all clients ğ‘– Ì¸âˆˆ ğ‘†ğ‘¡ âŠ† {1, . . . ,ğ‘›} that are not selected for communication at iteration ğ‘¡. In practice, ğ‘ğ‘¡ğ‘– = 0 means that client ğ‘– does not take part in the ğ‘¡-th communication round. Here the set ğ‘†ğ‘¡ âŠ† {1, . . . ,ğ‘›} is formed randomly such that Prob(ğ‘– âˆˆ ğ‘†ğ‘¡) = ğ‘ğ‘– > 0 for all ğ‘– = 1, . . . , ğ‘›.

5

EF21 with Bells & Whistles

Oct 6, 2021

â—‡ EF21-BC: Error feedback and bidirectional compression. The simplicity of the EF21 mechanism allows us to naturally extend it to the case when it is desirable to have efï¬cient/compressed communication between the clients and the server in both directions. At each iteration of EF21-BC, clients compute and send to the master node ğ‘ğ‘¡ğ‘– = ğ’ğ‘¤(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡) and update ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1 = ğ‘”Ìƒï¸€ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘– in the usual way, i.e., workers apply the EF21 mechanism. The key difference between EF21 and EF21-BC is that the master node in EF21-BC also uses this mechanism: it computes and broadcasts to the workers the compressed vector
ğ‘ğ‘¡+1 = ğ’ğ‘€ (ğ‘”ğ‘¡+1 âˆ’ ğ‘”ğ‘¡) Ìƒï¸€

and updates

ğ‘”ğ‘¡+1 = ğ‘”ğ‘¡ + ğ‘ğ‘¡+1,

where ğ‘”ğ‘¡+1 Ìƒï¸€

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1.

Vector ğ‘”ğ‘¡

is maintained by the master and workers.

Therefore,

the

clients are able to update it via using ğ‘”ğ‘¡+1 = ğ‘”ğ‘¡ + ğ‘ğ‘¡+1 and compute ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡ once they

receive ğ‘ğ‘¡+1.

â—‡ EF21-HB: Error feedback with momentum. We consider classical Heavy-ball method (Polyak, 1964) with EF21 estimator ğ‘”ğ‘¡:

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘£ğ‘¡,

ğ‘£ğ‘¡+1 = ğœ‚ğ‘£ğ‘¡ + ğ‘”ğ‘¡+1,

ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–,

ğ‘¡+1

1

ğ‘›
âˆ‘ï¸

ğ‘¡+1

ğ‘¡ 1 âˆ‘ğ‘›ï¸ ğ‘¡

ğ‘”= ğ‘›

ğ‘”ğ‘–

=ğ‘” + ğ‘›

ğ‘ğ‘– .

ğ‘–=1

ğ‘–=1

The resulting method is not better than EF21 in terms of the complexity of ï¬nding ğœ€-stationary point, i.e., momentum does not improve the theoretical convergence rate. Unfortunately, this is common issue for a wide range of results for momentum methods Loizou & RichtÃ¡rik (2020). However, it is important to theoretically analyze momentum-extensions such as EF21-HB due to their importance in practice and generalization behaviour.

â—‡ EF21-Prox: Error feedback for composite problems. Finally, we make EF21 applicable to the
composite optimization problems (6) by simply taking the prox-operator from the right-hand side of the ğ‘¥ğ‘¡+1 update rule (7):

ğ‘¥ğ‘¡+1 = prox

(ï¸€ğ‘¥ğ‘¡

âˆ’

ğ›¾ğ‘”ğ‘¡)ï¸€

=

arg

min

{ï¸‚ ğ›¾ğ‘Ÿ(ğ‘¥)

+

1 â€–ğ‘¥

âˆ’

(ğ‘¥ğ‘¡

âˆ’

}ï¸‚ ğ›¾ğ‘”ğ‘¡)â€–2

.

ğ›¾ğ‘Ÿ

ğ‘¥âˆˆRğ‘‘

2

This trick is simple, but, surprisingly, EF21-Prox is the ï¬rst distributed method with error-feedback that provably converges for composite problems (6).

4 THEORETICAL CONVERGENCE RESULTS
In this section, we formulate a single corollary derived from the main convergence theorems for our six enhancements of EF21, and formulate the assumptions that we use in the analysis. The complete statements of the theorems and their proofs are provided in the appendices. In Table 1 we compare our new results with existing results.

4.1 ASSUMPTIONS In this subsection, we list and discuss the assumptions that we use in the analysis.

4.1.1 GENERAL ASSUMPTIONS

To derive our convergence results, we invoke the following standard smoothness assumption. Assumption 1 (Smoothness and lower boundedness). Every ğ‘“ğ‘– has ğ¿ğ‘–-Lipschitz gradient, i.e.,
â€–âˆ‡ğ‘“ğ‘–(ğ‘¥) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¦)â€– â‰¤ ğ¿ğ‘– â€–ğ‘¥ âˆ’ ğ‘¦â€–

for

all

ğ‘–

âˆˆ

[ğ‘›], ğ‘¥, ğ‘¦

âˆˆ

Rğ‘‘,

and

ğ‘“ inf

def
=

inf ğ‘¥âˆˆRğ‘‘

ğ‘“ (ğ‘¥)

>

âˆ’âˆ.

We also assume that the compression operators used by all algorithms satisfy the following property.

6

EF21 with Bells & Whistles

Oct 6, 2021

Deï¬nition 1 (Contractive compressors). We say that a (possibly randomized) map ğ’ : Rğ‘‘ â†’ Rğ‘‘ is a contractive compression operator, or simply contractive compressor, if there exists a constant 0 < ğ›¼ â‰¤ 1 such that

E [ï¸€â€–ğ’(ğ‘¥) âˆ’ ğ‘¥â€–2]ï¸€ â‰¤ (1 âˆ’ ğ›¼) â€–ğ‘¥â€–2, âˆ€ğ‘¥ âˆˆ Rğ‘‘.

(8)

We emphasize that we do not assume ğ’ to be unbiased. Hence, our theory works with the Top-ğ‘˜ (Alistarh et al., 2018) and the Rank-ğ‘Ÿ (Safaryan et al., 2021) compressors, for example.

4.1.2 ADDTIONAL ASSUMPTIONS FOR EF21-SGD

We analyze EF21-SGD under the assumption that local stochastic gradients âˆ‡ğ‘“ğœ‰ğ‘¡ (ğ‘¥ğ‘¡) satisfy the ğ‘–ğ‘—
following inequality (see Assumption 2 of Khaled & RichtÃ¡rik (2020)).

Assumption 2 (General assumption for stochastic gradients). We assume that for all ğ‘– = 1, . . . ,ğ‘› there exist parameters ğ´ğ‘–, ğ¶ğ‘– â‰¥ 0, ğµğ‘– â‰¥ 1 such that

[ï¸

]ï¸

E â€–âˆ‡ğ‘“ğœ‰ğ‘¡ (ğ‘¥ğ‘¡)â€–2 | ğ‘¥ğ‘¡ â‰¤ 2ğ´ğ‘– (ï¸€ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ ğ‘“ğ‘–inf )ï¸€ + ğµğ‘–â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2 + ğ¶ğ‘–,

(9)

ğ‘–ğ‘—

where1 ğ‘“ğ‘–inf = infğ‘¥âˆˆRğ‘‘ ğ‘“ğ‘–(ğ‘¥) > âˆ’âˆ.

Below we provide two examples of stochastic gradients ï¬tting this assumption (for more detail, see (Khaled & RichtÃ¡rik, 2020)).
Example 1. Consider âˆ‡ğ‘“ğœ‰ğ‘¡ (ğ‘¥ğ‘¡) such that ğ‘–ğ‘—

[ï¸

]ï¸

[ï¸‚ âƒ¦

âƒ¦2 ]ï¸‚

E âˆ‡ğ‘“ğœ‰ğ‘–ğ‘¡ğ‘— (ğ‘¥ğ‘¡) | ğ‘¥ğ‘¡ = âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) and E âƒ¦âƒ¦âˆ‡ğ‘“ğœ‰ğ‘–ğ‘¡ğ‘— (ğ‘¥ğ‘¡) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦ | ğ‘¥ğ‘¡ â‰¤ ğœğ‘–2

for some ğœğ‘– â‰¥ 0. Then, due to variance decomposition,(9) holds with ğ´ğ‘– = 0, ğµğ‘– = 0, ğ¶ğ‘– = ğœğ‘–2.
Example 2. Let ğ‘“ğ‘–(ğ‘¥) = ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1 ğ‘“ğ‘–ğ‘— (ğ‘¥), ğ‘“ğ‘–ğ‘— be ğ¿ğ‘–ğ‘— -smooth and ğ‘“ğ‘–iğ‘—nf = infğ‘¥âˆˆRğ‘‘ ğ‘“ğ‘–ğ‘— (ğ‘¥) > âˆ’âˆ. Following Gower et al. (2019), we consider a stochastic reformulation

â¡

â¤

1

ğ‘šğ‘–
âˆ‘ï¸

ğ‘“ğ‘–(ğ‘¥) = Eğ‘£ğ‘–âˆ¼ğ’Ÿğ‘– [ğ‘“ğ‘£ğ‘– (ğ‘¥)] = Eğ‘£ğ‘–âˆ¼ğ’Ÿğ‘– â£ ğ‘šğ‘– ğ‘“ğ‘£ğ‘–ğ‘— (ğ‘¥)â¦ ,

(10)

ğ‘—=1

where Eğ‘£ğ‘–âˆ¼ğ’Ÿğ‘– [ğ‘£ğ‘–ğ‘—] = 1. One can show (see Proposition 2 of Khaled & RichtÃ¡rik (2020)) that under

the assumption that Eğ‘£ğ‘–âˆ¼ğ’Ÿğ‘– [ï¸€ğ‘£ğ‘–2ğ‘—]ï¸€ is ï¬nite for all ğ‘— stochastic gradient âˆ‡ğ‘“ğœ‰ğ‘¡ (ğ‘¥ğ‘¡) = âˆ‡ğ‘“ğ‘£ğ‘¡ (ğ‘¥ğ‘¡) with

ğ‘–ğ‘—

ğ‘–

ğ‘£ğ‘–ğ‘¡ sampled from ğ’Ÿğ‘– satisï¬es (9) with ğ´ğ‘– = maxğ‘— ğ¿ğ‘–ğ‘—Eğ‘£ğ‘–âˆ¼ğ’Ÿğ‘– [ï¸€ğ‘£ğ‘–2ğ‘—]ï¸€, ğµğ‘– = 1, ğ¶ğ‘– = 2ğ´ğ‘–âˆ†iğ‘–nf , where

âˆ†iğ‘–nf = ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1(ğ‘“ğ‘–inf âˆ’ ğ‘“ğ‘–iğ‘—nf ). In particular, if Prob(âˆ‡ğ‘“ğœ‰ğ‘–ğ‘¡ğ‘— (ğ‘¥ğ‘¡) = âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡)) = âˆ‘ï¸€ğ‘š ğ‘™=ğ¿ğ‘–1ğ‘— ğ¿ğ‘–ğ‘™ , then ğ´ğ‘– = ğ¿ğ‘– = ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1 ğ¿ğ‘–ğ‘— , ğµğ‘– = 1, and ğ¶ğ‘– = 2ğ´ğ‘–âˆ†iğ‘–nf .

Stochastic gradient ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡) is computed using a mini-batch of ğœğ‘– independent samples satisfying (9):

ğ‘¡

def

1

ğœğ‘–
âˆ‘ï¸

ğ‘¡

ğ‘”Ë†ğ‘–(ğ‘¥ ) =

âˆ‡ğ‘“ğœ‰ğ‘¡ (ğ‘¥ ).

ğœğ‘– ğ‘—=1 ğ‘–ğ‘—

4.1.3 ADDITIONAL ASSUMPTIONS FOR EF21-PAGE

In the analysis of EF21-PAGE, we rely on the following assumption.

Assumption 3 (Average â„’-smoothness). Let every ğ‘“ğ‘– have the form (3). Assume that for all ğ‘¡ â‰¥ 0, ğ‘– = 1, . . . , ğ‘›, and batch ğ¼ğ‘–ğ‘¡ (of size ğœğ‘–), the minibatch stochastic gradients difference

âˆ†Ìƒï¸€ ğ‘¡

def
=

1

âˆ‘ï¸ (âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡))

ğ‘– ğœğ‘–

ğ‘—âˆˆğ¼ğ‘–ğ‘¡

1When ğ´ğ‘– = 0 one can ignore the ï¬rst term in the right-hand side of (9), i.e., assumption infğ‘¥âˆˆRğ‘‘ ğ‘“ğ‘–(ğ‘¥) > âˆ’âˆ is not required in this case.

7

EF21 with Bells & Whistles

Oct 6, 2021

[ï¸

]ï¸

computed on the node ğ‘–, satisï¬es E âˆ†Ìƒï¸€ ğ‘¡ğ‘– | ğ‘¥ğ‘¡,ğ‘¥ğ‘¡+1 = âˆ†ğ‘¡ğ‘– and

[ï¸‚ âƒ¦
ğ‘¡

âƒ¦2
ğ‘¡

]ï¸‚
ğ‘¡ ğ‘¡+1

â„’2ğ‘– ğ‘¡+1

ğ‘¡2

E âƒ¦âƒ¦âˆ†Ìƒï¸€ ğ‘– âˆ’ âˆ†ğ‘–âƒ¦âƒ¦ | ğ‘¥ , ğ‘¥

â‰¤ â€–ğ‘¥ âˆ’ ğ‘¥ â€– ğœğ‘–

(11)

with

some

â„’ğ‘–

â‰¥

0,

where

âˆ†ğ‘¡ğ‘–

def
=

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡).

We

also

deï¬ne

def
â„’Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

(1âˆ’ğ‘ğ‘–)â„’2ğ‘– .
ğœ

ğ‘–

This assumption is satisï¬ed for many standard/popular sampling strategies. For example, if ğ¼ğ‘–ğ‘¡ is a full batch, then â„’ğ‘– = 0. Another example is uniform sampling on {1, . . . , ğ‘š}, and each ğ‘“ğ‘–ğ‘— is
ğ¿ğ‘–ğ‘—-smooth. In this regime, one may verify that â„’ğ‘– â‰¤ max1â‰¤ğ‘—â‰¤ğ‘š ğ¿ğ‘–ğ‘—.

4.2 MAIN RESULTS

Below we formulate the corollary establishing the complexities for each method. The complete version of this result is formulated and rigorously derived for each method in the appendix.

Corollary 1. (a) Suppose that Assumption 1 holds. Then, there exist appropriate choices of
parameters for EF21-PP, EF21-BC, EF21-HB, EF21-Prox such that the number of communi-
cation rounds ğ‘‡ and the (expected) number of gradient computations at each node #grad for these methods to ï¬nd an ğœ€-stationary point, i.e., a point ğ‘¥Ë†ğ‘‡ such that

E [ï¸€â€–âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )â€–2]ï¸€ â‰¤ ğœ€2

for EF21-PP, EF21-BC, EF21-HB and

E [ï¸€â€–ğ’¢ğ›¾(ğ‘¥Ë†ğ‘‡ )â€–2]ï¸€ â‰¤ ğœ€2

for EF21-Prox, where ğ’¢ğ›¾(ğ‘¥) = 1/ğ›¾ (ï¸€ğ‘¥ âˆ’ proxğ›¾ğ‘Ÿ(ğ‘¥ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥)))ï¸€, are

EF21-PP:

(ï¸ƒ )ï¸ƒ

(ï¸ƒ )ï¸ƒ

ğ¿Ìƒï¸€ğ›¿0

ğ¿Ìƒï¸€ğ›¿0

ğ‘‡ = ğ’ª ğ‘ğ›¼ğœ€2 , #grad = ğ’ª ğ›¼ğœ€2

EF21-BC: EF21-HB:

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ğ›¿0

ğ‘‡ = #grad = ğ’ª ğ›¼ğ‘¤ğ›¼ğ‘€ ğœ€2

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ğ›¿0 (ï¸‚ 1 1 )ï¸‚

ğ‘‡ = #grad = ğ’ª ğœ€2 ğ›¼ + 1 âˆ’ ğœ‚

EF21-Prox:

(ï¸ƒ )ï¸ƒ ğ¿Ìƒï¸€ğ›¿0
ğ‘‡ = #grad = ğ’ª ğ›¼ğœ€2 ,

where ğ¿Ìƒï¸€ d=ef âˆšï¸ ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğ¿2ğ‘– , ğ›¿0 d=ef ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf (for EF21-Prox, ğ›¿0 = Î¦(ğ‘¥0) âˆ’ Î¦ğ‘–ğ‘›ğ‘“ ), ğ‘ is the probability of sampling the client in EF21-PP, ğ›¼ğ‘¤ and ğ›¼ğ‘€ are contraction factors for compressors applied on the workersâ€™ and the masterâ€™s sides respectively in EF21-BC, and ğœ‚ âˆˆ [0,1) is the momentum parameter in EF21-HB.

(b) If Assumptions 1 and 2 in the setup from Example 1 hold, then there exist appropriate choices of parameters for EF21-SGD such that the corresponding ğ‘‡ and the averaged number of gradient computations at each node #grad are

EF21-SGD:

(ï¸ƒ )ï¸ƒ

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ğ›¿0

ğ¿Ìƒï¸€ğ›¿0 ğ¿Ìƒï¸€ğ›¿0ğœ2

ğ‘‡ = ğ’ª ğ›¼ğœ€2 , #grad = ğ’ª ğ›¼ğœ€2 + ğ›¼3ğœ€4 ,

where

ğœ

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğœğ‘–2.

(c) If Assumptions 1 and 3 hold, then there exist appropriate choices of parameters for EF21-

PAGE such that the corresponding ğ‘‡ and #grad are

EF21-PAGE:

(ï¸ƒ (ğ¿ + â„’)ğ›¿0

âˆšğ‘šâ„’ğ›¿0 )ï¸ƒ

Ìƒï¸€ Ìƒï¸€

Ìƒï¸€

ğ‘‡ = ğ’ª ğ›¼ğœ€2 + ğœ€2 ,

(ï¸ƒ (ğ¿ + â„’)ğ›¿0 âˆšğ‘šâ„’ğ›¿0 )ï¸ƒ

Ìƒï¸€ Ìƒï¸€

Ìƒï¸€

#grad = ğ’ª ğ‘š + ğ›¼ğœ€2 + ğœ€2 ,

8

EF21 with Bells & Whistles

Oct 6, 2021

âˆšï¸

where â„’Ìƒï¸€ =

1âˆ’ğ‘ ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

â„’2ğ‘–

and

ğœğ‘–

â‰¡

ğœ

=

1.

Remark: We highlight some points for our results in Corollary 1 as follows:
â€¢ For EF21-PP and EF21-Prox, none of previous error feedback methods work on these two settings (partial participation and proximal/composite case). Thus, we provide the ï¬rst convergence results for them. Moreover, we show that the gradient (computation) complexity for both EF21-PP and EF21-Prox is ğ’ª(1/ğ›¼ğœ€), matching the original vanilla EF21. It means that we extend EF21 to both settings for free.
â€¢ For EF21-BC, we show ğ’ª(1/ğ›¼ğ‘¤ğ›¼ğ‘€ ğœ€2) complexity result. In particular, if one uses constant ratio of compression (e.g., 10%), then ğ›¼ â‰ˆ 0.1. Then the result will be ğ’ª(1/ğœ€2). However, previous result of DoubleSqueeze is ğ’ª(Î”/ğœ€3) and it also uses more strict assumption for the compressors (E [â€–ğ’(ğ‘¥) âˆ’ ğ‘¥â€–] â‰¤ âˆ†). Even if we ignore this, our results for EF21-BC is better than the one for DoubleSqueeze by a large factor 1/ğœ€.
â€¢ Similarly, our result for EF21-HB is roughly ğ’ª(1/ğœ€2) (note that the momentum parameter ğœ‚ is usually constant such as 0.2, 0.4, 0.9 used in our experiments). However, previous result of M-CSER is roughly ğ’ª(ğº/ğœ€3) and it is proven under an additional bounded gradient assumption. Similarly, our EF21-HB is better by a large factor 1/ğœ€.
â€¢ For EF21-SGD and EF21-PAGE, we want to reduce the gradient complexity by using (variance-reduced) stochastic gradients instead of full gradient in the vanilla EF21. Note that ğœ2 and âˆ†inf in EF21-SGD could be much smaller than ğº in Choco-SGD since ğº always depends on the dimension (and can be even inï¬nite), while ğœ2 and âˆ†inf are mostly dimension-free parameters (particularly, they are very small if the functions/data samples are similar/close). Thus, for high dimensional problems (e.g., deep neural networks), EF21-SGD can be better than Choco-SGD. Besides, in the ï¬nite-sum case (3), especially if the number of data samples ğ‘š on eachâˆšclient is not very large, then EF21-PAGE is much better since its complexity is roughly ğ’ª( ğ‘š/ğœ€2) while EF21-SGD ones is roughly ğ’ª(ğœ2/ğœ€4).

5 EXPERIMENTS

In this section, we consider a logistic regression problem with a non-convex regularizer

â§

â«

â¨

1 âˆ‘ğ‘ï¸ (ï¸€

(ï¸€

âŠ¤ )ï¸€)ï¸€

ğ‘‘
âˆ‘ï¸

ğ‘¥2ğ‘—

â¬

min ğ‘“ (ğ‘¥) =

ğ‘‘

ğ‘

log 1 + exp âˆ’ğ‘ğ‘–ğ‘ğ‘– ğ‘¥ + ğœ†

1 + ğ‘¥2 ,

(12)

ğ‘¥âˆˆR â© ğ‘–=1

ğ‘—=1

ğ‘—â­

where ğ‘ğ‘– âˆˆ Rğ‘‘, ğ‘ğ‘– âˆˆ {âˆ’1,1} are the training data, and ğœ† > 0 is the regularization parameter, which is set to ğœ† = 0.1 in all experiments. For all methods the stepsizes are initially chosen as the largest stepsize predicted by theory for EF21 (see Theorem 1), then they are tuned individually for each parameter setting. We provide more details on the datasets, hardware, experimental setups, and additional experiments, including deep learning experiments in Appendix A.

5.1 EXPERIMENT 1: FAST CONVERGENCE WITH VARIANCE REDUCTION
In our ï¬rst experiment, we showcase the computation and communication superiority of EF21-PAGE (Alg. 3) over EF21-SGD.
Figure 1 illustrates that, in all cases, EF21-PAGE perfectly reduces the accumulated variance and converges to the desired tolerance, whereas EF21-SGD is stuck at some accuracy level. Moreover, EF21-PAGE turns out to be surprisingly efï¬cient with small bathsizes (eg, 1.5% of the local data ) both in terms of the number of epochs and the # bits sent to the server per client. Interestingly, for most datasets, a further increase of bathsize does not considerably improve the convergence.

9

EF21 with Bells & Whistles

Oct 6, 2021

|| f(xt)||2

|| f(xt)||2

100 10 2 10 4 10 6
0
100 10 2 10 4 10 6
0

mushrooms, Top-2 100

w8a, Top-2

a9a, Top-2 100

phishing, Top-1 100

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 128x; 1.5% EF21-PAGE; 512x; 25% EF21-PAGE; 1024x; 12.5% EF21-PAGE; 1024x; 1.5%
20e0pochs 400

|| f(xt)||2

10 2 10 4 10 6
0

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 512x; 1.5% EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5% EF21-PAGE; 64x; 1.5%
10e0pochs 200

|| f(xt)||2

10 2 10 4 10 6
0

EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5% EF21-SGD; 512x; 1.5% EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5% EF21-PAGE; 128x; 1.5%
20 epoc4h0s 60

|| f(xt)||2

10 2 10 4 10 6
0

EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5% EF21-SGD; 16x; 1.5% EF21-PAGE; 16x; 25% EF21-PAGE; 16x; 12.5% EF21-PAGE; 16x; 1.5%
20 epoc4h0s 60

mushrooms, Top-2
EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 128x; 1.5% EF21-PAGE; 512x; 25% EF21-PAGE; 1024x; 12.5% EF21-PAGE; 1024x; 1.5%
100#0b0i0ts/n 2(C000S0)0 300000

|| f(xt)||2

100 10 2 10 4 10 6
0

(a) Convergence in epochs.

w8a, Top-2

a9a, Top-2

100

100

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 512x; 1.5% EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5% EF21-PAGE; 64x; 1.5%
100#0b0i0ts/n 2(C000S0)0 300000

|| f(xt)||2

10 2 10 4 10 6
0

10 2

|| f(xt)||2

EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5%

10 4

EF21-SGD; 512x; 1.5%

EF21-PAGE; 64x; 25% EF21-PAGE; 64x; 12.5%

10 6

EF21-PAGE; 128x; 1.5%

#bit5s0/n00(C0 S) 100000 0

phishing, Top-1
EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5% EF21-SGD; 16x; 1.5% EF21-PAGE; 16x; 25% EF21-PAGE; 16x; 12.5% EF21-PAGE; 16x; 1.5%
#b2it0s0/n00(C S) 40000

(b) Convergence in terms of total number of bits sent from Clients to the Server divided by ğ‘›.

Figure 1: Comparison of EF21-PAGE and EF21-SGD with tuned parameters. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21. By 25%, 12.5% and 1.5% we refer to batchsizes equal âŒŠ0.25ğ‘ğ‘–âŒ‹, âŒŠ0.125ğ‘ğ‘–âŒ‹ and âŒŠ0.015ğ‘ğ‘–âŒ‹ for all clients ğ‘– = 1, . . . ,ğ‘›, where ğ‘ğ‘– denotes the size of local dataset.

5.2 EXPERIMENT 2: ON THE EFFECT OF PARTIAL PARTICIPATION OF CLIENTS
This experiment shows that EF21-PP (Alg. 4) can reduce communication costs and can be more practical than EF21. For this comparison, we consider ğ‘› = 100 and, therefore, apply a different data partitioning, see Table 5 from Appendix A for more details.
It is predicted by our theory (Corollary 1) that, in terms of the number of iterations/communication rounds, partial participation slows down the convergence of EF21 by a fraction of participating clients . We observe this behavior in practice as well (see Figure 2a). However, since for EF21-PP the communications are considerably cheaper it outperforms EF21 in terms of # number of bits sent to the server per client on average (see Figure 2).

100 10 2 10 4 10 6
0
100 10 2 10 4 10 6
0

mushrooms, Top-2

EF21; 1024x EF21-PP; 512x; 50%

100

EF21-PP; 256x; 25%

EF21-PP; 128x; 12.5% EF21-PP; 64x; 6.5%

10 2

w8a, Top-2

a9a, Top-2 100
10 2

phishing, Top-1 100
10 2

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 4 EF21; 64x

10 4 EF21; 128x

10 4 EF21; 16x

EF21-PP; 64x; 50%

EF21-PP; 64x; 50%

EF21-PP; 8x; 50%

10 6

EF21-PP; 32x; 25% EF21-PP; 16x; 12.5%

10 6

EF21-PP; 32x; 25% EF21-PP; 16x; 12.5%

10 6

EF21-PP; 4x; 25% EF21-PP; 2x; 12.5%

EF21-PP; 8x; 6.5%

EF21-PP; 8x; 6.5%

EF21-PP; 1x; 6.5%

com5m00u0nicatio1n0r0o0u0nds 15000 0 com1m00u0nication20ro0u0nds 3000 0 com25m0unicatio5n0r0ounds 750

0 com2m5u0nication5r0o0unds 750

mushrooms, Top-2
EF21; 1024x EF21-PP; 512x; 50% EF21-PP; 256x; 25% EF21-PP; 128x; 12.5% EF21-PP; 64x; 6.5%

(a) Convergence in communication rounds.

w8a, Top-2

a9a, Top-2

100

100

10 2

10 2

|| f(xt)||2

|| f(xt)||2

#bit1s/0n0(0C00 S)

10 4 10 6 200000 0

EF21; 64x EF21-PP; 64x; 50% EF21-PP; 32x; 25% EF21-PP; 16x; 12.5% EF21-PP; 8x; 6.5%
#bits5/0n0(0C0 S)

10 4 10 6 100000 0

EF21; 128x EF21-PP; 64x; 50% EF21-PP; 32x; 25% EF21-PP; 16x; 12.5% EF21-PP; 8x; 6.5%
10#0b0i0ts/n (C20S0)00

|| f(xt)||2

100 10 2 10 4 10 6 30000 0

phishing, Top-1
EF21; 16x EF21-PP; 8x; 50% EF21-PP; 4x; 25% EF21-PP; 2x; 12.5% EF21-PP; 1x; 6.5%
#b5i0ts0/0n (C S) 10000

(b) Convergence in terms of total number of bits sent from Clients to the Server divided by ğ‘›.

Figure 2: Comparison of EF21-PP and EF21 with tuned parameters. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21. By 50%, 25% , 12.5% and 6.5% we refer to a number of participating clients equal to âŒŠ0.5ğ‘›âŒ‹, âŒŠ0.25ğ‘›âŒ‹, âŒŠ0.125ğ‘›âŒ‹ and âŒŠ0.065ğ‘›âŒ‹.

10

|| f(xt)||2

|| f(xt)||2

EF21 with Bells & Whistles

Oct 6, 2021

REFERENCES
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communicationefï¬cient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems (NIPS), pp. 1709â€“1720, 2017.
Dan Alistarh, Torsten Hoeï¬‚er, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and CÃ©dric Renggli. The convergence of sparsiï¬ed gradient methods. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
Zeyuan Allen-Zhu. Katyusha: The ï¬rst direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1200â€“ 1205. ACM, 2017.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018.
Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, 2017.
Aleksandr Beznosikov, Samuel HorvÃ¡th, Peter RichtÃ¡rik, and Mher Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
LÃ©on Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proceedings of the symposium on learning and data science, Paris, volume 8, pp. 2624â€“2633, 2009.
LÃ©on Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pp. 421â€“436. Springer, 2012.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1â€“27, 2011.
Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence analysis and power-of-choice selection strategies. arXiv preprint arXiv:2010.01243v1, 2020.
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter RichtÃ¡rik. Linearly converging error compensated SGD. In 34th Conference on Neural Information Processing Systems (NeurIPS), 2020.
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter RichtÃ¡rik. MARINA: Faster nonconvex distributed learning with compression. In International Conference on Machine Learning, pp. 3788â€“3798. PMLR, 2021. arXiv:2102.07845.
Robert M Gower, Mark Schmidt, Francis Bach, and Peter RichtÃ¡rik. Variance-reduced methods for machine learning. Proceedings of the IEEE, 108(11):1968â€“1983, 2020.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter RichtÃ¡rik. SGD: General analysis and improved rates. In International Conference on Machine Learning, pp. 5200â€“5209. PMLR, 2019.
Priya Goyal, Piotr DollÃ¡r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770â€“778, 2016.
11

EF21 with Bells & Whistles

Oct 6, 2021

Samuel HorvÃ¡th and Peter RichtÃ¡rik. A better alternative to error feedback for communicationefï¬cient distributed learning. In 9th International Conference on Learning Representations (ICLR), 2021.
Samuel HorvÃ¡th, Chen-Yu Ho, Lâ€™udovÃ­t HorvÃ¡th, Atal Narayan Sahu, Marco Canini, and Peter RichtÃ¡rik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019a.
Samuel HorvÃ¡th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter RichtÃ¡rik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019b.
Rustem Islamov, Xun Qian, and Peter RichtÃ¡rik. Distributed second order methods with fast rates and compressed communication. arXiv preprint arXiv:2102.07158, 2021.
Peter et al Kairouz. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback ï¬xes SignSGD and other gradient compression schemes. In 36th International Conference on Machine Learning (ICML), 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In Proceedings of the 37th International Conference on Machine Learning, 2020.
Ahmed Khaled and Peter RichtÃ¡rik. Gradient descent with compressed iterates. In NeurIPS Workshop on Federated Learning for Data Privacy and Conï¬dentiality, 2019.
Ahmed Khaled and Peter RichtÃ¡rik. Better theory for SGD in the nonconvex world. arXiv preprint arXiv:2002.03329, 2020.
Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Anastasia Koloskova, Tao Lin, S. Stich, and Martin Jaggi. Decentralized deep learning with arbitrary communication compression. In International Conference on Learning Representations (ICLR), 2020.
Jakub KonecË‡nÃ½, H. Brendan McMahan, Felix Yu, Peter RichtÃ¡rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: strategies for improving communication efï¬ciency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Toronto, 2009.
Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint arXiv:1507.02000, 2015.
Guanghui Lan, Zhize Li, and Yi Zhou. A uniï¬ed variance-reduced accelerated gradient method for convex optimization. In Advances in Neural Information Processing Systems, pp. 10462â€“10472, 2019.
Zhize Li. ANITA: An optimal loopless accelerated variance-reduced gradient method. arXiv preprint arXiv:2103.11333, 2021a.
Zhize Li. A short note of page: Optimal convergence rates for nonconvex optimization. arXiv preprint arXiv:2106.09663, 2021b.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp. 5569â€“5579, 2018.
12

EF21 with Bells & Whistles

Oct 6, 2021

Zhize Li and Peter RichtÃ¡rik. A uniï¬ed analysis of stochastic gradient methods for nonconvex federated optimization. arXiv preprint arXiv:2006.07013, 2020.
Zhize Li and Peter RichtÃ¡rik. CANITA: Faster rates for distributed convex optimization with communication compression. arXiv preprint arXiv:2107.09461, 2021a.
Zhize Li and Peter RichtÃ¡rik. ZeroSARAH: Efï¬cient nonconvex ï¬nite-sum optimization with zero full gradient computation. arXiv preprint arXiv:2103.01447, 2021b.
Zhize Li, Dmitry Kovalev, Xun Qian, and Peter RichtÃ¡rik. Acceleration for compressed gradient descent in distributed and federated optimization. In International Conference on Machine Learning (ICML), pp. 5895â€“5904. PMLR, 2020.
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter RichtÃ¡rik. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International Conference on Machine Learning (ICML), pp. 6286â€“6295. PMLR, 2021. arXiv:2008.10898.
Nicolas Loizou and Peter RichtÃ¡rik. Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods. Computational Optimization and Applications, 77:653â€“710, 2020.
Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les Ã©quations aux dÃ©rivÃ©es partielles, 117(87-89):2, 1963.
Konstantin Mishchenko, Eduard Gorbunov, Martin TakÃ¡cË‡, and Peter RichtÃ¡rik. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshufï¬‚ing: Simple analysis with vast improvements. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17309â€“17320. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ c8cc6e90ccbff44c9cee23611711cdc4-[]Paper.pdf.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o (1/kË† 2). In Doklady AN USSR, volume 269, pp. 543â€“547, 1983.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Constantin Philippenko and Aymeric Dieuleveut. Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees. arXiv preprint arXiv:2006.14591, 2020.
Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864â€“878, 1963.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics, 4(5):1â€“17, 1964.
Xun Qian, Peter RichtÃ¡rik, and Tong Zhang. Error compensated distributed SGD can be accelerated. arXiv preprint arXiv:2010.00091, 2020.
Zheng Qu and Peter RichtÃ¡rik. Coordinate descent with arbitrary sampling ii: Expected separable overapproximation. arXiv preprint arXiv:1412.8063, 2014.
Peter RichtÃ¡rik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and practically faster error feedback. arXiv preprint arXiv:2106.05203, 2021.
Mher Safaryan, Rustem Islamov, Xun Qian, and Peter RichtÃ¡rik. FedNL: Making Newton-type methods applicable to federated learning. arXiv preprint arXiv:2106.02969, 2021.
13

EF21 with Bells & Whistles

Oct 6, 2021

Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from theory to algorithms. Cambridge University Press, 2014.
Sebastian U. Stich, J.-B. Cordonnier, and Martin Jaggi. Sparsiï¬ed SGD with memory. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
Hanlin Tang, Xiangru Lian, Chen Yu, Tong Zhang, and Ji Liu. DoubleSqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2020.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient compression for distributed optimization. In Neural Information Processing Systems, 2019.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster stochastic variance reduction algorithms. arXiv preprint arXiv:1810.10690, 2018.
Cong Xie, Shuai Zheng, Oluwasanmi Koyejo, Indranil Gupta, Mu Li, and Haibin Lin. CSER: Communication-efï¬cient SGD with error reset. In Advances in Neural Information Processing Systems (NeurIPS), pp. 12593â€“12603, 2020.
Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-iid federated learning. arXiv preprint arXiv:2101.11203v3, 2021.
Tianbao Yang, Qihang Lin, and Zhe Li. Uniï¬ed convergence analysis of stochastic momentum methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Syx4wnEtvH.
Haoyu Zhao, Zhize Li, and Peter RichtÃ¡rik. FedPAGE: A fast local stochastic gradient method for communication-efï¬cient federated learning. arXiv preprint arXiv:2108.04755, 2021.

14

EF21 with Bells & Whistles

Oct 6, 2021

APPENDIX

TABLE OF CONTENTS

1 Introduction

1

2 Our Contributions

2

3 Methods

4

4 Theoretical Convergence Results

6

4.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

4.1.1 General Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

4.1.2 Addtional Assumptions for EF21-SGD . . . . . . . . . . . . . . . . . . . 7

4.1.3 Additional Assumptions for EF21-PAGE . . . . . . . . . . . . . . . . . . . 7

4.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

5 Experiments

9

5.1 Experiment 1: Fast convergence with variance reduction . . . . . . . . . . . . . . 9

5.2 Experiment 2: On the effect of partial participation of clients . . . . . . . . . . . . 10

A Extra Experiments

16

A.1 Non-Convex Logistic Regression: Additional Experiments and Details . . . . . . . 16

A.2 Experiments with Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

A.3 Deep Learning Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B Notations and Assumptions

22

C EF21

23

C.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 23

C.2 Convergence under Polyak-Åojasiewicz Condition . . . . . . . . . . . . . . . . . . 25

D Stochastic Gradients

27

D.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 28

D.2 Convergence under Polyak-Åojasiewicz Condition . . . . . . . . . . . . . . . . . . 32

E Variance Reduction

35

E.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 37

E.2 Convergence under Polyak-Åojasiewicz Condition . . . . . . . . . . . . . . . . . . 41

F Partial Participation

44

F.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 47

F.2 Convergence under Polyak-Åojasiewicz Condition . . . . . . . . . . . . . . . . . . 48

G Bidirectional Compression

49

G.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 51

G.2 Convergence under Polyak-Åojasiewicz Condition . . . . . . . . . . . . . . . . . . 53

H Heavy Ball Momentum

55

H.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 57

I Composite Case

63

I.1 Convergence for General Non-Convex Functions . . . . . . . . . . . . . . . . . . 65

I.2 Convergence under Polyak-Åojasiewicz Condition . . . . . . . . . . . . . . . . . . 67

15

EF21 with Bells & Whistles

Oct 6, 2021

J Useful Auxiliary Results

70

J.1 Basic Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

J.2 Useful Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

A EXTRA EXPERIMENTS

In this section, we give missing details on the experiments from Section 5, and provide additional experiments.

A.1 NON-CONVEX LOGISTIC REGRESSION: ADDITIONAL EXPERIMENTS AND DETAILS
Datasets, hardware and implementation. We use standard LibSVM datasets (Chang & Lin, 2011), and split each dataset among ğ‘› clients. For experiments 1, 3, 4 and 5, we chose ğ‘› = 20 whereas for the experiment 2 we consider ğ‘› = 100. The ï¬rst ğ‘› âˆ’ 1 clients own equal parts, and the remaining part, of size ğ‘ âˆ’ ğ‘› Â· âŒŠğ‘/ğ‘›âŒ‹, is assigned to the last client. We consider the heterogeneous data distribution regime (i.e. we do not make any additional assumptions on data similarity between workers). A summary of datasets and details of splitting data among workers can be found in Tables 3 and 5. The algorithms are implemented in Python 3.8; we use 3 different CPU cluster node types in all experiments: 1) AMD EPYC 7702 64-Core; 2) Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz; 3) Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. In all algorithms involving compression, we use Top-ğ‘˜ (Alistarh et al., 2017) as a canonical example of contractive compressor ğ’, and ï¬x the compression ratio ğ‘˜/ğ‘‘ â‰ˆ 0.01, where ğ‘‘ is the number of features in the dataset. For all algorithms, at each iteration we compute the squared norm of the exact/full gradient for comparison of the methods performance. We terminate our algorithms either if they reach the certain number of iterations or the following stopping criterion is satisï¬ed: â€–âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2 â‰¤ 10âˆ’7.
In all experiments, the stepsize is set to the largest stepsize predicted by theory for EF21 multiplied by some constant multiplier which was individually tuned in all cases.

Dataset

ğ‘› ğ‘ (total # of datapoints) ğ‘‘ (# of features) k ğ‘ğ‘–

mushrooms 20

w8a

20

a9a

20

phishing 20

8,120 49,749 32,560 11,055

112 2 406 300 2 2,487 123 2 1,628
68 1 552

Table 3: Summary of the datasets and splitting of the data among clients for Experiments 1, 3, 4, and 5. Here ğ‘ğ‘– denotes the number of datapoints per client.

Experiment 1: Fast convergence with variance reductions (extra details). The parameters ğ‘ğ‘– of the PAGE estimator are set to ğ‘ğ‘– = ğ‘ d=ef ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğœğ‘–+ğœğ‘–ğ‘ğ‘– , where ğœğ‘– is the batchsize for clients ğ‘– = 1, . . . ,ğ‘› (see Table 4 for details). In our experiments, we assume that the sampling of Bernoulli random variable is performed on server side (which means that at each iteration for all clients ğ‘ğ‘¡ğ‘– = 1 or ğ‘ğ‘¡ğ‘– = 0). And if ğ‘ğ‘¡ğ‘– = 0, then in line 5 of Algorithm 3 ğ¼ğ‘–ğ‘¡ is sampled without replacement uniformly at random. Table 4 shows the selection of parameter ğ‘ for each experiment.
For each batchsize from the set2
{95%, 50%, 25%,12.5%, 6.5%,3%} ,
we tune the stepsize multiplier for EF21-PAGE within the set
{0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .
The best pair (batchsize, stepsize multiplier) is chosen in such a way that it gives the best convergence in terms of #bits/ğ‘›(ğ¶ â†’ ğ‘†). In the rest of the experiments, ï¬ne tuning is performed in a similar fashion.
2By 50%, 25% (and so on) we refer to a batchsize, which is equals to âŒŠ0.5ğ‘ğ‘–âŒ‹, âŒŠ0.25ğ‘ğ‘–âŒ‹ (and so on) for all clients ğ‘– = 1, . . . ,ğ‘›.

16

EF21 with Bells & Whistles

Dataset
mushrooms w8a a9a phishing

25%
0.1992 0.1998 0.2 0.2

12.5%
0.1097 0.1108 0.1109 0.1111

1.5%
0.0146 0.0147 0.0145 0.0143

Table 4: Summary of the parameter choice of ğ‘.

Oct 6, 2021

Experiment 2: On the effect of partial participation of clients (extra details) In this experiment, we consider ğ‘› = 100 and, therefore, a different data partitioning, see Table 5 for the summary.

Dataset

ğ‘› ğ‘ (total # of datapoints) ğ‘‘ (# of features) k ğ‘ğ‘–

mushrooms 100

w8a

100

a9a

100

phishing 100

8,120 49,749 32,560 11,055

112 2 81 300 2 497 123 2 325 68 1 110

Table 5: Summary of the datasets and splitting of the data among clients for Experiment 5. Here ğ‘ğ‘– denotes the number of datapoints per client.

We tune the stepsize multiplier for EF21-PP within the following set:
{0.125,0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096} .
Experiment 3: On the advantages of bidirectional biased compression. Our next experiment demonstrates that the application of the Server â†’ Clients compression in EF21-BC (Alg. 5) does not signiï¬cantly slow down the convergence in terms of the communication rounds but requires much less bits to be transmitted. Indeed, Figure 3a illustrates that that it is sufï¬cient to communicate only 5% âˆ’ 15% of data to perform similarly to EF21 (Alg. 1).3 Note that EF21 communicates full vectors from the Server â†’ Clients, and, therefore, may have slower communication at each round. In Figure 3b we take into account only the number of bits sent from clients to the server, and therefore we observe the same behavior as in Figure 3a. However, if we care about the total number of bits (see Figure 3c), then EF21-BC considerably outperforms EF21 in all cases.

3The range 5% âˆ’ 15% comes from the fractions ğ‘˜/ğ‘‘ for each dataset. 17

EF21 with Bells & Whistles

Oct 6, 2021

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

mushrooms, Top-2

100

EF21; 1024x EF21-BC; Top-2(S C); 128x

100

EF21-BC; Top-4(S C); 256x

10 2

EF21-BC; Top-8(S C); 1024x EF21-BC; Top-16(S C); 1024x

10 2

10 4

10 4

w8a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

10 4

a9a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

10 4

phishing, Top-1
EF21; 16x EF21-BC; Top-2(S C); 16x EF21-BC; Top-4(S C); 16x EF21-BC; Top-8(S C); 16x EF21-BC; Top-16(S C); 16x

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 6 0

com1m00u0nication20ro0u0nds

10 6 3000 0

com10m0u0nicatio2n0r0o0unds 3000

10 6 0

communic5a0t0ion rounds

10 6 1000 0

comm2u0n0ication rou4n0d0s

(a) Convergence in communication rounds. mushrooms, Top-2

100

EF21; 1024x EF21-BC; Top-2(S C); 128x

100

EF21-BC; Top-4(S C); 256x

10 2

EF21-BC; Top-8(S C); 1024x EF21-BC; Top-16(S C); 1024x

10 2

w8a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

a9a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

10 4

10 4

10 4

10 4

phishing, Top-1
EF21; 16x EF21-BC; Top-2(S C); 16x EF21-BC; Top-4(S C); 16x EF21-BC; Top-8(S C); 16x EF21-BC; Top-16(S C); 16x

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 6 0

#bit1s/0n0(0C00 S)

10 6 200000 0

#bi1ts0/0n0(0C0 S)

10 6 200000 0

20#00b0its/n (C400S0)0

10 6

60000

0

50#00bits/n1(C000S0) 15000

(b) Convergence in terms of total number of bits sent from Clients to the Server divided by ğ‘›. mushrooms, Top-2

100

EF21; 1024x EF21-BC; Top-2(S C); 128x

100

EF21-BC; Top-4(S C); 256x

10 2

EF21-BC; Top-8(S C); 1024x EF21-BC; Top-16(S C); 1024x

10 2

w8a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

a9a, Top-2

EF21; 64x EF21-BC; Top-2(S C); 32x

100

EF21-BC; Top-4(S C); 64x

EF21-BC; Top-8(S C); 64x EF21-BC; Top-16(S C); 64x

10 2

phishing, Top-1
EF21; 16x EF21-BC; Top-2(S C); 16x EF21-BC; Top-4(S C); 16x EF21-BC; Top-8(S C); 16x EF21-BC; Top-16(S C); 16x

10 4

10 4

10 4

10 4

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 6 0

#bits/n (C1 S+S C)

10 6 1e26 0.0

#bits/n (C0.5S+S C)

10 6

11e.06

0

#bi2ts0/n00(0C0 S+S 40C0)000

10 6 0

#bits/n2(0C000S0+S C) 400000

(c) Convergence in terms of total number of bits sent from Clients to the Server plus the total number of bits broadcasted from Server to Clients divided by ğ‘›.

Figure 3: Comparison of EF21-BC and EF21 with tuned stepsizes . By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21 (see the Theorem 1) .

For each parameter ğ‘˜ in Server-Clients compression, we tune the stepsize multiplier for EF21-BC within the following set:
{0.125,0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .
Experiment 4: On the cheaper computations via EF21-SGD. The fourth experiment (see Figure 4a) illustrates that EF21-SGD (Alg. 2) is the more preferable choice than EF21 for the cases when full gradient computations are costly. For each batchsize from the set4
{95%, 50%, 25%,12.5%, 6.5%,3%} ,
we tune the stepsize multiplier for EF21-SGD within the following set:
{0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .
Figure 4a illustrates that EF21-SGD is able to reach a moderate tolerance in 5 âˆ’ 10 epochs.
4By 50%, 25% (and so on) we refer to a batchsize, which is equals to âŒŠ0.5ğ‘ğ‘–âŒ‹, âŒŠ0.25ğ‘ğ‘–âŒ‹ (and so on) for all clients ğ‘– = 1, . . . ,ğ‘›.
18

EF21 with Bells & Whistles

Oct 6, 2021

|| f(xt)||2

|| f(xt)||2

mushrooms, Top-2

100

100

w8a, Top-2

a9a, Top-2 100

phishing, Top-1 100

10 2

10 2

10 2

10 2

|| f(xt)||2

|| f(xt)||2

|| f(xt)||2

10 4 EF21; 1024x

10 4 EF21; 64x

10 4 EF21; 64x

10 4 EF21; 16x

EF21-SGD; 2048x; 50%

EF21-SGD; 32x; 50%

EF21-SGD; 32x; 50%

EF21-SGD; 16x; 50%

10 6

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5%

10 6

EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5%

10 6

EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5%

10 6

EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5%

EF21-SGD; 128x; 1.5%

EF21-SGD; 512x; 1.5%

EF21-SGD; 512x; 1.5%

EF21-SGD; 16x; 1.5%

0

ep2o0chs

40

0

ep2o0chs

40

0

ep2o0chs

40

0

ep2o0chs

40

100 10 2 10 4 10 6
0

mushrooms, Top-2
EF21; 1024x EF21-SGD; 2048x; 50% EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 128x; 1.5%
1#0b0it0s0/n0(C S)200000

|| f(xt)||2

100 10 2 10 4 10 6
0

(a) Convergence in epochs.

w8a, Top-2

a9a, Top-2

100

10 2

|| f(xt)||2

EF21; 64x EF21-SGD; 32x; 50% EF21-SGD; 2048x; 25% EF21-SGD; 2048x; 12.5% EF21-SGD; 512x; 1.5%
5#00b0it0s/n (C 10S0)000

10 4 10 6
0

EF21; 64x EF21-SGD; 32x; 50% EF21-SGD; 128x; 25% EF21-SGD; 64x; 12.5% EF21-SGD; 512x; 1.5%
20#00b0its/n (4C000S0)

60000

|| f(xt)||2

100 10 2 10 4 10 6
0

phishing, Top-1
EF21; 16x EF21-SGD; 16x; 50% EF21-SGD; 16x; 25% EF21-SGD; 4x; 12.5% EF21-SGD; 16x; 1.5%
10#00b0its/n (2C000S0) 30000

(b) Convergence in terms of the number of bits sent from Clients to the Server by each client.

Figure 4: Comparison of EF21-SGD and EF21 with tuned stepsizes. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21. By 50%, 25% (and so on) we refer to a batchsize, which is equals to âŒŠ0.5ğ‘ğ‘–âŒ‹, âŒŠ0.25ğ‘ğ‘–âŒ‹ (and so on) for all clients ğ‘– = 1, . . . ,ğ‘›.

However, due to the accumulated variance introduced by SGD, estimator EF21-SGD is stuck at some accuracy level (see Figure 4b), showing the usual behavior of the SGD observed in practice.

Experiment 5: On the effect of heavy ball momentum. In this experiment (see Figure 5), we show that for the majority of the considered datasets heavy ball acceleration used in EF21-HB (Alg. 6) improves the convergence of EF21 method. For every dataset (and correspondingly chosen parameter ğ‘˜) we tune momentum parameter ğœ‚ in EF21-HB by making a grid search over all possible parameter values from 0.05 to 0.99 with the step 0.05. Finally, for our plots we pick ğœ‚ âˆˆ {0.05, 0.2, 0.25, 0.4, 0.9} since the ï¬rst four values shows the best performance and ğœ‚ = 0.9 is a popular choice in practice.
For each parameter ğœ‚ from the set
{0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99} .
we perform a grid search of stepsize multiplier within the powers of 2:
{0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048} .

100 10 2 10 4 10 6
0

mushrooms, Top-2

EF21; 1024x EF21-HB; 1024x; = 0.05

100

EF21-HB; 512x; = 0.2

EF21-HB; 512x; = 0.25 EF21-HB; 512x; = 0.4

10 2

EF21-HB; 16x; = 0.9

10 4

#bit1s/0n0(0C00 S)

10 6 200000 0

w8a, Top-2

EF21; 64x EF21-HB; 64x; = 0.05 EF21-HB; 64x; = 0.2 EF21-HB; 64x; = 0.25 EF21-HB; 32x; = 0.4 EF21-HB; 4x; = 0.9
#50b0it0s0/n (C

S1)00000

100 10 2 10 4 10 6
0

a9a, Top-2
EF21; 64x EF21-HB; 64x; = 0.05 EF21-HB; 64x; = 0.2 EF21-HB; 64x; = 0.25 EF21-HB; 32x; = 0.4 EF21-HB; 4x; = 0.9
#2b0it0s0/n0(C S) 40000

100 10 2 10 4 10 6
0

phishing, Top-1
EF21; 16x EF21-HB; 16x; = 0.05 EF21-HB; 16x; = 0.2 EF21-HB; 8x; = 0.25 EF21-HB; 8x; = 0.4 EF21-HB; 1x; = 0.9

#bits1/n00(0C0 S)

20000

Figure 5: Comparison of EF21-HB and EF21 with tuned parameters in terms of total number of bits sent from Clients to the Server divided by ğ‘›. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21 (see the Theorem 1) .

A.2 EXPERIMENTS WITH LEAST SQUARES

In this section, we conduct the experiments on a function satisfying the PÅ-condition (see Assumption 4). In particular, we consider the least squares problem:

{ï¸ƒ

1

ğ‘›
âˆ‘ï¸

}ï¸ƒ

min ğ‘“ (ğ‘¥) =

(ğ‘âŠ¤ğ‘– ğ‘¥ âˆ’ ğ‘ğ‘–)2 ,

ğ‘¥âˆˆRğ‘‘

ğ‘›

ğ‘–=1

19

|| f(xt)||2 || f(xt)||2 || f(xt)||2 || f(xt)||2

EF21 with Bells & Whistles

Oct 6, 2021

f(xt) f(x * )

|| f(xt)||2

where ğ‘ğ‘– âˆˆ Rğ‘‘, ğ‘ğ‘– âˆˆ {âˆ’1,1} are the training data. We use the same datasets as for the logistic regression problem.

Experiment: On the effect of heavy ball momentum in PÅ-setting. For PÅ-setting, EF21-HB also improves the convergence over EF21 for the majority of the datasets (see Figure 6). Stepsize and momentum parameter ğœ‚ are chosen using the same strategy as for the logistic regression experiments (see section A.1).

104 102 100 10 2 10 4
0.0
103 100 10 3 10 6
0.0

mushrooms, Top-2

104

EF21; 2048x

EF21-HB; 2048x; = 0.05 EF21-HB; 2048x; = 0.15

102

EF21-HB; 2048x; = 0.3

EF21-HB; 8x; = 0.85

100

f(xt) f(x * )

10 2

#bits/n0.(5C S)

10 4 1e16.0 0.0

mushrooms, Top-2

EF21; 2048x EF21-HB; 2048x; = 0.05

103

EF21-HB; 2048x; = 0.15 EF21-HB; 2048x; = 0.3

100

EF21-HB; 8x; = 0.85

10 3

|| f(xt)||2

10 6

#bits/n0.(5C S)

1e16.0 0.0

w8a, Top-2
EF21; 4096x EF21-HB; 4096x; = 0.05 EF21-HB; 4096x; = 0.15 EF21-HB; 4096x; = 0.3 EF21-HB; 32x; = 0.85
#bi0ts.5/n (C S) 1.0 1e6

f(xt) f(x * )

104 102 100 10 2 10 4
0

w8a, Top-2

EF21; 4096x EF21-HB; 4096x; = 0.05

103

EF21-HB; 4096x; = 0.15 EF21-HB; 4096x; = 0.3

100

EF21-HB; 32x; = 0.85

10 3

|| f(xt)||2

10 6

#bi0ts.5/n (C S) 1.0 1e6

0

a9a, Top-2

104

EF21; 1024x

EF21-HB; 1024x; = 0.05 EF21-HB; 1024x; = 0.15

102

EF21-HB; 1024x; = 0.3

EF21-HB; 16x; = 0.85

100

f(xt) f(x * )

10 2

250#0b0it0s/n (C500S0)00

10 4 750000 0

a9a, Top-2

EF21; 1024x EF21-HB; 1024x; = 0.05

103

EF21-HB; 1024x; = 0.15 EF21-HB; 1024x; = 0.3

100

EF21-HB; 16x; = 0.85

10 3

|| f(xt)||2

10 6

250#0b0it0s/n (C500S0)00 750000 0

phishing, Top-1
EF21; 4096x EF21-HB; 2048x; = 0.05 EF21-HB; 2048x; = 0.15 EF21-HB; 1024x; = 0.3 EF21-HB; 4x; = 0.85
50#0b0its/n (1C000S0) 15000 phishing, Top-1
EF21; 4096x EF21-HB; 2048x; = 0.05 EF21-HB; 2048x; = 0.15 EF21-HB; 1024x; = 0.3 EF21-HB; 4x; = 0.85
50#0b0its/n (1C000S0) 15000

Figure 6: Comparison of EF21-HB and EF21 with tuned parameters in terms of total number of bits sent from Clients to the Server divided by ğ‘›. By 1Ã—, 2Ã—, 4Ã— (and so on) we indicate that the stepsize was set to a multiple of the largest stepsize predicted by theory for EF21 (see the Theorem 2) .

A.3 DEEP LEARNING EXPERIMENTS
In this experiment, the exact/full gradient âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘˜+1) in the algorithm EF21-HB is replaced by its stochastic estimator (we later refer to this method as EF21-SGD-HB). We compare the resulting method with some existing baselines on a deep learning multi-class image classiï¬cation task. In particular, we compare our EF21-SGD-HB method to EF21+-SGD-HB5 , EF-SGD-HB6, EF21-SGD and EF-SGD on the problem of training ResNet18 (He et al., 2016) model on CIFAR-10 (Krizhevsky et al., 2009) dataset. For more details about the EF21+ and EF type methods and their applications in deep learning we refer reader to (RichtÃ¡rik et al., 2021). We implement the algorithms in PyTorch (Paszke et al., 2019) and run the experiments on a single GPU NVIDIA GeForce RTX 2080 Ti. The dataset is split into ğ‘› = 8 equal parts. Total train set size for CIFAR-10 is 50,000. The test set for evaluation has 10,000 data points. The train set is split into batches of size ğœ = 32. The ï¬rst seven workers own an equal number of batches of data, while the last worker gets the rest. In our experiments, we ï¬x ğ‘˜ â‰ˆ 0.05ğ‘‘, ğœ = 32 and momentum parameter ğœ‚ = 0.9.7 As it is usually done in deep learning applications, stochastic gradients are generated via so-called â€œshufï¬‚e onceâ€ strategy, i.e., workers randomly shufï¬‚e their datasets and then select minibatches using the obtained order (Bottou, 2009; 2012; Mishchenko et al., 2020). We tune the stepsize ğ›¾ within the range {0.0625, 0.125, 0.25, 0.5, 1} and for each method we individually chose the one ğ›¾ giving the highest accuracy score on test. For momentum methods, the best stepsize was 0.5, whereas for the non-momentum ones it was 0.125.
The experiments show (see Figure 7) that the train loss for momentum methods decreases slower than for the non-momentum ones, whereas for the test loss situation is the opposite. Finally, momentum methods show a considerable improvement in the accuracy score on the test set over the existing EF21-SGD and EF-SGD.
5EF21+-SGD-HB is the method obtained from EF21-SGD-HB via replacing EF21 by EF21+ compressor 6EF-SGD-HB is the method obtained from EF21-SGD-HB via replacing EF21 by EF compressor 7Here, ğ‘‘ is the number of model parameters. For ResNet18, ğ‘‘ = 11,511,784.
20

EF21 with Bells & Whistles

Oct 6, 2021

Test accuracy Train loss Test loss

80

60

40 20
0

1000

2000 3000 4000 communication rounds

EF21+-SGD-HB EF21-SGD-HB EF-SGD-HB EF21-SGD EF-SGD
5000 6000

101
EF21+-SGD-HB

EF21+-SGD-HB

EF21-SGD-HB

EF21-SGD-HB

EF-SGD-HB

2 Ã— 100

EF-SGD-HB

100

EF21-SGD

EF21-SGD

EF-SGD

EF-SGD

10âˆ’1

10âˆ’2 0

1000

2000 3000 4000 5000 communication rounds

6000

100 0 1000 2000 3000 4000 5000 6000 communication rounds

Figure 7: Comparison of EF-SGD and EF21-SGD with EF-SGD-HB, EF21-SGD-HB, and EF21+SGD-HB with tuned stepsizes applied to train ResNet18 on CIFAR10.

21

EF21 with Bells & Whistles

Oct 6, 2021

EF21 EF21-SGD EF21-PP
EF21-PAGE
EF21-BC EF21-HB
EF21-Prox

ğ‘…ğ‘¡ = âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2, ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf

ğ‘…ğ‘¡ = âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 , ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf ,

ğ‘ƒğ‘–ğ‘¡ = â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ ğ‘£ğ‘–ğ‘¡â€–2 , ğ‘‰ğ‘–ğ‘¡ = â€–ğ‘£ğ‘–ğ‘¡ âˆ’ ğ‘”ğ‘–ğ‘¡â€–2 ğ‘ƒğ‘–ğ‘¡ = â€–ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2, ğ‘…ğ‘¡ = âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 , ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf
Ìƒï¸€

ğ‘…ğ‘¡ = (1 âˆ’ ğœ‚)2 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 , ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf

ğ‘…ğ‘¡ = âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2, Î¦(ğ‘¥) = ğ‘“ (ğ‘¥) + ğ‘Ÿ(ğ‘¥), ğ›¿ğ‘¡ = Î¦(ğ‘¥ğ‘¡) âˆ’ Î¦ğ‘–ğ‘›ğ‘“ ,

ğ’¢ğ›¾ (ğ‘¥)

=

1 ğ›¾

(ï¸€ğ‘¥

âˆ’

proxğ›¾ ğ‘Ÿ (ğ‘¥

âˆ’

ğ›¾âˆ‡ğ‘“ (ğ‘¥)))ï¸€

Table 6: Summary of frequently used notations in the proofs.

B NOTATIONS AND ASSUMPTIONS

We now introduce an additional assumption, which enables us to obtain a faster linear convergence result in different settings.
Assumption 4 (Polyak-Åojasiewicz). There exists ğœ‡ > 0 such that ğ‘“ (ğ‘¥) âˆ’ ğ‘“ (ğ‘¥â‹†) â‰¤ 21ğœ‡ â€–âˆ‡ğ‘“ (ğ‘¥)â€–2 for all ğ‘¥ âˆˆ Rğ‘‘, where ğ‘¥â‹† = arg minğ‘¥âˆˆRğ‘‘ ğ‘“ .

Table 6 summarizes the most frequently used notations in our analysis. Additionally, we comment on

the main quantities here. We deï¬ne ğ›¿ğ‘¡ d=ef ğ‘“ (ğ‘¥ğ‘¡)âˆ’ğ‘“ inf 8, ğ‘…ğ‘¡ d=ef âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2. In the analysis of EF21-

HB, it is useful to adapt this notation to ğ‘…ğ‘¡ d=ef (1 âˆ’ ğœ‚)2 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2, where {ğ‘§ğ‘¡}ğ‘¡â‰¥0 is the sequence

of

virtual

iterates

introduced

in

Section

H.

We

denote

ğºğ‘¡ğ‘–

d=ef

â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘–ğ‘¡â€–2,

ğºğ‘¡

d=ef

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğºğ‘¡ğ‘–

following RichtÃ¡rik et al. (2021), where ğ‘”ğ‘–ğ‘¡ is an EF21 estimator at a node ğ‘–. Throughout the paper

ğ¿Ìƒï¸€2

d=ef

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

where

ğ¿ğ‘–

is

a

smoothness

constant

for

ğ‘“ğ‘–(Â·),

ğ‘–

=

1, . . . , ğ‘›

(see

Assumption

1).

8If, additionally, Assumption 4 holds, then ğ‘“ inf can be replaced by ğ‘“ (ğ‘¥â‹†) for ğ‘¥â‹† = arg minğ‘¥âˆˆRğ‘‘ ğ‘“ (ğ‘¥). 22

EF21 with Bells & Whistles

Oct 6, 2021

C EF21

For completeness, we provide here the detailed proofs for EF21 (RichtÃ¡rik et al., 2021).

Algorithm 1 EF21

1:

Input:

starting point ğ‘¥0

âˆˆ Rğ‘‘; ğ‘”ğ‘–0

âˆˆ Rğ‘‘

for ğ‘– =

1, . . . , ğ‘› (known by nodes); ğ‘”0

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–0

(known by master); learning rate ğ›¾ > 0

2: for ğ‘¡ = 0,1, 2, . . . , ğ‘‡ âˆ’ 1 do

3: Master computes ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡ and broadcasts ğ‘¥ğ‘¡+1 to all nodes

4: for all nodes ğ‘– = 1, . . . , ğ‘› in parallel do

5:

Compress ğ‘ğ‘¡ğ‘– = ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡) and send ğ‘ğ‘¡ğ‘– to the master

6:

Update local state ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

7: end for

8:

Master

computes

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

via

ğ‘”ğ‘¡+1

=

ğ‘”ğ‘¡

+

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ğ‘¡ğ‘–

9: end for

Lemma 1. Let ğ’ be a contractive compressor, then for all ğ‘– = 1, . . . , ğ‘›

E [ï¸€ğºğ‘¡ğ‘–+1]ï¸€

â‰¤

[ï¸ (1 âˆ’ ğœƒ)E [ï¸€ğºğ‘¡ğ‘–]ï¸€ + ğ›½ğ¿2ğ‘– E âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡

âƒ¦2 âƒ¦

]ï¸

,

and

(13)

E

[ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğœƒ)E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›½ğ¿Ìƒï¸€2E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

,

(14)

def
where ğœƒ = 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ),

ğ›½

def
=

(1

âˆ’

ğ›¼)

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

for any ğ‘  > 0.

Proof. Deï¬ne ğ‘Š ğ‘¡ d=ef {ğ‘”1ğ‘¡ , . . . , ğ‘”ğ‘›ğ‘¡ , ğ‘¥ğ‘¡, ğ‘¥ğ‘¡+1}, then

E [ï¸€ğºğ‘¡ğ‘–+1]ï¸€

=

E

[ï¸€ E

[ï¸€ğºğ‘¡ğ‘–+1

|

ğ‘Š ğ‘¡]ï¸€]ï¸€

=

E

[ï¸ E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡+1

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2

]ï¸]ï¸ | ğ‘Šğ‘¡

=

E

[ï¸ E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡

+

ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

ğ‘”ğ‘–ğ‘¡)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2

]ï¸]ï¸ | ğ‘Šğ‘¡

(â‰¤8) (1 âˆ’ ğ›¼)E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸

(â‰¤ğ‘–) (1 âˆ’ ğ›¼)(1 + ğ‘ )E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸

+(1

âˆ’

ğ›¼)

(ï¸€1

+

ğ‘ âˆ’1

)ï¸€

âƒ¦ âƒ¦âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡

âƒ¦2 )âƒ¦

(15)

(â‰¤ğ‘–ğ‘–) (1 âˆ’ ğ›¼)(1 + ğ‘ )E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸

+(1

âˆ’

ğ›¼)

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

ğ¿2ğ‘– E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

(ğ‘–â‰¤ğ‘–ğ‘–) (1 âˆ’ ğœƒ)E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›½ğ¿2ğ‘– E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸ ,

where (ğ‘–) follows by Youngâ€™s inequality (118), (ğ‘–ğ‘–) holds by Assumption 1, and in (ğ‘–ğ‘–ğ‘–) we apply the deï¬nition of ğœƒ and ğ›½. Averaging the above inequalities over ğ‘– = 1, . . . , ğ‘›, we obtain (14).

C.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 1. Let Assumption 1 hold, and let the stepsize in Algorithm 1 be set as

(ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1

ğ›½

0 < ğ›¾ â‰¤ ğ¿ + ğ¿Ìƒï¸€

.

(16)

ğœƒ

Fix ğ‘‡ â‰¥ 1 and let ğ‘¥Ë†ğ‘‡ be chosen from the iterates ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random. Then

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2 (ï¸€ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf )ï¸€ + E [ï¸€ğº0]ï¸€ , (17)

ğ›¾ğ‘‡

ğœƒğ‘‡

23

EF21 with Bells & Whistles

Oct 6, 2021

âˆšï¸

where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğœƒ

=

1

âˆ’

(1

âˆ’

ğ›¼)(1

+

ğ‘ ),

ğ›½

=

(1

âˆ’

ğ›¼)

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

for

any

ğ‘ 

>

0.

Proof. According to our notation, for Algorithm 1 ğ‘…ğ‘¡ = âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2. By Lemma 1, we have

E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒ) E [ï¸€ğºğ‘¡]ï¸€ + ğ›½ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ .

(18)

Next, using Lemma 16 and Jensenâ€™s inequality (119), we obtain the bound

(ï¸‚

)ï¸‚

âƒ¦ğ‘›

âƒ¦2

ğ‘“ (ğ‘¥ğ‘¡+1) â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾2 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ 21ğ›¾ âˆ’ ğ¿2 ğ‘…ğ‘¡ + ğ›¾2 âƒ¦âƒ¦âƒ¦ ğ‘›1 âˆ‘ï¸ (ï¸€ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡))ï¸€âƒ¦âƒ¦âƒ¦

âƒ¦ ğ‘–=1

âƒ¦

ğ‘¡ ğ›¾âƒ¦

ğ‘¡ âƒ¦2 (ï¸‚ 1 ğ¿ )ï¸‚ ğ‘¡ ğ›¾ 1 âˆ‘ğ‘›ï¸ âƒ¦ ğ‘¡

ğ‘¡ âƒ¦2

â‰¤ ğ‘“ (ğ‘¥ ) âˆ’ âƒ¦âˆ‡ğ‘“ (ğ‘¥ )âƒ¦ âˆ’ âˆ’ ğ‘… +

2

2ğ›¾ 2

2ğ‘›

âƒ¦ğ‘”ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

ğ‘–=1

= ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾ ğºğ‘¡. (19)

2

2ğ›¾ 2

2

Subtracting ğ‘“ inf from both sides of the above inequality, taking expectation and using the notation ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf , we get

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ . (20)

2

2ğ›¾ 2

2

Then by adding (20) with a 2ğ›¾ğœƒ multiple of (18) we obtain

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€

2ğœƒ

2

2ğ›¾ 2

2

ğ›¾ +

(ï¸

)ï¸

ğ›½ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ + (1 âˆ’ ğœƒ)E [ï¸€ğºğ‘¡]ï¸€

2ğœƒ

= E [ï¸€ğ›¿ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

2ğœƒ

2

(ï¸‚ 1 âˆ’

ğ¿ âˆ’âˆ’

ğ›¾

)ï¸‚ ğ›½ğ¿2 E [ï¸€ğ‘…ğ‘¡]ï¸€

Ìƒï¸€

2ğ›¾ 2 2ğœƒ

â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ .

2ğœƒ

2

The last inequality follows from the bound ğ›¾2 ğ›½ğ¿ğœƒÌƒï¸€2 + ğ¿ğ›¾ â‰¤ 1, which holds because of Lemma 15 and our assumption on the stepsize. By summing up inequalities for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1, we get

0 â‰¤ E [ï¸ğ›¿ğ‘‡ + ğ›¾ ğºğ‘‡ ]ï¸ â‰¤ ğ›¿0 + ğ›¾ E [ï¸€ğº0]ï¸€ âˆ’ ğ›¾ ğ‘‡âˆ‘âˆ’ï¸1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ .

2ğœƒ

2ğœƒ

2

ğ‘¡=0

Multiplying both sides by ğ›¾2ğ‘‡ , after rearranging we get

ğ‘‡âˆ‘âˆ’ï¸1 1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ â‰¤ 2ğ›¿0 + E [ï¸€ğº0]ï¸€ .

ğ‘‡

ğ›¾ğ‘‡

ğœƒğ‘‡

ğ‘¡=0

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦2 âƒ¦

]ï¸

,

where

ğ‘¥Ë†ğ‘‡

is

chosen

from ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random.

Corollary 2. Let assumptions of Theorem 1 hold,
ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›, (ï¸ âˆšï¸€ )ï¸âˆ’1
ğ›¾ = ğ¿ + ğ¿Ìƒï¸€ ğ›½/ğœƒ .

24

EF21 with Bells & Whistles

Oct 6, 2021

Then,

after

ğ‘‡

iterations/communication

rounds

of

EF21

we

have

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

(

ğ‘¥Ë†

ğ‘‡

)âƒ¦âƒ¦2

]ï¸

â‰¤

ğœ€2.

It

requires

(ï¸ƒ )ï¸ƒ ğ¿Ìƒï¸€ğ›¿0
ğ‘‡ = #grad = ğ’ª ğ›¼ğœ€2

âˆšï¸

iterations/communications rounds/gradint computations at each node, where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğ›¿0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ ğ‘–ğ‘›ğ‘“ .

Proof. Since ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘› , we have ğº0 = 0 and by Theorem 1

(ï¸ƒ (ğ‘–) 2ğ›¿0 (ğ‘–ğ‘–) 2ğ›¿0

âˆšï¸‚ )ï¸ƒ ğ›½

(ğ‘–ğ‘–ğ‘–)

2ğ›¿0

(ï¸‚

(ï¸‚ 2 )ï¸‚)ï¸‚

#grad = ğ‘‡ â‰¤ ğ›¾ğœ€2 â‰¤ ğœ€2

ğ¿ + ğ¿Ìƒï¸€ ğœƒ

â‰¤ ğœ€2 ğ¿ + ğ¿Ìƒï¸€ ğ›¼ âˆ’ 1

(ï¸ƒ

)ï¸ƒ

(ï¸ƒ

)ï¸ƒ

2ğ›¿0

2ğ¿Ìƒï¸€ (ğ‘–ğ‘£) 2ğ›¿0 ğ¿Ìƒï¸€ 2ğ¿Ìƒï¸€

6ğ¿Ìƒï¸€ğ›¿0

â‰¤ ğœ€2 ğ¿ + ğ›¼ â‰¤ ğœ€2 ğ›¼ + ğ›¼ = ğ›¼ğœ€2 ,

where in (ğ‘–) is due to the rate (17) given by Theorem 1. In two (ğ‘–ğ‘–) we plug in the stepsize, in (ğ‘–ğ‘–ğ‘–) we use Lemma 17, and (ğ‘–ğ‘£) follows by the inequalities ğ›¼ â‰¤ 1, and ğ¿ â‰¤ ğ¿Ìƒï¸€.

C.2 CONVERGENCE UNDER POLYAK-ÅOJASIEWICZ CONDITION

Theorem 2. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 1 be set as

â§ (ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1 â«

â¨

2ğ›½

ğœƒâ¬

0 < ğ›¾ â‰¤ min ğ¿ + ğ¿Ìƒï¸€

,

.

(21)

â© ğœƒ 2ğœ‡ â­

Let Î¨ğ‘¡ d=ef ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥â‹†) + ğ›¾ğœƒ ğºğ‘¡. Then for any ğ‘‡ â‰¥ 0, we have

E [ï¸€Î¨ğ‘‡ ]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ E [ï¸€Î¨0]ï¸€ ,

(22)

âˆšï¸

where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğœƒ

=

1

âˆ’

(1

âˆ’

ğ›¼)(1

+

ğ‘ ),

ğ›½

=

(1

âˆ’

ğ›¼)

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

for

any

ğ‘ 

>

0.

Proof. We proceed as in the previous proof, but use the PL inequality, subtract ğ‘“ (ğ‘¥â‹†) from both sides of (19) and utilize the notation ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥â‹†)

ğ›¿ğ‘¡+1 â‰¤ ğ›¿ğ‘¡ âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾ ğºğ‘¡

2

2ğ›¾ 2

2

â‰¤ ğ›¿ğ‘¡ âˆ’ ğ›¾ğœ‡ (ï¸€ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥â‹†))ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾ ğºğ‘¡.

2ğ›¾ 2

2

= (1 âˆ’ ğ›¾ğœ‡)ğ›¿ğ‘¡ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾ ğºğ‘¡.

2ğ›¾ 2

2

Take expectation on both sides of the above inequality and add it with a ğ›¾ğœƒ multiple of (18), then

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + E [ï¸ ğ›¾ ğºğ‘¡+1]ï¸ â‰¤ (1 âˆ’ ğ›¾ğœ‡)E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€

ğœƒ

2ğ›¾ 2

2

ğ›¾ +

(ï¸ (1

âˆ’

ğœƒ)E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›½ğ¿Ìƒï¸€2E

)ï¸ [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒ

=

(1

âˆ’

ğ›¾ğœ‡)E

[ï¸€ğ›¿ğ‘¡]ï¸€

+

ğ›¾

(ï¸‚ 1

âˆ’

ğœƒ

)ï¸‚

E

[ï¸€ğºğ‘¡]ï¸€

ğœƒ

2

(ï¸ƒ

)ï¸ƒ

âˆ’ 1 âˆ’ ğ¿ âˆ’ ğ›½ğ¿Ìƒï¸€2ğ›¾ E [ï¸€ğ‘…ğ‘¡]ï¸€ .

2ğ›¾ 2 ğœƒ

25

EF21 with Bells & Whistles

Oct 6, 2021

Note that our assumption on the stepsize implies that 1 âˆ’ ğœƒ2 â‰¤ 1 âˆ’ ğ›¾ğœ‡ and 21ğ›¾ âˆ’ ğ¿2 âˆ’ ğ›½ğ¿Ìƒğœƒï¸€2ğ›¾ â‰¥ 0. The last inequality follows from the bound ğ›¾2 2ğ›½ğœƒğ¿Ìƒï¸€2 + ğ›¾ğ¿ â‰¤ 1, which holds because of Lemma 15 and our assumption on the stepsize. Thus,

E

[ï¸ ğ›¿ğ‘¡+1

+

ğ›¾

]ï¸ ğºğ‘¡+1

â‰¤

(1

âˆ’

ğ›¾ğœ‡)E

[ï¸ ğ›¿ğ‘¡

+

ğ›¾

]ï¸ ğºğ‘¡

.

ğœƒ

ğœƒ

It remains to unroll the recurrence.

Corollary 3. Let assumptions of Theorem 2 hold,

ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

â§ (ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1 â«

â¨

2ğ›½

ğœƒâ¬

ğ›¾ = min ğ¿ + ğ¿Ìƒï¸€

,

.

â© ğœƒ 2ğœ‡ â­

Then, after ğ‘‡ iterations/communication rounds of EF21 we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥â‹†)]ï¸€ â‰¤ ğœ€. It requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ (ï¸‚ ğ›¿0 )ï¸‚

ğ‘‡ = #grad = ğ’ª

log

(23)

ğ›¼ğœ‡

ğœ€

âˆšï¸

iterations/communications rounds/gradint computations at each node, where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğ›¿0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ ğ‘–ğ‘›ğ‘“ .

Proof. Notice that

â§ (ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1 â«

â¨

2ğ›½

ğœƒâ¬

(ğ‘–)

{ï¸ƒ (ï¸‚

âˆš (ï¸‚ 2 )ï¸‚)ï¸‚âˆ’1 1 âˆ’ âˆš1 âˆ’ ğ›¼ }ï¸ƒ

min ğ¿ + ğ¿Ìƒï¸€

,

ğœ‡ â‰¥ min ğœ‡ ğ¿ + ğ¿Ìƒï¸€ 2 âˆ’ 1

,

â© ğœƒ 2ğœ‡ â­

ğ›¼

2

â§ (ï¸ƒ

âˆš )ï¸ƒâˆ’1 â«

(ğ‘–ğ‘–)

â¨

2 2ğ¿Ìƒï¸€

ğ›¼â¬

â‰¥ min ğœ‡ ğ¿ +

,

â© ğ›¼ 4â­

â§ (ï¸ƒ

âˆš )ï¸ƒâˆ’1 â«

(ğ‘–ğ‘–ğ‘–)

â¨ (1 + 2 2)ğ¿Ìƒï¸€

ğ›¼â¬

â‰¥ min ğœ‡

,

â© ğ›¼ 4â­

{ï¸ƒ

}ï¸ƒ

ğ›¼ğœ‡

ğ›¼

= min

âˆš,

(1 + 2 2)ğ¿Ìƒï¸€ 4

{ï¸‚ ğ›¼ğœ‡ ğ›¼ }ï¸‚ â‰¥ min ,
4ğ¿Ìƒï¸€ 4

ğ›¼ğœ‡

=

,

4ğ¿Ìƒï¸€

âˆš wheâˆšre in (ğ‘–) we apply Lemma 17, and plug in ğœƒ = 1 âˆ’ 1 âˆ’ ğ›¼ according to Lemma 17, (ğ‘–ğ‘–) follows by 1 âˆ’ ğ›¼ â‰¤ 1 âˆ’ ğ›¼/2, (ğ‘–ğ‘–ğ‘–) follows by the inequalities ğ›¼ â‰¤ 1, and ğ¿ â‰¤ ğ¿Ìƒï¸€.

Let ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘› , then ğº0 = 0. Thus using (22) and the above computations, we arrive at

log (ï¸ ğ›¿0 )ï¸
ğœ€

(ğ‘–) 1

(ï¸‚ ğ›¿0 )ï¸‚ 4ğ¿ (ï¸‚ ğ›¿0 )ï¸‚

Ìƒï¸€

#grad

=

ğ‘‡ â‰¤ log (1 âˆ’ ğ›¾ğœ‡)âˆ’1

â‰¤

log ğ›¾ğœ‡

ğœ€

â‰¤ log ğ›¼ğœ‡

ğœ€

,

where (ğ‘–) is due to (122).

26

EF21 with Bells & Whistles

Oct 6, 2021

D STOCHASTIC GRADIENTS

In this section, we study the extension of EF21 to the case when stochastic gradients are used instead of full gradients.

Algorithm 2 EF21-SGD

1:

Input: starting point ğ‘¥0

âˆˆ Rğ‘‘; ğ‘”ğ‘–0

âˆˆ Rğ‘‘ (known by nodes); ğ‘”0

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–0

(known

by

master);

learning rate ğ›¾ > 0

2: for ğ‘¡ = 0,1, 2, . . . , ğ‘‡ âˆ’ 1 do

3: Master computes ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡ and broadcasts ğ‘¥ğ‘¡+1 to all nodes

4: for all nodes ğ‘– = 1, . . . , ğ‘› in parallel do 5: Compute a stochastic gradient ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1) = ğœ1 âˆ‘ï¸€ğœğ‘—=1 âˆ‡ğ‘“ğœ‰ğ‘–ğ‘¡ğ‘— (ğ‘¥ğ‘¡+1)

6:

Compress ğ‘ğ‘¡ğ‘– = ğ’(ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡) and send ğ‘ğ‘¡ğ‘– to the master

7:

Update local state ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

8: end for

9:

Master

computes

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

via

ğ‘”ğ‘¡+1

=

ğ‘”ğ‘¡

+

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ğ‘¡ğ‘–

10: end for

Lemma 2. Let Assumptions 1 and 2 hold. Then for all ğ‘¡ â‰¥ 0 and all constants ğœŒ,ğœˆ > 0 EF21-SGD satisï¬es

E [ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›½Ë†1ğ¿Ìƒï¸€2E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

+ğ´Ìƒï¸€ğ›½Ë†2E [ï¸€ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ inf ]ï¸€ + ğ¶Ìƒï¸€ğ›½Ë†2,

(24)

where ğœƒË† d=ef 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğœŒ)(1 + ğœˆ), ğ›½Ë†1 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€,

ğ›½Ë†2 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€ + (ï¸1 + ğœŒ1 )ï¸, ğ´Ìƒï¸€ = maxğ‘–=1,...,ğ‘› 2(ğ´ğ‘–+ğ¿ğœğ‘–ğ‘–(ğµğ‘–âˆ’1)) ,

ğ‘› (ï¸

)ï¸

ğ¶Ìƒï¸€ = ğ‘›1 âˆ‘ï¸€ 2(ğ´ğ‘–+ğ¿ğœğ‘–ğ‘–(ğµğ‘–âˆ’1)) (ï¸€ğ‘“ inf âˆ’ ğ‘“ğ‘–inf )ï¸€ + ğ¶ğœğ‘–ğ‘– .

ğ‘–=1

Proof. For all ğœŒ,ğœˆ > 0 we have

E [ï¸€ğºğ‘¡ğ‘–+1]ï¸€

=

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡+1

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

â‰¤

(1

+

ğœŒ)E

[ï¸ âƒ¦ âƒ¦ğ’

(ï¸€ğ‘”Ë†ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

ğ‘”ğ‘–ğ‘¡

)ï¸€

âˆ’

(ï¸€ğ‘”Ë†ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

ğ‘”ğ‘–ğ‘¡

)ï¸€âƒ¦2 âƒ¦

]ï¸

+

(ï¸‚ 1

+

1 )ï¸‚

E

[ï¸ âƒ¦âƒ¦ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

ğœŒ

â‰¤

(1

âˆ’

ğ›¼)

(1

+

ğœŒ)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡

âˆ’

ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

+

(ï¸‚ 1

+

1 )ï¸‚

E

[ï¸ âƒ¦âƒ¦ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

ğœŒ

â‰¤

(1

âˆ’

ğ›¼)

(1

+

ğœŒ)(1

+

ğœˆ)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

+2 (1

âˆ’

ğ›¼) (1

+

(ï¸‚ ğœŒ) 1

+

1

)ï¸‚

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

ğ‘–

(ğ‘¥

ğ‘¡

+1

)

âˆ’

ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

ğœˆ

+2 (1

âˆ’

ğ›¼) (1

+

(ï¸‚ ğœŒ) 1

+

1

)ï¸‚

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

ğ‘–

(ğ‘¥

ğ‘¡

+1

)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

ğœˆ

+

(ï¸‚ 1

+

1 )ï¸‚

E

[ï¸ âƒ¦âƒ¦ğ‘”Ë†ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

ğœŒ

â‰¤

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡ğ‘– ]ï¸€

+

ğ›½Ë†1ğ¿2ğ‘– E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

+ğ›½Ë†2

E

[ï¸ âƒ¦ âƒ¦ğ‘”Ë†ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

âƒ¦2 )âƒ¦

]ï¸

,

27

EF21 with Bells & Whistles

Oct 6, 2021

where we introduced ğœƒË† d=ef 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğœŒ)(1 + ğœˆ), ğ›½Ë†1 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€, ğ›½Ë†2 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€ + (ï¸1 + ğœŒ1 )ï¸. Next we use independence of âˆ‡ğ‘“ğœ‰ğ‘–ğ‘¡ğ‘— (ğ‘¥ğ‘¡), variance decomposition, and (9) to estimate the last term:

E [ï¸€ğºğ‘¡ğ‘–+1]ï¸€

â‰¤

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡ğ‘– ]ï¸€

+

ğ›½Ë†1ğ¿2ğ‘– E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

ğ›½Ë†2

ğœğ‘–
âˆ‘ï¸

[ï¸‚ âƒ¦

ğ‘¡+1

âƒ¦2]ï¸‚
ğ‘¡+1

+ ğœğ‘–2 ğ‘—=1 E âƒ¦âƒ¦âˆ‡ğ‘“ğœ‰ğ‘–ğ‘¡ğ‘— (ğ‘¥ ) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦âƒ¦

=

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡ğ‘– ]ï¸€

+

ğ›½Ë†1ğ¿2ğ‘– E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

ğ›½Ë†2

ğœğ‘–
âˆ‘ï¸

(ï¸‚

[ï¸‚ âƒ¦

âƒ¦2]ï¸‚ [ï¸

ğ‘¡+1

âƒ¦

)ï¸‚ ğ‘¡+1 âƒ¦2]ï¸

+ ğœğ‘–2 ğ‘—=1 E âƒ¦âƒ¦âˆ‡ğ‘“ğœ‰ğ‘–ğ‘¡ğ‘— (ğ‘¥ )âƒ¦âƒ¦ âˆ’ E âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

(â‰¤9) (1 âˆ’ ğœƒË†)E [ï¸€ğºğ‘¡ğ‘–]ï¸€ + ğ›½Ë†1ğ¿2ğ‘– E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

2ğ´ğ‘–ğ›½Ë†2 [ï¸€ ğ‘¡+1

inf ]ï¸€

ğ›½Ë†2(ğµğ‘– âˆ’ 1)

[ï¸ âƒ¦

ğ‘¡+1 âƒ¦2]ï¸ ğ¶ğ‘–ğ›½Ë†2

+ ğœğ‘– E ğ‘“ğ‘–(ğ‘¥ ) âˆ’ ğ‘“ğ‘– +

ğœğ‘–

E âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦ + ğœğ‘–

â‰¤

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡ğ‘– ]ï¸€

+

ğ›½Ë†1ğ¿2ğ‘– E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

2(ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1))ğ›½Ë†2 [ï¸€ ğ‘¡+1

inf ]ï¸€ ğ¶ğ‘–ğ›½Ë†2

+

ğœğ‘–

E ğ‘“ğ‘–(ğ‘¥

) âˆ’ ğ‘“ğ‘–

+ ğœğ‘–

Averaging the obtained inequality for ğ‘– = 1, . . . ,ğ‘› we get

E [ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›½Ë†1ğ¿Ìƒï¸€2E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

1

ğ‘›
âˆ‘ï¸

(ï¸ƒ 2(ğ´ğ‘–

+

ğ¿ğ‘–(ğµğ‘–

âˆ’

1))ğ›½Ë†2

[ï¸€

ğ‘¡+1

inf ]ï¸€ ğ¶ğ‘–ğ›½Ë†2 )ï¸ƒ

+ ğ‘›

ğœğ‘–

E ğ‘“ğ‘–(ğ‘¥

) âˆ’ ğ‘“ğ‘–

+ ğœğ‘–

ğ‘–=1

â‰¤

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›½Ë†1ğ¿Ìƒï¸€2E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

+ 1 âˆ‘ğ‘›ï¸ (ï¸ƒ 2(ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1))ğ›½Ë†2 E [ï¸€ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ inf ]ï¸€)ï¸ƒ ğ‘› ğ‘–=1 ğœğ‘–

ğ›½Ë†2

ğ‘›
âˆ‘ï¸

(ï¸‚ 2(ğ´ğ‘–

+

ğ¿ğ‘–(ğµğ‘–

âˆ’

1))

(ï¸€

inf

inf )ï¸€ ğ¶ğ‘– )ï¸‚

+ ğ‘›

ğœğ‘–

ğ‘“

âˆ’ ğ‘“ğ‘–

+ ğœğ‘–

ğ‘–=1

â‰¤

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›½Ë†1ğ¿Ìƒï¸€2E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

+

ğ´Ìƒï¸€ğ›½Ë†2E

[ï¸€ğ‘“

(ğ‘¥ğ‘¡+1)

âˆ’

ğ‘“

inf

]ï¸€

+

ğ¶Ìƒï¸€ğ›½Ë†2

D.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 3. Let Assumptions 1 and 2 hold, and let the stepsize in Algorithm 2 be set as

â›

âˆšï¸ƒ ââˆ’1

ğ›½Ë†1

0 < ğ›¾ â‰¤ âğ¿ + ğ¿Ìƒï¸€ ğœƒË† â  ,

(25)

where ğ¿Ìƒï¸€ = âˆšï¸ ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğ¿2ğ‘– , ğœƒË† d=ef 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğœŒ)(1 + ğœˆ), ğ›½Ë†1 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€, and

ğœŒ,ğœˆ

>

0 are some positive numbers.

Assume that batchsizes ğœ1, . . . ,ğœğ‘–

are such that

ğ›¾ğ´Ìƒï¸€ğ›½^2 ^

<

1, where

2ğœƒ

ğ´Ìƒï¸€ = maxğ‘–=1,...,ğ‘› 2(ğ´ğ‘–+ğ¿ğœğ‘–ğ‘–(ğµğ‘–âˆ’1)) and ğ›½Ë†2 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€ + (ï¸1 + ğœŒ1 )ï¸. Fix ğ‘‡ â‰¥ 1 and

let ğ‘¥Ë†ğ‘‡ be chosen from the iterates ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 with following probabilities:

Prob {ï¸€ğ‘¥Ë†ğ‘‡ = ğ‘¥ğ‘¡}ï¸€ = ğ‘¤ğ‘¡ , ğ‘Šğ‘‡

(ï¸ƒ ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒğ‘¡ ğ‘¤ğ‘¡ = 1 âˆ’ 2ğœƒË† ,

ğ‘‡
âˆ‘ï¸ ğ‘Šğ‘‡ = ğ‘¤ğ‘¡.
ğ‘¡=0

28

EF21 with Bells & Whistles

Oct 6, 2021

Then

[ï¸ âƒ¦

ğ‘‡ âƒ¦2]ï¸ 2(ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf )

E [ï¸€ğº0]ï¸€

ğ¶Ìƒï¸€ğ›½Ë†2

E âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë† )âƒ¦ â‰¤ (ï¸ ğ›¾ğ´ğ›½^ )ï¸ğ‘‡ + Ë† (ï¸ ğ›¾ğ´ğ›½^ )ï¸ğ‘‡ + ğœƒË† ,

(26)

ğ›¾ğ‘‡

1âˆ’

Ìƒï¸€ 2 2ğœƒ^

ğœƒğ‘‡

1âˆ’

Ìƒï¸€ 2 2ğœƒ^

ğ‘› (ï¸

)ï¸

where ğ¶Ìƒï¸€ = ğ‘›1 âˆ‘ï¸€ 2(ğ´ğ‘–+ğ¿ğœğ‘–ğ‘–(ğµğ‘–âˆ’1)) (ï¸€ğ‘“ inf âˆ’ ğ‘“ğ‘–inf )ï¸€ + ğ¶ğœğ‘–ğ‘– .

ğ‘–=1

Proof. We notice that inequality (20) holds for EF21-SGD as well, i.e., we have

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ .

2

2ğ›¾ 2

2

Summing up the above inequality with a

ğ›¾ ^

multiple

of

(24),

we

derive

2ğœƒ

E [ï¸‚ğ›¿ğ‘¡+1 + 2ğ›¾ğœƒË†ğºğ‘¡+1]ï¸‚ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾2 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 21ğ›¾ âˆ’ ğ¿2 )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾2 E [ï¸€ğºğ‘¡]ï¸€

ğ›¾ +

(1 âˆ’ ğœƒË†)E [ï¸€ğºğ‘¡]ï¸€ +

ğ›¾

ğ›½Ë†1ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€

2ğœƒË†

2ğœƒË†

ğ›¾ +

ğ´Ìƒï¸€ğ›½Ë†2E [ï¸€ğ›¿ğ‘¡+1]ï¸€ +

ğ›¾

ğ¶Ìƒï¸€ğ›½Ë†2

2ğœƒË†

2ğœƒË†

â‰¤

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + E [ï¸‚ğ›¿ğ‘¡ +

ğ›¾

]ï¸‚ ğºğ‘¡

âˆ’

ğ›¾

E

[ï¸âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦2]ï¸

+

ğ›¾

ğ¶ ğ›½Ë†

2ğœƒË†

2ğœƒË†

âƒ¦ 2

âƒ¦ 2ğœƒË† Ìƒï¸€ 2

(ï¸ƒ 1

ğ¿ ğ›¾ğ›½Ë†1ğ¿Ìƒï¸€2 )ï¸ƒ [ï¸€ ğ‘¡]ï¸€

âˆ’

âˆ’âˆ’ 2ğ›¾ 2

2ğœƒË†

Eğ‘…

(25)
â‰¤

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + E [ï¸‚ğ›¿ğ‘¡ +

ğ›¾

]ï¸‚ ğºğ‘¡

âˆ’

ğ›¾

E

[ï¸âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦2]ï¸

+

ğ›¾

ğ¶ ğ›½Ë†

,

2ğœƒË†

2ğœƒË†

âƒ¦ 2

âƒ¦ 2ğœƒË† Ìƒï¸€ 2

where ğœƒË† d=ef 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğœŒ)(1 + ğœˆ), ğ›½Ë†1 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€, ğ›½Ë†2 d=ef 2 (1 âˆ’ ğ›¼) (1 + (ï¸ )ï¸
ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€ + 1 + ğœŒ1 , and ğœŒ,ğœˆ > 0 are some positive numbers. Next, we rearrange the terms

[ï¸ âƒ¦

ğ‘¡ âƒ¦2]ï¸

(ï¸ƒ

(ï¸ƒ

2

[ï¸‚
ğ‘¡

ğ›¾

]ï¸‚
ğ‘¡

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒ [ï¸€ ğ‘¡+1]ï¸€

)ï¸ƒ ğ›¾ [ï¸€ ğ‘¡+1]ï¸€

E âƒ¦âˆ‡ğ‘“ (ğ‘¥ )âƒ¦ â‰¤ ğ›¾ E ğ›¿ + 2ğœƒË†ğº âˆ’ 1 âˆ’ 2ğœƒË† E ğ›¿ âˆ’ 2ğœƒË†E ğº

ğ¶Ìƒï¸€ğ›½Ë†2 + ğœƒË†

(ï¸ƒ

(ï¸ƒ

2

[ï¸‚
ğ‘¡

ğ›¾

]ï¸‚
ğ‘¡

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒ

[ï¸‚
ğ‘¡+1

)ï¸ƒ

ğ›¾

]ï¸‚ [ï¸€ ğ‘¡+1]ï¸€

â‰¤ ğ›¾ E ğ›¿ + 2ğœƒË†ğº âˆ’ 1 âˆ’ 2ğœƒË† E ğ›¿ + 2ğœƒË†E ğº

ğ¶Ìƒï¸€ğ›½Ë†2 + ğœƒË† ,

sum up the obtained inequalities for ğ‘¡ = 0,1, . . . ,ğ‘‡ with weights ğ‘¤ğ‘¡/ğ‘Šğ‘‡ , and use the deï¬nition of ğ‘¥Ë†ğ‘‡

E

[ï¸

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦2 âƒ¦

]ï¸

=

1 âˆ‘ğ‘‡ï¸ ğ‘¤ğ‘¡E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

ğ‘Šğ¾ ğ‘¡=0

2

ğ‘‡ (ï¸‚ âˆ‘ï¸

[ï¸‚

ğ›¾ ]ï¸‚

[ï¸‚

ğ›¾

]ï¸‚)ï¸‚

â‰¤

ğ‘¤ğ‘¡E ğ›¿ğ‘¡ + ğºğ‘¡ âˆ’ ğ‘¤ğ‘¡+1E ğ›¿ğ‘¡+1 + E [ï¸€ğºğ‘¡+1]ï¸€

ğ›¾ğ‘Šğ‘‡ ğ‘¡=0 2ğœƒË† 2ğœƒË†

ğ¶Ìƒï¸€ğ›½Ë†2 + ğœƒË† 2ğ›¿0 E [ï¸€ğº0]ï¸€ ğ¶Ìƒï¸€ğ›½Ë†2 â‰¤ ğ›¾ğ‘Šğ‘‡ + ğœƒË†ğ‘Šğ‘‡ + ğœƒË† .

29

EF21 with Bells & Whistles

Oct 6, 2021

Finally, we notice

ğ‘‡
âˆ‘ï¸

(ï¸ƒ ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒğ‘‡

ğ‘Šğ‘‡ =

ğ‘¤ğ‘¡ â‰¥ (ğ‘‡ + 1) min ğ‘¤ğ‘¡ > ğ‘‡
ğ‘¡=0,1,...,ğ‘‡

1âˆ’

2ğœƒË†

ğ‘¡=0

that ï¬nishes the proof.

Corollary 4. Let assumptions of Theorem 3 hold, ğœŒ = ğ›¼/2, ğœˆ = ğ›¼/4,

1

ğ›¾=

,

ğ¿ + ğ¿Ìƒï¸€âˆšï¸ ğ›½^ğœƒ^1

âŒˆï¸ƒ {ï¸ƒ 2ğ‘‡ ğ›¾ (ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1)) ğ›½Ë†2 8 (ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1)) ğ›½Ë†2 inf 4ğ¶ğ‘–ğ›½Ë†2 }ï¸ƒâŒ‰ï¸ƒ

ğœğ‘– = max 1,

ğœƒË†

,

ğœƒË†ğœ€2

ğ›¿ğ‘– , ğœƒË†ğœ€2

,

âŒˆï¸ƒ

{ï¸ƒ 16ğ›¿0

8E [ï¸€ğº0]ï¸€ }ï¸ƒâŒ‰ï¸ƒ

ğ‘‡ = max ğ›¾ğœ€2 , ğœƒË†ğœ€2

,

where ğ›¿ğ‘–inf = ğ‘“ inf âˆ’ ğ‘“ğ‘–inf , ğ›¿0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf . Then, after ğ‘‡ iterations of EF21-SGD we have

E

[ï¸ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2.

It

requires

(ï¸ƒ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ )ï¸ƒ ğ‘‡ = ğ’ª ğ›¼ğœ€2

iterations/communications rounds,

#gradğ‘– = ğœğ‘–ğ‘‡

(ï¸

)ï¸ (ï¸

)ï¸

(ï¸ƒ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ ğ´Ë†ğ‘–(ğ›¿0 + ğ›¿ğ‘–inf ) + ğ¶ğ‘–

=ğ’ª

ğ›¼ğœ€2 +

ğ›¼3ğœ€4

(ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€)ğ´Ë†ğ‘–E [ï¸€ğº0]ï¸€ )ï¸ƒ +
ğ›¼2(ğ›¼ğ¿ + ğ¿Ìƒï¸€)ğœ€4

stochastic oracle calls for worker ğ‘–, and

1

ğ‘›
âˆ‘ï¸

#grad = ğ‘› ğœğ‘–ğ‘‡

ğ‘–=1

(ï¸

)ï¸ (ï¸

)ï¸

(ï¸ƒ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€

1

ğ‘›
âˆ‘ï¸

ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€

ğ´Ë†ğ‘–(ğ›¿0 + ğ›¿ğ‘–inf ) + ğ¶ğ‘–

=ğ’ª

ğ›¼ğœ€2 + ğ‘›

ğ›¼3ğœ€4

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

(ğ¿Ìƒï¸€ğ›¿0

+

E

[ï¸€ğº0]ï¸€)ğ´Ë†ğ‘–E

[ï¸€ğº0]ï¸€ )ï¸ƒ

+

ğ‘› ğ‘–=1

ğ›¼2(ğ›¼ğ¿ + ğ¿Ìƒï¸€)ğœ€4

stochastic oracle calls per worker on average, where ğ´Ë†ğ‘– = ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1).

Proof. The given choice of ğœğ‘– ensures that (ï¸1 âˆ’ ğ›¾ğ´2Ìƒï¸€ğœƒ^ğ›½^2 )ï¸ğ‘‡ = ğ’ª(1) and ğ¶Ìƒï¸€ğ›½^2/ğœƒ^ â‰¤ ğœ€/2. Next, the choice of ğ‘‡ ensures that the right-hand side of (26) is smaller than ğœ€. Finally, after simple computation we get the expression for ğœğ‘–ğ‘‡ .

Corollary 5. Consider the setting described in Example 1. Let assumptions of Theorem 3 hold, ğœŒ = ğ›¼/2, ğœˆ = ğ›¼/4,

1

ğ›¾=

,

ğ¿ + ğ¿Ìƒï¸€âˆšï¸ ğ›½^ğœƒ^1

âŒˆï¸ƒ {ï¸ƒ 4ğœğ‘–2ğ›½Ë†2 }ï¸ƒâŒ‰ï¸ƒ ğœğ‘– = max 1, ğœƒË†ğœ€2 ,

âŒˆï¸ƒ

{ï¸ƒ 16ğ›¿0

8E [ï¸€ğº0]ï¸€ }ï¸ƒâŒ‰ï¸ƒ

ğ‘‡ = max ğ›¾ğœ€2 , ğœƒË†ğœ€2

,

30

EF21 with Bells & Whistles

Oct 6, 2021

where

ğ›¿0

=

ğ‘“ (ğ‘¥0)

âˆ’

ğ‘“ inf .

Then,

after

ğ‘‡

iterations

of

EF21-SGD

we

have

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

(

ğ‘¥Ë†

ğ‘‡

)âƒ¦âƒ¦2

]ï¸

â‰¤

ğœ€2.

It

requires

(ï¸ƒ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ )ï¸ƒ ğ‘‡ = ğ’ª ğ›¼ğœ€2

iterations/communications rounds,

â›

(ï¸

)ï¸ â

ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ ğœğ‘–2

#gradğ‘– = ğœğ‘–ğ‘‡ = ğ’ª â ğ›¼ğœ€2

+

ğ›¼3ğœ€4

â 

stochastic oracle calls for worker ğ‘–, and

â›

(ï¸

)ï¸ â

1

ğ‘›
âˆ‘ï¸

ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ ğœ2

#grad = ğ‘›

ğœğ‘–ğ‘‡ = ğ’ª â

ğ›¼ğœ€2

+

ğ›¼3ğœ€4

â 

ğ‘–=1

stochastic

oracle

calls

per

worker

on

average,

where

ğœ2

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğœğ‘–2.

Corollary 6. Consider the setting described in Example 2. Let assumptions of Theorem 3 hold,

ğœŒ = ğ›¼/2, ğœˆ = ğ›¼/4,

1

ğ›¾=

,

ğ¿ + ğ¿Ìƒï¸€âˆšï¸ ğ›½^ğœƒ^1

âŒˆï¸ƒ

{ï¸ƒ

2ğ‘‡ ğ›¾ğ¿ğ‘–ğ›½Ë†2

8ğ¿ğ‘–ğ›½Ë†2

inf

8ğ¿ğ‘–

âˆ†

inf ğ‘–

ğ›½Ë†2

}ï¸ƒâŒ‰ï¸ƒ

ğœğ‘– = max 1, ğœƒË† , ğœƒË†ğœ€2 ğ›¿ğ‘– , ğœƒË†ğœ€2

,

âŒˆï¸ƒ

{ï¸ƒ 16ğ›¿0

8E [ï¸€ğº0]ï¸€ }ï¸ƒâŒ‰ï¸ƒ

ğ‘‡ = max ğ›¾ğœ€2 , ğœƒË†ğœ€2

,

where ğ›¿ğ‘–inf = ğ‘“ inf âˆ’ ğ‘“ğ‘–inf , ğ›¿0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , ğ¿ğ‘– = ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1 ğ¿ğ‘–ğ‘— , âˆ†iğ‘–nf = ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1(ğ‘“ğ‘–inf âˆ’ ğ‘“ğ‘–iğ‘—nf ).

Then,

after

ğ‘‡

iterations

of

EF21-SGD

we

have

E

[ï¸

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦2 âƒ¦

]ï¸

â‰¤

ğœ€2.

It

requires

(ï¸ƒ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ )ï¸ƒ ğ‘‡ = ğ’ª ğ›¼ğœ€2

iterations/communications rounds,

#gradğ‘– = ğœğ‘–ğ‘‡

(ï¸

)ï¸

(ï¸ƒ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€ (ï¸€ğ¿ğ‘–(ğ›¿0 + ğ›¿ğ‘–inf ) + ğ¿ğ‘–âˆ†iğ‘–nf )ï¸€

=ğ’ª

ğ›¼ğœ€2 +

ğ›¼3ğœ€4

(ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€)ğ¿ğ‘–E [ï¸€ğº0]ï¸€ )ï¸ƒ +
ğ›¼2(ğ›¼ğ¿ + ğ¿Ìƒï¸€)ğœ€4

stochastic oracle calls for worker ğ‘–, and

1

ğ‘›
âˆ‘ï¸

#grad = ğ‘› ğœğ‘–ğ‘‡

ğ‘–=1

(ï¸

)ï¸

(ï¸ƒ ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€

1

ğ‘›
âˆ‘ï¸

ğ¿Ìƒï¸€ğ›¿0 + E [ï¸€ğº0]ï¸€

(ï¸€ğ¿ğ‘–(ğ›¿0 + ğ›¿ğ‘–inf ) + ğ¿ğ‘–âˆ†iğ‘–nf )ï¸€

=ğ’ª

ğ›¼ğœ€2 + ğ‘›

ğ›¼3ğœ€4

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

(ğ¿Ìƒï¸€ğ›¿0

+

E

[ï¸€ğº0]ï¸€)ğ¿ğ‘–E

[ï¸€ğº0]ï¸€ )ï¸ƒ

+

ğ‘› ğ‘–=1

ğ›¼2(ğ›¼ğ¿ + ğ¿Ìƒï¸€)ğœ€4

stochastic oracle calls per worker on average.

31

EF21 with Bells & Whistles

Oct 6, 2021

D.2 CONVERGENCE UNDER POLYAK-ÅOJASIEWICZ CONDITION

Theorem 4. Let Assumptions 1, 2, and 4 hold, and let the stepsize in Algorithm 2 be set as

â§ â›

âˆšï¸ƒ ââˆ’1 â«

âª â¨

2ğ›½Ë†1

ğœƒË†

âª â¬

0 < ğ›¾ â‰¤ min âªâğ¿ + ğ¿Ìƒï¸€ ğœƒË† â  , 2ğœ‡ âª , (27)

â©

â­

where ğ¿Ìƒï¸€ = âˆšï¸ ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğ¿2ğ‘– , ğœƒË† d=ef 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğœŒ)(1 + ğœˆ), ğ›½Ë†1 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€, and

ğœŒ,ğœˆ

>

0 are some positive numbers.

Assume that batchsizes ğœ1, . . . ,ğœğ‘– are such that

2ğ´Ìƒï¸€ğ›½^2 ^

â‰¤

ğœ‡2 ,

ğœƒ

where ğ´Ìƒï¸€ = maxğ‘–=1,...,ğ‘› 2(ğ´ğ‘–+ğ¿ğœğ‘–ğ‘–(ğµğ‘–âˆ’1)) and ğ›½Ë†2 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€ + (ï¸1 + ğœŒ1 )ï¸. Then for

all ğ‘‡ â‰¥ 1

[ï¸‚ E ğ›¿ğ‘‡

+

ğ›¾ ğºğ‘‡ ]ï¸‚

â‰¤

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸ğ‘‡

[ï¸‚ E ğ›¿0

+

ğ›¾ ğº0]ï¸‚

+

4 ğ¶Ìƒï¸€ğ›½Ë†2,

(28)

ğœƒË†

2

ğœƒË†

ğœ‡ğœƒË†

ğ‘› (ï¸

)ï¸

where ğ¶Ìƒï¸€ = ğ‘›1 âˆ‘ï¸€ 2(ğ´ğ‘–+ğ¿ğœğ‘–ğ‘–(ğµğ‘–âˆ’1)) (ï¸€ğ‘“ inf âˆ’ ğ‘“ğ‘–inf )ï¸€ + ğ¶ğœğ‘–ğ‘– .

ğ‘–=1

Proof. We notice that inequality (20) holds for EF21-SGD as well, i.e., we have

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€

2

2ğ›¾ 2

2

PÅ
â‰¤

(1 âˆ’ ğ›¾ğœ‡)E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ .

2ğ›¾ 2

2

Summing

up

the

above

inequality

with

a

ğ›¾ ^

multiple

of

(24),

we

derive

ğœƒ

E

[ï¸‚ ğ›¿ğ‘¡+1

+

ğ›¾

]ï¸‚ ğºğ‘¡+1

â‰¤

(1 âˆ’ ğ›¾ğœ‡)E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€

ğœƒË†

2ğ›¾ 2

2

ğ›¾ +

(1

âˆ’

ğœƒË†)E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›¾

ğ›½Ë†1ğ¿Ìƒï¸€2E

[ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒË†

ğœƒË†

ğ›¾ +

ğ´Ìƒï¸€ğ›½Ë†2E

[ï¸€ğ›¿ğ‘¡+1]ï¸€

+

ğ›¾

ğ¶Ìƒï¸€ğ›½Ë†2

ğœƒË†

ğœƒË†

â‰¤

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2

E

[ï¸€ğ›¿ğ‘¡+1]ï¸€

+

(1

âˆ’

ğ›¾ğœ‡)E

[ï¸€ğ›¿ğ‘¡]ï¸€

+

(ï¸ƒ 1

âˆ’

ğœƒË†)ï¸ƒ

E

[ï¸‚ ğ›¾

]ï¸‚ ğºğ‘¡

+

ğ›¾

ğ¶ ğ›½Ë†

ğœƒË†

2

ğœƒË†

ğœƒË† Ìƒï¸€ 2

(ï¸ƒ 1

ğ¿ ğ›¾ğ›½Ë†1ğ¿Ìƒï¸€2 )ï¸ƒ [ï¸€ ğ‘¡]ï¸€

âˆ’ âˆ’âˆ’ 2ğ›¾ 2

ğœƒË†

Eğ‘…

(27)
â‰¤

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 E [ï¸€ğ›¿ğ‘¡+1]ï¸€

+

(1

âˆ’

[ï¸‚ ğ›¾ğœ‡)E ğ›¿ğ‘¡

+

ğ›¾ ğºğ‘¡]ï¸‚

+

ğ›¾ ğ¶ğ›½Ë†

,

ğœƒË†

ğœƒË†

ğœƒË† Ìƒï¸€ 2

where ğœƒË† d=ef 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğœŒ)(1 + ğœˆ), ğ›½Ë†1 d=ef 2 (1 âˆ’ ğ›¼) (1 + ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€, ğ›½Ë†2 d=ef 2 (1 âˆ’ ğ›¼) (1 + (ï¸ )ï¸
ğœŒ) (ï¸€1 + ğœˆ1 )ï¸€ + 1 + ğœŒ1 , and ğœŒ,ğœˆ > 0 are some positive numbers. Next, we rearrange the terms

(ï¸ƒ

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒ

[ï¸‚
ğ‘¡+1

ğ›¾

]ï¸‚
ğ‘¡+1

[ï¸ƒ(ï¸ƒ

ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒ ğ‘¡+1

]ï¸ƒ ğ›¾ ğ‘¡+1

1 âˆ’ ğœƒË† E ğ›¿ + ğœƒË†ğº

â‰¤ E 1 âˆ’ ğœƒË† ğ›¿ + ğœƒË†ğº

â‰¤

(1

âˆ’

ğ›¾ğœ‡)E

[ï¸‚ ğ›¿ğ‘¡

+

ğ›¾

]ï¸‚ ğºğ‘¡

+

ğ›¾

ğ¶Ìƒï¸€ğ›½Ë†2

ğœƒË†

ğœƒË†

and divide both sides of the inequality by (ï¸1 âˆ’ ğ›¾ğ´Ìƒğœƒ^ï¸€ğ›½^2 )ï¸:

E

[ï¸‚ ğ›¿ğ‘¡+1

+

ğ›¾

]ï¸‚ ğºğ‘¡+1

â‰¤

1 âˆ’ ğ›¾ğœ‡

E

[ï¸‚ ğ›¿ğ‘¡

+

ğ›¾

]ï¸‚ ğºğ‘¡

+

ğ›¾

ğ¶Ìƒï¸€ğ›½Ë†2

ğœƒË† 1 âˆ’ ğ›¾ğ´Ìƒğœƒ^ï¸€ğ›½^2 ğœƒË† ğœƒË† (ï¸1 âˆ’ ğ›¾ğ´Ìƒğœƒ^ï¸€ğ›½^2 )ï¸

(120)
â‰¤

(1

âˆ’

(ï¸ƒ ğ›¾ğœ‡) 1

+

2ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒ E [ï¸‚ğ›¿ğ‘¡

+

ğ›¾ ğºğ‘¡]ï¸‚

+

(ï¸ƒ 1

+

2ğ›¾ğ´Ìƒï¸€ğ›½Ë†2 )ï¸ƒ

ğ›¾ ğ¶ğ›½Ë†

.

ğœƒË†

ğœƒË†

ğœƒË† ğœƒË† Ìƒï¸€ 2

32

EF21 with Bells & Whistles

Oct 6, 2021

Since 2ğ´Ìƒğœƒ^ï¸€ğ›½^2 â‰¤ ğœ‡2 and ğ›¾ â‰¤ ğœ‡2 , we have

E

[ï¸‚ ğ›¿ğ‘¡+1

+

ğ›¾

]ï¸‚ ğºğ‘¡+1

â‰¤

ğœƒË†

(121)
â‰¤

Unrolling the recurrence, we get

(1

âˆ’

ğ›¾ğœ‡)

(ï¸ 1

+

ğ›¾ğœ‡ )ï¸

E

[ï¸‚ ğ›¿ğ‘¡

+

ğ›¾

]ï¸‚ ğºğ‘¡

+

2ğ›¾

ğ¶Ìƒï¸€ğ›½Ë†2

2

ğœƒË†

ğœƒË†

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸

E

[ï¸‚ ğ›¿ğ‘¡

+

ğ›¾

]ï¸‚ ğºğ‘¡

+

2ğ›¾

ğ¶Ìƒï¸€ğ›½Ë†2.

2

ğœƒË†

ğœƒË†

[ï¸‚ E ğ›¿ğ‘‡

+

ğ›¾ ğºğ‘‡ ]ï¸‚

â‰¤

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸ğ‘‡

E

[ï¸‚ ğ›¿0

+

ğ›¾

]ï¸‚ ğº0

+

2ğ›¾

ğ¶ ğ›½Ë†

ğ‘‡ âˆ’1
âˆ‘ï¸ (ï¸

ğ›¾ğœ‡ )ï¸ğ‘¡

1âˆ’

ğœƒË† 2 ğœƒË† ğœƒË† Ìƒï¸€ 2 ğ‘¡=0 2

â‰¤

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸ğ‘‡

E

[ï¸‚ ğ›¿0

+

ğ›¾

]ï¸‚ ğº0

+

2ğ›¾

ğ¶ ğ›½Ë†

âˆ
âˆ‘ï¸ (ï¸

ğ›¾ğœ‡ )ï¸ğ‘¡

1âˆ’

2 ğœƒË† ğœƒË† Ìƒï¸€ 2 ğ‘¡=0 2

=

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸ğ‘‡

E

[ï¸‚ ğ›¿0

+

ğ›¾

]ï¸‚ ğº0

+

4 ğ¶Ìƒï¸€ğ›½Ë†2

2

ğœƒË†

ğœ‡ğœƒË†

that ï¬nishes the proof.

Corollary 7. Let assumptions of Theorem 4 hold, ğœŒ = ğ›¼/2, ğœˆ = ğ›¼/4,

â§

â«

â¨1

ğœƒË† â¬

ğ›¾ = min

,

,

â© ğ¿ + ğ¿Ìƒï¸€âˆšï¸ ğ›½^ğœƒ^1 2ğœ‡ â­

âŒˆï¸ƒ {ï¸ƒ 8 (ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1)) ğ›½Ë†2 64 (ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1)) ğ›½Ë†2 inf 32ğ¶ğ‘–ğ›½Ë†2 }ï¸ƒâŒ‰ï¸ƒ

ğœğ‘– = max 1,

ğœ‡ğœƒË†

,

ğœƒË†ğœ€ğœ‡

ğ›¿ğ‘– , ğœƒË†ğœ€ğœ‡

,

âŒˆï¸‚ 2 (ï¸‚ 2ğ›¿0 [ï¸‚ 2ğ›¾ğº0 ]ï¸‚)ï¸‚âŒ‰ï¸‚

ğ‘‡=

ln ğ›¾ğœ‡

ğœ€ + E ğœƒË†ğœ€

,

where ğ›¿ğ‘–inf = ğ‘“ inf âˆ’ ğ‘“ğ‘–inf , ğ›¿0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf . Then, after ğ‘‡ iterations of EF21-SGD we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ inf ]ï¸€ â‰¤ ğœ€. It requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ (ï¸‚ ğ›¿0 [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

ğ‘‡ =ğ’ª

ln + E

ğœ‡ğ›¼ ğœ€

ğ¿Ìƒï¸€ğœ€

iterations/communications rounds,

#gradğ‘– = ğœğ‘–ğ‘‡

â›â› ğ¿

ğ¿Ìƒï¸€

(ï¸ ğ´Ë†ğ‘–

(ğœ€

+

ğ›¿ğ‘–inf

)

+

ğ¶ğ‘–

)ï¸

â

(ï¸‚ ğ›¿0

â [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

Ìƒï¸€

= ğ’ª ââ + ğœ‡ğ›¼

ğœ‡2ğ›¼3ğœ€

â  ln ğœ€ + E ğ¿ğœ€ â 

Ìƒï¸€

stochastic oracle calls for worker ğ‘–, and

1

ğ‘›
âˆ‘ï¸

#grad = ğ‘› ğœğ‘–ğ‘‡

ğ‘–=1

â›â› ğ¿

1

ğ‘›

ğ¿Ìƒï¸€

(ï¸ ğ´Ë†ğ‘–

(ğœ€

+

ğ›¿ğ‘–inf

)

+

ğ¶ğ‘–

)ï¸

â

(ï¸‚ ğ›¿0

â [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

Ìƒï¸€

âˆ‘ï¸

= ğ’ª ââ + ğœ‡ğ›¼ ğ‘›

ğœ‡2ğ›¼3ğœ€

â  ln ğœ€ + E ğ¿ğœ€ â 

ğ‘–=1

Ìƒï¸€

stochastic oracle calls per worker on average, where ğ´Ë†ğ‘– = ğ´ğ‘– + ğ¿ğ‘–(ğµğ‘– âˆ’ 1).

Proof.

The given choice of ğœğ‘– ensures that

2ğ´Ìƒï¸€ğ›½^2 ^

â‰¤

ğœ‡ 2

and 4ğ¶Ìƒï¸€ğ›½^2/ğœ‡ğœƒ^ â‰¤

ğœ€/2.

Next, the choice of ğ‘‡

ğœƒ

ensures that the right-hand side of (28) is smaller than ğœ€. Finally, after simple computation we get the

expression for ğœğ‘–ğ‘‡ .

33

EF21 with Bells & Whistles

Oct 6, 2021

Corollary 8. Consider the setting described in Example 1. Let assumptions of Theorem 4 hold, ğœŒ = ğ›¼/2, ğœˆ = ğ›¼/4,

â§

â«

â¨1

ğœƒË† â¬

ğ›¾ = min

,

,

â© ğ¿ + ğ¿Ìƒï¸€âˆšï¸ ğ›½^ğœƒ^1 2ğœ‡ â­

âŒˆï¸ƒ {ï¸ƒ 32ğ¶ğ‘–ğ›½Ë†2 }ï¸ƒâŒ‰ï¸ƒ

ğœğ‘– = max 1, ğœƒË†ğœ€ğœ‡

,

âŒˆï¸‚ 2 (ï¸‚ 2ğ›¿0 [ï¸‚ 2ğ›¾ğº0 ]ï¸‚)ï¸‚âŒ‰ï¸‚

ğ‘‡ = ln ğ›¾ğœ‡

ğœ€ +E

ğœƒË†ğœ€

,

where ğ›¿0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf . Then, after ğ‘‡ iterations of EF21-SGD we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ inf ]ï¸€ â‰¤ ğœ€. It

requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ (ï¸‚ ğ›¿0 [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

ğ‘‡ =ğ’ª

ln + E

ğœ‡ğ›¼ ğœ€

ğ¿Ìƒï¸€ğœ€

iterations/communications rounds,

#gradğ‘– = ğœğ‘–ğ‘‡

(ï¸ƒ(ï¸ƒ

)ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ ğ¿Ìƒï¸€ğœğ‘–2

(ï¸‚ ğ›¿0 [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

= ğ’ª ğœ‡ğ›¼ + ğœ‡2ğ›¼3ğœ€ ln ğœ€ + E ğ¿ğœ€

Ìƒï¸€

stochastic oracle calls for worker ğ‘–, and

1

ğ‘›
âˆ‘ï¸

#grad = ğ‘› ğœğ‘–ğ‘‡

ğ‘–=1

(ï¸ƒ(ï¸ƒ

)ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ ğ¿Ìƒï¸€ğœ2

(ï¸‚ ğ›¿0 [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

= ğ’ª ğœ‡ğ›¼ + ğœ‡2ğ›¼3ğœ€ ln ğœ€ + E ğ¿ğœ€

Ìƒï¸€

stochastic

oracle

calls

per

worker

on

average,

where

ğœ2

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğœğ‘–2.

Corollary 9. Consider the setting described in Example 2. Let assumptions of Theorem 4 hold,

ğœŒ = ğ›¼/2, ğœˆ = ğ›¼/4,

â§

â«

â¨1

ğœƒË† â¬

ğ›¾ = min

,

,

â© ğ¿ + ğ¿Ìƒï¸€âˆšï¸ ğ›½^ğœƒ^1 2ğœ‡ â­

âŒˆï¸ƒ

{ï¸ƒ

8ğ¿ğ‘–ğ›½Ë†2

64ğ¿ğ‘–ğ›½Ë†2

inf

64

ğ¿ğ‘–

âˆ†

inf ğ‘–

ğ›½Ë†2

}ï¸ƒâŒ‰ï¸ƒ

ğœğ‘– = max 1, ğœ‡ğœƒË† , ğœƒË†ğœ€ğœ‡ ğ›¿ğ‘– , ğœƒË†ğœ€ğœ‡

,

âŒˆï¸‚ 2 (ï¸‚ 2ğ›¿0 [ï¸‚ 2ğ›¾ğº0 ]ï¸‚)ï¸‚âŒ‰ï¸‚

ğ‘‡=

ln ğ›¾ğœ‡

ğœ€ + E ğœƒË†ğœ€

,

where ğ›¿ğ‘–inf = ğ‘“ inf âˆ’ ğ‘“ğ‘–inf , ğ›¿0 = ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf , ğ¿ğ‘– = ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1 ğ¿ğ‘–ğ‘— , âˆ†iğ‘–nf = ğ‘š1ğ‘– âˆ‘ï¸€ğ‘š ğ‘—=ğ‘–1(ğ‘“ğ‘–inf âˆ’ ğ‘“ğ‘–iğ‘—nf ). Then, after ğ‘‡ iterations of EF21-SGD we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ inf ]ï¸€ â‰¤ ğœ€. It requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ (ï¸‚ ğ›¿0 [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

ğ‘‡ =ğ’ª

ln + E

ğœ‡ğ›¼ ğœ€

ğ¿Ìƒï¸€ğœ€

iterations/communications rounds,

#gradğ‘– = ğœğ‘–ğ‘‡

(ï¸ƒ(ï¸ƒ ğ¿Ìƒï¸€

ğ¿Ìƒï¸€ğ¿ğ‘– (ï¸€ğœ€ + ğ›¿ğ‘–inf + âˆ†iğ‘–nf )ï¸€ )ï¸ƒ (ï¸‚ ğ›¿0

)ï¸ƒ [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

=ğ’ª

+

ğœ‡ğ›¼

ğœ‡2ğ›¼3ğœ€

ln ğœ€ + E ğ¿ğœ€

Ìƒï¸€

stochastic oracle calls for worker ğ‘–, and

1

ğ‘›
âˆ‘ï¸

#grad = ğ‘› ğœğ‘–ğ‘‡

ğ‘–=1

(ï¸ƒ(ï¸ƒ ğ¿Ìƒï¸€

1

ğ‘›
âˆ‘ï¸

ğ¿Ìƒï¸€ğ¿ğ‘–

(ï¸€ğœ€

+

ğ›¿ğ‘–inf

+

âˆ†iğ‘–nf )ï¸€ )ï¸ƒ

(ï¸‚ ğ›¿0

)ï¸ƒ [ï¸‚ 2ğº0 ]ï¸‚)ï¸‚

=ğ’ª

+

ğœ‡ğ›¼ ğ‘›

ğœ‡2ğ›¼3ğœ€

ln ğœ€ + E ğ¿ğœ€

ğ‘–=1

Ìƒï¸€

stochastic oracle calls per worker on average.

34

EF21 with Bells & Whistles

Oct 6, 2021

E VARIANCE REDUCTION

In this part, we modify the EF21 framework to better handle ï¬nite-sum problems with smooth summands. Unlike the online/streaming case where SGD has the optimal complexity (without additional assumption on the smoothness of stochastic trajectories) (Arjevani et al., 2019), in the ï¬nite sum regime, it is well-known that one can hope for convergence to the exact stationary point rather than its neighborhood. To achieve this, variance reduction techniques are instrumental. One approach is to apply a PAGE-estimator (Li et al., 2021) instead of a random minibatch applied in SGD. Note that PAGE has optimal comâˆšplexity for nonconvex problems of the form (3). With Corollary 10, we illustrate that this ğ‘‚ (ğ‘š + ğ‘š/ğœ€2) complexity is recovered for our Algorithm 3 when no compression is applied and ğ‘› = 1.

We show how to combine PAGE estimator with EF21 mechanism and call the new method EF21-
PAGE. At each step of EF21-PAGE, clients (nodes) either compute (with probability ğ‘) full gradients or use a recursive estimator ğ‘£ğ‘–ğ‘¡ + ğœ1ğ‘– âˆ‘ï¸€ğ‘—âˆˆğ¼ğ‘–ğ‘¡ (ï¸€âˆ‡ğ‘“ğ‘–ğ‘—(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘—(ğ‘¥ğ‘¡))ï¸€ (with probability 1 âˆ’ ğ‘). Then each client applies a Markov compressor/EF21-estimator and sends the result to the master node.
Typically the number of data points ğ‘š is large, and ğ‘ < 1/ğ‘š. As a result, computation of full gradients
rarely happens during optimization procedure, on average, only once in every ğ‘š iterations.

Notice that unlike VR-MARINA (Gorbunov et al., 2021), which is a state-of-the-art distributed optimization method designed speciï¬cally for unbiased compressors and which also uses PAGEestimator, EF21-PAGE does not require the communication of full (not compressed) vectors at all. This is an important property of the algorithm since, in some distributed networks, and especially when ğ‘‘ is very large, as is the case in modern over-parameterized deep learning, full vector communication is prohibitive. However, unlike the rate of VR-MARINA, the rate of EF21-PAGE does not improve with the growth of ğ‘›. This is not a ï¬‚aw of our method, but rather an inevitable drawback of the distributed methods that use biased compressions only.

Notations for this section. In this section, we use the following additional notations ğ‘ƒğ‘–ğ‘¡ d=ef

â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ ğ‘£ğ‘–ğ‘¡â€–2, ğ‘ƒ ğ‘¡

d=ef

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ƒğ‘–ğ‘¡,

ğ‘‰ğ‘–ğ‘¡

d=ef

â€–ğ‘£ğ‘–ğ‘¡ âˆ’ ğ‘”ğ‘–ğ‘¡â€–2, ğ‘‰ ğ‘¡

d=ef

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘‰ğ‘–ğ‘¡,

where

ğ‘£ğ‘–ğ‘¡

is a PAGE

estimator.

Recall

that

ğºğ‘¡

d=ef

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğºğ‘¡ğ‘– ,

ğºğ‘¡ğ‘–

d=ef

â€–âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)

âˆ’

ğ‘”ğ‘–ğ‘¡â€–2.

The main idea of the analysis in this section is to split the error in two parts ğºğ‘¡ğ‘– â‰¤ 2ğ‘ƒğ‘–ğ‘¡ + 2ğ‘‰ğ‘–ğ‘¡, and bound them separetely.

Algorithm 3 EF21-PAGE

1:

Input: starting point ğ‘¥0

âˆˆ Rğ‘‘; ğ‘”ğ‘–0, ğ‘£ğ‘–0

âˆˆ Rğ‘‘ for ğ‘– = 1, . . . , ğ‘› (known by nodes); ğ‘”0

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–0

(known by master); learning rate ğ›¾ > 0; probabilities ğ‘ğ‘– âˆˆ (0,1]; batch-sizes 1 â‰¤ ğœğ‘– â‰¤ ğ‘šğ‘–

2: for ğ‘¡ = 0,1, 2, . . . , ğ‘‡ âˆ’ 1 do

3: Master computes ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

4: for all nodes ğ‘– = 1, . . . , ğ‘› in parallel do

5:

Sample ğ‘ğ‘¡ğ‘– âˆ¼ Be(ğ‘ğ‘–)

6:

If ğ‘ğ‘¡ğ‘– = 0, sample a minibatch of data samples ğ¼ğ‘–ğ‘¡ with |ğ¼ğ‘–ğ‘¡| = ğœğ‘–

â§â¨âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

if ğ‘ğ‘¡ğ‘– = 1,

7:

ğ‘£ğ‘–ğ‘¡+1 =

ğ‘£ğ‘–ğ‘¡

+

1 ğœ

âˆ‘ï¸€ (ï¸€âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡))ï¸€

if ğ‘ğ‘¡ğ‘– = 0

â©

ğ‘– ğ‘—âˆˆğ¼ğ‘¡

ğ‘–

8:

Compress ğ‘ğ‘¡ğ‘– = ğ’(ğ‘£ğ‘–ğ‘¡+1 âˆ’ ğ‘”ğ‘–ğ‘¡) and send ğ‘ğ‘¡ğ‘– to the master

9:

Update local state ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

10: end for

11:

Master

computes

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

via

ğ‘”ğ‘¡+1

=

ğ‘”ğ‘¡

+

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ğ‘¡ğ‘–

12: end for

13: Output: ğ‘¥Ë†ğ‘‡ chosen uniformly from {ğ‘¥ğ‘¡}ğ‘¡âˆˆ[ğ‘‡ ]

Lemma 3. Let Assumption 3 hold, and let ğ‘£ğ‘–ğ‘¡+1 be a PAGE estimator, i. e. for ğ‘ğ‘¡ğ‘– âˆ¼ Be(ğ‘ğ‘–)

â§â¨âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

if ğ‘ğ‘¡ğ‘– = 1,

ğ‘£ğ‘–ğ‘¡+1 =

ğ‘£ğ‘–ğ‘¡

+

1 ğœ

âˆ‘ï¸€ (ï¸€âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡))ï¸€

if

ğ‘ğ‘¡ğ‘– = 0,

(29)

â©

ğ‘– ğ‘—âˆˆğ¼ğ‘¡

ğ‘–

35

EF21 with Bells & Whistles

Oct 6, 2021

for all ğ‘– = 1, . . . , ğ‘›, ğ‘¡ â‰¥ 0. Then

E

[ï¸€ğ‘ƒ

ğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğ‘min)E

[ï¸€ğ‘ƒ

ğ‘¡]ï¸€

+

â„’Ìƒï¸€2E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

,

(30)

where â„’Ìƒï¸€ = ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 (1âˆ’ğœğ‘ğ‘–ğ‘–)â„’2ğ‘– , ğ‘min = minğ‘–=1,...,ğ‘› ğ‘ğ‘–.

Proof.

E

[ï¸€ğ‘ƒ

ğ‘¡ ğ‘–

+1

]ï¸€

=

E

[ï¸ âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡+1

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

â¡ âƒ¦

âƒ¦2â¤

=

âƒ¦ (1 âˆ’ ğ‘ğ‘–)E â¢âƒ¦ğ‘£ğ‘¡ +

1

âƒ¦ âˆ‘ï¸ (âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡)) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦

â¥

âƒ¦ â£

ğ‘–

âƒ¦

ğœğ‘–

âƒ¦ â¦
âƒ¦

âƒ¦

ğ‘—âˆˆğ¼ğ‘–ğ‘¡

âƒ¦

[ï¸‚ âƒ¦

âƒ¦2]ï¸‚

= (1 âˆ’ ğ‘ğ‘–)E âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) + âˆ†Ìƒï¸€ ğ‘¡ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) + âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦

[ï¸‚ âƒ¦

âƒ¦2]ï¸‚

= (1 âˆ’ ğ‘ğ‘–)E âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) + âˆ†Ìƒï¸€ ğ‘¡ğ‘– âˆ’ âˆ†ğ‘¡ğ‘–âƒ¦âƒ¦

(=ğ‘–) (1 âˆ’ ğ‘ğ‘–)E [ï¸âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ + (1 âˆ’ ğ‘ğ‘–)E [ï¸‚âƒ¦âƒ¦âƒ¦âˆ†Ìƒï¸€ ğ‘¡ğ‘– âˆ’ âˆ†ğ‘¡ğ‘–âƒ¦âƒ¦âƒ¦2]ï¸‚

(ğ‘–ğ‘–)

[ï¸€ ğ‘¡]ï¸€

(1 âˆ’ ğ‘ğ‘–)â„’2ğ‘–

[ï¸ âƒ¦ ğ‘¡+1

ğ‘¡âƒ¦2]ï¸

â‰¤ (1 âˆ’ ğ‘ğ‘–)E ğ‘ƒğ‘– + ğœğ‘– E âƒ¦ğ‘¥ âˆ’ ğ‘¥ âƒ¦

[ï¸€ ğ‘¡]ï¸€

(1 âˆ’ ğ‘ğ‘–)â„’2ğ‘–

[ï¸ âƒ¦ ğ‘¡+1

ğ‘¡âƒ¦2]ï¸

â‰¤ (1 âˆ’ ğ‘min)E ğ‘ƒğ‘– + ğœğ‘– E âƒ¦ğ‘¥ âˆ’ ğ‘¥ âƒ¦ ,

(31)

[ï¸

]ï¸

where equality (ğ‘–) holds because E âˆ†Ìƒï¸€ ğ‘¡ğ‘– âˆ’ âˆ†ğ‘¡ğ‘– | ğ‘¥ğ‘¡, ğ‘¥ğ‘¡+1, ğ‘£ğ‘–ğ‘¡ = 0, and (ğ‘–ğ‘–) holds by Assumption 3.

It remains to average the above inequality over ğ‘– = 1, . . . , ğ‘›.

Lemma 4. Let Assumptions 1 and 3 hold, let ğ‘£ğ‘–ğ‘¡+1 be a PAGE estimator, i. e. for ğ‘ğ‘¡ğ‘– âˆ¼ Be(ğ‘ğ‘–) and for all ğ‘– = 1, . . . , ğ‘›, ğ‘¡ â‰¥ 0

â§â¨âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

if ğ‘ğ‘¡ğ‘– = 1,

ğ‘£ğ‘–ğ‘¡+1 =

ğ‘£ğ‘–ğ‘¡

+

1 ğœ

âˆ‘ï¸€ (ï¸€âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘— (ğ‘¥ğ‘¡))ï¸€

if

ğ‘ğ‘¡ğ‘– = 0,

(32)

â©

ğ‘– ğ‘—âˆˆğ¼ğ‘¡

ğ‘–

and let ğ‘”ğ‘–ğ‘¡+1 be an EF21 estimator, i. e.

ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ’(ğ‘£ğ‘–ğ‘¡+1 âˆ’ ğ‘”ğ‘–ğ‘¡), ğ‘”ğ‘–0 = ğ’ (ï¸€ğ‘£ğ‘–0)ï¸€

(33)

for all ğ‘– = 1, . . . , ğ‘›, ğ‘¡ â‰¥ 0. Then

E

[ï¸€ğ‘‰

ğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğœƒ)E

[ï¸€ğ‘‰

ğ‘¡]ï¸€

+

2ğ›½ğ‘maxE

[ï¸€ğ‘ƒ

ğ‘¡]ï¸€

+

ğ›½

(ï¸ 2ğ¿Ìƒï¸€2

+

)ï¸ â„’Ìƒï¸€2

E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

,

(34)

where â„’Ìƒï¸€ = ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 (1âˆ’ğœğ‘ğ‘–ğ‘–)â„’2ğ‘– , ğ‘max = maxğ‘–=1,...,ğ‘› ğ‘ğ‘–, ğœƒ = 1âˆ’(1âˆ’ğ›¼)(1+ğ‘ ), ğ›½ = (1âˆ’ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ for any ğ‘  > 0.

Proof. Following the steps in proof of Lemma 1, but with âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) and âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) being substituted by their estimators ğ‘£ğ‘–ğ‘¡+1 and ğ‘£ğ‘–ğ‘¡, we end up with an analogue of (15)

E [ï¸âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡+1 âˆ’ ğ‘£ğ‘–ğ‘¡+1âƒ¦âƒ¦2]ï¸

â‰¤

(1

âˆ’

ğœƒ)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡

âˆ’

ğ‘£ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸

+

ğ›½E

[ï¸ âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡+1

âˆ’

ğ‘£ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸

,

(35)

36

EF21 with Bells & Whistles

Oct 6, 2021

where ğœƒ = 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ), ğ›½ = (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ for any ğ‘  > 0. Then

E

[ï¸€ğ‘‰

ğ‘¡ ğ‘–

]ï¸€

=

E [ï¸âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡+1 âˆ’ ğ‘£ğ‘–ğ‘¡+1âƒ¦âƒ¦2]ï¸

(â‰¤35) (1 âˆ’ ğœƒ)E [ï¸âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡ âˆ’ ğ‘£ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›½E [ï¸âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡+1 âˆ’ ğ‘£ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸

=

(1

âˆ’

ğœƒ)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡

âˆ’

ğ‘£ğ‘–ğ‘¡âƒ¦âƒ¦2]ï¸

+

ğ›½E

[ï¸ E

[ï¸ âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡+1

âˆ’

ğ‘£ğ‘–ğ‘¡âƒ¦âƒ¦2

|

ğ‘£ğ‘–ğ‘¡,

]ï¸]ï¸ ğ‘¥ğ‘¡,ğ‘¥ğ‘¡+1

(ğ‘–)
=

(1

âˆ’

ğœƒ)E

[ï¸€ğ‘‰ğ‘–ğ‘¡]ï¸€

+

ğ›½ğ‘ğ‘–E

[ï¸ âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸

â¡ âƒ¦

âƒ¦2â¤

+ğ›½(1 âˆ’ ğ‘ğ‘–)E â¢âƒ¦âƒ¦âƒ¦ 1 âˆ‘ï¸ (ï¸€âˆ‡ğ‘“ğ‘–ğ‘—(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–ğ‘—(ğ‘¥ğ‘¡))ï¸€âƒ¦âƒ¦âƒ¦ â¥

â£âƒ¦ ğœğ‘– âƒ¦ ğ‘—âˆˆğ¼ğ‘–ğ‘¡

â¦ âƒ¦ âƒ¦

= (1 âˆ’ ğœƒ)E [ï¸€ğ‘‰ğ‘–ğ‘¡]ï¸€ + ğ›½ğ‘ğ‘–E [ï¸âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2]ï¸ + ğ›½(1 âˆ’ ğ‘ğ‘–)E [ï¸‚âƒ¦âƒ¦âƒ¦âˆ†Ìƒï¸€ ğ‘¡ğ‘–âƒ¦âƒ¦âƒ¦2]ï¸‚

(ğ‘–ğ‘–)
=

(1

âˆ’

ğœƒ)E

[ï¸€ğ‘‰ğ‘–ğ‘¡]ï¸€

+

2ğ›½ğ‘ğ‘–E

[ï¸ âƒ¦âƒ¦ğ‘£ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

+2ğ›½ğ‘ğ‘–E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ + ğ›½(1 âˆ’ ğ‘ğ‘–)E [ï¸‚âƒ¦âƒ¦âƒ¦âˆ†Ìƒï¸€ ğ‘¡ğ‘–âƒ¦âƒ¦âƒ¦2]ï¸‚

= (1 âˆ’ ğœƒ)E [ï¸€ğ‘‰ğ‘–ğ‘¡]ï¸€ + 2ğ›½ğ‘ğ‘–E [ï¸€ğ‘ƒğ‘–ğ‘¡]ï¸€ + 2ğ›½ğ‘ğ‘–E [ï¸âƒ¦âƒ¦âˆ†ğ‘¡ğ‘–âƒ¦âƒ¦2]ï¸ + ğ›½(1 âˆ’ ğ‘ğ‘–)E [ï¸‚âƒ¦âƒ¦âƒ¦âˆ†Ìƒï¸€ ğ‘¡ğ‘–âƒ¦âƒ¦âƒ¦2]ï¸‚

(ğ‘–ğ‘–ğ‘–)
=

(1 âˆ’ ğœƒ)E [ï¸€ğ‘‰ğ‘–ğ‘¡]ï¸€ + 2ğ›½ğ‘ğ‘–E [ï¸€ğ‘ƒğ‘–ğ‘¡]ï¸€ + ğ›½(2ğ‘ğ‘– + 1 âˆ’ ğ‘ğ‘–)E [ï¸âƒ¦âƒ¦âˆ†ğ‘¡ğ‘–âƒ¦âƒ¦2]ï¸

[ï¸‚ âƒ¦

âƒ¦2]ï¸‚

+ğ›½(1 âˆ’ ğ‘ğ‘–)E âƒ¦âƒ¦âˆ†Ìƒï¸€ ğ‘¡ğ‘– âˆ’ âˆ†ğ‘¡ğ‘–âƒ¦âƒ¦

(â‰¤ğ‘–ğ‘£) (1 âˆ’ ğœƒ)E [ï¸€ğ‘‰ğ‘–ğ‘¡]ï¸€ + 2ğ›½ğ‘ğ‘–E [ï¸€ğ‘ƒğ‘–ğ‘¡]ï¸€ + ğ›½(1 + ğ‘ğ‘–)ğ¿2ğ‘– E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

+ğ›½ (1 âˆ’ ğ‘ğ‘–)â„’2ğ‘– E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸ ğœğ‘–

[ï¸€ ğ‘¡]ï¸€

[ï¸€ ğ‘¡]ï¸€

(ï¸‚
2

(1 âˆ’ ğ‘ğ‘–)â„’2ğ‘– )ï¸‚

[ï¸ âƒ¦ ğ‘¡+1

ğ‘¡âƒ¦2]ï¸

â‰¤ (1 âˆ’ ğœƒ)E ğ‘‰ğ‘– + 2ğ›½ğ‘maxE ğ‘ƒğ‘– + ğ›½ 2ğ¿ğ‘– + ğœğ‘–

E âƒ¦ğ‘¥ âˆ’ ğ‘¥ âƒ¦ ,

where in (ğ‘–) we use the deï¬nition of PAGE estimator (32), (ğ‘–ğ‘–) applies (119) with ğ‘  = 1, (ğ‘–ğ‘–ğ‘–) is due to bias-variance decomposition (123), (ğ‘–ğ‘£) makes use of Assumptions 1 and 3, and the last step is due to ğ‘ğ‘– â‰¤ 1, ğ‘ğ‘– â‰¤ ğ‘max .

It remains to average the above inequality over ğ‘– = 1, . . . , ğ‘›.

E.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 5. Let Assumptions 1 and 3 hold, and let the stepsize in Algorithm 3 be set as

(ï¸ƒ âˆšï¸ƒ

)ï¸ƒâˆ’1

0<ğ›¾ â‰¤ ğ¿+

4ğ›½ ğ¿2

+

(ï¸‚ 3ğ›½ 2

ğ‘max

+

1 )ï¸‚ â„’2

.

(36)

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğœƒ ğ‘min ğ‘min

Fix ğ‘‡ â‰¥ 1 and let ğ‘¥Ë†ğ‘‡ be chosen from the iterates ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random. Then

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2Î¨0 , (37) ğ›¾ğ‘‡

where Î¨ğ‘¡ d=ef ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf + ğ›¾ğœƒ ğ‘‰ ğ‘¡ + ğ‘mğ›¾in (ï¸1 + 2ğ›½ğ‘ğœƒmin )ï¸ ğ‘ƒ ğ‘¡, ğ‘max = maxğ‘–=1,...,ğ‘› ğ‘ğ‘–, ğ‘min =

âˆšï¸

minğ‘–=1,...,ğ‘› ğ‘ğ‘–, ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğœƒ

=

1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ),

ğ›½

=

(1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€

for

any

ğ‘  > 0.

37

EF21 with Bells & Whistles

Oct 6, 2021

Proof. We apply Lemma 16 and split the error â€–ğ‘”ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2 in two parts

ğ‘“ (ğ‘¥ğ‘¡+1) â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾ âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2

2

2ğ›¾ 2

2

â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡

2

2ğ›¾ 2

+ğ›¾âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘£ğ‘¡âƒ¦âƒ¦2

+

ğ›¾E

[ï¸ âƒ¦âƒ¦ğ‘£ğ‘¡

âˆ’

âˆ‡ğ‘“

(ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸

â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡

2

2ğ›¾ 2

1 âˆ‘ğ‘›ï¸ âƒ¦ ğ‘¡ ğ‘¡âƒ¦2 1 âˆ‘ğ‘›ï¸ âƒ¦ ğ‘¡

ğ‘¡ âƒ¦2

+ğ›¾ ğ‘›

âƒ¦ğ‘”ğ‘– âˆ’ ğ‘£ğ‘– âƒ¦

+ğ›¾ ğ‘›

âƒ¦ğ‘£ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

ğ‘–=1

ğ‘–=1

= ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾ğ‘‰ ğ‘¡ + ğ›¾ğ‘ƒ ğ‘¡, (38)

2

2ğ›¾ 2

where we used notation ğ‘…ğ‘¡ = â€–ğ›¾ğ‘”ğ‘¡â€–2 = âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2, and applied (118) and (119).
Subtracting ğ‘“ inf from both sides of the above inequality, taking expectation and using the notation ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ inf , we get

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾E [ï¸€ğ‘‰ ğ‘¡]ï¸€ + ğ›¾E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ . (39)

2

2ğ›¾ 2

Further, Lemma 3 and 4 provide the recursive bounds for the last two terms of (39)

E [ï¸€ğ‘ƒ ğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğ‘min)E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ + â„’Ìƒï¸€2E [ğ‘…ğ‘¡] ,

(40)

(ï¸

)ï¸

E [ï¸€ğ‘‰ ğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒ)E [ï¸€ğ‘‰ ğ‘¡]ï¸€ + ğ›½ 2ğ¿Ìƒï¸€2 + â„’Ìƒï¸€2 E [ğ‘…ğ‘¡] + 2ğ›½ğ‘maxE [ï¸€ğ‘ƒ ğ‘¡]ï¸€ .

(41)

Adding (39) with a ğ›¾ğœƒ multiple of (41) we obtain

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + ğ›¾ E [ï¸€ğ‘‰ ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾E [ï¸€ğ‘‰ ğ‘¡]ï¸€

ğœƒ

2

2ğ›¾ 2

+ğ›¾E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ + ğ›¾ (ï¸€(1 âˆ’ ğœƒ) E [ï¸€ğ‘‰ ğ‘¡]ï¸€ + ğ´ğ‘Ÿğ‘¡ + ğ¶E [ï¸€ğ‘ƒ ğ‘¡]ï¸€)ï¸€ ğœƒ

â‰¤ ğ›¿ğ‘¡ + ğ›¾ E [ï¸€ğ‘‰ ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğ›¾ğ´ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒ

2

2ğ›¾ 2 ğœƒ

+ğ›¾

(ï¸‚ 1

+

ğ¶

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

,

ğœƒ

where

we

denote

ğ´

d=ef

ğ›½

(ï¸ 2ğ¿Ìƒï¸€2

+

)ï¸ â„’Ìƒï¸€2 ,

ğ¶

d=ef

2ğ›½ğ‘max.

Then adding the above inequality with a ğ‘mğ›¾in (ï¸€1 + ğ¶ğœƒ )ï¸€ multiple of (40), we get

38

EF21 with Bells & Whistles

Oct 6, 2021

E [ï¸€Î¦ğ‘¡+1]ï¸€

=

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + ğ›¾ E [ï¸€ğ‘‰ ğ‘¡+1]ï¸€ +

ğ›¾

(ï¸‚ 1

+

ğ¶

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡+1]ï¸€

ğœƒ

ğ‘min

ğœƒ

â‰¤ ğ›¿ğ‘¡ + ğ›¾ E [ï¸€ğ‘‰ ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğ›¾ğ´ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒ

2

2ğ›¾ 2 ğœƒ

+ğ›¾

(ï¸‚ 1

+

ğ¶

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

ğœƒ

ğ›¾ +

(ï¸‚ 1

+

ğ¶

)ï¸‚

(ï¸ (1

âˆ’

ğ‘min)E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

+

â„’Ìƒï¸€2E

)ï¸ [ï¸€ğ‘…ğ‘¡]ï¸€

ğ‘min

ğœƒ

â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğ‘‰ ğ‘¡]ï¸€ + ğ›¾ (ï¸‚1 + ğ¶ )ï¸‚ E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸

ğœƒ

ğ‘min

ğœƒ

2

(ï¸‚ 1 ğ¿ ğ›¾ğ´ âˆ’ âˆ’âˆ’ âˆ’

ğ›¾

(ï¸‚ 1

+

ğ¶

)ï¸‚

)ï¸‚ â„’2

E

[ï¸€ğ‘…ğ‘¡]ï¸€

Ìƒï¸€

2ğ›¾ 2 ğœƒ ğ‘min

ğœƒ

=

E

[ï¸€Î¦ğ‘¡]ï¸€

âˆ’

ğ›¾

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸

2

(ï¸‚ 1 ğ¿ ğ›¾ğ´ âˆ’ âˆ’âˆ’ âˆ’

ğ›¾

(ï¸‚ 1

+

ğ¶

)ï¸‚

)ï¸‚ â„’2

E

[ï¸€ğ‘…ğ‘¡]ï¸€

.

(42)

Ìƒï¸€

2ğ›¾ 2 ğœƒ ğ‘min

ğœƒ

The coefï¬cient in front of E [ğ‘…ğ‘¡] simpliï¬es after substitution by ğ´ and ğ¶

ğ›¾ğ´ +

ğ›¾

(ï¸‚ 1

+

ğ¶ )ï¸‚ â„’2

â‰¤

2ğ›½ ğ¿2

+

(ï¸‚ 3ğ›½

ğ‘max

+

1

)ï¸‚ â„’2.

Ìƒï¸€

Ìƒï¸€

Ìƒï¸€

ğœƒ ğ‘min

ğœƒ

ğœƒ

ğœƒ ğ‘min ğ‘min

Thus by Lemma 15 and the stepsize choice

(ï¸ƒ âˆšï¸ƒ

)ï¸ƒâˆ’1

0<ğ›¾ â‰¤ ğ¿+

4ğ›½ ğ¿2

+

(ï¸‚ 3ğ›½ 2

ğ‘max

+

1 )ï¸‚ â„’2

(43)

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğœƒ ğ‘min ğ‘min

the last term in (42) is not positive. By summing up inequalities for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1, we get

0 â‰¤ E [ï¸€Î¦ğ‘‡ ]ï¸€ â‰¤ E [ï¸€Î¦0]ï¸€ âˆ’ ğ›¾ ğ‘‡âˆ‘âˆ’ï¸1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ . 2
ğ‘¡=0

Multiplying both sides by ğ›¾2ğ‘‡ and rearranging we get

ğ‘‡âˆ‘âˆ’ï¸1 1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ â‰¤ 2E [ï¸€Î¦0]ï¸€ .

ğ‘‡

ğ›¾ğ‘‡

ğ‘¡=0

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦2 âƒ¦

]ï¸

,

where

ğ‘¥Ë†ğ‘‡

is

chosen

from ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random.

Corollary 10. Let assumptions of Theorem 5 hold,

ğ‘£ğ‘–0 = ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

(ï¸ƒ âˆšï¸ƒ

)ï¸ƒâˆ’1

ğ›¾=

ğ¿+

4ğ›½ ğ¿2

+

(ï¸‚ 3ğ›½ 2

ğ‘max

+

1 )ï¸‚ â„’2

,

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğœƒ ğ‘min ğ‘min

ğ‘ğ‘– = ğœğ‘– , ğ‘– = 1, . . . , ğ‘›. ğœğ‘– + ğ‘šğ‘–

Then,

after

ğ‘‡

iterations/communication

rounds

of

EF21-PAGE

we

have

E

[ï¸

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦2 âƒ¦

]ï¸

â‰¤

ğœ€2.

It

requires

39

EF21 with Bells & Whistles

Oct 6, 2021

(ï¸ƒ (ğ¿Ìƒï¸€ + â„’Ìƒï¸€)ğ›¿0 âˆšï¸‚ ğ‘max

âˆšğ‘šmaxâ„’Ìƒï¸€ğ›¿0 )ï¸ƒ

ğ‘‡ = ğ’ª ğ›¼ğœ€2

ğ‘min + ğœ€2

iterations/communications rounds,

(ï¸ƒ

ğœğ‘–(ğ¿Ìƒï¸€ + â„’Ìƒï¸€)ğ›¿0 âˆšï¸‚ ğ‘max ğœğ‘–âˆšğ‘šmaxâ„’Ìƒï¸€ğ›¿0 )ï¸ƒ

#gradğ‘– = ğ’ª ğ‘šğ‘– + ğ›¼ğœ€2

ğ‘min + ğœ€2

stochastic oracle calls for worker ğ‘–, and

(ï¸ƒ ğœ (ğ¿Ìƒï¸€ + â„’Ìƒï¸€)ğ›¿0 âˆšï¸‚ ğ‘max ğœ âˆšğ‘šmaxâ„’Ìƒï¸€ğ›¿0 )ï¸ƒ

#grad = ğ’ª ğ‘š + ğ›¼ğœ€2

ğ‘min + ğœ€2

stochastic

oracle

calls

per

worker

on

average,

where

ğœ

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğœğ‘–

,

ğ‘š

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘šğ‘–

,

ğ‘šmax

=

maxğ‘–=1,...,ğ‘› ğ‘šğ‘–, ğ‘max = maxğ‘–=1,...,ğ‘› ğ‘ğ‘–, ğ‘min = minğ‘–=1,...,ğ‘› ğ‘ğ‘–.

Proof. Notice that by Lemma 17 we have

âˆšï¸ƒ

âˆšï¸ƒ

ğ¿+

4ğ›½ ğ¿2

+

(ï¸‚ 3ğ›½ 2

ğ‘max

+

1 )ï¸‚ â„’2

â‰¤

ğ¿+

16 ğ¿2

+

(ï¸‚ 12 2

ğ‘max

+

1 )ï¸‚ â„’2

Ìƒï¸€

Ìƒï¸€

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğœƒ ğ‘min ğ‘min

ğ›¼2

ğ›¼2 ğ‘min ğ‘min

4 âˆšï¸‚ 24 ğ‘max 2

â‰¤ ğ¿ + ğ¿Ìƒï¸€ +

+ â„’Ìƒï¸€

ğ›¼ âˆš ğ›¼2 ğ‘min ğ‘min âˆš

4

24 âˆšï¸‚ ğ‘max

2

â‰¤ ğ¿ + ğ¿Ìƒï¸€ +

â„’Ìƒï¸€ + âˆš â„’Ìƒï¸€

ğ›¼

ğ›¼ ğ‘min

ğ‘min

âˆš

âˆš

5

24 âˆšï¸‚ ğ‘max

2

â‰¤ ğ¿Ìƒï¸€ +

â„’Ìƒï¸€ + âˆš â„’Ìƒï¸€

ğ›¼

ğ›¼ ğ‘min

ğ‘min

âˆš

5 âˆšï¸‚ ğ‘max (ï¸

)ï¸

2

â‰¤

ğ¿Ìƒï¸€ + â„’Ìƒï¸€ + âˆš â„’Ìƒï¸€

ğ›¼ ğ‘min

ğ‘min

5 âˆšï¸‚ ğ‘max (ï¸

)ï¸ âˆš

â‰¤

ğ¿Ìƒï¸€ + â„’Ìƒï¸€ + 2 ğ‘šmaxâ„’Ìƒï¸€,

ğ›¼ ğ‘min

âˆš

âˆšâˆš

where we used ğ¿ â‰¤ ğ¿Ìƒï¸€, ğ‘min â‰¤ ğ‘max, and the fact that ğ‘ + ğ‘ â‰¤ ğ‘ + ğ‘ for ğ‘, ğ‘ â‰¥ 0.

Then the number of communication rounds

2ğ›¿0 ğ‘‡ â‰¤ ğ›¾ğœ€2

2ğ›¿0 (ï¸‚ 5 âˆšï¸‚ ğ‘max (ï¸

)ï¸ âˆš

)ï¸‚

â‰¤

ğœ€2

ğ›¼

ğ¿Ìƒï¸€ + â„’Ìƒï¸€ + 2 ğ‘šmaxâ„’Ìƒï¸€ ğ‘min

(ï¸ƒ (ğ¿Ìƒï¸€ + â„’Ìƒï¸€)ğ›¿0 âˆšï¸‚ ğ‘max

âˆšğ‘šmaxâ„’Ìƒï¸€ğ›¿0 )ï¸ƒ

= ğ’ª ğ›¼ğœ€2 ğ‘min + ğœ€2 .

At each worker, we have

#gradğ‘– = ğ‘šğ‘– + ğ‘‡ (ğ‘ğ‘–ğ‘šğ‘– + (1 âˆ’ ğ‘ğ‘–)ğœğ‘–) = ğ‘šğ‘– + 2ğ‘šğ‘–ğœğ‘– ğ‘‡ ğœğ‘– + ğ‘šğ‘– â‰¤ ğ‘šğ‘– + 2ğœğ‘–ğ‘‡.

Averaging over ğ‘– = 1, . . . ,ğ‘›, we get

40

EF21 with Bells & Whistles

Oct 6, 2021

#grad â‰¤ ğ‘š + 2ğœ ğ‘‡ (ï¸ƒ ğœ (ğ¿Ìƒï¸€ + â„’Ìƒï¸€)ğ›¿0 âˆšï¸‚ ğ‘max ğœ âˆšğ‘šmaxâ„’Ìƒï¸€ğ›¿0 )ï¸ƒ
= ğ’ª ğ‘š + ğ›¼ğœ€2 ğ‘min + ğœ€2 .

E.2 CONVERGENCE UNDER POLYAK-ÅOJASIEWICZ CONDITION Theorem 6. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 3 be set as

{ï¸‚ ğœƒ ğ‘min }ï¸‚

0 < ğ›¾ â‰¤ min ğ›¾0, ,

,

(44)

2ğœ‡ 2ğœ‡

(ï¸‚ âˆšï¸‚

)ï¸‚âˆ’1

def
where ğ›¾0 = 0 < ğ›¾ â‰¤

ğ¿+

(ï¸

)ï¸

8ğœƒğ›½ ğ¿Ìƒï¸€2 + 4 5ğœƒğ›½ ğ‘ğ‘mmainx + ğ‘m1in â„’Ìƒï¸€2

âˆšï¸

, ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğœƒ

=

1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ), ğ›½ = (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ for any ğ‘  > 0.

Let

Î¨ğ‘¡

def
=

ğ‘“ (ğ‘¥ğ‘¡)

âˆ’

ğ‘“ (ğ‘¥â‹†)

+

2ğ›¾ ğ‘‰

ğ‘¡

+

2ğ›¾

(ï¸

)ï¸

1 + 4ğ›½ğ‘max ğ‘ƒ ğ‘¡. Then for any ğ‘‡ â‰¥ 0, we have

ğœƒ

ğ‘min

ğœƒ

E [ï¸€Î¨ğ‘‡ ]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ E [ï¸€Î¨0]ï¸€ .

(45)

Proof. Similarly to the proof of Theorem 5 the inequalities (39), (40), (41) hold with ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥â‹†).
Adding (39) with a 2ğœƒğ›¾ multiple of (41) we obtain

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + 2ğ›¾ E [ï¸€ğ‘‰ ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾E [ï¸€ğ‘‰ ğ‘¡]ï¸€

ğœƒ

2

2ğ›¾ 2

+ğ›¾E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ + 2ğ›¾ (ï¸€(1 âˆ’ ğœƒ) E [ï¸€ğ‘‰ ğ‘¡]ï¸€ + ğ´ğ‘Ÿğ‘¡ + ğ¶E [ï¸€ğ‘ƒ ğ‘¡]ï¸€)ï¸€ ğœƒ

â‰¤

ğ›¿ğ‘¡

+

2ğ›¾

E

[ï¸€ğ‘‰

ğ‘¡]ï¸€

(ï¸‚ 1

âˆ’

ğœƒ )ï¸‚

ğœƒ

2

âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ 2ğ›¾ğ´ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

2

2ğ›¾ 2 ğœƒ

+ğ›¾

(ï¸‚ 1

+

2ğ¶

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

,

ğœƒ

where

ğ´

d=ef

ğ›½

(ï¸ 2ğ¿Ìƒï¸€2

+

)ï¸ â„’Ìƒï¸€2 ,

ğ¶

d=ef

2ğ›½ğ‘max.

Then adding the above inequality with a ğ‘2mğ›¾in (ï¸€1 + 2ğœƒğ¶ )ï¸€ multiple of (40), we get

41

EF21 with Bells & Whistles

Oct 6, 2021

E [ï¸€Î¨ğ‘¡+1]ï¸€

=

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + 2ğ›¾ E [ï¸€ğ‘‰ ğ‘¡+1]ï¸€ +

2ğ›¾

(ï¸‚ 1

+

2ğ¶

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡+1]ï¸€

ğœƒ

ğ‘min

ğœƒ

â‰¤ ğ›¿ğ‘¡ + ğ›¾ E [ï¸€ğ‘‰ ğ‘¡]ï¸€ (ï¸‚1 âˆ’ ğœƒ )ï¸‚ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ 2ğ›¾ğ´ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒ

22

2ğ›¾ 2 ğœƒ

+ğ›¾

(ï¸‚ 1

+

2ğ¶

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

ğœƒ

2ğ›¾ +

(ï¸‚ 1

+

2ğ¶

)ï¸‚

(ï¸ (1

âˆ’

ğ‘min)E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

+

â„’Ìƒï¸€2E

)ï¸ [ï¸€ğ‘…ğ‘¡]ï¸€

ğ‘min

ğœƒ

â‰¤

E

[ï¸€ğ›¿ğ‘¡]ï¸€

+

2ğ›¾

E

[ï¸€ğ‘‰

ğ‘¡]ï¸€

(ï¸‚ 1

âˆ’

ğœƒ

)ï¸‚

+

2ğ›¾

(ï¸‚ 1

+

2ğ¶

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

(ï¸ 1

âˆ’

ğ‘min

)ï¸

ğœƒ

2

ğ‘min

ğœƒ

2

ğ›¾

[ï¸ âƒ¦

âˆ’ E âˆ‡ğ‘“

(ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦2]ï¸

âˆ’

(ï¸‚

1

ğ¿ 2ğ›¾ğ´

âˆ’âˆ’

âˆ’

2ğ›¾

(ï¸‚ 1

+

2ğ¶

)ï¸‚

)ï¸‚ â„’2

E

[ï¸€ğ‘…ğ‘¡]ï¸€

.

âƒ¦

âƒ¦

Ìƒï¸€

2

2ğ›¾ 2 ğœƒ ğ‘min

ğœƒ

(46)

PL inequality implies that ğ›¿ğ‘¡ âˆ’ ğ›¾2 â€–âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2 â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ›¿ğ‘¡. In view of the above inequality and our assumption on the stepsize ( ğ›¾ â‰¤ 2ğœƒğœ‡ , ğ›¾ â‰¤ ğ‘2mğœ‡in ) , we get

E [ï¸€Î¨ğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)E [ï¸€Î¨ğ‘¡]ï¸€ âˆ’ (ï¸‚ 1

ğ¿ 2ğ›¾ğ´

âˆ’âˆ’

âˆ’

2ğ›¾

(ï¸‚ 1

+

2ğ¶

)ï¸‚

)ï¸‚ â„’2

E

[ï¸€ğ‘…ğ‘¡]ï¸€

.

Ìƒï¸€

2ğ›¾ 2 ğœƒ ğ‘min

ğœƒ

The coefï¬cient in front of E [ğ‘…ğ‘¡] simpliï¬es after substitution by ğ´ and ğ¶

2ğ›¾ğ´ +

2ğ›¾

(ï¸‚ 1

+

2ğ¶

)ï¸‚

â„’2

=

4ğ›½ ğ¿2 + (ï¸‚ 2ğ›½ +

2

+ 8ğ›½ ğ‘max )ï¸‚ â„’2

Ìƒï¸€

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğ‘min

ğœƒ

ğœƒ

ğœƒ ğ‘min ğœƒ ğ‘min

â‰¤

4ğ›½ ğ¿2

+

(ï¸‚ 5ğ›½ 2

ğ‘max

+

1

)ï¸‚ â„’2.

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğœƒ ğ‘min ğ‘min

Thus by Lemma 15 and the stepsize choice

(ï¸ƒ âˆšï¸ƒ

)ï¸ƒâˆ’1

0<ğ›¾ â‰¤ ğ¿+

8ğ›½ ğ¿2

+

(ï¸‚ 5ğ›½ 4

ğ‘max

+

1 )ï¸‚ â„’2

(47)

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğœƒ ğ‘min ğ‘min

the last term in (46) is not positive.

E [ï¸€Î¨ğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)E [ï¸€Î¨ğ‘¡]ï¸€ . It remains to unroll the recurrence.

Corollary 11. Let assumptions of Theorem 6 hold,

ğ‘£ğ‘–0 = ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

{ï¸‚ ğœƒ ğ‘min }ï¸‚

ğ›¾ = min ğ›¾0, ,

,

2ğœ‡ 2ğœ‡

(ï¸ƒ âˆšï¸ƒ

)ï¸ƒâˆ’1

ğ›¾ = ğ¿+

8ğ›½ ğ¿2

+

(ï¸‚ 5ğ›½ 4

ğ‘max

+

1 )ï¸‚ â„’2

,

0

Ìƒï¸€

Ìƒï¸€

ğœƒ

ğœƒ ğ‘min ğ‘min

ğ‘ğ‘– = ğœğ‘– , ğ‘– = 1, . . . , ğ‘›. ğœğ‘– + ğ‘šğ‘–

Then, after ğ‘‡ iterations of EF21-PAGE we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ inf ]ï¸€ â‰¤ ğœ€. It requires

(ï¸ƒ (ï¸ƒ

)ï¸ƒ

)ï¸ƒ

1 ğ¿Ìƒï¸€ + â„’Ìƒï¸€âˆšï¸‚ ğ‘max âˆš

(ï¸‚ ğ›¿0 )ï¸‚

ğ‘‡ =ğ’ª

+ ğ‘šmaxâ„’Ìƒï¸€ ln

ğœ‡

ğ›¼

ğ‘min

ğœ€

42

EF21 with Bells & Whistles

Oct 6, 2021

iterations/communications rounds,

(ï¸ƒ

(ï¸ƒ

)ï¸ƒ

)ï¸ƒ

ğœğ‘– ğ¿Ìƒï¸€ + â„’Ìƒï¸€âˆšï¸‚ ğ‘max âˆš

(ï¸‚ ğ›¿0 )ï¸‚

#gradğ‘– = ğ’ª ğ‘šğ‘– + ğœ‡

ğ›¼

+ ğ‘šmaxâ„’Ìƒï¸€ ln

ğ‘min

ğœ€

stochastic oracle calls for worker ğ‘–, and

(ï¸ƒ

(ï¸ƒ

)ï¸ƒ

)ï¸ƒ

ğœ ğ¿Ìƒï¸€ + â„’Ìƒï¸€âˆšï¸‚ ğ‘max âˆš

(ï¸‚ ğ›¿0 )ï¸‚

#grad = ğ’ª ğ‘š +

+ ğ‘šmaxâ„’Ìƒï¸€ ln

ğœ‡

ğ›¼

ğ‘min

ğœ€

stochastic

oracle

calls

per

worker

on

average,

where

ğœ

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğœğ‘–

,

ğ‘š

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘šğ‘–

,

ğ‘šmax

=

maxğ‘–=1,...,ğ‘› ğ‘šğ‘–, ğ‘max = maxğ‘–=1,...,ğ‘› ğ‘ğ‘–, ğ‘min = minğ‘–=1,...,ğ‘› ğ‘ğ‘–.

43

EF21 with Bells & Whistles

Oct 6, 2021

F PARTIAL PARTICIPATION

In this section, we provide an option for partial participation of the clients â€“ a feature important in federated learning. Most of the works in compressed distributed optimization deal with full worker participation, i.e., the case when all clients are involved in computation and communication at every iteration. However, in the practice of federated learning, only a subset of clients are allowed to participate at each training round. This limitation comes mainly due to the following two reasons. First, clients (e.g., mobile devices) may wish to join or leave the network randomly. Second, it is often prohibitive to wait for all available clients since stragglers can signiï¬cantly slow down the training process. Although many existing works (Gorbunov et al., 2021; HorvÃ¡th & RichtÃ¡rik, 2021; Philippenko & Dieuleveut, 2020; Karimireddy et al., 2020; Yang et al., 2021; Cho et al., 2020) allow for partial participation, they assume either unbiased compressors or no compression at all. We provide a simple analysis of partial participation, which works with biased compressors and builds upon the EF21 mechanism.
The modiï¬ed method (Algorithm 4) is called EF21-PP . At each iteration of EF21-PP , the master samples a subset ğ‘†ğ‘¡ of clients (nodes), which are required to perform computation. Note, that all other clients (nodes) ğ‘– âˆˆ/ ğ‘†ğ‘¡ participate neither in the computation nor in communication at iteration ğ‘¡.
We allow for an arbitrary sampling strategy of a subset ğ‘†ğ‘¡ at the master node. The only requirement is that Prob (ğ‘– âˆˆ ğ‘†ğ‘¡) = ğ‘ğ‘– > 0 for all ğ‘– = 1, . . . , ğ‘›, which is often referred to as a proper arbitrary sampling.9 Clearly, many poplular sampling procedures fell into this setting, for instance, independent sampling with/without replacement, ğœ -nice sampling. We do not discuss particular sampling strategies here, more on samplings can be found in (Qu & RichtÃ¡rik, 2014).

Algorithm 4 EF21-PP (EF21 with partial participation)

1:

Input:

starting point ğ‘¥0

âˆˆ Rğ‘‘; ğ‘”ğ‘–0

âˆˆ Rğ‘‘

for ğ‘– =

1, . . . , ğ‘› (known by nodes); ğ‘”0

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–0

(known by master); learning rate ğ›¾ > 0

2: for ğ‘¡ = 0,1, 2, . . . , ğ‘‡ âˆ’ 1 do

3: Master computes ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

4: Master samples a subset ğ‘†ğ‘¡ of nodes (|ğ‘†ğ‘¡| â‰¤ ğ‘›) such that Prob (ğ‘– âˆˆ ğ‘†ğ‘¡) = ğ‘ğ‘– 5: Master broadcasts ğ‘¥ğ‘¡+1 to the nodes with ğ‘– âˆˆ ğ‘†ğ‘¡

6: for all nodes ğ‘– = 1, . . . , ğ‘› in parallel do

7:

if ğ‘– âˆˆ ğ‘†ğ‘¡ then

8:

Compress ğ‘ğ‘¡ğ‘– = ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡) and send ğ‘ğ‘¡ğ‘– to the master

9:

Update local state ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

10:

end if

11:

if ğ‘– âˆˆ/ ğ‘†ğ‘¡ then

12:

Do not change local state ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡

13:

end if

14: end for

15: Master updates ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡, ğ‘ğ‘¡ğ‘– = 0 for ğ‘– âˆˆ/ ğ‘†ğ‘¡

16:

Master

computes

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

via

ğ‘”ğ‘¡+1

=

ğ‘”ğ‘¡

+

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ğ‘¡ğ‘–

17: end for

Lemma 5. Then for Algorithm 4 holds

E

[ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğœƒğ‘)E

[ï¸€ğºğ‘¡]ï¸€

+

ğµE

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

(48)

with ğœƒğ‘

def
=

ğœŒğ‘ğ‘šğ‘–ğ‘› + ğœƒğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğœŒ âˆ’ (ğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğ‘ğ‘šğ‘–ğ‘›), ğµ

def
=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

(ï¸€ğ›½ğ‘ğ‘–

+

(ï¸€1

+

ğœŒâˆ’1)ï¸€

(1

âˆ’

ğ‘ğ‘–))ï¸€

ğ¿2ğ‘– ,

ğ‘ğ‘šğ‘ğ‘¥ d=ef max1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğ‘ğ‘šğ‘–ğ‘› d=ef min1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğœƒ = 1 âˆ’ (1 + ğ‘ )(1 âˆ’ ğ›¼), ğ›½ = (ï¸€1 + 1ğ‘  )ï¸€ (1 âˆ’ ğ›¼) and small enough ğœŒ, ğ‘  > 0.

Proof. By (13) in Lemma 1, we have for all ğ‘– âˆˆ ğ‘†ğ‘¡

E

[ï¸€ğºğ‘¡ğ‘–+1

|

ğ‘–

âˆˆ

ğ‘†ğ‘¡]ï¸€

â‰¤

(1

âˆ’

ğœƒ)E

[ï¸€ğºğ‘¡ğ‘– ]ï¸€

+

ğ›½ğ¿2ğ‘– E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2

|

ğ‘–

âˆˆ

]ï¸ ğ‘†ğ‘¡

(49)

9It is natural to focus on proper samplings only since otherwise there is a node ğ‘–, which never communicaties. This would be a critical issue when trying to minimize (1) as we do not assume any similarity between ğ‘“ğ‘–(Â·).

44

EF21 with Bells & Whistles

Oct 6, 2021

with ğœƒ = 1 âˆ’ (1 + ğ‘ )(1 âˆ’ ğ›¼), ğ›½ = (ï¸€1 + 1ğ‘  )ï¸€ (1 âˆ’ ğ›¼) and arbitrary ğ‘  > 0.

Deï¬ne ğ‘Š ğ‘¡ d=ef {ğ‘”1ğ‘¡ , . . . , ğ‘”ğ‘›ğ‘¡ , ğ‘¥ğ‘¡, ğ‘¥ğ‘¡+1} and let ğ‘– âˆˆ/ ğ‘†ğ‘¡, then

E [ï¸€ğºğ‘¡ğ‘–+1 | ğ‘– âˆˆ/ ğ‘†ğ‘¡]ï¸€

=

E

[ï¸€ E

[ï¸€ğºğ‘¡ğ‘–+1

|

ğ‘Š ğ‘¡]ï¸€

|

ğ‘–

âˆˆ/

ğ‘†ğ‘¡]ï¸€

=

E

[ï¸ E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡+1

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)âƒ¦âƒ¦2

|

]ï¸ ğ‘Šğ‘¡

|

ğ‘–

âˆˆ/

]ï¸ ğ‘†ğ‘¡

â‰¤

(1

+

ğœŒ)E

[ï¸ E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2

|

ğ‘Š

]ï¸
ğ‘¡

|

ğ‘–

âˆˆ/

]ï¸ ğ‘†ğ‘¡

+

(ï¸€1

+

ğœŒâˆ’1)ï¸€

E

[ï¸ E

[ï¸ âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2

|

]ï¸ ğ‘Šğ‘¡

|

ğ‘–

âˆˆ/

]ï¸ ğ‘†ğ‘¡

â‰¤ (1 + ğœŒ)E [ï¸€ğºğ‘¡ğ‘–]ï¸€

+

(ï¸€1

+

ğœŒâˆ’1)ï¸€

E

[ï¸ âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)âƒ¦âƒ¦2

|

ğ‘–

âˆˆ/

]ï¸ ğ‘†ğ‘¡

â‰¤

(1

+

ğœŒ)E

[ï¸€ğºğ‘¡ğ‘– ]ï¸€

+

(ï¸€1

+

ğœŒâˆ’1)ï¸€

ğ¿2ğ‘– E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

.

(50)

Combining (49) and (50), we get

E [ï¸€ğºğ‘¡+1]ï¸€

= =
(49),(50)
â‰¤
(ğ‘–)
â‰¤
=

1

ğ‘›
âˆ‘ï¸

[ï¸€ ğ‘¡+1]ï¸€

ğ‘› E ğºğ‘–

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

[ï¸€ ğ‘¡+1

]ï¸€ 1 âˆ‘ğ‘›ï¸

[ï¸€ ğ‘¡+1

]ï¸€

ğ‘› ğ‘ğ‘–E ğºğ‘– | ğ‘– âˆˆ ğ‘†ğ‘¡ + ğ‘› (1 âˆ’ ğ‘ğ‘–) E ğºğ‘– | ğ‘– âˆˆ/ ğ‘†ğ‘¡

ğ‘–=1

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

(ï¸ƒ

1

ğ‘›
âˆ‘ï¸

)ï¸ƒ [ï¸

2]ï¸

(1 âˆ’ ğœƒ) ğ‘›

ğ‘ğ‘–E [ï¸€ğºğ‘¡ğ‘–]ï¸€ + ğ›½ ğ‘›

ğ‘ğ‘–ğ¿2ğ‘– E âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦

ğ‘–=1

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

+ (1 + ğœŒ)

(1 âˆ’ ğ‘ğ‘–) E [ï¸€ğºğ‘¡ğ‘–]ï¸€

ğ‘›

ğ‘–=1

(ï¸ƒ

1

ğ‘›
âˆ‘ï¸

)ï¸ƒ [ï¸

2]ï¸

+ (ï¸€1 + ğœŒâˆ’1)ï¸€ ğ‘›

(1 âˆ’ ğ‘ğ‘–)ğ¿2ğ‘– E âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

(ï¸ƒ

1

ğ‘›
âˆ‘ï¸

)ï¸ƒ [ï¸

2]ï¸

(1 âˆ’ ğœƒ)ğ‘ğ‘šğ‘ğ‘¥ ğ‘› E [ï¸€ğºğ‘¡ğ‘–]ï¸€ + ğ›½ ğ‘› ğ‘ğ‘–ğ¿2ğ‘– E âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦

ğ‘–=1

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

+ (1 + ğœŒ) (1 âˆ’ ğ‘ğ‘šğ‘–ğ‘›)

E [ï¸€ğºğ‘¡ğ‘–]ï¸€

ğ‘›

ğ‘–=1

(ï¸ƒ

1

ğ‘›
âˆ‘ï¸

)ï¸ƒ [ï¸

2]ï¸

+ (ï¸€1 + ğœŒâˆ’1)ï¸€ ğ‘›

(1 âˆ’ ğ‘ğ‘–)ğ¿2ğ‘– E âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦

ğ‘–=1

(ï¸‚

)ï¸‚

(1 âˆ’ ğœƒ)ğ‘ğ‘šğ‘ğ‘¥ + (1 + ğœŒ)(1 âˆ’ ğ‘ğ‘šğ‘–ğ‘›) E [ï¸€ğºğ‘¡]ï¸€

(ï¸ƒ

1

ğ‘›
âˆ‘ï¸

)ï¸ƒ

[ï¸

2]ï¸

+ ğ‘›

(ï¸€ğ›½ğ‘ğ‘– + (ï¸€1 + ğœŒâˆ’1)ï¸€ (1 âˆ’ ğ‘ğ‘–))ï¸€ ğ¿2ğ‘– E âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦

ğ‘–=1

(ï¸‚

)ï¸‚

=

1 âˆ’ (ğœŒğ‘ğ‘šğ‘–ğ‘› + ğœƒğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğœŒ âˆ’ (ğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğ‘ğ‘šğ‘–ğ‘›)) E [ï¸€ğºğ‘¡]ï¸€

(ï¸ƒ

1

ğ‘›
âˆ‘ï¸

)ï¸ƒ

[ï¸

2]ï¸

+ ğ‘›

(ï¸€ğ›½ğ‘ğ‘– + (ï¸€1 + ğœŒâˆ’1)ï¸€ (1 âˆ’ ğ‘ğ‘–))ï¸€ ğ¿2ğ‘– E âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦ .

ğ‘–=1

=

(1

âˆ’

ğœƒğ‘)

E

[ï¸€ğºğ‘¡]ï¸€

+

ğµE

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

,

45

EF21 with Bells & Whistles

Oct 6, 2021

Lemma 6. [To simplify the rates for partial participation] Let ğµ and ğœƒğ‘ be deï¬ned as in Theorems 7 and Theorems 8, and let ğ‘ğ‘– = ğ‘ > 0 for all ğ‘– = 1, . . . , ğ‘› . Then there exist ğœŒ, ğ‘  > 0 such that
ğ‘ğ›¼ ğœƒğ‘ â‰¥ 2 , (51)

(ï¸ƒ )ï¸ƒ2

ğµ 4ğ¿Ìƒï¸€

0< â‰¤

.

(52)

ğœƒğ‘

ğ‘ğ›¼

Proof. Under the assumption that ğ‘ğ‘– = ğ‘ for all ğ‘– = 1, . . . , ğ‘›, the constants simplify to

ğœƒğ‘ = ğœŒğ‘ + ğœƒğ‘ âˆ’ ğœŒ,

ğµ = (ï¸€ğ›½ğ‘ + (ï¸€1 + ğœŒâˆ’1)ï¸€ (1 âˆ’ ğ‘))ï¸€ ğ¿Ìƒï¸€2,

ğ‘ğ‘šğ‘ğ‘¥ = ğ‘ğ‘šğ‘–ğ‘› = ğ‘.

Case I: let ğ›¼ = 1, ğ‘ = 1, then the result holds trivially. âˆš
Case II: let 0 < ğ›¼ < 1, ğ‘ = 1, then ğµ = ğ›½ğ¿Ìƒï¸€2 , ğœƒğ‘ = ğœƒ = 1 âˆ’ 1 âˆ’ ğ›¼ â‰¥ ğ›¼2 and (52) follows by Lemma 17.

Case III: let ğ›¼ = 1, and 0 < ğ‘ < 1, then ğœƒ = 1 , ğ›½ = 0 , ğµ = (ï¸€1 + ğœŒâˆ’1)ï¸€ (1âˆ’ğ‘)ğ¿Ìƒï¸€2, ğœƒğ‘ = ğ‘âˆ’ğœŒ(1âˆ’ğ‘). Then the choice ğœŒ = 2(1ğ‘âˆ’ğ›¼ğ‘) simpliï¬es
ğ‘ ğœƒğ‘ = 2 ,

ğµ (ï¸€1 + ğœŒâˆ’1)ï¸€ (1 âˆ’ ğ‘)ğ¿Ìƒï¸€2 2(1 âˆ’ ğ‘)ğ¿Ìƒï¸€2 (ï¸‚ 2 )ï¸‚ 4ğ¿Ìƒï¸€2

=

=

ğœƒğ‘

ğ‘ âˆ’ ğœŒ(1 âˆ’ ğ‘)

ğ‘

ğ‘ âˆ’ 1 â‰¤ ğ‘2 .

Case IV: let 0 < ğ›¼ < 1,and 0 < ğ‘ < 1.Then the choice of constants ğœƒ = 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğ‘ ), ğ›½ = (1 âˆ’ ğ›¼) (ï¸€1 + 1ğ‘  )ï¸€, ğœŒ = 4(1ğ‘âˆ’ğ›¼ğ‘) , ğ‘  = 4(1ğ›¼âˆ’ğ›¼) yields

ğ‘ğœŒ + ğœƒğ‘ âˆ’ ğœŒ = ğ‘(ğœŒ + 1 âˆ’ (1 âˆ’ ğ›¼) (1 + ğ‘ )) âˆ’ ğœŒ

= ğ‘ğ›¼ âˆ’ ğ‘(1 âˆ’ ğ›¼)ğ‘  âˆ’ (1 âˆ’ ğ‘)ğœŒ

1

= ğ‘ğ›¼.

(53)

2

Also

1 4 âˆ’ 3ğ›¼ 4

1 4(1 âˆ’ ğ‘) + ğ›¼ğ‘ 4 âˆ’ ğ‘(4 âˆ’ ğ›¼) 4

1+ =

â‰¤ , 1+ =

=

â‰¤.

ğ‘ 

ğ›¼

ğ›¼

ğœŒ

ğ‘ğ›¼

ğ‘ğ›¼

ğ‘ğ›¼

Thus

(ï¸ )ï¸

(ï¸ )ï¸

ğµ ğ‘ğ›½ + (1 âˆ’ ğ‘) 1 + ğœŒ1

ğ‘(1 âˆ’ ğ›¼) (ï¸€1 + 1ğ‘  )ï¸€ + (1 âˆ’ ğ‘) 1 + ğœŒ1

=

ğ¿Ìƒï¸€2 =

ğ¿Ìƒï¸€2

ğœƒğ‘

ğ‘(ğœŒ + ğœƒ) âˆ’ ğœŒ

12 ğ‘ğ›¼

ğ‘(1 âˆ’ ğ›¼) 4 + (1 âˆ’ ğ‘) 4

â‰¤

ğ›¼

ğ‘ğ›¼ ğ¿Ìƒï¸€2

12 ğ‘ğ›¼

4+ 4 â‰¤ ğ›¼ ğ‘ğ›¼ ğ¿Ìƒï¸€2
12 ğ‘ğ›¼

8
â‰¤ 1ğ‘ğ‘ğ›¼ğ›¼ ğ¿Ìƒï¸€2
2

16ğ¿Ìƒï¸€2

â‰¤ ğ‘2ğ›¼2 .

(54)

46

EF21 with Bells & Whistles

Oct 6, 2021

F.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 7. Let Assumption 1 hold, and let the stepsize in Algorithm 4 be set as

(ï¸ƒ âˆšï¸ƒ )ï¸ƒâˆ’1

ğµ

0<ğ›¾ â‰¤ ğ¿+

.

(55)

ğœƒğ‘

Fix ğ‘‡ â‰¥ 1 and let ğ‘¥Ë†ğ‘‡ be chosen from the iterates ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random. Then

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2 (ï¸€ğ‘“ (ğ‘¥0) âˆ’ ğ‘“ inf )ï¸€ + E [ï¸€ğº0]ï¸€ (56)

ğ›¾ğ‘‡

ğœƒğ‘ğ‘‡

with ğœƒğ‘

=

ğœŒğ‘ğ‘šğ‘–ğ‘› + ğœƒğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğœŒ âˆ’ (ğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğ‘ğ‘šğ‘–ğ‘›), ğµ

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

(ï¸€ğ›½ğ‘ğ‘–

+

(ï¸€1

+

ğœŒâˆ’1)ï¸€

(1

âˆ’

ğ‘ğ‘–))ï¸€

ğ¿2ğ‘– ,

ğ‘ğ‘šğ‘ğ‘¥ = max1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğ‘ğ‘šğ‘–ğ‘› = min1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğœƒ = 1 âˆ’ (1 + ğ‘ )(1 âˆ’ ğ›¼), ğ›½ = (ï¸€1 + 1ğ‘  )ï¸€ (1 âˆ’ ğ›¼) and

ğœŒ, ğ‘  > 0.

Proof. By (20), we have

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ . (57)

2

2ğ›¾ 2

2

Lemma 5 states that

E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒğ‘)E [ï¸€ğºğ‘¡]ï¸€ + ğµE [ï¸€ğ‘…ğ‘¡]ï¸€

(58)

with ğœƒğ‘

=

ğœŒğ‘ğ‘šğ‘–ğ‘› + ğœƒğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğœŒ âˆ’ (ğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğ‘ğ‘šğ‘–ğ‘›), ğµ

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

(ï¸€ğ›½ğ‘ğ‘–

+

(ï¸€1

+

ğœŒâˆ’1)ï¸€

(1

âˆ’

ğ‘ğ‘–))ï¸€

ğ¿2ğ‘– ,

ğ‘ğ‘šğ‘ğ‘¥ = max1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğ‘ğ‘šğ‘–ğ‘› = min1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğœƒ = 1 âˆ’ (1 + ğ‘ )(1 âˆ’ ğ›¼), ğ›½ = (ï¸€1 + 1ğ‘  )ï¸€ (1 âˆ’ ğ›¼) and

small enough ğœŒ, ğ‘  > 0.

Adding (57) with a 2ğ›¾ğœƒ2 multiple of (58) and rearranging terms in the right hand side, we have

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€

2ğœƒğ‘

2ğœƒğ‘

âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğ›¾ğµ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

2

2ğ›¾ 2 2ğœƒ

â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ + ğ›¾ E [ï¸€ğºğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ .

2ğœƒğ‘

2

The last inequality follows from the bound ğ›¾2 ğœƒğµğ‘ + ğ¿ğ›¾ â‰¤ 1, which holds because of Lemma 15 and our assumption on the stepsize. By summing up inequalities for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1, we get

0 â‰¤ E [ï¸ğ›¿ğ‘‡ + ğ›¾ ğºğ‘‡ ]ï¸ â‰¤ ğ›¿0 + ğ›¾ E [ï¸€ğº0]ï¸€ âˆ’ ğ›¾ ğ‘‡âˆ‘âˆ’ï¸1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ .

2ğœƒ

2ğœƒ

2

ğ‘¡=0

Multiplying both sides by ğ›¾2ğ‘‡ , after rearranging we get

ğ‘‡âˆ‘âˆ’ï¸1 1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ â‰¤ 2ğ›¿0 + E [ï¸€ğº0]ï¸€ .

ğ‘‡

ğ›¾ğ‘‡

ğœƒğ‘‡

ğ‘¡=0

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦2 âƒ¦

]ï¸

,

where

ğ‘¥Ë†ğ‘‡

is

chosen

from ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random.

Corollary 12.

Let assumptions of Theorem 7 hold,

ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

(ï¸ƒ âˆšï¸ƒ )ï¸ƒâˆ’1

ğµ

ğ›¾ = ğ¿+

,

ğœƒğ‘

ğ‘ğ‘– = ğ‘, ğ‘– = 1, . . . , ğ‘›,

47

EF21 with Bells & Whistles

Oct 6, 2021

where ğµ and ğœƒğ‘ are given in Theorem 7. Then, after ğ‘‡ iterations/communication rounds of EF21-PP

we

have

E

[ï¸ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2.

It

requires

(ï¸ƒ )ï¸ƒ

ğ¿Ìƒï¸€ğ›¿0

ğ‘‡ = #grad = ğ’ª ğ‘ğ›¼ğœ€2

(59)

iterations/communications rounds/gradint computations at each node.

Proof. Let ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘› , then ğº0 = 0 and by Theorem 7

(ï¸ƒ

âˆšï¸ƒ )ï¸ƒ

(ï¸ƒ

)ï¸ƒ

(ğ‘–) 2ğ›¿0 (ğ‘–ğ‘–) 2ğ›¿0

ğµ (ğ‘–ğ‘–ğ‘–) 2ğ›¿0

4ğ¿Ìƒï¸€

#grad = ğ‘‡ â‰¤ ğ›¾ğœ€2 â‰¤ ğœ€2

ğ¿ + ğ¿Ìƒï¸€ ğœƒğ‘

â‰¤ ğœ€2 ğ¿ + ğ‘ğ›¼

(ï¸ƒ

)ï¸ƒ

(ï¸ƒ

)ï¸ƒ

2ğ›¿0

4ğ¿Ìƒï¸€ (ğ‘–ğ‘£) 2ğ›¿0 ğ¿Ìƒï¸€ 4ğ¿Ìƒï¸€

5ğ¿Ìƒï¸€ğ›¿0

â‰¤ ğœ€2 ğ¿ + ğ‘ğ›¼ â‰¤ ğœ€2 ğ‘ğ›¼ + ğ‘ğ›¼ = ğ‘ğ›¼ğœ€2 ,

where (ğ‘–) is due to the rate (56) given by Theorem 7. In two (ğ‘–ğ‘–) we use the largest possible stepsize (55), in (ğ‘–ğ‘–ğ‘–) we utilize Lemma 6, and (ğ‘–ğ‘£) follows by the inequalities ğ›¼ â‰¤ 1, ğ‘ â‰¤ 1 and ğ¿ â‰¤ ğ¿Ìƒï¸€.

F.2 CONVERGENCE UNDER POLYAK-ÅOJASIEWICZ CONDITION

Theorem 8. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 4 be set as

â§ (ï¸ƒ

âˆšï¸ƒ )ï¸ƒâˆ’1

â«

0 < ğ›¾ â‰¤ min â¨ ğ¿ + 2ğµ , ğœƒğ‘ â¬ . (60)

â©

ğœƒğ‘

2ğœ‡ â­

Let Î¨ğ‘¡ d=ef ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥â‹†) + ğœƒğ›¾ğ‘ ğºğ‘¡. Then for any ğ‘‡ â‰¥ 0, we have

E [ï¸€Î¨ğ‘‡ ]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ E [ï¸€Î¨0]ï¸€

(61)

with ğœƒğ‘

=

ğœŒğ‘ğ‘šğ‘–ğ‘› + ğœƒğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğœŒ âˆ’ (ğ‘ğ‘šğ‘ğ‘¥ âˆ’ ğ‘ğ‘šğ‘–ğ‘›), ğµ

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

(ï¸€ğ›½ğ‘ğ‘–

+

(ï¸€1

+

ğœŒâˆ’1)ï¸€

(1

âˆ’

ğ‘ğ‘–))ï¸€

ğ¿2ğ‘– ,

ğ‘ğ‘šğ‘ğ‘¥ = max1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğ‘ğ‘šğ‘–ğ‘› = min1â‰¤ğ‘–â‰¤ğ‘› ğ‘ğ‘–, ğœƒ = 1 âˆ’ (1 + ğ‘ )(1 âˆ’ ğ›¼), ğ›½ = (ï¸€1 + 1ğ‘  )ï¸€ (1 âˆ’ ğ›¼) and

ğœŒ, ğ‘  > 0.

Proof. Following the same steps as in the proof of Theorem 2, but using (58), and assumption on the stepsize (60), we obtain the result.

Corollary 13. Let assumptions of Theorem 8 hold,

ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

â§ (ï¸ƒ

âˆšï¸ƒ )ï¸ƒâˆ’1

â«

â¨

2ğµ

ğ›¾ = min ğ¿ +

â©

ğœƒğ‘

, ğœƒğ‘ â¬ , 2ğœ‡ â­

ğ‘ğ‘– = ğ‘, ğ‘– = 1, . . . , ğ‘›,

where ğµ and ğœƒğ‘ are given in Theorem 8. Then, after ğ‘‡ iterations/communication rounds of EF21-PP we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥â‹†)]ï¸€ â‰¤ ğœ€. It requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ (ï¸‚ ğ›¿0 )ï¸‚

ğ‘‡ = #grad = ğ’ª

log

(62)

ğ‘ğ›¼ğœ‡

ğœ€

iterations/communications rounds/gradint computations at each node.

Proof. The proof is the same as for Corollary 3. The only difference is that Lemma 6 is needed to upper bound the quantities 1/ğœƒğ‘ and ğµ/ğœƒğ‘, which appear in Theorem 8.

48

EF21 with Bells & Whistles

Oct 6, 2021

G BIDIRECTIONAL COMPRESSION

In the majority of applications, the uplink (Client â†’ Server) communication is the bottleneck.

However, in some settings the downlink (Server â†’ Client) communication can also slowdown

training. Tang et al. (2020) construct a mechanism which allows bidirectional biased compression.

(ï¸ )ï¸

Their method builds upon the original EF meachanism and they prove ğ’ª

1
2

rate for general

ğ‘‡ /3

nonconvex objectives. However, the main defï¬ciency of this approach is that it requires an additional

assumption

of

bounded

magnitude

of

error

(there

exists

âˆ†

>

0

such

that

[ï¸ E â€–ğ’(ğ‘¥)

âˆ’

ğ‘¥â€–2]ï¸

â‰¤

âˆ†

for all ğ‘¥). In this section, we lift this limitation and propose a new method EF21-BC (Algorithm 5), which enjoys the desirable ğ’ª (ï¸€ ğ‘‡1 )ï¸€, and does not rely on additional assumptions.

Algorithm 5 EF21-BC (EF21 with bidirectional biased compression)

1:

Input:

starting

point

ğ‘¥0

âˆˆ

Rğ‘‘;

ğ‘”0,

ğ‘0,

ğ‘”Ìƒï¸€ğ‘–0

âˆˆ

Rğ‘‘

for

ğ‘–

=

1, . . . , ğ‘›

(known

by

nodes);

ğ‘”0 Ìƒï¸€

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”Ìƒï¸€ğ‘–0

(known

by

master)

;

learning

rate

ğ›¾

>

0

2: for ğ‘¡ = 0,1, 2, . . . , ğ‘‡ âˆ’ 1 do

3: Master updates ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡

4: for all nodes ğ‘– = 1, . . . , ğ‘› in parallel do

5:

Update ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡, ğ‘”ğ‘¡+1 = ğ‘”ğ‘¡ + ğ‘ğ‘¡,

6:

compress ğ‘ğ‘¡ğ‘– = ğ’ğ‘¤(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡), send ğ‘ğ‘¡ğ‘– to the master, and

7:

update local state ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1 = ğ‘”Ìƒï¸€ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

8: end for

9:

Master

computes

ğ‘”ğ‘¡+1 Ìƒï¸€

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1

via

ğ‘”ğ‘¡+1 Ìƒï¸€

=

ğ‘”ğ‘¡ Ìƒï¸€

+

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ğ‘¡ğ‘– ,

10: compreses ğ‘ğ‘¡+1 = ğ’ğ‘€ (ğ‘”ğ‘¡+1 âˆ’ ğ‘”ğ‘¡), broadcast ğ‘ğ‘¡+1 to workers ,

Ìƒï¸€

11: and updates ğ‘”ğ‘¡+1 = ğ‘”ğ‘¡ + ğ‘ğ‘¡+1

12: end for

Note that ğ’ğ‘€ and ğ’ğ‘¤ stand for contractive compressors of the type 1 of master and workers respectively. In general, different ğ›¼ğ‘€ and ğ›¼ğ‘¤ are accepted.

Notations

for

this

section:

ğ‘ƒğ‘–ğ‘¡

d=ef

â€–ğ‘”Ìƒï¸€ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2,

ğ‘ƒğ‘¡

d=ef

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ƒğ‘–ğ‘¡.

Lemma 7. Let Assumption 1 hold, ğ’ğ‘¤ be a contractive compressor, and ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1 be an EF21 estimator of âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1), i. e.

ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1 = ğ‘”Ìƒï¸€ğ‘–ğ‘¡ + ğ’ğ‘¤(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡)

(63)

for arbitrary ğ‘”Ìƒï¸€ğ‘–0 and all all ğ‘– = 1, . . . , ğ‘›, ğ‘¡ â‰¥ 0. Then

E [ï¸€ğ‘ƒ ğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒğ‘¤)E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ + ğ›½ğ‘¤ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ ,

(64)

def
where ğœƒğ‘¤ = 1 âˆ’ (1 âˆ’ ğ›¼ğ‘¤)(1 + ğ‘ ),

ğ›½ğ‘¤

def
=

(1

âˆ’

ğ›¼ğ‘¤ )

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

for any ğ‘  > 0.

Proof. The proof is the same as for Lemma 1.

Lemma 8. Let Assumption 1 hold, ğ’ğ‘€ , ğ’ğ‘¤ be contractive compressors. Let ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1 be an EF21 estimator of âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1), i. e.

ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1 = ğ‘”Ìƒï¸€ğ‘–ğ‘¡ + ğ’ğ‘¤(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡),

(65)

and

let

ğ‘”ğ‘¡+1

be

an

EF21

estimator

of

ğ‘”ğ‘¡+1 Ìƒï¸€

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”Ìƒï¸€ğ‘–ğ‘¡+1,

i.

e.

ğ‘”ğ‘¡+1 = ğ‘”ğ‘¡ + ğ’ğ‘€ (ğ‘”ğ‘¡+1 âˆ’ ğ‘”ğ‘¡)

(66)

Ìƒï¸€

for arbitrary ğ‘”0, ğ‘”Ìƒï¸€ğ‘–0 and all ğ‘– = 1, . . . , ğ‘›, ğ‘¡ â‰¥ 0. Then

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡+1âƒ¦âƒ¦2]ï¸

â‰¤

(1

âˆ’

ğœƒğ‘€

)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

+

8ğ›½ğ‘€

E

[ï¸€ğ‘ƒ

ğ‘¡]ï¸€

+

8ğ›½ğ‘€

ğ¿Ìƒï¸€2E

[ï¸€ğ‘…ğ‘¡]ï¸€

,

(67)

Ìƒï¸€

Ìƒï¸€

where

ğ‘”ğ‘¡

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡,

ğ‘”ğ‘¡ Ìƒï¸€

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”Ìƒï¸€ğ‘–ğ‘¡

,

ğœƒğ‘€

=

1 âˆ’ (1 âˆ’ ğ›¼ğ‘€ )(1 + ğœŒ),

ğ›½ğ‘€

=

(1 âˆ’ ğ›¼ğ‘€ ) (ï¸€1 +

ğœŒâˆ’1)ï¸€

for any ğœŒ > 0.

49

EF21 with Bells & Whistles

Oct 6, 2021

Proof. Similarly to the proof of Lemma 1, deï¬ne ğ‘Š ğ‘¡ d=ef {ğ‘”1ğ‘¡ , . . . , ğ‘”ğ‘›ğ‘¡ , ğ‘¥ğ‘¡, ğ‘¥ğ‘¡+1} and

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡+1âƒ¦âƒ¦2]ï¸

=

E

[ï¸ E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡+1âƒ¦âƒ¦2

|

]ï¸]ï¸ ğ‘Šğ‘¡

Ìƒï¸€

Ìƒï¸€

=

E

[ï¸ E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

+

ğ’ğ‘€ (ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡)

âˆ’

ğ‘”ğ‘¡+1âƒ¦âƒ¦2

|

]ï¸]ï¸ ğ‘Šğ‘¡

Ìƒï¸€

Ìƒï¸€

(8)
â‰¤

(1

âˆ’

ğ›¼ğ‘€

)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

Ìƒï¸€

(ğ‘–)
â‰¤

(1

âˆ’

ğ›¼ğ‘€

)(1

+

ğœŒ)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

Ìƒï¸€

+(1 âˆ’ ğ›¼ğ‘€ ) (ï¸€1 + ğœŒâˆ’1)ï¸€ âƒ¦âƒ¦ğ‘”ğ‘¡+1 âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2

Ìƒï¸€

Ìƒï¸€

=

(1

âˆ’

ğœƒğ‘€

)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

+

ğ›½ğ‘€

âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2

,

(68)

Ìƒï¸€

Ìƒï¸€

Ìƒï¸€

where (ğ‘–) follows by Youngâ€™s inequality (118), and in (ğ‘–ğ‘–) we use the deï¬nition of ğœƒğ‘€ and ğ›½ğ‘€ .

Further we bound the last term in (68). Recall that

1

ğ‘›
âˆ‘ï¸

ğ‘”ğ‘¡+1 = ğ‘”ğ‘¡ +

Ìƒï¸€

Ìƒï¸€

ğ‘ğ‘¡ğ‘– .

(69)

ğ‘›

ğ‘–=1

where

ğ‘ğ‘¡ğ‘–

=

ğ’ğ‘¤ (âˆ‡ğ‘“ğ‘– (ğ‘¥ğ‘¡+1 )

âˆ’

ğ‘”Ìƒï¸€ğ‘–ğ‘¡)

and

ğ‘”ğ‘¡ Ìƒï¸€

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”Ìƒï¸€ğ‘–ğ‘¡

.

Then

â¡ âƒ¦

ğ‘›

âƒ¦2â¤

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

(=69)

E

âƒ¦ âƒ¦ğ‘”ğ‘¡

+

1

âˆ‘ï¸ ğ‘ğ‘¡

âˆ’

âƒ¦ ğ‘”ğ‘¡âƒ¦

Ìƒï¸€

Ìƒï¸€

â£âƒ¦Ìƒï¸€ ğ‘› ğ‘– Ìƒï¸€ âƒ¦ â¦

âƒ¦

ğ‘–=1

âƒ¦

â¡ âƒ¦

ğ‘›

âƒ¦2â¤

= E â£âƒ¦âƒ¦âƒ¦ ğ‘›1 âˆ‘ï¸ ğ‘ğ‘¡ğ‘–âƒ¦âƒ¦âƒ¦ â¦

âƒ¦ ğ‘–=1 âƒ¦

(ğ‘–)

1

ğ‘›
âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡âƒ¦2]ï¸

â‰¤ ğ‘›

E âƒ¦ğ‘ğ‘–âƒ¦

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

= ğ‘›

E âƒ¦âƒ¦ğ‘ğ‘¡ğ‘– âˆ’ (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡)ï¸€ + (ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡)ï¸€âƒ¦âƒ¦

ğ‘–=1

(118)

1

ğ‘›
âˆ‘ï¸

[ï¸

[ï¸

2 ]ï¸]ï¸

â‰¤2 ğ‘›

E

E

âƒ¦ âƒ¦ğ’ğ‘¤

(ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

ğ‘”Ìƒï¸€ğ‘–ğ‘¡)ï¸€

âˆ’

(ï¸€âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1)

âˆ’

ğ‘”Ìƒï¸€ğ‘–ğ‘¡)ï¸€âƒ¦âƒ¦

| ğ‘Šğ‘¡

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

+2 ğ‘›

E

âƒ¦ âƒ¦âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

ğ‘”Ìƒï¸€ğ‘–ğ‘¡

âƒ¦ âƒ¦

ğ‘–=1

(8)

1

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

1

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

â‰¤ 2(1 âˆ’ ğ›¼ğ‘¤) ğ‘›

E âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”Ìƒï¸€ğ‘–ğ‘¡âƒ¦âƒ¦ + 2 ğ‘›

E

âƒ¦ âƒ¦âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

ğ‘”Ìƒï¸€ğ‘–ğ‘¡

âƒ¦ âƒ¦

ğ‘–=1

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

= 2(2 âˆ’ ğ›¼ğ‘¤) ğ‘›

E

âƒ¦ âƒ¦âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

ğ‘”Ìƒï¸€ğ‘–ğ‘¡

âƒ¦ âƒ¦

ğ‘–=1

(ğ‘–ğ‘–)

1

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

<4 ğ‘›

E

âƒ¦ âƒ¦âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡+1

)

âˆ’

ğ‘”Ìƒï¸€ğ‘–ğ‘¡

âƒ¦ âƒ¦

ğ‘–=1

1

ğ‘›
âˆ‘ï¸

[ï¸

2]ï¸

=4 ğ‘›

E âƒ¦âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡) âˆ’ (ï¸€ğ‘”Ìƒï¸€ğ‘–ğ‘¡ âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡))ï¸€âƒ¦âƒ¦

ğ‘–=1

(118) 1 âˆ‘ğ‘›ï¸ âƒ¦ ğ‘¡

ğ‘¡ âƒ¦2 1 âˆ‘ğ‘›ï¸ [ï¸âƒ¦

ğ‘¡+1

ğ‘¡ âƒ¦2]ï¸

â‰¤8 ğ‘›

âƒ¦ğ‘”Ìƒï¸€ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

+8 ğ‘›

E âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ ) âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

ğ‘–=1

ğ‘–=1

(ğ‘–ğ‘–ğ‘–) 1 âˆ‘ğ‘›ï¸ [ï¸âƒ¦ ğ‘¡

ğ‘¡ âƒ¦2]ï¸

[ï¸ 2 âƒ¦ ğ‘¡+1

ğ‘¡âƒ¦2]ï¸

â‰¤8 ğ‘›

E âƒ¦ğ‘”Ìƒï¸€ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦ + 8ğ¿Ìƒï¸€ E âƒ¦ğ‘¥ âˆ’ ğ‘¥ âƒ¦

ğ‘–=1

= 8E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ + 8ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ ,

(70)

50

EF21 with Bells & Whistles

Oct 6, 2021

where in (ğ‘–) we use (119), (ğ‘–ğ‘–) is due to ğ›¼ğ‘¤ > 0, (ğ‘–ğ‘–ğ‘–) holds by Assumption 1. In the last step we

apply

the

deï¬nition

of

ğ‘ƒğ‘¡

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

â€–ğ‘”Ìƒï¸€ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2,

and

ğ‘…ğ‘¡

=

âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2

Finally, plugging (70) into (68), we conclude the proof.

G.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 9. Let Assumption 1 hold, and let the stepsize in Algorithm 5 be set as

(ï¸ƒ

âˆšï¸ƒ

)ï¸ƒâˆ’1

16ğ›½ğ‘€ 2ğ›½ğ‘¤ (ï¸‚ 8ğ›½ğ‘€ )ï¸‚

0 < ğ›¾ â‰¤ ğ¿ + ğ¿Ìƒï¸€

+

1+

(71)

ğœƒğ‘€

ğœƒğ‘¤

ğœƒğ‘€

Fix ğ‘‡ â‰¥ 1 and let ğ‘¥Ë†ğ‘‡ be chosen from the iterates ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random. Then

E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡ )âƒ¦âƒ¦2]ï¸ â‰¤ 2E [ï¸€Î¨0]ï¸€ , (72) ğ›¾ğ‘‡

where

Î¨ğ‘¡

def
=

ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ inf

+

ğ›¾

â€–ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡â€–2 + ğ›¾

(ï¸ 1

+

8ğ›½ğ‘€

)ï¸

ğ‘ƒ ğ‘¡,

ğ¿Ìƒï¸€

=

âˆšï¸

1

âˆ‘ï¸€ğ‘›

ğ¿2,

ğœƒğ‘¤

def
=

1 âˆ’ (1 âˆ’

ğœƒğ‘€

Ìƒï¸€

ğœƒğ‘¤

ğœƒğ‘€

ğ‘› ğ‘–=1 ğ‘–

ğ›¼ğ‘¤)(1 + ğ‘ ),

ğ›½ğ‘¤

def
=

(1 âˆ’ ğ›¼ğ‘¤) (ï¸€1

+

ğ‘ âˆ’1)ï¸€,

ğœƒğ‘€

def
=

1 âˆ’ (1 âˆ’ ğ›¼ğ‘€ )(1 + ğœŒ),

ğ›½ğ‘€

def
=

(1 âˆ’ ğ›¼ğ‘€ ) (ï¸€1

+

ğœŒâˆ’1)ï¸€

for any ğœŒ, ğ‘  > 0.

Proof. We apply Lemma 16 and split the error â€–ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)â€–2 in two parts

ğ‘“ (ğ‘¥ğ‘¡+1) â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾ âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2

2

2ğ›¾ 2

2

â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡

2

2ğ›¾ 2

+ğ›¾âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + ğ›¾ âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2

Ìƒï¸€

Ìƒï¸€

â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡

2

2ğ›¾ 2

1 âˆ‘ğ‘›ï¸ âƒ¦ ğ‘¡ ğ‘¡âƒ¦2 1 âˆ‘ğ‘›ï¸ âƒ¦ ğ‘¡

ğ‘¡ âƒ¦2

+ğ›¾ ğ‘›

âƒ¦ğ‘” âˆ’ ğ‘” âƒ¦ + ğ›¾

Ìƒï¸€

ğ‘›

âƒ¦ğ‘”Ìƒï¸€ğ‘– âˆ’ âˆ‡ğ‘“ğ‘–(ğ‘¥ )âƒ¦

ğ‘–=1

ğ‘–=1

= ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ ğ‘…ğ‘¡ + ğ›¾âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + ğ›¾ğ‘ƒ ğ‘¡, (73)

2

2ğ›¾ 2

Ìƒï¸€

where

we

used

notation

ğ‘…ğ‘¡

=

â€–ğ›¾ğ‘”ğ‘¡â€–2

=

âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2,

ğ‘ƒğ‘¡

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

â€–ğ‘”Ìƒï¸€ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2

and

applied

(118) and (119).

Subtracting ğ‘“ inf from both sides of the above inequality, taking expectation and using the notation ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡+1) âˆ’ ğ‘“ inf , we get

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + ğ›¾E [ï¸âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›¾E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ .

2

2ğ›¾ 2

Ìƒï¸€

(74)

Further, Lemma 7 and 8 provide the recursive bounds for the last two terms of (74)

E [ï¸€ğ‘ƒ ğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒğ‘¤)E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ + ğ›½ğ‘¤ğ¿Ìƒï¸€2E [ğ‘…ğ‘¡] ,

(75)

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡+1

âˆ’

ğ‘”ğ‘¡+1âƒ¦âƒ¦2]ï¸

â‰¤

(1

âˆ’

ğœƒğ‘€

)E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

+

8ğ›½ğ‘€

ğ¿Ìƒï¸€2E

[ğ‘…ğ‘¡]

+

8ğ›½ğ‘€

E

[ï¸€ğ‘ƒ

ğ‘¡]ï¸€

.

(76)

Ìƒï¸€

Ìƒï¸€

51

EF21 with Bells & Whistles

Oct 6, 2021

Summing up (74) with a ğœƒğ›¾ğ‘€ multiple of (76) we obtain

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + ğ›¾ E [ï¸âƒ¦âƒ¦ğ‘”ğ‘¡+1 âˆ’ ğ‘”ğ‘¡+1âƒ¦âƒ¦2]ï¸ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒğ‘€

Ìƒï¸€

2

2ğ›¾ 2

+ğ›¾E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

+

ğ›¾E

[ï¸€ğ‘ƒ

ğ‘¡]ï¸€

Ìƒï¸€

ğ›¾ +

(ï¸ (1

âˆ’

ğœƒğ‘€

)

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸)ï¸

ğœƒğ‘€

Ìƒï¸€

ğ›¾ +

(ï¸

)ï¸

8ğ›½ğ‘€ ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ + 8ğ›½ğ‘€ E [ï¸€ğ‘ƒ ğ‘¡]ï¸€

ğœƒğ‘€

â‰¤

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

ğ›¾

E

[ï¸ âƒ¦âƒ¦ğ‘”ğ‘¡

âˆ’

ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸

âˆ’

ğ›¾

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸

ğœƒğ‘€

Ìƒï¸€

2

(ï¸ƒ

)ï¸ƒ

âˆ’ 1 âˆ’ ğ¿ âˆ’ 8ğ›¾ğ›½ğ‘€ ğ¿Ìƒï¸€2 E [ï¸€ğ‘…ğ‘¡]ï¸€

2ğ›¾ 2

ğœƒğ‘€

+ğ›¾

(ï¸‚ 1

+

8ğ›½ğ‘€

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

.

ğœƒğ‘€

(ï¸

)ï¸

Then adding the above inequality with a ğœƒğ›¾ğ‘¤ 1 + 8ğœƒğ›½ğ‘€ğ‘€ multiple of (75), we get

E [ï¸€Î¨ğ‘¡+1]ï¸€ = E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + ğ›¾ E [ï¸âƒ¦âƒ¦ğ‘”ğ‘¡+1 âˆ’ ğ‘”ğ‘¡+1âƒ¦âƒ¦2]ï¸ + ğ›¾ (ï¸‚1 + 8ğ›½ğ‘€ )ï¸‚ E [ï¸€ğ‘ƒ ğ‘¡+1]ï¸€

ğœƒğ‘€

Ìƒï¸€

ğœƒğ‘¤

ğœƒğ‘€

(ï¸ƒ

)ï¸ƒ

â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ + ğ›¾ E [ï¸âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ âˆ’ 1 âˆ’ ğ¿ âˆ’ 8ğ›¾ğ›½ğ‘€ ğ¿Ìƒï¸€2 E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒğ‘€

Ìƒï¸€

2

2ğ›¾ 2

ğœƒğ‘€

+ğ›¾

(ï¸‚ 1

+

8ğ›½ğ‘€

)ï¸‚

E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

ğœƒğ‘€

ğ›¾ +

(ï¸‚ 1

+

8ğ›½ğ‘€

)ï¸‚

(ï¸ (1

âˆ’

ğœƒğ‘¤ )E

[ï¸€ğ‘ƒ ğ‘¡]ï¸€

+

ğ›½ğ‘¤ ğ¿Ìƒï¸€ 2 E

)ï¸ [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒğ‘¤

ğœƒğ‘€

â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ + ğ›¾ E [ï¸âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›¾ (ï¸‚1 + 8ğ›½ğ‘€ )ï¸‚ E [ï¸€ğ‘ƒ ğ‘¡]ï¸€ âˆ’ ğ›¾ E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸

ğœƒğ‘€

Ìƒï¸€

ğœƒğ‘¤

ğœƒğ‘€

2

(ï¸ƒ

)ï¸ƒ

âˆ’ 1 âˆ’ ğ¿ âˆ’ 8ğ›¾ğ›½ğ‘€ ğ¿Ìƒï¸€2 âˆ’ ğ›¾ (ï¸‚1 + 8ğ›½ğ‘€ )ï¸‚ ğ›½ğ‘¤ğ¿Ìƒï¸€2 E [ï¸€ğ‘…ğ‘¡]ï¸€

2ğ›¾ 2

ğœƒğ‘€

ğœƒğ‘¤

ğœƒğ‘€

=

E

[ï¸€Î¨ğ‘¡]ï¸€

âˆ’

ğ›¾

E

[ï¸ âƒ¦ âƒ¦âˆ‡ğ‘“

(ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸

2

(ï¸ƒ

)ï¸ƒ

âˆ’ 1 âˆ’ ğ¿ âˆ’ 8ğ›¾ğ›½ğ‘€ ğ¿Ìƒï¸€2 âˆ’ ğ›¾ğ›½ğ‘¤ğ¿Ìƒï¸€2 (ï¸‚1 + 8ğ›½ğ‘€ )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ . (77)

2ğ›¾ 2

ğœƒğ‘€

ğœƒğ‘¤

ğœƒğ‘€

Thus by Lemma 15 and the stepsize choice

(ï¸ƒ

âˆšï¸ƒ

)ï¸ƒâˆ’1

16ğ›½ğ‘€ 2ğ›½ğ‘¤ (ï¸‚ 8ğ›½ğ‘€ )ï¸‚

0 < ğ›¾ â‰¤ ğ¿ + ğ¿Ìƒï¸€

+

1+

(78)

ğœƒğ‘€

ğœƒğ‘¤

ğœƒğ‘€

the last term in (77) is not positive. By summing up inequalities for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1, we get

0 â‰¤ E [ï¸€Î¨ğ‘‡ ]ï¸€ â‰¤ E [ï¸€Î¨0]ï¸€ âˆ’ ğ›¾ ğ‘‡âˆ‘âˆ’ï¸1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ . 2
ğ‘¡=0
Multiplying both sides by ğ›¾2ğ‘‡ and rearranging we get

ğ‘‡âˆ‘âˆ’ï¸1 1 E [ï¸âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2]ï¸ â‰¤ 2E [ï¸€Î¨0]ï¸€ .

ğ‘‡

ğ›¾ğ‘‡

ğ‘¡=0

52

EF21 with Bells & Whistles

Oct 6, 2021

It

remains

to

notice

that

the

left

hand

side

can

be

interpreted

as

E

[ï¸ âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦2 âƒ¦

]ï¸

,

where

ğ‘¥Ë†ğ‘‡

is

chosen

from ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random.

Corollary 14. Let assumption of Theorem 9 hold,

ğ‘”0 = âˆ‡ğ‘“ (ğ‘¥0), ğ‘”Ìƒï¸€ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

(ï¸ƒ

âˆšï¸ƒ

)ï¸ƒâˆ’1

16ğ›½ğ‘€ 2ğ›½ğ‘¤ (ï¸‚ 8ğ›½ğ‘€ )ï¸‚

ğ›¾ = ğ¿ + ğ¿Ìƒï¸€

+

1+

,

ğœƒğ‘€

ğœƒğ‘¤

ğœƒğ‘€

Then, after ğ‘‡

iterations/communication

rounds

of

EF21-BC

we

have

E

[ï¸ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2.

It

requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ğ›¿0

ğ‘‡ = #grad = ğ’ª ğ›¼ğ‘¤ğ›¼ğ‘€ ğœ€2

(79)

iterations/communications rounds/gradint computations at each node.

Proof. Note that by Lemma 17 and ğ›¼ğ‘€ , ğ›¼ğ‘¤ â‰¤ 1, we have

16ğ›½ğ‘€ 2ğ›½ğ‘¤ (ï¸‚ 8ğ›½ğ‘€ )ï¸‚

4

4 (ï¸‚

4 )ï¸‚

+

1+

ğœƒğ‘€

ğœƒğ‘¤

ğœƒğ‘€

â‰¤ 16 ğ›¼2 + 2 ğ›¼2 1 + 8 ğ›¼2

ğ‘€

ğ‘¤

ğ‘€

64 8 33

â‰¤ ğ›¼2 + ğ›¼2 ğ›¼2

ğ‘€

ğ‘¤ğ‘€

64 + 8 Â· 33 â‰¤ ğ›¼2 ğ›¼2 .
ğ‘¤ğ‘€

It remains to apply the steps similar to those in the proof of Corollary 2.

G.2 CONVERGENCE UNDER POLYAK-ÅOJASIEWICZ CONDITION

Theorem 10. Let Assumptions 1 and 4 hold, and let the stepsize in Algorithm 3 be set as

{ï¸‚ ğœƒğ‘€ ğœƒğ‘¤ }ï¸‚

0 < ğ›¾ â‰¤ min ğ›¾0, ,

,

(80)

2ğœ‡ 2ğœ‡

(ï¸‚

âˆšï¸‚

)ï¸‚âˆ’1

where ğ›¾0 d=ef ğ¿ + ğ¿Ìƒï¸€ 32ğœƒğ›½ğ‘€ğ‘€ + 4ğ›½ğœƒğ‘¤ğ‘¤ğ¿Ìƒï¸€2 (ï¸1 + 16ğœƒğ›½ğ‘€ğ‘€ )ï¸ , ğ¿Ìƒï¸€ = âˆšï¸ ğ‘›1 âˆ‘ï¸€ğ‘›ğ‘–=1 ğ¿2ğ‘– , ğœƒğ‘¤ d=ef 1âˆ’(1âˆ’ğ›¼ğ‘¤)(1+

ğ‘ ),

ğ›½ğ‘¤

def
=

(1 âˆ’ ğ›¼ğ‘¤) (ï¸€1

+

ğ‘ âˆ’1)ï¸€,

ğœƒğ‘€

def
=

1 âˆ’ (1 âˆ’ ğ›¼ğ‘€ )(1 + ğœŒ),

ğ›½ğ‘€

def
=

(1 âˆ’ ğ›¼ğ‘€ ) (ï¸€1

+

ğœŒâˆ’1)ï¸€

for

any

ğœŒ, ğ‘  > 0.

Let

Î¨ğ‘¡

def
=

ğ‘“ (ğ‘¥ğ‘¡)

âˆ’

ğ‘“ inf

+

ğ›¾

â€–ğ‘”ğ‘¡ âˆ’ ğ‘”ğ‘¡â€–2 +

ğ›¾

(ï¸

)ï¸

1 + 8ğ›½ğ‘€ ğ‘ƒ ğ‘¡. Then for any ğ‘‡ â‰¥ 0, we have

ğœƒğ‘€

Ìƒï¸€

ğœƒğ‘¤

ğœƒğ‘€

E [ï¸€Î¨ğ‘‡ ]ï¸€ â‰¤ (1 âˆ’ ğ›¾ğœ‡)ğ‘‡ E [ï¸€Î¨0]ï¸€ .

(81)

Proof. Similarly to the proof of Theorem 9 the inequalities (74), (75), (76) hold with ğ›¿ğ‘¡ = ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘“ (ğ‘¥â‹†).

It remains to apply the steps similar to those in the proof of Theorem 6.

Corollary 15. Let assumption of Theorem 10 hold,

ğ‘”0 = âˆ‡ğ‘“ (ğ‘¥0), ğ‘”Ìƒï¸€ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

â›

âˆšï¸ƒ

ââˆ’1

{ï¸‚ ğœƒğ‘€ ğœƒğ‘¤ }ï¸‚

ğ›¾ = min ğ›¾0, ,

,

2ğœ‡ 2ğœ‡

32ğ›½ğ‘€ 4ğ›½ğ‘¤ğ¿Ìƒï¸€2 (ï¸‚ 16ğ›½ğ‘€ )ï¸‚

ğ›¾0 = âğ¿ + ğ¿Ìƒï¸€

+

1+

â ,

ğœƒğ‘€

ğœƒğ‘¤

ğœƒğ‘€

53

EF21 with Bells & Whistles

Oct 6, 2021

Then, after ğ‘‡ iterations of EF21-PAGE we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ inf ]ï¸€ â‰¤ ğœ€. It requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€

(ï¸‚ ğ›¿0 )ï¸‚

ğ‘‡ = #grad = ğ’ª

ln

ğœ‡ğ›¼ğ‘¤ ğ›¼ğ‘€

ğœ€

iterations/communications rounds/gradint computations at each node.

54

EF21 with Bells & Whistles

Oct 6, 2021

H HEAVY BALL MOMENTUM
Notations for this section: ğ‘…ğ‘¡ = â€–ğ›¾ğ‘”ğ‘¡â€–2 = (1 âˆ’ ğœ‚)2 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2.
In this section, we study the momentum version of EF21. In particular, we focus on Polyak style momentum (Polyak, 1964; Yang et al., 2016). Let ğ‘”ğ‘¡ be a gradient estimator at iteration ğ‘¡, then the update rule of heavy ball (HB) is given by

ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡ + ğœ‚ (ï¸€ğ‘¥ğ‘¡ âˆ’ ğ‘¥ğ‘¡âˆ’1)ï¸€ ,
where ğ‘¥âˆ’1 = ğ‘¥0, ğœ‚ âˆˆ [0, 1) is called the momentum parameter, and ğ›¾ > 0 is the stepsize. The above update rule can be viewed as a combination of the classical gradient step
ğ‘¦ğ‘¡ = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡ followed by additional momentum step
ğ‘¥ğ‘¡+1 = ğ‘¦ğ‘¡ + ğœ‚ (ï¸€ğ‘¥ğ‘¡ âˆ’ ğ‘¥ğ‘¡âˆ’1)ï¸€ . Here the momentum term is added to accelerate the convergence and make the trajectory look like a smooth descent to the bottom of the ravine, rather than zigzag.
Equivalently, the update of HB can be implemented by the following two steps (Yang et al., 2016): {ï¸‚ ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘£ğ‘¡ ğ‘£ğ‘¡+1 = ğœ‚ğ‘£ğ‘¡ + ğ‘”ğ‘¡+1.
We are now ready to present the distributed variant of heavy ball method enhanced with a contractive compressor ğ’, and EF21 mechanism, which we call EF21-HB (Algorithm 6). We present the complexity results in Theorem 11 and Corollary 16.

Algorithm 6 EF21-HB

1: Input: starting point ğ‘¥0 âˆˆ Rğ‘‘; ğ‘”ğ‘–0 âˆˆ Rğ‘‘ for ğ‘– = 1, . . . , ğ‘› (known by nodes); ğ‘£0 = ğ‘”0 =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–0

(known

by

master);

learning

rate

ğ›¾

>

0;

momentum

parameter

0

â‰¤

ğœ‚

<

1

2: for ğ‘¡ = 0,1, 2, . . . , ğ‘‡ âˆ’ 1 do

3: Master computes ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘£ğ‘¡ and broadcasts ğ‘¥ğ‘¡+1 to all nodes

4: for all nodes ğ‘– = 1, . . . , ğ‘› in parallel do

5:

Compress ğ‘ğ‘¡ğ‘– = ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡) and send ğ‘ğ‘¡ğ‘– to the master

6:

Update local state ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

7: end for

8:

Master

computes

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

via

ğ‘”ğ‘¡+1

=

ğ‘”ğ‘¡ +

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ğ‘¡ğ‘– ,

and

ğ‘£ğ‘¡+1

=

ğœ‚ğ‘£ğ‘¡ + ğ‘”ğ‘¡+1

9: end for

In the analysis of EF21-HB, we assume by default that ğ‘£âˆ’1 = 0.
Lemma 9. Let sequences {ğ‘¥ğ‘¡}ğ‘¡â‰¥0 , and {ğ‘£ğ‘¡}ğ‘¡â‰¥0 be generated by Algorithm 6 and let the sequence {ğ‘§ğ‘¡}ğ‘¡â‰¥0 be deï¬ned as ğ‘§ğ‘¡+1 d=ef ğ‘¥ğ‘¡+1 âˆ’ 1ğ›¾âˆ’ğœ‚ğœ‚ ğ‘£ğ‘¡ with 0 â‰¤ ğœ‚ < 1. Then for all ğ‘¡ â‰¥ 0
ğ‘§ğ‘¡+1 = ğ‘§ğ‘¡ âˆ’ ğ›¾ ğ‘”ğ‘¡. 1âˆ’ğœ‚

Proof.

ğ‘§ğ‘¡+1

(ğ‘–)
=

ğ‘¥ğ‘¡+1 âˆ’ ğ›¾ğœ‚ ğ‘£ğ‘¡

1âˆ’ğœ‚

(ğ‘–ğ‘–)
=

ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘£ğ‘¡ âˆ’ ğ›¾ğœ‚ ğ‘£ğ‘¡

1âˆ’ğœ‚

(ğ‘–ğ‘–ğ‘–)
=

ğ‘§ğ‘¡ +

ğ›¾ğœ‚ ğ‘£ğ‘¡âˆ’1 âˆ’

ğ›¾

ğ‘£ğ‘¡

1âˆ’ğœ‚

1âˆ’ğœ‚

= ğ‘§ğ‘¡ âˆ’ ğ›¾ (ï¸€ğ‘£ğ‘¡ âˆ’ ğœ‚ğ‘£ğ‘¡âˆ’1)ï¸€ 1âˆ’ğœ‚

= ğ‘§ğ‘¡ âˆ’ ğ›¾ ğ‘”ğ‘¡, 1âˆ’ğœ‚

55

EF21 with Bells & Whistles

Oct 6, 2021

where in (ğ‘–) and (ğ‘–ğ‘–ğ‘–) we use the deï¬nition of ğ‘§ğ‘¡+1 and ğ‘§ğ‘¡, in (ğ‘–ğ‘–) we use the step ğ‘¥ğ‘¡+1 = ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘£ğ‘¡ (line 3 of Algorithm 6). Finally, the last equality follows by the update ğ‘£ğ‘¡+1 = ğœ‚ğ‘£ğ‘¡ + ğ‘”ğ‘¡+1 (line 8 of
Algorithm 6).

Lemma 10. Let the sequence {ğ‘£ğ‘¡}ğ‘¡â‰¥0 be deï¬ned as ğ‘£ğ‘¡+1 = ğœ‚ğ‘£ğ‘¡ + ğ‘”ğ‘¡+1 with 0 â‰¤ ğœ‚ < 1. Then

ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘£ğ‘¡âƒ¦âƒ¦2 â‰¤ 1 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘”ğ‘¡âƒ¦âƒ¦2 .

(1 âˆ’ ğœ‚)2

ğ‘¡=0

ğ‘¡=0

Proof. Unrolling the given recurrence and noticing that ğ‘£âˆ’1 = 0, we have ğ‘£ğ‘¡ = âˆ‘ï¸€ğ‘¡ğ‘™=0 ğœ‚ğ‘¡âˆ’ğ‘™ğ‘”ğ‘™. Deï¬ne ğ» d=ef âˆ‘ï¸€ğ‘¡ğ‘™=0 ğœ‚ğ‘™ â‰¤ 1âˆ’1 ğœ‚ . Then by Jensenâ€™s inequality

ğ‘‡ âˆ’1
âˆ‘ï¸ 2

ğ‘‡ âˆ’1
âˆ‘ï¸

âƒ¦ğ‘¡ âƒ¦âˆ‘ï¸

ğœ‚ğ‘¡âˆ’ğ‘™

âƒ¦2 âƒ¦

âƒ¦âƒ¦ğ‘£ğ‘¡âƒ¦âƒ¦ = ğ»2 âƒ¦

ğ‘”ğ‘™âƒ¦

ğ‘¡=0 ğ‘¡=0 âƒ¦âƒ¦ ğ‘™=0 ğ» âƒ¦âƒ¦

ğ‘‡ âˆ’1
âˆ‘ï¸

ğ‘¡
âˆ‘ï¸

ğœ‚ğ‘¡âˆ’ğ‘™

2

â‰¤ ğ»2

âƒ¦âƒ¦ğ‘”ğ‘™âƒ¦âƒ¦

ğ‘¡=0 ğ‘™=0 ğ»

ğ‘‡ âˆ’1 ğ‘¡
= ğ» âˆ‘ï¸ âˆ‘ï¸ ğœ‚ğ‘¡âˆ’ğ‘™ âƒ¦âƒ¦ğ‘”ğ‘™âƒ¦âƒ¦2

ğ‘¡=0 ğ‘™=0

1

ğ‘‡ âˆ’1 ğ‘¡
âˆ‘ï¸ âˆ‘ï¸

2

â‰¤

ğœ‚ğ‘¡âˆ’ğ‘™

âƒ¦âƒ¦ğ‘”ğ‘™

âƒ¦ âƒ¦

1 âˆ’ ğœ‚ ğ‘¡=0 ğ‘™=0

1

ğ‘‡ âˆ’1
âˆ‘ï¸

ğ‘‡ âˆ’1
2 âˆ‘ï¸

=

âƒ¦âƒ¦ğ‘”ğ‘™âƒ¦âƒ¦

ğœ‚ğ‘¡âˆ’ğ‘™

1 âˆ’ ğœ‚ ğ‘™=0 ğ‘¡=ğ‘™

â‰¤ 1 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘”ğ‘¡âƒ¦âƒ¦2 . (1 âˆ’ ğœ‚)2
ğ‘¡=0

Lemma 11. Let the sequence {ğ‘§ğ‘¡}ğ‘¡â‰¥0 be deï¬ned as ğ‘§ğ‘¡+1 d=ef ğ‘¥ğ‘¡+1 âˆ’ 1ğ›¾âˆ’ğœ‚ğœ‚ ğ‘£ğ‘¡ with 0 â‰¤ ğœ‚ < 1. Then

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

âˆ‘ï¸

E

[ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğœƒ)

âˆ‘ï¸

E

[ï¸€ğºğ‘¡]ï¸€

+

2ğ›½ğ¿Ìƒï¸€2(1

+

4ğœ‚2)

âˆ‘ï¸

E

[ï¸ âƒ¦âƒ¦ğ‘§ğ‘¡+1

âˆ’

ğ‘§ğ‘¡âƒ¦âƒ¦2]ï¸

,

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

where ğœƒ = 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ), ğ›½ = (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€ for any ğ‘  > 0.

Proof. Summing up the inequality in Lemma 1 (for EF21 estimator) for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1, we have

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

âˆ‘ï¸

E

[ï¸€ğºğ‘¡+1]ï¸€

â‰¤

(1

âˆ’

ğœƒ)

âˆ‘ï¸

E

[ï¸€ğºğ‘¡]ï¸€

+

ğ›½ğ¿Ìƒï¸€2

âˆ‘ï¸

E

[ï¸ âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

.

(82)

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

It remains to bound âˆ‘ï¸€ğ‘¡ğ‘‡=âˆ’01 E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸. Notice that by deï¬nition of {ğ‘§ğ‘¡}ğ‘¡â‰¥0, we have ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡ = ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡ + ğ›¾ğœ‚ (ï¸€ğ‘£ğ‘¡ âˆ’ ğ‘£ğ‘¡âˆ’1)ï¸€ . 1âˆ’ğœ‚

56

EF21 with Bells & Whistles

Oct 6, 2021

Thus

ğ‘‡ âˆ’1
âˆ‘ï¸ [ï¸

ğ‘‡ âˆ’1

2]ï¸

âˆ‘ï¸ [ï¸

2]ï¸

2ğ›¾2ğœ‚2

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸

2]ï¸

âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦ â‰¤ 2 E âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦ +

E âƒ¦âƒ¦ğ‘£ğ‘¡ âˆ’ ğ‘£ğ‘¡âˆ’1âƒ¦âƒ¦

Eâƒ¦

(1 âˆ’ ğœ‚)2

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

ğ‘‡ âˆ’1
âˆ‘ï¸ [ï¸

2]ï¸

2ğ›¾2ğœ‚2

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸

2]ï¸

= 2 E âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦ +

E âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ (1 âˆ’ ğœ‚)ğ‘£ğ‘¡âˆ’1âƒ¦âƒ¦

(1 âˆ’ ğœ‚)2

ğ‘¡=0

ğ‘¡=0

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦ ğ‘¡+1

ğ‘¡âƒ¦2]ï¸

4ğ›¾2ğœ‚2

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡âƒ¦2]ï¸

â‰¤ 2 E âƒ¦ğ‘§ âˆ’ ğ‘§ âƒ¦ + (1 âˆ’ ğœ‚)2 E âƒ¦ğ‘” âƒ¦

ğ‘¡=0

ğ‘¡=0

4ğ›¾2ğœ‚2

ğ‘‡ âˆ’1
âˆ‘ï¸

+

(1 âˆ’ ğœ‚)2E [ï¸€â€–ğ‘£ğ‘¡âˆ’1â€–2]ï¸€

(1 âˆ’ ğœ‚)2

ğ‘¡=0

(ğ‘–)

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦ ğ‘¡+1

ğ‘¡âƒ¦2]ï¸

4ğ›¾2ğœ‚2

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡âƒ¦2]ï¸

â‰¤ 2 E âƒ¦ğ‘§ âˆ’ ğ‘§ âƒ¦ + (1 âˆ’ ğœ‚)2 E âƒ¦ğ‘” âƒ¦

ğ‘¡=0

ğ‘¡=0

4ğ›¾2ğœ‚2

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡âƒ¦2]ï¸

+ (1 âˆ’ ğœ‚)2 E âƒ¦ğ‘” âƒ¦

ğ‘¡=0

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦ ğ‘¡+1

ğ‘¡âƒ¦2]ï¸

8ğ›¾2ğœ‚2

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡âƒ¦2]ï¸

= 2 E âƒ¦ğ‘§ âˆ’ ğ‘§ âƒ¦ + (1 âˆ’ ğœ‚)2 E âƒ¦ğ‘” âƒ¦

ğ‘¡=0

ğ‘¡=0

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

(ğ‘–ğ‘–)
=

2

âˆ‘ï¸

E

[ï¸ âƒ¦âƒ¦ğ‘§ğ‘¡+1

âˆ’

ğ‘§ğ‘¡âƒ¦âƒ¦2]ï¸

+

8ğœ‚2

âˆ‘ï¸

E

[ï¸ âƒ¦âƒ¦ğ‘§ğ‘¡+1

âˆ’

ğ‘§ğ‘¡âƒ¦âƒ¦2]ï¸

ğ‘¡=0

ğ‘¡=0

ğ‘‡ âˆ’1

=

2(1

+

4ğœ‚2)

âˆ‘ï¸

E

[ï¸ âƒ¦âƒ¦ğ‘§ğ‘¡+1

âˆ’

ğ‘§ğ‘¡âƒ¦âƒ¦2]ï¸

,

ğ‘¡=0

where in (ğ‘–) we apply Lemma 10, and in (ğ‘–ğ‘–) Lemma 9 is utilized.

It remains to plug in the above inequality into (82)

Lemma 12. Let the sequence {ğ‘§ğ‘¡}ğ‘¡â‰¥0 be generated as in Lemma 9, i.e., ğ‘§ğ‘¡+1 = ğ‘§ğ‘¡ âˆ’ 1âˆ’ğ›¾ ğœ‚ ğ‘”ğ‘¡, then for all ğ‘¡ â‰¥ 0
âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 â‰¤ 2ğºğ‘¡ + 2(1 âˆ’ ğœ‚)2 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾2

with

ğºğ‘¡

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

â€–âˆ‡ğ‘“ğ‘–

(ğ‘¥ğ‘¡

)

âˆ’

ğ‘”ğ‘–ğ‘¡â€–2.

Proof. Notice that for ğ›¾ > 0 we have âˆ‡ğ‘“ (ğ‘¥ğ‘¡) = âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡ âˆ’ 1âˆ’ğ›¾ ğœ‚ (ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡). Then

âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 â‰¤ 2 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + 2 (1 âˆ’ ğœ‚)2 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾2

2 âˆ‘ğ‘›ï¸ âƒ¦

ğ‘¡

ğ‘¡âƒ¦2 2(1 âˆ’ ğœ‚)2 âƒ¦ ğ‘¡+1 ğ‘¡âƒ¦2

â‰¤ ğ‘›

âƒ¦âˆ‡ğ‘“ğ‘–(ğ‘¥ ) âˆ’ ğ‘”ğ‘– âƒ¦ +

ğ›¾2

âƒ¦ğ‘§ âˆ’ ğ‘§ âƒ¦ ,

ğ‘–=1

where the inequalities hold due to (118) with ğ‘  = 1, and (119).

H.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Theorem 11. Let Assumption 1 hold, and let the stepsize in Algorithm 6 be set as

(ï¸ƒ

âˆšï¸‚

)ï¸ƒâˆ’1

(1 + ğœ‚)ğ¿ ğ¿Ìƒï¸€ 2ğ›½

def

0 < ğ›¾ < 2(1 âˆ’ ğœ‚)2 + 1 âˆ’ ğœ‚ ğœƒ (1 + 4ğœ‚2) = ğ›¾0,

(83)

57

EF21 with Bells & Whistles

Oct 6, 2021

where 0 â‰¤ ğœ‚ < 1, ğœƒ = 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ), ğ›½ = (1 âˆ’ ğ›¼) (ï¸€1 + ğ‘ âˆ’1)ï¸€, and ğ‘  > 0. Fix ğ‘‡ â‰¥ 1 and let ğ‘¥Ë†ğ‘‡ be chosen from the iterates ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random. Then

â›

â

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡ âƒ¦2]ï¸

3ğ›¿0(1 âˆ’ ğœ‚) E [ï¸€ğº0]ï¸€

1 3(1 âˆ’ ğœ‚)

E âƒ¦âˆ‡ğ‘“ (ğ‘¥ )âƒ¦ â‰¤

(ï¸

)ï¸ +

â2 +

(ï¸

)ï¸ â  , (84)

ğ›¾

ğœƒğ‘‡

2ğœ†1 ğ›¾ 1 âˆ’ ğ›¾

ğ‘¡=0 ğ‘‡ ğ›¾ 1 âˆ’ ğ›¾0 ğ›¾0

where ğœ†1 d=ef ğ¿Ìƒï¸€âˆšï¸ 2ğœƒğ›½ (1 + 4ğœ‚2). If the stepsize is set to 0 < ğ›¾ â‰¤ ğ›¾0/2, then

â›

â

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡ âƒ¦2]ï¸

6ğ›¿0(1 âˆ’ ğœ‚) E [ï¸€ğº0]ï¸€

3(1 âˆ’ ğœ‚)

E âƒ¦âˆ‡ğ‘“ (ğ‘¥ )âƒ¦ â‰¤

+ â2 + âˆšï¸

â  . (85)

ğ‘¡=0

ğ›¾ğ‘‡

ğ‘‡ğœƒ

ğ›¾ğ¿Ìƒï¸€ 2ğ›½ (1 + 4ğœ‚2)

ğœƒ

Proof. Consider the sequence ğ‘§ğ‘¡+1 d=ef ğ‘¥ğ‘¡+1 âˆ’ 1ğ›¾âˆ’ğœ‚ğœ‚ ğ‘£ğ‘¡ with 0 â‰¤ ğœ‚ < 1. Then Lemma 9 states that ğ‘§ğ‘¡+1 = ğ‘§ğ‘¡ âˆ’ 1âˆ’ğ›¾ ğœ‚ ğ‘”ğ‘¡. By ğ¿-smoothness of ğ‘“ (Â·)

ğ‘“ (ğ‘§ğ‘¡+1) âˆ’ ğ‘“ (ğ‘§ğ‘¡) â‰¤ âŸ¨âˆ‡ğ‘“ (ğ‘§ğ‘¡), ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âŸ© + ğ¿ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 2

= âŸ¨âˆ‡ğ‘“ (ğ‘§ğ‘¡) âˆ’ ğ‘”ğ‘¡, ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âŸ© + âŸ¨ğ‘”ğ‘¡, ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âŸ© + ğ¿ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 2

(=ğ‘–) âŸ¨âˆ‡ğ‘“ (ğ‘§ğ‘¡) âˆ’ ğ‘”ğ‘¡, ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âŸ© âˆ’ 1 âˆ’ ğœ‚ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 + ğ¿ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2

ğ›¾

2

= âŸ¨âˆ‡ğ‘“ (ğ‘§ğ‘¡) âˆ’ ğ‘”ğ‘¡, ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âŸ© âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ )ï¸‚ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾2

= âŸ¨âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡, ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âŸ© + âŸ¨âˆ‡ğ‘“ (ğ‘§ğ‘¡) âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡), ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âŸ©

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ )ï¸‚ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾2

(â‰¤ğ‘–ğ‘–) 1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + ğœ†1 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 + 1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘§ğ‘¡) âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2

2ğœ†1

2

2ğœ†2

+ ğœ†2 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ )ï¸‚ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2

2

ğ›¾2

= 1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + 1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘§ğ‘¡) âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2

2ğœ†1

2ğœ†2

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 )ï¸‚ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾ 22 2

(ğ‘–â‰¤ğ‘–ğ‘–) 1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + ğ¿2 âƒ¦âƒ¦ğ‘§ğ‘¡ âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2

2ğœ†1

2ğœ†2

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 )ï¸‚ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾ 22 2

(â‰¤ğ‘–ğ‘£) 1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + ğ›¾2ğœ‚2ğ¿2 âƒ¦âƒ¦ğ‘£ğ‘¡âˆ’1âƒ¦âƒ¦2

2ğœ†1

2ğœ†2(1 âˆ’ ğœ‚)2

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 )ï¸‚ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 , ğ›¾ 22 2

where in (ğ‘–) Lemma 9 is applied, in (ğ‘–ğ‘–) the inequality (115) is applied twice for ğœ†1, ğœ†2 > 0, (ğ‘–ğ‘–ğ‘–) holds due to Assumption 1, and (ğ‘–ğ‘£) holds by deï¬nition of ğ‘§ğ‘¡ = ğ‘¥ğ‘¡ âˆ’ 1ğ›¾âˆ’ğœ‚ğœ‚ ğ‘£ğ‘¡âˆ’1.

58

EF21 with Bells & Whistles

Oct 6, 2021

Summing up the above inequalities for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1 (assuming ğ‘£âˆ’1 = 0), we have

ğ‘“ (ğ‘§ğ‘‡ ) â‰¤ ğ‘“ (ğ‘§0) + 1 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + ğ›¾2ğœ‚2ğ¿2 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘£ğ‘¡âƒ¦âƒ¦2

2ğœ†1 ğ‘¡=0

2ğœ†2(1 âˆ’ ğœ‚)2 ğ‘¡=0

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 )ï¸‚ ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾ 22 2
ğ‘¡=0

(â‰¤ğ‘–) ğ‘“ (ğ‘§0) + 1 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 + ğ›¾2ğœ‚2ğ¿2 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘”ğ‘¡âƒ¦âƒ¦2

2ğœ†1 ğ‘¡=0

2ğœ†2(1 âˆ’ ğœ‚)4 ğ‘¡=0

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 )ï¸‚ ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾ 22 2
ğ‘¡=0

(ğ‘–ğ‘–)

0

1 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦

ğ‘¡

ğ‘¡âƒ¦2

ğ›¾2ğœ‚2ğ¿2 ğ‘‡âˆ‘âˆ’ï¸1 (1 âˆ’ ğœ‚)2 âƒ¦ ğ‘¡+1 ğ‘¡âƒ¦2

= ğ‘“ (ğ‘§ ) +

âƒ¦âˆ‡ğ‘“ (ğ‘¥ ) âˆ’ ğ‘” âƒ¦ +

âƒ¦ğ‘§ âˆ’ ğ‘§ âƒ¦

2ğœ†1 ğ‘¡=0

2ğœ†2(1 âˆ’ ğœ‚)4 ğ‘¡=0 ğ›¾2

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 )ï¸‚ ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾ 22 2
ğ‘¡=0

= ğ‘“ (ğ‘§0) + 1 ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡âƒ¦âƒ¦2 2ğœ†1 ğ‘¡=0

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 âˆ’

ğœ‚2ğ¿2

)ï¸‚ ğ‘‡ âˆ’1 âˆ‘ï¸ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2

ğ›¾ 2 2 2 2ğœ†2(1 âˆ’ ğœ‚)2 ğ‘¡=0

(ğ‘–â‰¤ğ‘–ğ‘–) ğ‘“ (ğ‘§0) + 1 ğ‘‡âˆ‘âˆ’ï¸1 ğºğ‘¡ 2ğœ†1 ğ‘¡=0

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ†2 âˆ’

ğœ‚2ğ¿2

)ï¸‚ ğ‘‡ âˆ’1 âˆ‘ï¸ âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2

ğ›¾ 2 2 2 2ğœ†2(1 âˆ’ ğœ‚)2 ğ‘¡=0

= ğ‘“ (ğ‘§0) + 1 ğ‘‡âˆ‘âˆ’ï¸1 ğºğ‘¡ 2ğœ†1 ğ‘¡=0

âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ‚ğ¿ )ï¸‚ ğ‘‡âˆ‘âˆ’ï¸1 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2 ğ›¾ 2 2 (1 âˆ’ ğœ‚)
ğ‘¡=0

= ğ‘“ (ğ‘§0) + 1 ğ‘‡âˆ‘âˆ’ï¸1 ğºğ‘¡ âˆ’ (ï¸‚ 1 âˆ’ ğœ‚ âˆ’ ğ¿ âˆ’ ğœ†1 âˆ’ ğœ‚ğ¿ )ï¸‚ 1 ğ‘‡âˆ‘âˆ’ï¸1 ğ‘…ğ‘¡,

2ğœ†1 ğ‘¡=0

ğ›¾

2 2 (1 âˆ’ ğœ‚) (1 âˆ’ ğœ‚)2

ğ‘¡=0

where (ğ‘–) holds due to Lemma 10, in (ğ‘–ğ‘–) Lemma 9 is applied, in (ğ‘–ğ‘–ğ‘–) we apply â€–âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘”ğ‘¡â€–2 â‰¤ ğºğ‘¡. Finally, in the last two steps we choose ğœ†2 = 1ğœ‚âˆ’ğ¿ğœ‚ , and recall the deï¬nition ğ‘…ğ‘¡ = â€–ğ›¾ğ‘”ğ‘¡â€–2 = (1 âˆ’ ğœ‚)2 âƒ¦âƒ¦ğ‘§ğ‘¡+1 âˆ’ ğ‘§ğ‘¡âƒ¦âƒ¦2.

Subtracting ğ‘“ inf from both sides of the above inequality, taking expectation and using the notation ğ›¿ğ‘¡ = ğ‘“ (ğ‘§ğ‘¡) âˆ’ ğ‘“ inf , we get

[ï¸€ ğ‘‡ ]ï¸€

[ï¸€ 0]ï¸€ 1 ğ‘‡âˆ‘âˆ’ï¸1 [ï¸€ ğ‘¡]ï¸€ (ï¸‚ 1 âˆ’ ğœ‚ ğ¿ ğœ†1

ğœ‚ğ¿ )ï¸‚ 1 ğ‘‡âˆ‘âˆ’ï¸1 [ï¸€ ğ‘¡]ï¸€

E ğ›¿ â‰¤ E ğ›¿ + 2ğœ†1 E ğº âˆ’ ğ›¾ âˆ’ 2 âˆ’ 2 âˆ’ (1 âˆ’ ğœ‚) (1 âˆ’ ğœ‚)2 E ğ‘… .

ğ‘¡=0

ğ‘¡=0

(86)

By Lemma 11, we have

ğ‘‡ âˆ’1
âˆ‘ï¸

ğ‘‡ âˆ’1
âˆ‘ï¸

2ğ›½ğ¿Ìƒï¸€2(1

+

4ğœ‚2)

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸€ğºğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒ) E [ï¸€ğºğ‘¡]ï¸€ +

E [ï¸€ğ‘…ğ‘¡]ï¸€ .

(87)

E

(1 âˆ’ ğœ‚)2

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

59

EF21 with Bells & Whistles

Oct 6, 2021

Next, we are going to add (86) with a 2ğœƒ1ğœ†1 multiple of (87). First, let us "forget", for a moment, about all the terms involving ğ‘…ğ‘¡ and denote their sum appearing on the right hand side by â„›, then

1

ğ‘‡ âˆ’1
âˆ‘ï¸

1

ğ‘‡ âˆ’1
âˆ‘ï¸

1

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸€ğ›¿ğ‘‡ ]ï¸€ +

E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿0]ï¸€ +

E [ï¸€ğºğ‘¡]ï¸€ + (1 âˆ’ ğœƒ)

E [ï¸€ğºğ‘¡]ï¸€ + â„›

E

2ğœƒğœ†1

2ğœ†1

2ğœ†1

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

1

ğ‘‡ âˆ’1
âˆ‘ï¸

= E [ï¸€ğ›¿0]ï¸€ +

E [ï¸€ğºğ‘¡]ï¸€ + â„›.

2ğœƒğœ†1 ğ‘¡=0

Canceling out the same terms in both sides of the above inequality, we get

E [ï¸€ğ›¿ğ‘‡ ]ï¸€ + 1 E [ï¸€ğºğ‘‡ ]ï¸€ â‰¤ E [ï¸€ğ›¿0]ï¸€ + 1 E [ï¸€ğº0]ï¸€ + â„›,

2ğœƒğœ†1

2ğœƒğœ†1

where â„› d=ef âˆ’ (ï¸ 1âˆ’ğ›¾ ğœ‚ âˆ’ ğ¿2 (ï¸1 + 12âˆ’ğœ‚ğœ‚ )ï¸ âˆ’ ğœ†21 âˆ’ ğ›½ğ¿Ìƒï¸€2(ğœƒ1ğœ†+14ğœ‚2) )ï¸ (1âˆ’1ğœ‚)2 âˆ‘ï¸€ğ‘‡ğ‘¡=âˆ’01 E [ğ‘…ğ‘¡].

âˆšï¸ Now choosing ğœ†1 = ğ¿Ìƒï¸€ 2ğœƒğ›½ (1 + 4ğœ‚2) and using the deï¬nition of ğ›¾0 given by (83), i.e., ğ›¾0 d=ef (ï¸‚ âˆšï¸ )ï¸‚âˆ’1
2((11+âˆ’ğœ‚ğœ‚))ğ¿2 + 1âˆ’ğ¿Ìƒï¸€ğœ‚ 2ğœƒğ›½ (1 + 4ğœ‚2) , we have

(ï¸ƒ

)ï¸ƒ

1 âˆ’ ğœ‚ âˆ’ ğ¿ (ï¸‚1 + 2ğœ‚ )ï¸‚ âˆ’ ğœ†1 âˆ’ ğ›½ğ¿Ìƒï¸€2(1 + 4ğœ‚2) 1

ğ›¾2

1âˆ’ğœ‚

2

ğœƒğœ†1

(1 âˆ’ ğœ‚)2

(ï¸ƒ 1 âˆ’ ğœ‚ ğ¿ (ï¸‚

2ğœ‚ )ï¸‚

âˆšï¸‚ 2ğ›½

)ï¸ƒ 1

= ğ›¾ âˆ’ 2 1 + 1 âˆ’ ğœ‚ âˆ’ ğ¿Ìƒï¸€ ğœƒ (1 + 4ğœ‚2) (1 âˆ’ ğœ‚)2

(ï¸ƒ

âˆšï¸‚

)ï¸ƒ

= 1 âˆ’ ğ¿ 1 + ğœ‚ âˆ’ ğ¿Ìƒï¸€ 2ğ›½ (1 + 4ğœ‚2) 1

ğ›¾ 2 (1 âˆ’ ğœ‚)2 1 âˆ’ ğœ‚ ğœƒ

1âˆ’ğœ‚

(ï¸‚ 1 1 )ï¸‚ 1

=âˆ’

.

ğ›¾ ğ›¾0 1 âˆ’ ğœ‚

Then

0 â‰¤ E [ï¸€Î¦ğ‘‡ ]ï¸€

d=ef

[ï¸‚ E ğ›¿ğ‘‡ +

1

]ï¸‚ ğºğ‘‡

2ğœƒğœ†1

[ï¸‚

1

]ï¸‚ (ï¸‚ 1

1 )ï¸‚

1

ğ‘‡ âˆ’1
âˆ‘ï¸

â‰¤ E ğ›¿0 +

ğº0 âˆ’ âˆ’

E [ï¸€ğ‘…ğ‘¡]ï¸€

2ğœƒğœ†1

ğ›¾ ğ›¾0 1 âˆ’ ğœ‚ ğ‘¡=0

(ï¸‚ 1

1 )ï¸‚

1

ğ‘‡ âˆ’1
âˆ‘ï¸

= E [ï¸€Î¦0]ï¸€ âˆ’ âˆ’

E [ï¸€ğ‘…ğ‘¡]ï¸€ .

ğ›¾ ğ›¾0 1 âˆ’ ğœ‚ ğ‘¡=0

After rearranging, we get

1 ğ‘‡âˆ‘âˆ’ï¸1 [ï¸€ğ‘…ğ‘¡]ï¸€ â‰¤ E [ï¸€Î¦0]ï¸€ (1 âˆ’ ğœ‚) .

ğ›¾2 E
ğ‘¡=0

(ï¸

)ï¸

ğ›¾ 1âˆ’ ğ›¾

ğ›¾0

Summing the result of Lemma 12 over ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1 and applying expectation, we get

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡ âƒ¦2]ï¸ ğ‘‡âˆ‘âˆ’ï¸1 [ï¸€ ğ‘¡]ï¸€ 2 ğ‘‡âˆ‘âˆ’ï¸1 [ï¸€ ğ‘¡]ï¸€

E âƒ¦âˆ‡ğ‘“ (ğ‘¥ )âƒ¦ â‰¤ 2 E ğº + ğ›¾2 E ğ‘… .

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

Due to Lemma 11, the conditions of Lemma 18 hold with ğ¶ d=ef 2ğ›½ğ¿Ìƒï¸€2 (11+âˆ’4ğœ‚ğœ‚)22 , ğ‘ ğ‘¡ = E [ğºğ‘¡], ğ‘Ÿğ‘¡ = E [ğ‘…ğ‘¡], thus

60

EF21 with Bells & Whistles

Oct 6, 2021

ğ‘‡ âˆ’1
âˆ‘ï¸

E [ï¸€ğº0]ï¸€

ğ¶

ğ‘‡ âˆ’1
âˆ‘ï¸

[ï¸€ğºğ‘¡]ï¸€ â‰¤

+

E [ï¸€ğ‘…ğ‘¡]ï¸€ .

E

ğœƒ

ğœƒ

ğ‘¡=0

ğ‘¡=0

Combining the above inequalities, we can continue with

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡ âƒ¦2]ï¸

ğ‘‡âˆ‘âˆ’ï¸1 [ï¸€ ğ‘¡]ï¸€ 2 ğ‘‡âˆ‘âˆ’ï¸1 [ï¸€ ğ‘¡]ï¸€

E âƒ¦âˆ‡ğ‘“ (ğ‘¥ )âƒ¦ â‰¤ 2 E ğº + ğ›¾2 E ğ‘…

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

2E [ï¸€ğº0]ï¸€ (ï¸‚

ğ›¾2ğ¶ )ï¸‚

1

ğ‘‡ âˆ’1
âˆ‘ï¸

â‰¤

+ 2+

E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒ

ğœƒ ğ›¾2

ğ‘¡=0

2E [ï¸€ğº0]ï¸€ (ï¸‚ ğ›¾2ğ¶ )ï¸‚ E [ï¸€Î¦0]ï¸€ (1 âˆ’ ğœ‚)

â‰¤

+ 2+

(ï¸

)ï¸ .

ğœƒ ğœƒ ğ›¾ 1 âˆ’ ğ›¾ğ›¾0

(ï¸

âˆšï¸ )ï¸âˆ’1

Note that for ğ›¾ < ğ›¾0 = 2((11+âˆ’ğœ‚ğœ‚))ğ¿2 +

ğ¶ ğœƒ

, we have

ğ›¾2ğ¶

ğ¶ ğœƒ

<

2 â‰¤ 1.

(88)

(ï¸

âˆšï¸ )ï¸

ğœƒ (1+ğœ‚)ğ¿2 + ğ¶

2(1âˆ’ğœ‚)

ğœƒ

Thus

ğ‘‡ âˆ’1

âˆ‘ï¸

[ï¸ âƒ¦

ğ‘¡ âƒ¦2]ï¸

2E [ï¸€ğº0]ï¸€ 3E [ï¸€Î¦0]ï¸€ (1 âˆ’ ğœ‚)

E âƒ¦âˆ‡ğ‘“ (ğ‘¥ )âƒ¦ â‰¤

+ (ï¸

)ï¸

ğœƒ

ğ›¾ 1âˆ’ ğ›¾

ğ‘¡=0

ğ›¾0

â›

â

3ğ›¿0(1 âˆ’ ğœ‚) E [ï¸€ğº0]ï¸€

1 3(1 âˆ’ ğœ‚)

= (ï¸

)ï¸ +

â2 +

(ï¸

)ï¸ â  ,

ğ›¾ 1 âˆ’ ğ›¾ğ›¾0 ğœƒ

2ğœ†1 ğ›¾ 1 âˆ’ ğ›¾ğ›¾0

âˆšï¸ where ğœ†1 = ğ¿Ìƒï¸€ 2ğœƒğ›½ (1 + 4ğœ‚2).

Corollary 16. Let assumptions of Theorem 11 hold,

ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

(ï¸ƒ

âˆšï¸‚

)ï¸ƒâˆ’1

ğ›¾ = (1 + ğœ‚)ğ¿ + ğ¿Ìƒï¸€ 2ğ›½ (1 + 4ğœ‚2) .

2(1 âˆ’ ğœ‚)2 1 âˆ’ ğœ‚ ğœƒ

Then, after ğ‘‡

iterations/communication

rounds

of

EF21-HB

we

have

E

[ï¸ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥Ë†ğ‘‡

âƒ¦2]ï¸ )âƒ¦

â‰¤

ğœ€2.

It

requires

(ï¸ƒ

)ï¸ƒ

ğ¿Ìƒï¸€ğ›¿0 (ï¸‚ 1 1 )ï¸‚

ğ‘‡ = #grad = ğ’ª ğœ€2 ğ›¼ + 1 âˆ’ ğœ‚

(89)

iterations/communications rounds/gradint computations at each node.

Proof. Notice that by using ğ¿ â‰¤ ğ¿Ìƒï¸€, ğœ‚ < 1 and Lemma 17, we have

âˆšï¸‚

âˆšï¸‚

(1 + ğœ‚)ğ¿ ğ¿Ìƒï¸€ +

2ğ›½ (1 + 4ğœ‚2) â‰¤

ğ¿Ìƒï¸€

ğ¿Ìƒï¸€ 10ğ›½

+

2(1 âˆ’ ğœ‚)2 1 âˆ’ ğœ‚ ğœƒ

(1 âˆ’ ğœ‚)2 1 âˆ’ ğœ‚ ğœƒ

(ï¸ƒ

âˆš )ï¸ƒ

ğ¿Ìƒï¸€

1 2 10

â‰¤

+

.

1âˆ’ğœ‚ 1âˆ’ğœ‚ ğ›¼

61

EF21 with Bells & Whistles

Oct 6, 2021

Using the above inequality, (85), and (83), we get

(ï¸ƒ

âˆš )ï¸ƒ

6ğ›¿0(1 âˆ’ ğœ‚) 6ğ›¿0(1 âˆ’ ğœ‚) ğ¿Ìƒï¸€

1 2 10

#grad = ğ‘‡ â‰¤ ğ›¾ğœ€2 â‰¤

ğœ€2

+ 1âˆ’ğœ‚ 1âˆ’ğœ‚ ğ›¼

(ï¸ƒ

âˆš )ï¸ƒ

6ğ¿Ìƒï¸€ğ›¿0 1 2 10

â‰¤ ğœ€2 1 âˆ’ ğœ‚ + ğ›¼ .

62

EF21 with Bells & Whistles

Oct 6, 2021

I COMPOSITE CASE

Now we focus on solving a composite optimization problem

def

1

ğ‘›
âˆ‘ï¸

min Î¦(ğ‘¥) =

ğ‘“ğ‘–(ğ‘¥) + ğ‘Ÿ(ğ‘¥),

(90)

ğ‘¥âˆˆRğ‘‘

ğ‘›

ğ‘–=1

where each ğ‘“ğ‘–(Â·) is ğ¿ğ‘–-smooth (possibly non-convex), ğ‘Ÿ(Â·) is convex, and Î¦inf = infğ‘¥âˆˆRğ‘‘ Î¦(ğ‘¥) > âˆ’âˆ. This is a standard and important generalization of setting (1). Namely, it includes three special
cases.

â€¢ Smooth unconstrained optimization. Set ğ‘Ÿ â‰¡ 0, then we recover the initially stated problem formulation (1).
â€¢ Smooth optimization over convex set. Let ğ‘Ÿ = ğ›¿ğ‘„ (indicator function of the set ğ‘„), where ğ‘„ is a nonempty closed convex set. Then (90) reduces to the problem of minimizing ï¬nite a sum of smooth (possibly non-convex) functions over a nonempty closed convex set

{ï¸ƒ

1

ğ‘›
âˆ‘ï¸

}ï¸ƒ

min ğ‘¥âˆˆğ‘„ ğ‘›

ğ‘“ğ‘–(ğ‘¥) .

ğ‘–=1

â€¢ ğ‘™1-regularized optimization. Choose ğ‘Ÿ(ğ‘¥) = ğœ†â€–ğ‘¥â€–1 with ğœ† > 0, then (90) amounts to the ğ‘™1-regularized (also known as LASSO) problem

{ï¸ƒ

1

ğ‘›
âˆ‘ï¸

}ï¸ƒ

min

ğ‘“ğ‘–(ğ‘¥) + ğœ†â€–ğ‘¥â€–1 .

ğ‘¥âˆˆRğ‘‘ ğ‘›

ğ‘–=1

For any ğ›¾ > 0, ğ‘¥ âˆˆ Rğ‘‘, deï¬ne a proximal mapping of function ğ‘Ÿ(Â·) (prox-operator) as

prox

{ï¸‚ (ğ‘¥) d=ef arg min ğ‘Ÿ(ğ‘¦) +

1

â€–ğ‘¦

âˆ’

}ï¸‚ ğ‘¥â€–2 .

(91)

ğ›¾ğ‘Ÿ ğ‘¦âˆˆRğ‘‘

2ğ›¾

Throughout this section, we assume that the master node can efï¬ciently compute prox-operator at every iteration. This is a reasonable assumption, and in many cases (choices of ğ‘Ÿ(Â·)) appearing in applications, there exists an analytical solution of (91), or its computation is cheap compared to the aggregation step.
To evaluate convergence in composite case, we deï¬ne the generalized gradient mapping at a point ğ‘¥ âˆˆ Rğ‘‘ with a parameter ğ›¾

def 1 (ï¸€

)ï¸€

ğ’¢ğ›¾(ğ‘¥) = ğ›¾ ğ‘¥ âˆ’ proxğ›¾ğ‘Ÿ(ğ‘¥ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥)) .

(92)

One can verify that the above quantity is a well-deï¬ned evaluation metric (Beck, 2017). Namely, for any ğ‘¥* âˆˆ Rğ‘‘, it holds that ğ’¢ğ›¾(ğ‘¥) = 0 if and only if ğ‘¥* is a stationary point of (90), and in a special
case when ğ‘Ÿ â‰¡ 0, we have ğ’¢ğ›¾(ğ‘¥) = âˆ‡ğ‘“ (ğ‘¥).

Notations for this section: in this section we re-deï¬ne ğ›¿ğ‘¡ d=ef Î¦ (ğ‘¥ğ‘¡) âˆ’ Î¦inf

Lemma 13 (Gradient mapping bound).

Let ğ‘¥ğ‘¡+1

def
=

proxğ›¾ ğ‘Ÿ (ğ‘¥ğ‘¡

âˆ’

ğ›¾ğ‘£ğ‘¡),

then

E

[ï¸ âƒ¦ âƒ¦ğ’¢ğ›¾

(ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸

â‰¤

2

[ï¸ E âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

+

[ï¸ 2E âƒ¦âƒ¦ğ‘£ğ‘¡

âˆ’

âˆ‡

ğ‘“

(

ğ‘¥

ğ‘¡

)

âƒ¦2 âƒ¦

]ï¸

.

(93)

ğ›¾2

63

EF21 with Bells & Whistles

Oct 6, 2021

Proof.

E [ï¸âƒ¦âƒ¦ğ’¢ğ›¾ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸ = ğ›¾12 E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡ âˆ’ proxğ›¾ğ‘Ÿ(ğ‘¥ğ‘¡ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥ğ‘¡))âƒ¦âƒ¦2]ï¸

â‰¤ ğ›¾22 E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸ + ğ›¾22 E [ï¸âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ proxğ›¾ğ‘Ÿ(ğ‘¥ğ‘¡ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥ğ‘¡))âƒ¦âƒ¦2]ï¸

=

2

[ï¸ E âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

ğ›¾2

+ ğ›¾22 E [ï¸âƒ¦âƒ¦proxğ›¾ğ‘Ÿ(ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘£ğ‘¡) âˆ’ proxğ›¾ğ‘Ÿ(ğ‘¥ğ‘¡ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥ğ‘¡))âƒ¦âƒ¦2]ï¸

â‰¤

2

[ï¸ E âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

ğ›¾2

+

2

[ï¸ E âƒ¦âƒ¦(ğ‘¥ğ‘¡

âˆ’

ğ›¾ğ‘£ğ‘¡)

âˆ’

(ğ‘¥ğ‘¡

âˆ’

ğ›¾âˆ‡ğ‘“ (ğ‘¥ğ‘¡))âƒ¦âƒ¦2]ï¸

ğ›¾2

=

2

[ï¸ E âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2]ï¸

+

[ï¸ 2E âƒ¦âƒ¦ğ‘£ğ‘¡

âˆ’

âˆ‡

ğ‘“

(

ğ‘¥

ğ‘¡

))

âƒ¦2 âƒ¦

]ï¸

,

(94)

ğ›¾2

where in the last inequality we apply non-expansiveness of prox-operator.

Lemma 14.

Let ğ‘¥ğ‘¡+1

def
=

proxğ›¾ ğ‘Ÿ (ğ‘¥ğ‘¡

âˆ’

ğ›¾ğ‘£ğ‘¡),

then

for

any

ğœ†

>

0,

Î¦ (ï¸€ğ‘¥ğ‘¡+1)ï¸€ â‰¤ Î¦ (ï¸€ğ‘¥ğ‘¡)ï¸€ + 1 âƒ¦âƒ¦ğ‘£ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğœ† )ï¸‚ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 . (95)

2ğœ†

ğ›¾22

Proof. Deï¬ne ğ‘ŸËœ(ğ‘¥) d=ef ğ‘Ÿ(ğ‘¥)+ 21ğ›¾ â€–ğ‘¥ âˆ’ ğ‘¥ğ‘¡ + ğ›¾ğ‘£ğ‘¡â€–2, and note that ğ‘¥ğ‘¡+1 = arg minğ‘¥âˆˆRğ‘‘ {ğ‘ŸËœ(ğ‘¥)}. Since ğ‘ŸËœ(Â·) is 1/ğ›¾ - strongly convex, we have
ğ‘ŸËœ(ğ‘¥ğ‘¡) â‰¥ ğ‘ŸËœ(ğ‘¥ğ‘¡+1) + 1 âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 , 2ğ›¾

ğ‘Ÿ(ğ‘¥ğ‘¡) + 1 âƒ¦âƒ¦ğ›¾ğ‘£ğ‘¡âƒ¦âƒ¦2 â‰¥ ğ‘Ÿ(ğ‘¥ğ‘¡+1) + 1 âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡ + ğ›¾ğ‘£ğ‘¡âƒ¦âƒ¦2 + 1 âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 .

2ğ›¾

2ğ›¾

2ğ›¾

Thus ğ‘Ÿ(ğ‘¥ğ‘¡+1) âˆ’ ğ‘Ÿ(ğ‘¥ğ‘¡) â‰¤ âˆ’ 1 âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 âˆ’ âŸ¨ğ‘£ğ‘¡, ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âŸ©. (96) ğ›¾

By ğ¿ smoothness of ğ‘“ (Â·), ğ‘“ (ï¸€ğ‘¥ğ‘¡+1)ï¸€ âˆ’ ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€ â‰¤ âŸ¨ï¸€âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€ , ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âŸ©ï¸€ + ğ¿ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 . (97) 2

Summing up (97) with (96) we obtain

Î¦ (ï¸€ğ‘¥ğ‘¡+1)ï¸€ âˆ’ Î¦ (ï¸€ğ‘¥ğ‘¡)ï¸€ â‰¤ âŸ¨âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘£ğ‘¡, ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âŸ© âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 ğ›¾2

â‰¤ 1 âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ‘£ğ‘¡âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğœ† )ï¸‚ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 .

2ğœ†

ğ›¾22

We are now ready to present EF21-Prox and provide its convergence guarantees in general non-convex case.
64

EF21 with Bells & Whistles

Oct 6, 2021

I.1 CONVERGENCE FOR GENERAL NON-CONVEX FUNCTIONS

Algorithm 7 EF21-Prox

1:

Input:

starting point ğ‘¥0

âˆˆ Rğ‘‘; ğ‘”ğ‘–0

âˆˆ Rğ‘‘

for ğ‘– =

1, . . . , ğ‘› (known by nodes); ğ‘”0

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–0

(known by master); learning rate ğ›¾ > 0

2: for ğ‘¡ = 0,1, 2, . . . , ğ‘‡ âˆ’ 1 do

3: Master computes ğ‘¥ğ‘¡+1 = proxğ›¾ğ‘Ÿ (ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡)

4: for all nodes ğ‘– = 1, . . . , ğ‘› in parallel do

5:

Compress ğ‘ğ‘¡ğ‘– = ğ’(âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡+1) âˆ’ ğ‘”ğ‘–ğ‘¡) and send ğ‘ğ‘¡ğ‘– to the master

6:

Update local state ğ‘”ğ‘–ğ‘¡+1 = ğ‘”ğ‘–ğ‘¡ + ğ‘ğ‘¡ğ‘–

7: end for

8:

Master

computes

ğ‘”ğ‘¡+1

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘”ğ‘–ğ‘¡+1

via

ğ‘”ğ‘¡+1

=

ğ‘”ğ‘¡

+

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ‘ğ‘¡ğ‘–

9: end for

10: Output: ğ‘¥Ë†ğ‘‡ chosen uniformly from {ğ‘¥ğ‘¡}ğ‘¡âˆˆ[ğ‘‡ ]

Theorem 12. Let Assumption 1 hold, ğ‘Ÿ(Â·) be convex and Î¦inf = infğ‘¥âˆˆRğ‘‘ Î¦(ğ‘¥) > âˆ’âˆ. Set the

stepsize in Algorithm 7 as

(ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1

ğ¿

ğ›½

def

0 < ğ›¾ < 2 + ğ¿Ìƒï¸€ ğœƒ = ğ›¾0, (98)

âˆšï¸

where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğœƒ

=

1

âˆ’

(1

âˆ’

ğ›¼)(1

+

ğ‘ ),

ğ›½

=

(1

âˆ’

ğ›¼)

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

for

any

ğ‘ 

>

0.

Fix ğ‘‡ â‰¥ 1 and let ğ‘¥Ë†ğ‘‡ be chosen from the iterates ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘‡ âˆ’1 uniformly at random. Then

â› âˆšï¸ƒ â

[ï¸ âƒ¦

ğ‘‡ âƒ¦2]ï¸

4 (ï¸€Î¦0 âˆ’ Î¦inf )ï¸€ 2E [ï¸€ğº0]ï¸€

1 1ğœƒ

E âƒ¦ğ’¢ğ›¾(ğ‘¥Ë† )âƒ¦ â‰¤

(ï¸

)ï¸ +

â1 + (ï¸

)ï¸

â  . (99)

ğ‘‡ ğ›¾ 1 âˆ’ ğ›¾ğ›¾0 ğœƒğ‘‡

ğ›¾ 1 âˆ’ ğ›¾ğ›¾0 ğ¿Ìƒï¸€ ğ›½

If the stepsize is set to 0 < ğ›¾ â‰¤ ğ›¾0/2, then

[ï¸

2]ï¸ 8 (ï¸€Î¦0 âˆ’ Î¦inf )ï¸€ 2E [ï¸€ğº0]ï¸€ (ï¸ƒ

âˆšï¸ƒ )ï¸ƒ 2ğœƒ

E

âƒ¦ âƒ¦ğ’¢

ğ›¾

(ğ‘¥Ë†

ğ‘‡

)

âƒ¦ âƒ¦

â‰¤

+

1+

.

ğ›¾ğ‘‡

ğœƒğ‘‡

ğ›¾ğ¿Ìƒï¸€ ğ›½

(100)

Proof. First, let us apply Lemma 14 with ğ‘£ğ‘¡ = ğ‘”ğ‘¡, ğœ† > 0

Î¦ (ï¸€ğ‘¥ğ‘¡+1)ï¸€ â‰¤ Î¦ (ï¸€ğ‘¥ğ‘¡)ï¸€ + 1 âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğœ† )ï¸‚ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 .

2ğœ†

ğ›¾22

(101)

Subtract Î¦inf from both sides, take expectation, and deï¬ne ğ›¿ğ‘¡ = Î¦ (ğ‘¥ğ‘¡) âˆ’ Î¦inf , ğºğ‘¡ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

â€–ğ‘”ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2,

ğ‘…ğ‘¡

=

âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2,

then

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğœ† )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + 1 E [ï¸€ğºğ‘¡]ï¸€ .

ğ›¾22

2ğœ†

(102)

Note that the proof of Lemma 1 does not rely on the update rule for ğ‘¥ğ‘¡+1, but only on the way the estimator ğ‘”ğ‘–ğ‘¡+1 is constructed. Therefore, (14) also holds for the composite case

E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒ)E [ï¸€ğºğ‘¡]ï¸€ + ğ›½ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ .

(103)

Adding (102) with a 2ğœƒ1ğœ† multiple of (103) , we obtain

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + 1 E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ + 1 E [ï¸€ğºğ‘¡]ï¸€ + 1 âˆ’ ğœƒ E [ï¸€ğºğ‘¡]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğœ† )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

2ğœƒğœ†

2ğœ†

2ğœƒğœ†

ğ›¾22

+ 1 ğ›½ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ 2ğœƒğœ†

=

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

1 E [ï¸€ğºğ‘¡]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğœ† âˆ’

ğ›½

)ï¸‚ ğ¿2 E [ï¸€ğ‘…ğ‘¡]ï¸€ .

Ìƒï¸€

2ğœƒğœ†

ğ›¾ 2 2 2ğœƒğœ†

65

EF21 with Bells & Whistles

Oct 6, 2021

By summing up inequalities for ğ‘¡ = 0, . . . , ğ‘‡ âˆ’ 1, we arrive at

1

1

(ï¸‚ 1 ğ¿ ğœ†

ğ›½

)ï¸‚ ğ‘‡ âˆ’1 âˆ‘ï¸

0 â‰¤ E [ï¸€ğ›¿ğ‘‡ ]ï¸€ + E [ï¸€ğºğ‘‡ ]ï¸€ â‰¤ ğ›¿0 + E [ï¸€ğº0]ï¸€ âˆ’ âˆ’ âˆ’ âˆ’ ğ¿Ìƒï¸€2

E [ï¸€ğ‘…ğ‘¡]ï¸€ .

2ğœƒğœ† 2ğœƒğœ† ğ›¾ 2 2 2ğœƒğœ† ğ‘¡=0

Thus

ğ‘‡ âˆ’1
âˆ‘ï¸

(ï¸‚

1

)ï¸‚ (ï¸‚ 1 ğ¿ ğœ† ğ›½ )ï¸‚âˆ’1

E [ï¸€ğ‘…ğ‘¡]ï¸€ â‰¤ ğ›¿0 + E [ï¸€ğº0]ï¸€

âˆ’ âˆ’ âˆ’ ğ¿Ìƒï¸€2

ğ‘¡=0 2ğœƒğœ† ğ›¾ 2 2 2ğœƒğœ†

(ï¸ƒ

âˆšï¸ƒ

)ï¸ƒ (ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1

= ğ›¿0 + 1 ğœƒ E [ï¸€ğº0]ï¸€ 1 âˆ’ ğ¿ âˆ’ ğ›½ ğ¿Ìƒï¸€2

2ğœƒ ğ›½ğ¿Ìƒï¸€2

ğ›¾2

ğœƒ

= ğ›¾2ğ¹0ğµ.

(104)

âˆšï¸ where in the ï¬rst equality we choose ğœ† = ğ›½ğœƒ ğ¿Ìƒï¸€2, and in the second we deï¬ne ğ¹ 0 d=ef ğ›¿0 +

1 âˆšï¸

ğœƒ

E [ï¸€ğº0]ï¸€,

ğµ

d=ef

(ï¸‚ ğ›¾

âˆ’

ğ¿ğ›¾2

âˆ’

âˆšï¸ )ï¸‚âˆ’1 ğ›½ ğ¿2ğ›¾2

=

(ï¸ ğ›¾

âˆ’

ğ›¾2 )ï¸âˆ’1.

2ğœƒ ğ›½ğ¿Ìƒï¸€2

2

ğœƒ Ìƒï¸€

ğ›¾0

By Lemma 13 with ğ‘£ğ‘¡ = ğ‘”ğ‘¡ we have

E

[ï¸ âƒ¦ âƒ¦ğ’¢ğ›¾

(ï¸€ğ‘¥Ë†ğ‘‡

)ï¸€âƒ¦2]ï¸ âƒ¦

=

1 ğ‘‡âˆ‘âˆ’ï¸1 E [ï¸âƒ¦âƒ¦ğ’¢ğ›¾ (ï¸€ğ‘¥ğ‘¡)ï¸€âƒ¦âƒ¦2]ï¸

ğ‘‡

ğ‘¡=0

2

ğ‘‡ âˆ’1
âˆ‘ï¸

2

ğ‘‡ âˆ’1
âˆ‘ï¸

â‰¤

E [ï¸€ğ‘…ğ‘¡]ï¸€ +

E [ï¸€ğºğ‘¡]ï¸€

ğ›¾2ğ‘‡

ğ‘‡

ğ‘¡=0

ğ‘¡=0

(ğ‘–)

2

ğ‘‡ âˆ’1
âˆ‘ï¸

2 E [ï¸€ğº0]ï¸€

2

ğ›½ğ¿Ìƒï¸€2

ğ‘‡ âˆ’1
âˆ‘ï¸

â‰¤

E [ï¸€ğ‘…ğ‘¡]ï¸€ +

+

E [ï¸€ğ‘…ğ‘¡]ï¸€

ğ›¾2ğ‘‡

ğ‘‡ğœƒ

ğ‘‡ğœƒ

ğ‘¡=0

ğ‘¡=0

(â‰¤ğ‘–ğ‘–) 2ğ¹ 0ğµ + 2 E [ï¸€ğº0]ï¸€ + 2 ğ›½ğ¿Ìƒï¸€2 ğ›¾2ğ¹ 0ğµ

ğ‘‡

ğ‘‡ğœƒ

ğ‘‡ğœƒ

(ï¸ƒ 2ğ¹ 0ğµ

)ï¸ƒ ğ›¾2ğ›½ğ¿Ìƒï¸€2

2 E [ï¸€ğº0]ï¸€

=

1+

+

ğ‘‡

ğœƒ

ğ‘‡ğœƒ

2ğ¹ 0

(ï¸ƒ

)ï¸ƒ ğ›¾2ğ›½ğ¿Ìƒï¸€2

2 E [ï¸€ğº0]ï¸€

=

(ï¸

)ï¸ 1 +

+

,

ğ‘‡ ğ›¾ 1 âˆ’ ğ›¾ğ›¾0

ğœƒ

ğ‘‡ğœƒ

where in (ğ‘–) we apply Lemma 18 with ğ¶ d=ef ğ›½ğ¿Ìƒï¸€2, ğ‘ ğ‘¡ d=ef E [ğºğ‘¡], ğ‘Ÿğ‘¡ d=ef E [ğ‘…ğ‘¡]. (ğ‘–ğ‘–) is due to (104).

(ï¸‚ âˆšï¸ )ï¸‚âˆ’1 Note that for ğ›¾ < ğ¿2 + ğ›½ğœƒ ğ¿Ìƒï¸€ , we have

ğ›¾2ğ›½ğ¿Ìƒï¸€2 <

ğ›½ğœƒ ğ¿Ìƒï¸€2 â‰¤ 1.

ğœƒ

(ï¸‚ âˆšï¸ )ï¸‚2

ğ¿ + ğ›½ ğ¿Ìƒï¸€

2

ğœƒ

(105)

Thus

[ï¸ âƒ¦

ğ‘‡ âƒ¦2]ï¸

4ğ¹ 0

2 E [ï¸€ğº0]ï¸€

E âƒ¦ğ’¢ğ›¾(ğ‘¥Ë† )âƒ¦ â‰¤

(ï¸

)ï¸ +

ğ›¾

ğ‘‡ğœƒ

ğ‘‡ ğ›¾ 1 âˆ’ ğ›¾0

4ğ›¿0

2E [ï¸€ğº0]ï¸€

2E [ï¸€ğº0]ï¸€

âˆšï¸ƒ 1ğœƒ

=

(ï¸

)ï¸ +

+ (ï¸

)ï¸

.

ğ‘‡ ğ›¾ 1 âˆ’ ğ›¾ğ›¾0

ğœƒğ‘‡ ğ‘‡ ğ›¾ 1 âˆ’ ğ›¾ğ›¾0 ğœƒ ğ›½ğ¿Ìƒï¸€2

Set ğ›¾ â‰¤ ğ›¾0/2, then the bound simpliï¬es to

[ï¸

2]ï¸

8ğ›¿0 2E [ï¸€ğº0]ï¸€ (ï¸ƒ

âˆšï¸ƒ )ï¸ƒ 2ğœƒ

E

âƒ¦ âƒ¦ğ’¢

ğ›¾

(ğ‘¥Ë†

ğ‘‡

âƒ¦ )âƒ¦

â‰¤

+

1+

.

ğ›¾ğ‘‡

ğœƒğ‘‡

ğ›¾ ğ›½ğ¿Ìƒï¸€2

(106)

66

EF21 with Bells & Whistles

Oct 6, 2021

Corollary 17. Let assumptions of Theorem 12 hold,
ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›, (ï¸ âˆšï¸€ )ï¸âˆ’1
ğ›¾ = ğ¿ + 2ğ¿Ìƒï¸€ ğ›½/ğœƒ .

Then,

after

ğ‘‡

iterations/communication

rounds

of

EF21-Prox

we

have

E

[ï¸

âƒ¦ âƒ¦âˆ‡

ğ‘“

(ğ‘¥Ë†

ğ‘‡

)âƒ¦âƒ¦2

]ï¸

â‰¤

ğœ€2.

It

requires

(ï¸ƒ )ï¸ƒ ğ¿Ìƒï¸€ğ›¿0
#grad = ğ’ª ğ›¼ğœ€2 ,

âˆšï¸

where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğ›¿0

=

Î¦(ğ‘¥0)

âˆ’

Î¦ğ‘–ğ‘›ğ‘“ .

Proof. The proof is the same as for Corollary 2.

I.2 CONVERGENCE UNDER POLYAK-ÅOJASIEWICZ CONDITION

In order to extend the analysis of Polyak-Åojasiewicz functions to composite optimization, we use the following Assumption 5 from (Li & Li, 2018; Wang et al., 2018).
Assumption 5 (Polyak-Åojasiewicz). There exists ğœ‡ > 0 such that

â€–ğ’¢ğ›¾(ğ‘¥)â€–2 â‰¥ 2ğœ‡ (Î¦(ğ‘¥) âˆ’ Î¦(ğ‘¥â‹†))

for all ğ‘¥ âˆˆ Rğ‘‘, where ğ‘¥â‹† = arg minğ‘¥ Î¦(ğ‘¥).
Theorem 13. Let Assumptions 1 and 5 hold, ğ‘Ÿ(Â·) be convex and Î¦inf = infğ‘¥âˆˆRğ‘‘ Î¦(ğ‘¥) > âˆ’âˆ. Set the stepsize in Algorithm 7 as

â§ (ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1

â«

â¨

2ğ›½

ğœƒâ¬

ğ›¾ â‰¤ min ğ¿ + 2ğ¿Ìƒï¸€

,

.

â©

ğœƒ

âˆšï¸ ğœ‡ + ğœƒğ¿Ìƒï¸€ 2ğ›½ â­

ğœƒ

(107)

Let

Î¨ğ‘¡

def
=

Î¦(ğ‘¥ğ‘¡)

âˆ’

Î¦(ğ‘¥â‹†)

+

âˆšï¸ 1 ğºğ‘¡ with ğœ† = 2ğ›½ ğ¿Ìƒï¸€. Then for any ğ‘‡

â‰¥ 0, we have

ğœƒğœ†

ğœƒ

E

[ï¸€Î¨ğ‘‡

]ï¸€

â‰¤

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸ğ‘‡

E

[ï¸€Î¨0]ï¸€

,

2

âˆšï¸

where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğœƒ

=

1

âˆ’

(1

âˆ’

ğ›¼)(1

+

ğ‘ ),

ğ›½

=

(1

âˆ’

ğ›¼)

(ï¸€1

+

ğ‘ âˆ’1)ï¸€

for

any

ğ‘ 

>

0.

(108)

Proof. We start as in the previous proof, but subtract Î¦(ğ‘¥â‹†) from both sides of (101) and deï¬ne

ğ›¿ğ‘¡

d=ef

Î¦ (ğ‘¥ğ‘¡)

âˆ’

Î¦ (ğ‘¥â‹†)

.

Recall

that

ğºğ‘¡

=

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

â€–ğ‘”ğ‘–ğ‘¡

âˆ’

âˆ‡ğ‘“ğ‘–(ğ‘¥ğ‘¡)â€–2,

ğ‘…ğ‘¡

=

âƒ¦âƒ¦ğ‘¥ğ‘¡+1

âˆ’

ğ‘¥ğ‘¡âƒ¦âƒ¦2.

Then

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ â‰¤ E [ï¸€ğ›¿ğ‘¡]ï¸€ âˆ’ (ï¸‚ 1 âˆ’ ğ¿ âˆ’ ğœ† )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€ + 1 E [ï¸€ğºğ‘¡]ï¸€ .

ğ›¾22

2ğœ†

By Lemma 1, we have

E [ï¸€ğºğ‘¡+1]ï¸€ â‰¤ (1 âˆ’ ğœƒ)E [ï¸€ğºğ‘¡]ï¸€ + ğ›½ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ .

Then by adding (109) with a ğœƒ1ğœ† multiple of (110) we obtain

67

(109) (110)

EF21 with Bells & Whistles

Oct 6, 2021

E [ï¸€ğ›¿ğ‘¡+1]ï¸€ + 1 E [ï¸€ğºğ‘¡+1]ï¸€

â‰¤

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

1

(ï¸‚ 1

âˆ’

ğœƒ

+

ğœƒ )ï¸‚ E [ï¸€ğºğ‘¡]ï¸€

âˆ’

(ï¸‚ 1

âˆ’

ğ¿

âˆ’

ğœ† )ï¸‚ E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒğœ†

ğœƒğœ†

2

ğ›¾22

+ 1 ğ›½ğ¿Ìƒï¸€2E [ï¸€ğ‘…ğ‘¡]ï¸€ ğœƒğœ†

=

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

1

(ï¸‚ 1

âˆ’

ğœƒ )ï¸‚ E [ï¸€ğºğ‘¡]ï¸€

âˆ’

(ï¸‚ 1

âˆ’

ğ¿

âˆ’

ğœ†

âˆ’

ğ›½

)ï¸‚ ğ¿2 E [ï¸€ğ‘…ğ‘¡]ï¸€

Ìƒï¸€

ğœƒğœ†

2

ğ›¾ 2 2 ğœƒğœ†

(ï¸ƒ

âˆšï¸‚ )ï¸ƒ

(ğ‘–)
=

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

1

(ï¸‚ 1

âˆ’

ğœƒ

)ï¸‚

E

[ï¸€ğºğ‘¡]ï¸€

âˆ’

1ğ¿ âˆ’âˆ’

2ğ›½ ğ¿

E [ï¸€ğ‘…ğ‘¡]ï¸€

Ìƒï¸€

ğœƒğœ†

2

ğ›¾2

ğœƒ

(ğ‘–ğ‘–)
â‰¤

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

1

(ï¸‚ 1

âˆ’

ğœƒ

)ï¸‚

E

[ï¸€ğºğ‘¡]ï¸€

âˆ’

1 E [ï¸€ğ‘…ğ‘¡]ï¸€ ,

ğœƒğœ†

2

2ğ›¾

(111)

âˆšï¸ where in (ğ‘–) we choose ğœ† = 2ğœƒğ›½ ğ¿Ìƒï¸€2, (ğ‘–ğ‘–) is due to the stepsize choice (the ï¬rst term in minimum).
Next, combining Assumption 5 with Lemma 13, we have

2ğœ‡ğ›¿ğ‘¡ = 2ğœ‡ (ï¸€Î¦(ğ‘¥ğ‘¡) âˆ’ Î¦(ğ‘¥â‹†))ï¸€ â‰¤ âƒ¦âƒ¦ğ’¢ğ›¾(ğ‘¥ğ‘¡)âƒ¦âƒ¦2 â‰¤ 2 ğ‘…ğ‘¡ + 2ğºğ‘¡, ğ›¾2

and

âˆ’ğ‘…ğ‘¡ â‰¤ âˆ’ğœ‡ğ›¾2ğ›¿ğ‘¡ + ğ›¾2ğºğ‘¡.

(112)

Thus (111) can be further bounded as

E [Î¨]

=

[ï¸‚ E ğ›¿ğ‘¡+1 +

1

]ï¸‚ ğºğ‘¡+1

ğœƒğœ†

â‰¤

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

1

(ï¸‚ 1

âˆ’

ğœƒ

)ï¸‚

E

[ï¸€ğºğ‘¡]ï¸€

âˆ’

1 E [ï¸€ğ‘…ğ‘¡]ï¸€

ğœƒğœ†

2

2ğ›¾

(112)
â‰¤

E [ï¸€ğ›¿ğ‘¡]ï¸€ +

1

(ï¸‚ 1

âˆ’

ğœƒ

)ï¸‚

E

[ï¸€ğºğ‘¡]ï¸€

âˆ’

ğ›¾ğœ‡ E

[ï¸€ğ›¿ğ‘¡]ï¸€

+

ğ›¾

E

[ï¸€ğºğ‘¡]ï¸€

ğœƒğœ†

2

2

2

=

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸

E

[ï¸€ğ›¿ğ‘¡]ï¸€

+

1

(ï¸‚ 1

âˆ’

ğœƒ

+

ğ›¾ğœƒğœ† )ï¸‚ E [ï¸€ğºğ‘¡]ï¸€

2

ğœƒğœ†

22

â‰¤

(ï¸ 1

âˆ’

ğ›¾ğœ‡ )ï¸

E

[ï¸‚ ğ›¿ğ‘¡

+

1

]ï¸‚ ğºğ‘¡ ,

2

ğœƒğœ†

(113)

where the last inequality follows by our assumption on the stepsize (the second term in minimum). It remains to unroll the recurrence.

Corollary 18. Let assumptions of Theorem 13 hold,

ğ‘”ğ‘–0 = âˆ‡ğ‘“ğ‘–(ğ‘¥0), ğ‘– = 1, . . . , ğ‘›,

â§ (ï¸ƒ

âˆšï¸‚ )ï¸ƒâˆ’1

â«

â¨

2ğ›½

ğœƒâ¬

ğ›¾ = min ğ¿ + 2ğ¿Ìƒï¸€

,

.

â©

ğœƒ

âˆšï¸ ğœ‡ + ğœƒğ¿Ìƒï¸€ 2ğ›½ â­

ğœƒ

Then, after ğ‘‡ iterations/communication rounds of EF21-Prox we have E [ï¸€ğ‘“ (ğ‘¥ğ‘‡ ) âˆ’ ğ‘“ (ğ‘¥â‹†)]ï¸€ â‰¤ ğœ€. It requires

(ï¸ƒ

)ï¸ƒ

ğœ‡ + ğ¿Ìƒï¸€ (ï¸‚ ğ›¿0 )ï¸‚

ğ‘‡ = #grad = ğ’ª

log

ğ›¼ğœ‡

ğœ€

(114)

âˆšï¸

iterations/communications rounds/gradint computations at each node, where ğ¿Ìƒï¸€ =

1 ğ‘›

âˆ‘ï¸€ğ‘›
ğ‘–=1

ğ¿2ğ‘– ,

ğ›¿0 = Î¦(ğ‘¥0) âˆ’ Î¦ğ‘–ğ‘›ğ‘“ .

68

EF21 with Bells & Whistles

Proof. Note that by Lemma 17 we have

âˆšï¸‚

âˆš

ğœ‡

2ğ›½

4ğœ‡ 2 2

+ ğ¿Ìƒï¸€

â‰¤

+ ğ¿Ìƒï¸€

ğœƒ

ğœƒ

ğ›¼

ğ›¼

(ï¸

)ï¸

4 ğœ‡ + ğ¿Ìƒï¸€

â‰¤

.

ğ›¼

The remainder of the proof is the same as for Corollary 3.

Oct 6, 2021

69

EF21 with Bells & Whistles

Oct 6, 2021

J USEFUL AUXILIARY RESULTS

J.1 BASIC FACTS

For all ğ‘, ğ‘, ğ‘¥1, . . . , ğ‘¥ğ‘› âˆˆ Rğ‘‘, ğ‘  > 0 and ğ‘ âˆˆ (0,1] the following inequalities hold

â€–ğ‘â€–2 ğ‘ â€–ğ‘â€–2

âŸ¨ğ‘, ğ‘âŸ© â‰¤

+

,

2ğ‘ 

2

âŸ¨ğ‘ âˆ’ ğ‘, ğ‘ + ğ‘âŸ© = â€–ğ‘â€–2 âˆ’ â€–ğ‘â€–2,

1 â€–ğ‘â€–2 âˆ’ â€–ğ‘â€–2 â‰¤ â€–ğ‘ + ğ‘â€–2, 2
â€–ğ‘ + ğ‘â€–2 â‰¤ (1 + ğ‘ )â€–ğ‘â€–2 + (1 + 1/ğ‘ )â€–ğ‘â€–2,

âƒ¦ ğ‘› âƒ¦2

ğ‘›

âƒ¦ 1 âˆ‘ï¸ âƒ¦

1 âˆ‘ï¸ 2

âƒ¦

ğ‘¥ğ‘–âƒ¦ â‰¤

â€–ğ‘¥ğ‘–â€– ,

âƒ¦âƒ¦ ğ‘› ğ‘–=1 âƒ¦âƒ¦

ğ‘› ğ‘–=1

(ï¸ ğ‘ )ï¸âˆ’1

1âˆ’

â‰¤ 1 + ğ‘,

2

(ï¸ ğ‘ )ï¸

ğ‘

1 + (1 âˆ’ ğ‘) â‰¤ 1 âˆ’ ,

2

2

log (1 âˆ’ ğ‘) â‰¤ âˆ’ğ‘.

(115) (116) (117) (118)
(119)
(120) (121) (122)

Bias-variance decomposition For a random vector ğœ‰ âˆˆ Rğ‘‘ and any deterministic vector ğ‘¥ âˆˆ Rğ‘‘, the variance of ğœ‰ can be decomposed as

E [ï¸€â€–ğœ‰ âˆ’ E[ğœ‰]â€–2]ï¸€ = E [ï¸€â€–ğœ‰â€–2]ï¸€ âˆ’ â€–E[ğœ‰]â€–2

(123)

Tower property of mathematical expectation. For random variables ğœ‰, ğœ‚ âˆˆ Rğ‘‘ we have E[ğœ‰] = E[E[ğœ‰ | ğœ‚]]
under assumption that all expectations in the expression above are well-deï¬ned.

(124)

J.2 USEFUL LEMMAS

Lemma 15 (Lemma 5 of (RichtÃ¡rik et al., 2021)). If 0 â‰¤ ğ›¾ â‰¤ âˆšğ‘1+ğ‘ , then ğ‘ğ›¾2 + ğ‘ğ›¾ â‰¤ 1. Moreover, {ï¸ }ï¸
the bound is tight up to the factor of 2 since âˆšğ‘1+ğ‘ â‰¤ min âˆš1ğ‘ , 1ğ‘ â‰¤ âˆšğ‘2+ğ‘ .

Lemma 16 (Lemma 2 of (Li et al., 2021)).

Suppose that function ğ‘“

is ğ¿-smooth and let ğ‘¥ğ‘¡+1

def
=

ğ‘¥ğ‘¡ âˆ’ ğ›¾ğ‘”ğ‘¡, where ğ‘”ğ‘¡ âˆˆ Rğ‘‘ is any vector, and ğ›¾ > 0 any scalar. Then we have

ğ‘“ (ğ‘¥ğ‘¡+1) â‰¤ ğ‘“ (ğ‘¥ğ‘¡) âˆ’ ğ›¾ âƒ¦âƒ¦âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 âˆ’ (ï¸‚ 1 âˆ’ ğ¿ )ï¸‚ âƒ¦âƒ¦ğ‘¥ğ‘¡+1 âˆ’ ğ‘¥ğ‘¡âƒ¦âƒ¦2 + ğ›¾ âƒ¦âƒ¦ğ‘”ğ‘¡ âˆ’ âˆ‡ğ‘“ (ğ‘¥ğ‘¡)âƒ¦âƒ¦2 . (125)

2

2ğ›¾ 2

2

Lemma 17 (Lemma 3 of (RichtÃ¡rik et al., 2021)). Let 0 < ğ›¼ < 1 and for ğ‘  > 0 let ğœƒ(ğ‘ ) and ğ›½(ğ‘ ) be deï¬ned as

def
ğœƒ(ğ‘ ) = 1 âˆ’ (1 âˆ’ ğ›¼)(1 + ğ‘ ),

ğ›½(ğ‘ )

def
=

(1

âˆ’

ğ›¼)(1

+

ğ‘ âˆ’1).

Then the solution of the optimization problem

{ï¸‚ ğ›½(ğ‘ )

ğ›¼ }ï¸‚

min

: 0<ğ‘ <

ğ‘  ğœƒ(ğ‘ )

1âˆ’ğ›¼

is given by ğ‘ * = âˆš 1 âˆ’ 1. Furthermore, ğœƒ(ğ‘ *) = 1 âˆ’ âˆš1 âˆ’ ğ›¼, ğ›½(ğ‘ *) = 1âˆšâˆ’ğ›¼ and

1âˆ’ğ›¼

1âˆ’ 1âˆ’ğ›¼

âˆšï¸ƒ

âˆš

ğ›½(ğ‘ *)

1

1 1âˆ’ğ›¼

2

ğœƒ(ğ‘ *) = âˆš1 âˆ’ ğ›¼ âˆ’ 1 = ğ›¼ + ğ›¼ âˆ’ 1 â‰¤ ğ›¼ âˆ’ 1.

(126) (127)

In the trivial case ğ›¼ = 1, we have ğ›½ğœƒ((ğ‘ ğ‘ )) = 0 for any ğ‘  > 0, and (127) is satisï¬ed.

70

EF21 with Bells & Whistles

Oct 6, 2021

Lemma 18. Let (arbitrary scalar) non-negative sequences {ğ‘ ğ‘¡}ğ‘¡â‰¥0, and {ğ‘Ÿğ‘¡}ğ‘¡â‰¥0 satisfy

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

âˆ‘ï¸ ğ‘ ğ‘¡+1 â‰¤ (1 âˆ’ ğœƒ) âˆ‘ï¸ ğ‘ ğ‘¡ + ğ¶ âˆ‘ï¸ ğ‘Ÿğ‘¡

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

for some parameters ğœƒ âˆˆ (0, 1], ğ¶ > 0. Then for all ğ‘‡ â‰¥ 0

ğ‘‡âˆ‘âˆ’ï¸1 ğ‘ ğ‘¡ â‰¤ ğ‘ 0 + ğ¶ ğ‘‡âˆ‘âˆ’ï¸1 ğ‘Ÿğ‘¡.

ğœƒğœƒ

ğ‘¡=0

ğ‘¡=0

Proof. We have

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

âˆ‘ï¸ ğ‘ ğ‘¡ âˆ’ ğ‘ 0 â‰¤ âˆ‘ï¸ ğ‘ ğ‘¡ + ğ‘ ğ‘‡ âˆ’ ğ‘ 0

ğ‘¡=0

ğ‘¡=0

ğ‘‡ âˆ’1
= âˆ‘ï¸ ğ‘ ğ‘¡+1

ğ‘¡=0

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

â‰¤ (1 âˆ’ ğœƒ) âˆ‘ï¸ ğ‘ ğ‘¡ + ğ¶ âˆ‘ï¸ ğ‘Ÿğ‘¡

ğ‘¡=0

ğ‘¡=0

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

ğ‘‡ âˆ’1

= âˆ‘ï¸ ğ‘ ğ‘¡ âˆ’ ğœƒ âˆ‘ï¸ ğ‘ ğ‘¡ + ğ¶ âˆ‘ï¸ ğ‘Ÿğ‘¡.

ğ‘¡=0

ğ‘¡=0

ğ‘¡=0

Dividing both sides by ğœƒ > 0 and rearranging the terms, we get

ğ‘‡âˆ‘âˆ’ï¸1 ğ‘ ğ‘¡ â‰¤ ğ‘ 0 + ğ¶ ğ‘‡âˆ‘âˆ’ï¸1 ğ‘Ÿğ‘¡.

ğœƒğœƒ

ğ‘¡=0

ğ‘¡=0

(128)

71

