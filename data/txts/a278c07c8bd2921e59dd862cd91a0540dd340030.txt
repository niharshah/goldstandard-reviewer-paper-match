arXiv:1906.02120v2 [stat.ML] 17 Oct 2019

Adapting Neural Networks for the Estimation of Treatment Effects
Claudia Shi1, David M. Blei1,2, and Victor Veitch2
1Department of Computer Science, Columbia University 2Department of Statistics, Columbia University
Abstract
This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we ﬁt models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these ﬁtted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the ﬁrst step. The question we address is: how can we adapt the design and training of the neural networks used in the ﬁrst step in order to improve the quality of the ﬁnal estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The ﬁrst is a new architecture, the Dragonnet, that exploits the sufﬁciency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties ‘out-of-the-box’. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at github.com/claudiashi57/dragonnet.
1 Introduction
We consider the estimation of causal effects from observational data. Observational data is often readily available in situations where randomized control trials are expensive or impossible. However, causal inference from observational data must address (possible) confounding factors that affect both treatment and outcome. Failure to adjust for confounders can lead to incorrect conclusions. To address this, a practitioner collects covariate information in addition to treatment and outcome status. The causal effect can be identiﬁed if the covariates contain all confounding variables. We will work in this ‘no hidden confounding’ setting throughout the paper. The task we consider is the estimation of the effect of a treatment T (e.g., a patient receives a drug) on an outcome Y (whether they recover) adjusting for covariates X (e.g., illness severity or socioeconomic status).
We consider how to use neural networks to estimate the treatment effect. The estimation of treatment effects proceeds in two stages. First, we ﬁt models for the conditional outcome Q(t, x) = [Y | t, x] and the propensity score g(x) = P(T = 1|x). Then, we plug these ﬁtted models into a downstream estimator. The strong predictive performance of neural networks motivates their use for effect estimation [e.g. Sha+16; Joh+16; Lou+17; AS17; Ala+17; Sch+18; Yoo+18; Far+18]. We will use neural networks as models for the conditional outcome and propensity score.
In principle, using neural networks for the conditional outcome and propensity score models is straightforward. We can use a standard net to predict the outcome Y from the treatment and covariates, and another to predict the treatment from the covariates. With
1

a suitable choice of training objective, the trained models will yield consistent estimates of the conditional outcomes and propensity scores. However, neural network research has focused on predictive performance. What is important for causal inference is the quality of the downstream estimation. This leads to our main question: how can we modify the design and training of neural networks in order to improve the quality of treatment effect estimation?

We address this question by adapting results from the statistical literature on the estimation of treatment effects. The contributions of this paper are:

1. A neural network architecture—the Dragonnet—based on the sufﬁciency of the propensity score for causal estimation.
2. A regularization procedure—targeted regularization—based on non-parametric estimation theory.
3. An empirical study of these methods on established benchmark datasets. We ﬁnd the methods substantially improve estimation quality in comparison to existing neuralnetwork based approaches. This holds even when the methods degrade predictive performance.

Setup. For concreteness, we consider the estimation of the average effect of a binary

treatment, though the methods apply broadly. The data are generated independently and

identically (Yi, Ti, Xi)

iid
∼

P.

The average treatment affect (ATE) ψ is

ψ = [Y | do(T = 1)] − [Y | do(T = 0)].

The use of Pearl’s do notation indicates that the effect of interest is causal. It corresponds to what happens if we intervene by assigning a new patient the drug. If the observed covariates X include all common causes of the treatment and outcome—i.e., block all backdoor paths— then the causal effect is equal to a parameter of the observational distribution P,

ψ = [ [Y | X , T = 1] − [Y | X , T = 0]].

(1.1)

We want to estimate ψ using a ﬁnite sample from P. Following equation 1.1, a natural

estimator is

ψˆQ = 1 ni

Qˆ(1, xi) − Qˆ(0, xi) ,

(1.2)

where Qˆ is an estimate of the conditional outcome Q(t, x) = [Y | t, x]. There are also more sophisticated estimators that additionally rely on estimates gˆ of the propensity score g(x) = P(T = 1 | x); see section 3.

We now state our question of interest plainly. We want to use neural networks to model Q and g. How should we adapt the design and training of these networks so that ψˆ is a good
estimate of ψ?

2 Dragonnet
Our starting point is a classic result, [RR83, Thm. 3], Theorem 2.1 (Sufﬁciency of Propensity Score). If the average treatment effect ψ is identiﬁable from observational data by adjusting for X , i.e., ψ = [ [Y | X , T = 1] − [Y | X , T = 0]], then adjusting for the propensity score also sufﬁces:
ψ = [ [Y | g(X ), T = 1] − [Y | g(X ), T = 0]]

2

In words: it sufﬁces to adjust for only the information in X that is relevant for predicting
the treatment. Consider the parts of X that are relevant for predicting the outcome but not
the treatment. Those parts are irrelevant for the estimation of the causal effect, and are
effectively noise for the adjustment. As such, we expect conditioning on these parts to hurt ﬁnite-sample performance —instead, we should discard this information.1 For example, when computing the expected-outcome-based estimator ψˆQ, (equation 1.2), we should train Qˆ to predict Y from only the part of X relevant for T , even though this may degrade the predictive performance of Qˆ.

Here is one way to use neural networks to ﬁnd the relevant parts of X . First, train a deep net to predict T . Then remove the ﬁnal (predictive) layer. Finally, use the activation of the remaining net as features for predicting the outcome. In other contexts (e.g., images) this is a standard procedure [e.g., Gir+14]. The hope is that the ﬁrst net will distill the covariates into the features relevant for treatment prediction, i.e., relevant to the propensity score gˆ. Then, conditioning on the features is equivalent to conditioning on the propensity score itself. However, this process is cumbersome. With ﬁnite data, estimation errors in the propensity score model gˆ may propagate to the conditional outcome model. Ideally, the model itself should choose a tradeoff between predictive accuracy and the propensity-score representation.

This method inspires Dragonnet,2 a three-headed ar-

chitecture that provides an end-to-end procedure for

predicting propensity score and conditional outcome

from covariates and treatment information. See Fig-

ure 1. We use a deep net to produce a representation

layer Z(X ) ∈ p, and then predict both the treatment

and outcome from this shared representation. We use

2-hidden layer neural networks for each of the outcome models Qˆ(0, ·) : p → and Qˆ(1, ·) : p → .

In contrast, we use a simple linear map (followed

Figure 1: Dragonnet architecture.

by a sigmoid) for the propensity score model gˆ. The

simple map forces the representation layer to tightly couple to the estimated propensity

scores.

Dragonnet has parameters θ and output heads Qnn(ti, xi; θ ) and gnn(xi; θ ). We train the model by minimizing an objective function,

θˆ = argmin Rˆ(θ ; X), where
θ

Rˆ(θ ; X) = 1 ni

(Qnn(ti, xi; θ ) − yi)2 + αCrossEntropy(gnn(xi; θ ), ti) ,

(2.1) (2.2)

where α ∈ + is a hyperparameter weighting the loss components. The ﬁtted model is Qˆ = Qnn(·, ·; θˆ) and gˆ = gnn(·; θˆ). With the ﬁtted outcome model Qˆ in hand, we can estimate the treatment effect with the estimator ψˆQ (equation 1.2).

In principle, the end-to-end training and high capacity of Dragonnet might allow it to avoid throwing away any information. In section 5, we study the Dragonnet’s behaviour empirically and ﬁnd evidence that it does indeed trade off prediction quality to achieve a good representation of the propensity score. Further, this trade-off improves ATE estimation even when we use a downstream estimator, such as ψˆQ, that does not use the estimated propensity scores.

If the propensity-score head is removed from Dragonnet, the resulting architecture is

1A caveat: this intuition applies to the complexity of learning the outcome model. In the case of high-variance
outcome and small sample size modeling all covariates may help as a variance reduction technique. 2“Dragonnet” because the dragon has three heads.

3

(essentially) the TARNET architecture from Shalit et al. [Sha+16]. We compare with TARNET in section 5. We also compare to the multiple-stage method described above.

3 Targeted Regularization

We now turn to targeted regularization, a modiﬁcation to the objective function used for neural network training. This modiﬁed objective is based on non-parametric estimation theory. It yields a ﬁtted model that, with a suitable downstream estimator, guarantees desirable asymptotic properties.

We review some necessary results from semi-parametric estimation theory, and then explain targeted regularization. The summary of this section is:
1. ψˆ has good asymptotic properties if it satisﬁes a certain equation (equation 3.1) with Qˆ and gˆ.
2. Targeted regularization (equation 3.2) is a modiﬁcation to the training objective. 3. Minimizing this objective forces (Qˆtreg, gˆ, ψˆtreg) to satisfy the required equation, where
Qˆtreg and ψˆtreg are particular choices for Qˆ and ψˆ.

Setup. Recall that the general recipe for estimating a treatment effect has two steps: (i) ﬁt
models for the conditional outcome Q and the propensity score g; (ii) plug the ﬁtted models Qˆ and gˆ into a downstream estimator ψˆ. The estimator ψˆQ in equation 1.2 is the simplest
example. There are a wealth of alternatives that, in theory, offer better performance.

Such estimators are studied in the semi-parametric estimation literature; see Kennedy
[Ken16] for a readable introduction. We restrict ourselves to the (simpler) fully non-
parametric case; i.e., we make no assumptions on the form of the true data generating
distribution. For our purposes, the key results from non-parametric theory are of the form: If the tuple (Qˆ, gˆ, ψˆ) satisﬁes a certain equation, (equation 3.1 below), then, asymptotically, the estimator ψˆ will have various good properties. For instance,

1. robustness in the double machine-learning sense [Che+17a; Che+17b]—ψˆ converges to ψ at a fast rate (in the sample complexity sense) even if Qˆ and gˆ converge slowly;
and 2. efﬁciency—asymptotically, ψˆ has the lowest variance of any consistent estimator of ψ.
That is, the estimator ψˆ is asymptotically the most data efﬁcient estimator possible.

These asymptotic guarantees hold if (i) Qˆ and gˆ are consistent estimators for the conditional

outcome and propensity scores, and (ii) the tuple satisﬁes the non-parametric estimating

equation,

0= 1 n

ϕ( yi, ti, xi; Qˆ, gˆ, ψˆ),

i

(3.1)

where ϕ is the efﬁcient inﬂuence curve of ψ,

ϕ( y, t, x; Q, g, ψ) = Q(1, x) − Q(0, x) + t − 1 − t { y − Q(t, x)} − ψ. g(x) 1 − g(x)

See, e.g., Chernozhukov et al. [Che+17b] and van der Laan and Rose [vR11] for details.
A natural way to construct a tuple satisfying the non-parametric estimating equation is to estimate Qˆ and gˆ in a manner agnostic to the downstream estimation task, and then choose ψˆ so that equation 3.1 is satisﬁed. This yields the A-IPTW estimator [Rob+00; Rob00]. Unfortunately, the presence of gˆ in the denominator of some terms can cause the A-IPTW be unstable in ﬁnite samples, despite its asymptotic optimality. (In our experiments, the A-IPTW estimator consistently under-performs the naive estimator ψˆQ.)

4

Targeted minimum loss estimation (TMLE) [vR11] is an alternative strategy that mitigates the ﬁnite-sample instability. The TMLE relies on (task-agnostic) ﬁtted models Qˆ and gˆ. The idea is to perturb the estimate Qˆ—with perturbation depending on gˆ—such that the simple estimator ψˆQ satisﬁes the non-parametric estimating equation (equation 3.1).
Because the simple estimator is free of gˆ in denominators, it is stable with ﬁnite data.
Thus, the TMLE yields an estimate that has both good asymptotic properties and good
ﬁnite-sample performance. The ideas that underpin TMLE are the main inspiration for
targeted regularization.

Targeted regularization. We now describe targeted regularization. We require Q and g to be modeled by a neural network (such as Dragonnet) with output heads Qnn(ti, xi; θ ) and gnn(xi; θ ). By default, the neural network is trained by minimizing a differentiable objective function Rˆ(θ ; X ), e.g., equation 2.2.

Targeted regularization is a modiﬁcation to the objective function. We introduce an extra model parameter and a regularization term γ( y, t, x; θ , ) deﬁned by

Q˜(ti, xi; θ , γ( yi, ti, xi; θ ,

) = Qnn(ti, xi; θ ) + ) = ( yi − Q˜(ti, xi; θ ,

ti − 1 − ti gnn(xi; θ ) 1 − gnn(xi; θ ) ))2.

We then train the model by minimizing the modiﬁed objective,

θˆ, ˆ = argmin Rˆ(θ ; X) + β 1 n

γ( yi, ti, xi; θ , ) .

θ,

i

The variable β ∈ + is a hyperparameter. Next, we deﬁne an estimator ψˆtreg as:

ψˆtreg = 1 n

Qˆtreg(1, xi) − Qˆtreg(0, xi),

i

Qˆtreg = Q˜(·, ·; θˆ, ˆ).

where

(3.2)
(3.3) (3.4)

The key observation is

0 = ∂ Rˆ(θ ; X ) + β 1 n

1 γ( yi, ti, xi; θ , ) ˆ = β n

i

ϕ( yi, ti, xi; Qˆtreg, gˆ, ψˆtreg). (3.5)

That is, minimizing the targeted regularization term forces Qˆtreg, gˆ, ψˆtreg to satisfy the non-parametric estimating equation equation 3.1.
Accordingly, the estimator ψˆtreg will have the good non-parametric asymptotic properties so long as Qˆtreg and gˆ are consistent. Consistency is plausible—even with the addition of the targeted regularization term—because the model can choose to set to 0, which (essentially) recovers the original training objective. For instance, if Qˆ and gˆ are consistent in the original model than the targeted regularization estimates will also be consistent. In detail, the targeted regularization model preserves ﬁnite VC dimension (we add only 1 parameter), so the limiting model is an argmin of the true (population) risk. The true risk for the targeted regularization loss has a minimum at Qˆ = [Y |x, t], gˆ = P(T = 1|x), and ˆ = 0. This is because the original risk is minimized at these values (by consistency), and the targeted regularization term (a squared error) is minimized at Qˆ + ˆH(gˆ) = [Y |x, t], which is achieved at ˆ = 0.

The key idea, equation 3.5, is inspired by TMLE. Like targeted regularization, TMLE introduces an extra model parameter . It then chooses ˆ so that a ˆ-perturbation of Qˆ satisﬁes the non-parametric estimating equation with ψˆQ. However, TMLE uses only the
parameter to ensure that the non-parametric estimating equation are satisﬁed, while
targeted regularization adapts the entire model. Both TMLE and targeted regularization

5

are designed to yield an estimate with stable ﬁnite-sample behavior and strong asymptotic guarantees. We compare these methods in section 5.
Finally, we note that estimators satisfying the non-parametric estimating equation are also ‘doubly robust’ in the sense that the effect estimate is consistent if either Qˆ or gˆ is consistent. This property also holds for the targeted regularization estimator, if either the modiﬁed outcome model or propensity score is consistent.
4 Related Work
The methods connect to different areas in causal inference and estimation theory.
Representations for causal inference. Dragonnet is related to papers using representation learning ideas for treatment effect estimation. The Dragonnet architecture resembles TARNET, a two-headed outcome-only model used as the baseline in Shalit et al. [Sha+16]. TARNET is Dragonnet without the propensity head. One approach in the literature emphasizes learning a covariate representation that has a balanced distribution across treatment and outcome; e.g., BNNs [Joh+16] and CFRNET [Sha+16]. Other work combines deep generative models with standard causal identiﬁcation results. CEVEA [Lou+17], GANITE [Yoo+18], and CMPGP [AS17] use VAEs, GANs, and multi-task gaussian processes, respectively, to estimate treatment effects. Another approach combines (pre-trained) propensity scores with neural networks; e.g., Propensity Dropout [Ala+17] and Perfect Matching [Sch+18]. Dragonnet complements these approaches. Exploiting the sufﬁciency of the propensity score is a distinct approach, and it may be possible to combine it with other strategies.
Non-parametric estimation and machine learning. Targeted regularization relates to a body of work combining machine learning methods with semi-parametric estimation theory. As mentioned above, the main inspiration for the method is targeted minimum loss estimation [vR11]. Chernozhukov et al. [Che+17a; Che+17b] develop theory for ‘double machine learning’, showing that if certain estimating equations are satisﬁed then treatment estimates will converge at a parametric (O(1/ n)) rate even if the conditional outcome and propensity models converge much more slowly. Farrell et al. [Far+18] prove that neural networks converge at a fast enough rate to invoke the double machine learning results. This gives theoretical justiﬁcation for the use of neural networks to model propensity scores and conditional expected outcomes. Targeted regularization is complementary: we rely on the asymptotic results for motivation, and address the ﬁnite-sample approach.
5 Experiments
Do Dragonnet and targeted regularization improve treatment effect estimation in practice? Dragonnet is a high-capacity model trained end-to-end: does it actually throw away information irrelevant to the propensity score? TMLE already offers an approach for balancing asymptotic guarantees with ﬁnite sample performance: does targeted regularization improve over this? We study the methods empirically using two semi-synthetic benchmarking tools.3 We ﬁnd that Dragonnet and targeted regularization substantially improve estimation quality. Moreover, we ﬁnd that Dragonnet exploits propensity score sufﬁciency, and that targeted regularization improves on TMLE.
3Code and data at github.com/claudiashi57/dragonnet
6

Table 1: Dragonnet with targeted regularization is state-of-the-art among neural network methods on
the IHDP benchmark data. Entries are mean absolute error (and standard error) across simulations. Estimators are computed with the training and validation data (∆in), heldout data (∆out ), and all data (∆all ). Note that using all the data for both training and estimation improves estimation relative to data splitting. Values from previous work are as reported in the cited papers.

Method
BNN [Joh+16] TARNET [Sha+16] CFR Wass[Sha+16] CEVAEs [Lou+17] GANITE [Yoo+18]
baseline (TARNET) baseline + t-reg Dragonnet Dragonnet + t-reg

∆in
0.37 ± .03 0.26 ± .01 0.25 ± .01 0.34 ± .01 0.43 ± .05
0.16 ± .01 0.15 ± .01 0.14 ± .01 0.14 ± .01

∆out
0.42 ± .03 0.28 ± .01 0.27 ± .01 0.46 ± .02 0.49 ± .05
0.21 ± .01 0.20 ± .01 0.21 ± .01 0.20 ± .01

∆all
— — — — —
0.13 ± .00 0.12 ± .00 0.12 ± .00 0.11 ± .00

5.1 Setup
Ground truth causal effects are rarely available for real-world data. Accordingly, empirical evaluation of causal estimation procedures rely on semi-synthetic data. For the conclusions to be useful, the semi-synthetic data must have good ﬁdelity to the real world. We use two pre-established causal benchmarking tools.
IHDP. Hill [Hil11] introduced a semi-synthetic dataset constructed from the Infant Health and Development Program (IHDP). This dataset is based on a randomized experiment investigating the effect of home visits by specialists on future cognitive scores. Following [Sha+16], we use 1000 realizations from the NPCI package [Dor16].4 The data has 747 observations.
ACIC 2018. We also use the IBM causal inference benchmarking framework, which was developed for the 2018 Atlantic Causal Inference Conference competition data (ACIC 2018) [Shi+18]. This is a collection of semi-synthetic datasets derived from the linked birth and infant death data (LBIDD) [MA98]. Importantly, the simulation is comprehensive— including 63 distinct data generating process settings—and the data are relatively large. Each competition dataset is a sample from a distinct distribution, which is itself drawn randomly according to the data generating process setting. For each data generating process setting, we randomly pick 3 datasets of size either 5k or 10k.
Some of the datasets have overlap violations. That is, P(T = 1|x) can be very close to 0 or 1 for many values of x. Although overlap violations are an important area of study, this is not our focus and the methods of this paper are not expected to be appropriate in this setting. As a simple heuristic, we exclude all datasets where the heldout treatment accuracy for Dragonnet is higher than 90%; high classiﬁcation accuracy indicates a strong separation between the treated and control populations. Subject to this criteria, 101 datasets remain.
Model and Baseline Settings. Our main baseline is an implementation of the 2-headed TARNET architecture from Shalit et al. [Sha+16]. This model predicts only the outcome, and is equivalent to the Dragonnet architecture with the propensity head removed.
For Dragonnet and targeted regularization, we set the hyperparameters α in equation 2.2
4There is a typo in Shalit et al. [Sha+16]. They use setting A of the NPCI package, which corresponds to setting B in Hill [Hil11]

7

Table 2: Dragonnet and targeted regularization improve estimation on average on ACIC 2018. Table entries are mean absolute error over all datasets.

Table 3: Dragonnet and targeted regularization improve over the baseline about half the time, but improvement is substantial when it does happen. Error values are mean absolute error on ACIC 2018.

Method

∆all

baseline (TARNET) 1.45

baseline + t-reg

1.40

Dragonnet

0.55

Dragonnet + t-reg 0.35

ψQ
baseline:
+ t-reg + dragon + dragon & t-reg

%i mp r o v e
0%
42% 63% 46%

↑av g
0
0.30 1.42 2.37

↓av g
0
0.11 0.01 0.01

and β in equation 3.2 to 1. For the targeted regularization baseline, we use TARNET as the outcome model and logistic regression as the propensity score model. We train TARNET and logistic regression jointly using the targeted regularization objective.
For all models, the hidden layer size is 200 for the shared representation layers and 100 for the conditional outcome layers. We train using stochastic gradient descent with momentum. Empirically, the choice of optimizer has a signiﬁcant impact on estimation performance for the baseline and for Dragonnet and targeted regularization. Among the optimizers we tried, stochastic gradient descent with momentum resulted in the best performance for the baseline.
For IHDP experiments, we follow established practice [e.g. Sha+16]. We randomly split the data into test/validation/train with proportion 63/27/10 and report the in sample and out of sample estimation errors. However, this procedure is not clearly motivated for parameter estimation, so we also report the estimation errors for using all the data for both training and estimation.
For the ACIC 2018 experiments, we re-run each estimation procedure 25 times, use all the data for training and estimation, and report the average estimate errors.
Estimators and metrics. For the ACIC experiments, we report mean absolute error of the average treatment effect estimate, ∆ = ψˆ − ψ . For IHDP, following established procedure, we report mean absolute difference between the estimate and the sample ATE, ∆ = ψˆ − 1/n i Q(1, xi) − Q(0, xi) . By default, we use ψˆQ as our estimator, except for models with targeted regularization, where we report ψˆtreg (equation 3.4). For estimation, we exclude any data point with estimated propensity score outside [0.01, 0.99].

5.2 Effect on Treatment Estimation
The IHDP simulation is the de-facto standard benchmark for neural network treatment effect estimation methods. In table 1 we report the estimation error of a number of approaches. Dragonnet with targeted regularization is state-of-the-art among these methods. However, the small sample size and limited simulation settings of IHDP make it difﬁcult to draw conclusions about the methods. The main takeaways of table 1 are: i) Our baseline method is a strong comparator and ii) reusing the same data for ﬁtting the model and computing the estimate works better than data splitting.
The remaining experiments use the Atlantic Causal Inference Conference 2018 competition (ACIC 2018) dataset. In table 2 we report the mean absolute error over the included datasets. The main observation is that Dragonnet improves estimation relative to the baseline (TARNET), and adding targeted regularization to Dragonnet improves estimation further. Additionally, we observe that despite its asymptotically optimal properties, TMLE

8

Figure 2: Dragonnet has worse prediction loss on the held out data than baseline, but better estimation quality. The estimation error and loss are from a separate run of the ACIC dataset where we held out 30% of data to compute the loss.

Figure 3: Dragonnet improves over the baseline if many covariates are irrelevant for treatment. The result is obtained using the ACIC datasets. We stratiﬁed the datasets by the number of irrelevant covariates and compared the median MAE across strata.

hurts more than it helps on average. Double robust estimators such as the TMLE are known to be sensitive to violations of assumptions in other contexts [KS07]. We note that targeted regularization can improve performance even where TMLE does not.
In table 2, we report average estimation error across simulations. We see that Dragonnet and targeted regularization improve the baseline estimation. Is this because of small improvement on most datasets or major improvement on a subset of datasets? In table 3 we present an alternative comparison. We divide the datasets according to whether each method improves estimation relative to the baseline. We report the average improvement in positive cases, and the average degradation in negative cases. We observe that Dragonnet and targeted regularization help about half the time. When the methods do help, the improvement is substantial. When the methods don’t help, the degradation is mild.

5.3 Why does Dragonnet work?
Dragonnet was motivated as an end-to-end version of a multi-stage approach. Does the end-to-end network work better? We now compare to the multi-stage procedure, which we call NEDnet.5 NEDnet has essentially the same architecture as Dragonnet. NEDnet is ﬁrst trained using a pure treatment prediction objective. The ﬁnal layer (treatment prediction head) is then removed, and replaced with an outcome-prediction neural network matching the one used by Dragonnet. The representation layers are then frozen, and the outcome-prediction network is trained on the pure outcome prediction task. NEDnet and Dragonnet are compared in table 4. The end-to-end Dragonnet produces more accurate estimates.
Table 4: Dragonnet produces more accurate estimates than NEDnet, a multi-stage alternative. Table entries are mean absolute error over all datasets.

IHDP
Dragonnet NEDnet

ψˆQ
0.12 ± 0.00 0.15 ± 0.01

ψˆTMLE
0.12 ± 0.00 0.12 ± 0.00

ACIC
Dragonnet NEDnet

ψˆQ
0.55 1.49

ψˆTMLE
1.97 2.80

We motivated the Dragonnet architecture by the sufﬁciency of the propensity score for causal adjustment. This architecture improves estimation performance. Is this because it is
5“NEDnet” because the network is beheaded after the ﬁrst stage.
9

exploiting the sufﬁciency? Three observations suggest this is the case.
First, compared to TARNET, Dragonnet has worse performance as a predictor for the outcome, but better performance as an estimator. See Figure 2. This is the case even when we use the simple estimator ψˆQ, which does not use the output of the propensity-score head of Dragonnet. This suggests that, as intended, the shared representation adapts to the treatment prediction task, at the price of worse predictive performance for the outcome prediction task.
Second, Dragonnet is supposed to predict the outcome from only information relevant to T . If this holds, we expect Dragonnet to improve signiﬁcantly over the baseline when there is a large number of covariates that inﬂuence only Y (i.e., not T ). These covariates are "noise" for the causal estimation since they are irrelevant for confounding. As illustrated in Figure 3, when most of the effect on Y is from confounding variables, the differences between Dragonnet and the baseline are not signiﬁcant. As the number of covariates that only inﬂuence Y increases, Dragonnet becomes a better estimator.
Third, with inﬁnite data, Dragonnet and TARNET should perform equally well. With ﬁnite data, we expect Dragonnet to be more data efﬁcient as it discards covariates that are irrelevant for confounding. We verify this intuition by comparing model performance with different amount of data. We ﬁnd Dragonnet’s improvement is more signiﬁcant with smaller-sized data. See Appendix A for details.

5.4 When does targeted regularization work?
The guarantees from non-parametric theory are asymptotic, and apply in regimes where the estimated models closely approximate the true values. We divide the datasets according to the error of the simple (Q-only) baseline estimator. In cases where the initial estimator is good, TMLE and targeted regularization behave similarly. This is as expected. In cases where the initial estimator is poor, TMLE signiﬁcantly degrades estimation quality, but targeted regularization does not. It appears that adapting the entire learning process to satisfy the non-parametric estimating equation avoids some bad ﬁnite sample effects. We do not have a satisfactory theoretical explanation for this. Understanding this phenomena is an important direction for future work.
Table 5: We divide the datasets according to the mean absolute ATE estimation error for the baseline simple estimator. We observe that the initial TMLE and targeted regularization are comparable if the initial estimate is good (∆AT E < 1 ). However, the TMLE makes the estimate worse if the initial estimate is bad ∆AT E < 1, whereas dragonnet with targeted regularization still improve it.

Baseline
+ t-reg + tmle
Dragonnet
+ t-reg + tmle

∆AT E < 1 0.03
0.04 0.03
0.02
0.03 0.02

∆AT E > 1 28.52
27.36 176.14
10.64
6.50 39.34

10

6 Discussion
The results above raise a number of interesting questions and directions for future work. Foremost, although TMLE and targeted regularization are conceptually similar, the methods have very different performance in our experiments. Understanding the root causes of this behavior may open new lines of attack for the practical use of non-parametric estimation methods. Relatedly, another promising avenue of development is to adapt the well-developed literature on TMLE [e.g., vR11; LG16] to more advanced targeted regularization methods. For instance, there are a number of TMLE approaches to estimating the average treatment effect on the treated (ATT), and it’s not clear a priori which of these, if any, will yield a good targeted-regularization type procedure. Generally, extending the methods here to other causal estimands and mediation analysis is an important problem. There are also important questions about Dragonnet-type architectures. We motivated Dragonnet with the intuition that we should use only the information in the confounders that is relevant to both the treatment assignment and outcome. Our empirical results support this intuition. However, in other contexts, this intuition breaks down. For example, in randomized control trials—where covariates only affect the outcome—adjustment can increase power [Saq+13]. This is because adjustment may reduce the effective variance of the outcome, and double robust methods can be used to ensure consistency (the treatment assignment model is known trivially). Our motivating intuition is well supported in the large-data, unknown propensity-score model case we consider. It would be valuable to have a clear articulation of the trade-offs involved and practical guidelines for choosing which covariates to adjust for in observational studies. As a sharp example of the need, recent papers have used Dragonnet-type models for causal adjustment with black-box embedding methods [Vei+19a; Vei+19b]. They achieve good estimation accuracy, but it remains unclear exactly what trade-offs may being made. In a different direction, our experiments do not support the routine use of data-splitting in effect estimation. Established benchmarks have commonly split the data into train and test sets and used predictions on the test set to compute the downstream estimator. This technique has some theoretical justiﬁcation [Che+17b] (in a K-fold variant), but signiﬁcantly degrades performance in our experiments. We note that this is also true in our preliminary (unreported) experiments with K-fold data splitting. A clearer understanding of why and when data splitting is not appropriate would be of substantial practical value. We note that Farrell et al. [Far+18] prove that data reuse does not invalidate estimation when using neural networks.
Acknowledgements
We are thankful to Yixin Wang, Dhanya Sridhar, Jackson Loper, Roy Adams, and Shira Mitchell for helpful comments and discussions. This work was supported by ONR N0001415-1-2209, ONR 133691-5102004 , NIH 5100481-5500001084, NSF CCF-1740833, FA 8750-14-2-0009, the Alfred P. Sloan Foundation, the John Simon Guggenheim Foundation, Facebook, Amazon, IBM, and the government of Canada through NSERC. The GPUs used for this research were donated by the NVIDIA Corporation.
11

References

[AS17] [Ala+17] [Bot+12] [Che+17a] [Che+17b] [CH08] [Dor16] [Far+18] [Gir+14] [Hil11] [Joh+16] [KS07] [Ken16] [LG16] [Lou+17] [MA98] [Pot93] [Rob00] [Rob+00] [RR83]

A. Alaa and M. van der Schaar. “Bayesian inference of individualized treatment effects using multi-task gaussian processes”. In: arXiv e-prints arXiv:1704.02801 (2017). A. M. Alaa, M. Weisz, and M. van der Schaar. “Deep counterfactual networks with propensity-dropout”. In: arXiv e-prints arXiv:1706.05966 (2017). L. Bottou, J. Peters, J. Quiñonero-Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray, P. Simard, and E. Snelson. “Counterfactual reasoning and learning systems”. In: arXiv preprint arXiv:1209.2355 (2012). V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, C. Hansen, W. Newey, and J. Robins. “Double/debiased machine learning for treatment and structural parameters”. In: The Econometrics Journal (2017). V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, C. Hansen, and W. Newey. “Double/debiased/neyman machine learning of treatment effects”. In: American Economic Review 5 (2017). S. R. Cole and M. A. Hernán. “Constructing inverse probability weights for marginal structural models.” In: American Journal of Epidemiology (2008). V. Dorie. Non-parametrics for Causal Inference. https://github.com/vdorie/ npci. 2016. M. H. Farrell, T. Liang, and S. Misra. “Deep Neural Networks for Estimation and Inference: Application to Causal Effects and Other Semiparametric Estimands”. In: arXiv e-prints: arxiv: 1809.09953 (2018). R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. “Rich feature hierarchies for accurate object detection and semantic segmentation”. In: 2014 IEEE Conference on Computer Vision and Pattern Recognition (2014). J. L. Hill. “Bayesian nonparametric modeling for causal inference”. In: Journal of Computational and Graphical Statistics 1 (2011). F. D. Johansson, U. Shalit, and D. Sontag. “Learning representations for counterfactual inference”. In: arXiv e-prints arXiv:1605.03661 (2016). J. D. Y. Kang and J. L. Schafer. “Demystifying double robustness: a comparison of alternative strategies for estimating a population mean from incomplete data”. In: Statist. Sci. 4 (2007). E. H. Kennedy. “Semiparametric theory and empirical processes in causal inference”. In: Statistical causal inferences and their applications in public health research. 2016. M van der Laan and S Gruber. “One-step targeted minimum loss-based estimation based on universal least favorable one-dimensional submodels”. In: The International Journal of Biostatistics (2016). C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling. “Causal effect inference with deep latent-variable models”. In: NEURIPS. 2017. M. F. MacDorman and J. O. Atkinson. “Infant mortality statistics from the linked birth/infant death”. In: Mon Vital Stat Rep, 46(suppl 2):1–22 (1998). F. J. Potter. “The effect of weight trimming on nonlinear survey estimates”. In: Proceedings of the American Statistical Association, Section on Survey Research Methods. 1993. J. M. Robins. “Robust estimation in sequentially ignorable missing data and causal inference models”. In: ASA Proceedings of the Section on Bayesian Statistical Science (2000). J. M. Robins, A. Rotnitzky, and M. van der Laan. “On proﬁle likelihood: comment”. In: Journal of the American Statistical Association 450 (2000). P. R. Rosenbaum and D. B. Rubin. “The central role of the propensity score in observational studies for causal effects”. In: Biometrika 1 (1983).

12

[Saq+13] [Sch+99] [Sch+18] [Sha+16] [Shi+18] [vR11] [Vei+19a] [Vei+19b] [Yoo+18]

N. Saquib, J. Saquib, and J. P. A. Ioannidis. “Practices and impact of primary outcome adjustment in randomized controlled trials: meta-epidemiologic study”. In: BMJ (2013). D. O. Scharfstein, A. Rotnitzky, and J. M. Robins. “Adjusting for nonignorable drop-out using semiparametric nonresponse models”. In: Journal of the American Statistical Association 448 (1999). P. Schwab, L. Linhardt, and W. Karlen. “Perfect match: a simple method for learning representations for counterfactual inference with neural networks”. In: arXiv e-prints arXiv:1810.00656 (2018). U. Shalit, F. D. Johansson, and D. Sontag. “Estimating individual treatment effect: generalization bounds and algorithms”. In: arXiv e-prints arXiv:1606.03976 (2016). Y. Shimoni, C. Yanover, E. Karavani, and Y. Goldschmnidt. “Benchmarking framework for performance-evaluation of causal inference analysis”. In: ArXiv preprint arXiv:1802.05046 (2018). M. van der Laan and S. Rose. Targeted Learning: Causal Inference for Observational and Experimental Data. 2011. V. Veitch, Y. Wang, and D. M. Blei. “Using embeddings to correct for unobserved confounding in networks”. In: Advances in Neural Information Processing Systems. 2019. V. Veitch, D. Sridhar, and D. M. Blei. “Using text embeddings for causal inference”. In: arXiv e-prints (2019). J. Yoon, J. Jordon, and M. van der Schaar. “Ganite: estimation of individualized treatment effects using generative adversarial nets”. In: International Conference on Learning Representations. 2018.

13

Appendix A
We motivated Dragonnet as a tool for better ﬁnite-sample performance. To examine this intuition, we randomly picked two large (50k) datasets from ACIC 2018.6 For each, we subsample the data at a number of rates and examine how estimation error changes with sample size. As shown in Figure 4, the difference between Dragonnet and TARNET’s estimation error become smaller as the amount of data increase. In Figure 5, since the initial estimate for both TARNET and Dragonnet are close to perfect, the improvement is less signiﬁcant.

Figure 4: The difference between Dragonnet and TARNET’s estimation error become smaller as we use an increasing amount of data.

Figure 5: The estimation error of TARNET and Dragonnet converge as the amount of data used increase. Note: the initial estimates are close to perfect.

Appendix B
As part of computing the estimates, we trimming the dataset by throwing away entries with extreme propensity scores. [e.g., Pot93; Sch+99; CH08; Bot+12]. In our experiments, we ﬁnd the choice of the trimming parameter makes a difference in effect estimation even for the simple estimator equation 1.2 as well as for the doubly robust TMLE estimator. table 6 shows the sensitivity of estimation procedures to the choice of truncation parameter.
Table 6: The truncation level used to select data for estimator computation has different effects on different methods and different estimators. i) TARNET produce better result with a steeper truncation (less data), whereas Dragonnet methods perform better when using more data. ii) TMLE estimator’s performance improves with steeper truncation. Table entries are ATE estimation errors (∆all averaged across the included ACIC 2018 datasets, using all the data for both training and estimation.

truncation level
ψQ
baseline (TARNET) baseline + t-reg Dragonnet Dragonnet + t-reg ψT M LE
baseline (TARNET) Dragonnet

[0.01, 0.99]
1.45 1.40 0.55 0.35
8.75 1.97

[0.03, 0.97]
0.66 0.53 0.74 0.37
2.12 2.11

[0.1, 0.9]
0.44 0.40 0.73 0.54
1.27 0.72

6261502077fbb40c9b57c9a5ccf6cd220 and d7a70d3bfd664374ae20484fe94e7fb7
14

