Learning Algebraic Recombination for Compositional Generalization
Chenyao Liu1âˆ— Shengnan An2âˆ— Zeqi Lin3â€  Qian Liu4âˆ— Bei Chen3 Jian-Guang LOU3 Lijie Wen1â€  Nanning Zheng 2 Dongmei Zhang3 1 School of Software, Tsinghua University 2 Xiâ€™an Jiaotong University
3 Microsoft Research Asia 4 Beihang University {liucy19@mails, wenlj@}.tsinghua.edu.cn {an1006634493@stu, nnzheng@mail}.xjtu.edu.cn {Zeqi.Lin, beichen, jlou, dongmeiz}@microsoft.com
qian.liu@buaa.edu.cn

arXiv:2107.06516v1 [cs.CL] 14 Jul 2021

Abstract
Neural sequence models exhibit limited compositional generalization ability in semantic parsing tasks. Compositional generalization requires algebraic recombination, i.e., dynamically recombining structured expressions in a recursive manner. However, most previous studies mainly concentrate on recombining lexical units, which is an important but not sufï¬cient part of algebraic recombination. In this paper, we propose LEAR, an end-toend neural model to learn algebraic recombination for compositional generalization. The key insight is to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic recombination. Speciï¬cally, we learn two modules jointly: a Composer for producing latent syntax, and an Interpreter for assigning semantic operations. Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model. The source code is publicly available at https://github.com/microsoft/ContextualSP.
1 Introduction
The principle of compositionality is an essential property of language: the meaning of a complex expression is fully determined by its structure and the meanings of its constituents (Pelletier, 2003; SzaboÂ´, 2004). Based on this principle, human intelligence exhibits compositional generalization â€” the algebraic capability to understand and produce a potentially inï¬nite number of novel expressions by dynamically recombining known components (Chomsky, 1957; Fodor and Pylyshyn, 1988; Fodor and Lepore, 2002). For example, people who know the meaning of â€œJohn teaches the girlâ€ and
âˆ— Work done during an internship at Microsoft Research. The ï¬rst two authors contributed equally to this paper.
â€  Corresponding author.

John teaches the girl. Tomâ€™s daughter.

â†’

Tom teaches Johnâ€™s daughterâ€™s daughter.

(a) Compositional generalization requires algebraic recombination, i.e., dynamically recombining structured expressions in a recursive manner.

John teaches the girl.

The girl teaches John.

(b) Most previous studies mainly concentrate on recombining lexical units, which is an important but not sufï¬cient part of algebraic recombination.
Figure 1: Compositional generalization.

John tea

Tomâ€™s

â€œTomâ€™s daughterâ€ must know the meaning of â€œTom

teaches Johnâ€™s daughterâ€™s daughterâ€ (Figure 1a),

even though they have never seen such complex

sentences before.

Tom teaches Johnâ€™s

In recent years, there has been accumulating

evidence that end-to-end deep learning models

lack such ability in semantic parsing (i.e., trans-

lating natural language expressions to machine in-

terpretable semantic meanings) tasks (Lake and Ba-

roni, 2018; Keysers et al., 2019; Kim and Linzen,

2020; Tsarkov et al., 2020).

Compositional generalization requires algebraic

recombination, i.e., dynamically recombining

structured expressions in a recursive manner. In

the example in Figure 1a, understanding â€œJohnâ€™s

daughterâ€™s daughterâ€ is a prerequisite for under-

standing â€œTom teaches Johnâ€™s daughterâ€™s daugh-

terâ€, while â€œJohnâ€™s daughterâ€™s daughterâ€ is also a

novel compound expression, which requires recom-

bining â€œJohnâ€ and â€œTomâ€™s daughterâ€ recursively. Most previous studies on compositional general-
ization mainly concentrate on recombining lexical units (e.g., words and phrases) (Lake, 2019; Li et al., 2019; Andreas, 2019; Gordon et al., 2020; AkyuÂ¨rek et al., 2020; Guo et al., 2020a; Russin et al., 2019), of which an example is shown in Figure 1b. This is a necessary part of algebraic recombination, but it is not sufï¬cient for compositional generalization. There have been some studies on algebraic recombination (Liu et al., 2020; Chen et al., 2020). However, they are highly speciï¬c to a relative simple domain SCAN (Lake and Baroni, 2018) and can hardly generalize to more complex domains.
In this paper, our main point to achieve algebraic recombination is to model semantic parsing as a homomorphism between a latent syntactic algebra and a semantic algebra (Montague, 1970; Marcus, 2019). Based on this formalism, we focus on learning the high-level mapping between latent syntactic operations and semantic operations, rather than the direct mapping between expression instances and semantic meanings.
Motivated by this idea, we propose LEAR (Learning Algebraic Recombination), an end-toend neural architecture for compositional generalization. LEAR consists of two modules: a Composer and an Interpreter. Composer learns to model the latent syntactic algebra, thus it can produce the latent syntactic structure of each expression in a bottom-up manner; Interpreter learns to assign semantic operations to syntactic operations, thus we can transform a syntactic tree to the ï¬nal composed semantic meaning.
Experiments on two realistic and comprehensive compositional generalization benchmarks (CFQ (Keysers et al., 2019) and COGS (Kim and Linzen, 2020)) demonstrate the effectiveness of our model: CFQ 67.3% â†’ 90.9%, COGS 35.0% â†’ 97.7%.
2 Compositionality: An Algebraic View
A semantic parsing task aims to learn a meaningassignment function m : L â†’ M , where L is the set of (simple and complex) expressions in the language, and M is the set of available semantic meanings for the expressions in L. Many end-toend deep learning models are built upon this simple and direct formalism, in which the principle of compositionality is not leveraged, thus exhibiting limited compositional generalization.

To address this problem, in this section we put forward the formal statement that â€œcompositionality requires the existence of a homomorphism between the expressions of a language and the meanings of those expressionsâ€ (Montague, 1970).
Let us consider a language as a partial algebra L = L, (fÎ³)Î³âˆˆÎ“ , where Î“ is the set of underlying syntactic (grammar) rules, and we use fÎ³ : Lk â†’ L to denote the syntactic operation with a ï¬xed arity k for each Î³ âˆˆ Î“. Note that fÎ³ is a partial function, which means that we allow fÎ³ be undeï¬ned for certain expressions. Therefore, L is a partial algebra, and we call it a syntactic algebra. In a semantic parsing task, L is latent, and we need to model it by learning from data.
Consider now M = M, G , where G are semantic operations upon M . M is also a partial algebra, and we call it a semantic algebra. In a semantic parsing task, we can easily deï¬ne this algebra (by enumerating all available semantic primitives and semantic operations), since M is a machineinterpretable formal system.
The key to compositionality is that the meaningassignment function m should be a homomorphism from L to M. That is, for each k-ary syntactic operation fÎ³ in L, there exists a k-ary semantic operation gÎ³ âˆˆ G such that whenever fÎ³(e1, ..., ek) is deï¬ned,
m(fÎ³(e1, ..., ek)) = gÎ³(m(e1), ..., m(ek)). (1)
Based on this formal statement, the task of learning the meaning-assignment function m can be transformed as two sub-tasks: (1) learning latent syntax of expressions (i.e., modeling the syntactic algebra L); (2) learning the operation assignment function (fÎ³)Î³âˆˆÎ“ â†’ G. Learning latent syntax. We need to learn a syntactic parser that can produce the syntactic structure of each given expression. To ensure compositional generalization, there must be an underlying grammar (i.e., Î“), and we hypothesize that Î“ is a context-free grammar. Learning operation assignment. In the syntax tree, for each nonterminal node with k nonterminal children, we assign a k-ary semantic operation to it. This operation assignment entirely depends on the underlying syntactic operation Î³ of this node.
In semantic parsing tasks, we do not have respective supervision for these two sub-tasks. Therefore, we need to jointly learning these two sub-tasks only from the end-to-end supervision D âŠ‚ L Ã— M .

Composer ğ‘ªğ‘ªğœ½ğœ½ ğ’›ğ’›|ğ’™ğ’™

ğ‘£ğ‘£14 ğ’©ğ’©3

ğ‘£ğ‘£13

ğ‘£ğ‘£23

ğ‘£ğ‘£12

ğ‘£ğ‘£22 ğ’©ğ’©2 ğ‘£ğ‘£32

ğ‘£ğ‘£11

ğ‘£ğ‘£21

ğ‘£ğ‘£31

ğ‘£ğ‘£41 ğ’©ğ’©1

ğ‘¥ğ‘¥: Who executive produced M0

ğ‘¦ğ‘¦ : SELECT DISTINCT x0
WHERE { x0 EXEC_PROD M0 }

Interpreter ğ‘°ğ‘°ğ“ğ“ ğ’ˆğ’ˆ|ğ’™ğ’™, ğ’›ğ’› JOIN

ğ‘šğ‘š(ğ‘¥ğ‘¥) ğ‘…ğ‘…ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘šğ‘š ğ‘¥ğ‘¥ , ğ‘¦ğ‘¦

Predicate: EXEC_PROD
Entity:M0

Figure 2: An overview of LEAR: (1) Composer CÎ¸(z|x) is a neural network based on latent TreeLSTM, which produces the latent syntax tree z of input expression x; (2) Interpreter IÏ†(g|x, z) is a neural network that assigns a semantic operation for each nonterminal node in z.

3 Model

At the t-th (1 â‰¤ t < T ) merge step, we have:

Ë†it = arg max Linear(Tree-LSTM(rti, rti+1)) (2)
1â‰¤iâ‰¤T âˆ’t

Here â€œTree-LSTMâ€ is the standard child-sum
tree-structured LSTM encoder (Tai et al., 2015). We use vit to denote the i-th cell at layer t (the t-th merge step is determined by the t-th layer), and use rti to denote the representation of vit:

r1i = Linear(Emb(xi))

(3)

ï£±ï£´rtiâˆ’1

ï£²

rti =

Tree-LSTM(r

tâˆ’1 i

,

r

tâˆ’1 i+1

)

t>1 ï£´ï£³rti+âˆ’11

i < Ë†itâˆ’1 i = Ë†itâˆ’1 (4) i > Ë†itâˆ’1

We propose a novel end-to-end neural model LEAR (Learning Algebraic Recombination) for compositional generalization in semantic parsing tasks. Figure 2 shows its overall architecture. LEAR consists of two parts: (1) Composer CÎ¸(z|x), which produces the latent syntax tree z of input expression x; (2) Interpreter IÏ†(g|x, z), which assigns a semantic operation for each nonterminal node in z. Î¸ and Ï† refers to learnable parameters in them respectively. We generate a semantic meaning m(x) according to the predicted z and g in a symbolic manner, then check whether it is semantic equivalent to the ground truth semantic meaning y to produce rewards for optimizing Î¸ and Ï†.
3.1 Composer
We use x = [x1, ..., xT ] to denote an input expression of length T . Composer CÎ¸(z|x) will produce a latent binary tree z given x.
3.1.1 Latent Tree-LSTM
We build up the latent binary tree z in a bottom-up manner based on Tree-LSTM encoder, called latent Tree-LSTM (Choi et al., 2018; Havrylov et al., 2019).
Given the input sequence x of length T , latent Tree-LSTM merges two nodes into one parent node at each merge step, constructing a binary tree after T âˆ’ 1 merge steps. The merge process is implemented by selecting the adjacent node pair which has the highest merging score.

Then we can obtain a unlabeled binary tree,

in

which

{

v

1 1

,

v21

,

...,

v

1 T

}

are

leaf

nodes,

and

{vË†i21 , vË†i32 ..., vË†iTT âˆ’1 } are non-leaf nodes.

3.1.2 Abstraction by Nonterminal Symbols
As discussed in Section 2, our hypothesis is that the underlying grammar Î“ is context-free. Therefore, each syntactic rule Î³ âˆˆ Î“ can be expressed in the form of:

A â†’ B, A âˆˆ N , B âˆˆ (N âˆª Î£)+

where N is a ï¬nite set of nonterminals, and Î£ is a ï¬nite set of terminal symbols.
Abstraction is an essential property of contextfree grammar: each compound expression e will be abstracted as a simple nonterminal symbol N (e), then it can be combined with other expressions to produce more complex expressions, no matter what details e originally has. This setup may beneï¬t the generalizability, thus we want to incorporate it as an inductive bias into our model.
Concretely, we assume that there are at most N latent nonterminals in language L (i.e., N = {N1, ..., NN }, where N is a hyper-parameter). For each node vit in tree z, we perform a (N + 1)-class classiï¬cation:

cË†vt

=

arg

max

Linear(r

t i

)

(5)

i

0â‰¤câ‰¤N

We assign the nonterminal NcË† t to vit when vi
cË†vit > 0. The collection of such nonterminal nodes

are denoted as Vz . Then we modify Equation 4:

ï£±ï£´ritâˆ’1 ï£² rti = Tree-LSTM(ritâˆ’1, rit+âˆ’11) t>1 ï£´ï£³rit+âˆ’11

i < Ë†itâˆ’1 i = Ë†itâˆ’1 i > Ë†itâˆ’1 (6)

rti =

Linear(Emb(

N

(v

t i

)))

rti

vit âˆˆ Vz vit âˆˆ Vz

Equation 6 means that: in nonterminal nodes, the
bottom-up message passing will be reduced from rti to a nonterminal symbol N (vit), thus mimicking the abstraction setup in context-free grammar.

3.2 Interpreter
For each nonterminal node v âˆˆ Vz , Interpreter IÏ†(g|x, z) assigns a semantic operation gv to it.
We divide nonterminal nodes into two categories: (1) lexical nodes, which refer to those containing no any other nonterminal node in the corresponding sub-trees; (2) algebraic nodes, which refer to the rest of nonterminal nodes.

Interpreting Lexical Nodes For each lexical

node v, Interpreter assigns a semantic primitive

(i.e., 0-ary semantic operation) to it. Take the CFQ

benchmark as an example: it uses SPARQL queries

to annotate semantic meanings, thus semantic prim-

itives in CFQ are entities (e.g., m.0gwm wy), predi-

cates (e.g., ns:ï¬lm.director.ï¬lm) and attributes (e.g.,

ns:people.person.gender m 05zppz).

We use a classiï¬er to predict the semantic primi-

tive:

gv = arg max Linear(hv,x)

(7)

gâˆˆGlex

where Glex is the collection of semantic primitives in the domain, and hv,x is the contextual representation of the span corresponding to v (implemented using Bi-LSTM). Contextually conditioned variation is an important phenomenon in language: the meaning of lexical units varies according to the contexts in which they appear (Allwood, 2003). For example, â€œeditorâ€ means a predicate â€œï¬lm.editor.ï¬lmâ€ in expression â€œIs M0 an editor of M1?â€, while it means an attribute â€œï¬lm.editorâ€ in expression â€œIs M0 an Italian editor?â€. This is the reason why we use contextual representation in Equation 7.

Interpreting Algebraic Nodes For each algebraic node v, Interpreter assigns a semantic operation to it. The collection of all possible semantic operations Gopr also depends on the domain. Take

Operation âˆ§(t1, t2) JOIN(t1, t2)

Args[t1, t2]â†’ Result Type
[P, P]â†’P [E, E]â†’E [A, A]â†’A [A, E]â†’E [E, A]â†’E [A, P]â†’P [P, A]â†’P [E, P]â†’E [P, E]â†’E [A, P]â†’E [P, A]â†’E

Example Who [direct and act] M0? Who direct [M0 and M1]? Is M0 an [Italian female]? Is [M0 an Italian female]?
Is M0 M3â€™s [Italian editor]?
Is M0 an [editor of M1]?
Who [marries an Italian]?

Table 1: Semantic operations in CFQ. A/P/E represents Attribute/Predicate/Entity.

the CFQ benchmark as an example1, this domain has two operations (detailed in Table 1): âˆ§ (conjunction) and JOIN.
We also use a classiï¬er to predict the semantic operation of v:

gv = arg max Linear(rv)

(8)

gâˆˆGopr

where rv is the latent Tree-LSTM representation of node v (see Equation 6).
In Equation 8, we do not use any contextual information from outside v. This setup is based on the assumption of semantic locality: each compound expression should mean the same thing in different contexts.

4 Training
Denote Ï„ = {z, g} as the trajectory produced by our model where z and g are actions produced from Composer and Interpreter, respectively, and R(Ï„ ) as the reward of trajectory Ï„ (elaborated in Sec. 4.1). Using policy gradient (Sutton et al., 2000) with the likelihood ratio trick, our model can be optimized by ascending the following gradient:

âˆ‡J (Î¸, Ï†) = EÏ„âˆ¼Ï€Î¸,Ï† R(Ï„ )âˆ‡ log Ï€Î¸,Ï† (Ï„ ) , (9)

where Î¸ and Ï† are learnable parameters in Composer and Interpreter respectively and âˆ‡ is the abbreviation of âˆ‡Î¸,Ï†. Furthermore, the REINFORCE algorithm (Williams, 1992) is leveraged to approximate Eq. 9 and the mean-reward baseline (Weaver and Tao, 2001) is employed to reduce variance.
1It is not difï¬cult to deï¬ne Glex and Gopr for each domain, as semantic meanings are always machine-interpretable. The semantic operations of another compositional generalization benchmark, COGS, are listed in the Appendix.

4.1 Reward Design
The reward R (Ï„ ) combines two parts as:
R (Ï„ ) = Î± Â· R1 (Ï„ ) + (1 âˆ’ Î±) Â· R2 (Ï„ ) , (10)
Logic-based Reward R1(Ï„ ). We use m(x) and y to denote the predicted semantic meaning and the ground truth semantic meaning respectively. Each semantic meaning can be converted to a conjunctive normal form2. We use Sm(x) and Sy to denote conjunctive components in m(x) and y, then deï¬ne R1(Ï„ ) based on Jaccard similarity (i.e., intersection over union):
R1 (Ï„ ) = Jaccard-Sim(Sm(x), Sy) (11)
Primitive-Based Reward R2(Ï„ ). We use Sm(x) and Sy to denote semantic primitives ocurred in m(x) and y. Then we deï¬ne R2(Ï„ ) as:
R2 (Ï„ ) = Jaccard-Sim(Sm(x), Sy) (12)
4.2 Reducing Search Space
To reduce the huge search space of Ï„ , we make two constraints as follows. Parameter Constraint. Consider v, a tree node with n(n > 0) nonterminal children. Composer will never make v a nonterminal node, if no semantic operation has n parameters. Phrase Table Constraint. Following the strategy proposed in Guo et al. (2020b), we build a â€œphrase tableâ€ consisting of lexical units (i.e., words and phrases) paired with semantic primitives that frequently co-occur with them3. Composer will never produce a lexical node outside of this table, and Interpreter will use this table to restrict candidates in Equation 7.
4.3 Curriculum Learning
To help the model converge better, we use a simple curriculum learning (Bengio et al., 2009) strategy to train the model. Speciï¬cally, we ï¬rst train the model on samples of input length less than a cut-off NCL, then further train it on the full train set.
2For example, the semantic meaning of â€œWho directed and edited M0 â€™s prequel and M1?â€ can be converted to a conjunctive normal form with four components: â€œx0 Â· DIRECT Â· x1 Â· PREQUEL Â· M0â€, â€œx0 Â· EDIT Â· x1 Â· PREQUEL Â· M0â€, â€œx0 Â· DIRECT Â· M1â€, and â€œx0 Â· EDIT Â· M1â€.
3Mainly based on statistical word alignment technique in machine translation, detailed in the Appendix.

CFQ x â€œDid a male film director edit and direct M0?â€

SELECT count ( * ) WHERE {

yy

?x0 ns:film.director.film M0 . ?x0 ns:film.editor.film M0 .

?x0 ns:people.person.gender m_05zppz }

COGS x â€œCharlotte was given the cake on a table.â€
cake(x_4) ; give.recipient (x_2, Charlotte)
y AND give.theme(x_2,x_4)
AND cake.nmod.on(x_4, x_7) AND table(x_7)

Figure 3: Examples of CFQ and COGS.

Statistics Train Size Dev Size Test Size Vocab Size Avg Input Len (Train/Test) Avg Output Len (Train/Test) Input Pattern Coveragea Output Pattern Coverage

CFQ 95,743 11,968 11,968
96 13.5/15.1 27.7/34.0
0.022 0.045

COGS 24,155 3,000 21,000
740 7.5/9.8 43.6/67.6 0.783 0.782

Table 2: Dataset statistics.
aInput/output pattern coverage is the percentage of test x/y whose patterns occur in the train data. Output patterns are determined by anonymizing semantic primitives, and input patterns are determined by anonymizing their lexical units.

5 Experimental Setup
Benchmarks. We mainly evaluate LEAR on CFQ (Keysers et al., 2019) and COGS (Kim and Linzen, 2020), two comprehensive and realistic benchmarks for measuring compositional generalization. They use different semantic formulations: CFQ uses SPARQL queries, and COGS uses logical queries (Figure 3 shows examples of them). We list dataset statistics in Table 2. The input/output pattern coverage indicates that: CFQ mainly measures the algebraic recombination ability, while COGS measures both lexical recombination (âˆ¼ 78%) and algebraic recombination (âˆ¼ 22%).
In addition to these two compositional generalization benchmarks in which utterances are synthesized by formal grammars, we also evaluate LEAR on GEO (Zelle and Mooney, 1996), a widely used semantic parsing benchmark, to see whether LEAR can generalize to utterances written by real users. We use the variable-free FunQL (Kate et al., 2005) as the semantic formalism, and we follow the compositional train/test split (Finegan-Dollak et al., 2018) to evaluate compositional generalization. Baselines. For CFQ, we consider 3 groups of models as our baselines: (1) sequence-to-sequence mod-

Models LSTM+Attention (Keysers et al., 2019) Transformer (Keysers et al., 2019) Universal Transformer (Keysers et al., 2019) Evolved Transformer (Furrer et al., 2020) T5-11B (Furrer et al., 2020) T5-11B-mod (Furrer et al., 2020) Neural Shufï¬‚e Exchange (Furrer et al., 2020) CGPS (Furrer et al., 2020; Li et al., 2019) HPD (Guo et al., 2020b) LEAR
w/o Abstraction w/o Semantic locality w/o Primitive-based reward w/o Curriculum learning w/o Tree-LSTM

MCD-MEAN 14.9Â±1.1 17.9Â±0.9 18.9Â±1.4 20.8Â±0.7 40.9Â±4.3 42.1Â±9.1 2.8Â±0.3 7.1Â±1.8 67.3Â±4.1 90.9Â±1.2 85.4Â±4.5 87.9Â±2.7 85.3Â±7.8 71.9Â±15.4 30.4Â±3.2

MCD1 28.9Â±1.8 34.9Â±1.1 37.4Â±2.2 42.4Â±1.0 61.4Â±4.8 61.6Â±12.4 5.1Â±0.4 13.2Â±3.9 72.0Â±7.5 91.7Â±1.0 88.4Â±1.6 89.8Â±1.7 77.0Â±19 59.7Â±23 40.1Â±1.9

MCD2 5.0Â±0.8 8.2Â±0.3 8.1Â±1.6 9.3Â±0.8 30.1Â±2.2 31.3Â±12.8 0.9Â±0.1 1.6Â±0.8 66.1Â±6.4 89.2Â±1.9 80.0Â±11 87.3Â±1.8 89.2Â±2.2 77.2Â±13.5 25.6Â±6.1

MCD3 10.8Â±0.6 10.6Â±1.1 11.3Â±0.3 10.8Â±0.2 31.2Â±5.7 33.3Â±2.3 2.3Â±0.3 6.6Â±0.6 63.9Â±5.7 91.7Â±0.6 87.9Â±0.8 86.5Â±4.6 89.7Â±2.1 78.8Â±9.6 25.4Â±1.8

Table 3: Accuracy on three splits (MCD1/MCD2/MCD3) of CFQ benchmark.

els based on deep encoder-decoder architecture, including LSTM+Attention (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2014), Transformer (Vaswani et al., 2017), Universal Transformer (Dehghani et al., 2018) and Evolved Transformer (So et al., 2019); (2) deep models with large pretrained encoder, such as T5 (Raffel et al., 2019); (3) Models that are specially designed for compositional generalization, which include Neural Shufï¬‚e Exchange Network (Freivalds et al., 2019), CGPS (Li et al., 2019), and state-of-the-art model HPD (Guo et al., 2020b). For COGS, we quote the baseline results in the original paper (Kim and Linzen, 2020). For GEO, we take the baseline results reported by Herzig and Berant (2020), and also compare with two specially designed methods: SpanBasedSP (Herzig and Berant, 2020) and PDE (Guo et al., 2020c).
Evaluation Metric. We use accuracy as the evaluation metric, i.e., the percentage test samples of which the predicted semantic meaning m(x) is semantically equivalent to the ground truth y.
Hyper-Parameters. We set N = 3/2/3 (the number of nonterminal symbols), and Î± = 0.5/1.0/0.9 for CFQ/COGS/GEO respectively. In CFQ, the curriculum cut-off NCL is set to 11, as we statistically ï¬nd that this is the smallest curriculum that contains the complete vocabulary. We do not apply curriculum learning strategy to COGS and GEO, as LEAR can work well without curriculum learning in both benchmarks. Learnable parameters (Î¸ and Ï†) are optimized with AdaDelta (Zeiler, 2012), and the setting of learning rate is discussed in Section 6.1. We take the model that performs best

Model Transformer (Kim and Linzen, 2020) LSTM (Bi) (Kim and Linzen, 2020) LSTM (Uni) (Kim and Linzen, 2020) LEAR
w/o Abstraction w/o Semantic locality w/o Tree-LSTM

Acc 35 Â± 6 16 Â± 8 32 Â± 6 97.7 Â± 0.7 94.5 Â± 2.8 94.0 Â± 3.6 80.7 Â± 4.3

Table 4: Accuracy on COGS benchmark.

Model

Acc

Seq2Seq (Herzig and Berant, 2020)

46.0

BERT2Seq (Herzig and Berant, 2020) 49.6

GRAMMAR (Herzig and Berant, 2020) 54.0

PDE (Guo et al., 2020c)

81.2

SpanBasedSP (Herzig and Berant, 2020) 82.2

LEAR

84.1

Table 5: Accuracy on GEO benchmark.

on the validation set for testing, and all results are obtained by averaging over 5 runs with different random seeds. See Appendix for more implementation details.
6 Results and Discussion
Table 3 shows average accuracy and 95% conï¬dence intervals on three splits of CFQ. LEAR achieves an average accuracy of 90.9% on these three splits, outperforming all baselines by a large margin. We list some observations as follows. Methods for lexical recombination cannot generalize to algebraic recombination. Many methods for compositional generalization have been

proved effective for lexical recombination. Neural Shufï¬‚e Exchange and CGPS are two representatives of them. However, experimental results show that they cannot generalize to CFQ, which focus on algebraic recombination. Knowledge of semantics is important for compositional generalization. Seq2seq models show poor compositional generalization ability (âˆ¼ 20%). Pre-training helps a lot (âˆ¼ 20% â†’âˆ¼ 40%), but still not satisfying. HPD and LEAR incorporate knowledge of semantics (i.e., semantic operations) into the models, rather than simply model semantic meanings as sequences. This brings large proï¬t. Exploring latent compositional structure in a bottom-up manner is key to compositional generalization. HPD uses LSTM to encode the input expressions, while LEAR uses latent TreeLSTM, which explicitly explores latent compositional structure of expressions. This is the key to the large accuracy proï¬t (67.3% â†’ 90.9%).
Table 4 shows the results on COGS benchmark. It proves that LEAR can well generalize to domains which use different semantic formalisms, by specifying domain-speciï¬c Glex (semantic primitives) and Gopr (semantic operations). Table 5 shows the results on GEO benchmark. It proves that LEAR can well generalize to utterances written by real users (i.e., non-synthetic utterances).
6.1 Ablation Study
Table 3 and 4 also report results of some ablation models. Our observations are as follows. Abstraction by nonterminal symbols brings proï¬t. We use â€œw/o abstractionâ€ to denote the ablation model in which Equation 6 is disabled. This ablation leads to 5.5%/3.2% accuracy drop on CFQ/COGS. Incorporating semantic locality into the model brings proï¬t. We use â€œw/o semantic localityâ€ to denote the ablation model in which a Bi-LSTM layer is added before the latent Tree-LSTM. This ablation leads to 3.0%/3.7% accuracy drop on CFQ/COGS. Tree-LSTM contributes signiï¬cantly to compositional generalization. In the ablation â€œw/o TreeLSTMâ€, we replace the Tree-LSTM encoder with a span-based encoder, in which each span is represented by concatenating its start and end LSTM representations (similar to Herzig and Berant (2020)). In Table 3 and 4, we can see that span-based encoder severely affects the performance and even

Accuracy

Ratio 1:1:1 1:0.5:0.1 1:0.1:0.1

MCD-MEAN 87.4Â±7.1 90.9Â±1.2 86.7Â±3.9

MCD1 91.5Â±2.1 91.7Â±1.0 89.4Â±1.6

MCD2 89.4Â±2.3 89.2Â±1.9 85.8Â±2.7

MCD3 81.2Â±17 91.7Â±0.6 84.9Â±7.5

Table 6: Results of different learning rate ratios of lexical Interpreter, Composer, and algebraic Interpreter.

100 80 60 40 20 0 0-5

LEAR
MCD1 MCD2 MCD3 5-10 10-15 15-20 20-25 25-30

HPD
MCD1 MCD2 MCD3 0-5 5-10 10-15 15-20 20-25 25-30

Figure 4: Performance by input length.

much worse than the results of â€œw/o abstractionâ€ and â€œw/o semantic localityâ€. This ablation hints that Tree-LSTM is the main inductive bias of compositionality in our model. Primitive-based reward helps the model converge better. The ablation â€œw/o primitive-based rewardâ€ leads to 5.6% accuracy drop on CFQ, and the model variance has become much larger. The key insight is: primitive-based reward guides the model to interpret polysemous lexical units more effectively, thus helping the model converge better. Curriculum learning helps the model converge better. The ablation â€œw/o curriculum learningâ€ leads to 19% accuracy drop on CFQ, and the model variance has become much larger. This indicates the importance of curriculum learning. On COGS, LEAR performs well without curriculum learning. We speculate that there are two main reasons: (1) expressions of COGS is much shorter than CFQ; (2) the input/output pattern coverage of COGS is much higher than CFQ. Higher component with smaller learning rate. Inspired by the differential update strategy used in Liu et al. (2020)(i.e., the higher level the component is positioned in the model, the slower the parameters in it should be updated), we set three different learning rates to three different components in LEAR (in bottom-up order): lexical Interpreter, Composer, and algebraic Interpreter. We ï¬x the learning rate of lexical Interpreter to 1, and adjust the ratio of the learning rates of Composer and algebraic Interpreter to lexical Interpreter. Table 6 shows the results on CFQ. The hierarchical learning rate setup (1 : 0.5 : 0.1) achieves the best performance.

Attribute: FILM_CINE
Predicate: PERSON_CHILD

JOIN
âˆ§

JOIN Predicate: ACTOR_FILM
Entity: M0

What parent of a cinematographer played M0
(a) Composer error. A correct syntax tree should compose â€œparent of a cinematographerâ€ as a constituent, while the preWdhiiccthedesdyitnotar xotrfeeMin2coinrrfelucetlnycecdomapnodseswâ€œaas ciinnfelumeantcoegdrapbhyerM0 played M0â€.

âˆ§
Predicate: EDIT_FILM
JOIN Entity: Predicate: M2 INFLU_BY

JOIN
âˆ§

Predicate: INFLU_BY
Entity: M0

Which editor of M2 influenced and was influenced by M0
(b) Interpreter error. In this expression, the ï¬rst â€œinï¬‚uencedâ€ should be assigned a semantic primitive â€œinï¬‚uence.inï¬‚uence node.inï¬‚uencedâ€, while Interpreter incorrectly assigns â€œinï¬‚uence.inï¬‚uence node.inï¬‚uenced byâ€ (abbreviated as â€œINFLU BYâ€ in this ï¬gure) to it.

Figure 5: Two error cases. We use solid nodes to denote predicted nonterminal nodes. Incorrect parts are colored red.

6.2 Closer Analysis
We also conduct closer analysis to the results of LEAR as follows.
6.2.1 Performance by Input Length
Intuitively, understanding longer expressions requires stronger algebraic recombination ability than shorter examples. Therefore, we expect that our model should keep a good and stable performance with the increasing of input length.
Figure 4 shows the performance of LEAR and HPD (the state-of-the-art model on CFQ) under different input lengths. Speciï¬cally, test instances are divided into 6 groups by length: [1, 5], [6, 10], ..., [26, 30]), and we report accuracy on each group separately. The results indicate that LEAR has stable high performance for different input lengths, with only a slow decline as length increases. Even on the group with the longest input length, LEAR can maintain an average 86.3% accuracy across three MCD-splits.
6.2.2 Error Analysis
To understand the source of errors, we take a closer look at the failed test instances of LEAR on CFQ. These failed test instances account for 9.1% of the test dataset. We category them into two error types:

Error Type CE IE

MCD1 45.70% 54.30%

MCD2 32.05% 67.95%

MCD3 39.83% 60.17%

Table 7: Distribution of CE (Composer Error) and IE (Interpreter Error).

Composer error (CE), i.e., test cases where Composer produces incorrect syntactic structures (only considering nonterminal nodes). Figure 5a shows an example. As we do not have ground-truth syntactic structures, we determine whether a failed test instance belongs to this category based on handcraft syntactic templates. Interpreter error (IE), i.e., test cases where Composer produces correct syntactic structures but Interpreter assigns one or more incorrect semantic primitives or operations. Figure 5b shows an example, which contains an incorrect semantic primitive assignment.
Table 7 shows the distribution of these two error types. On average, 39.19% of failed instances are composer errors, and the remaining 60.81% are interpreter errors.
6.3 Limitations
Our approach is implicitly build upon the assumption of primitive alignment, that is, each primitive in the meaning representation can align to at least one span in the utterance. This assumption holds in most cases of various semantic parsing tasks, including CFQ, COGS, and GEO. However, for robustness and generalizability, we also need to consider cases that do not meet this assumption. For example, consider this utterance â€œObamaâ€™s brotherâ€, of which the corresponding meaning representation is â€œSlibing(P eople[Obama]) âˆ§ Gender[M ale]â€. Neither â€œSlibingâ€ nor â€œGender[M ale]â€ can align to a span in the utterance, as the composed meaning of them is expressed by a single word (â€œbrotherâ€). Therefore, LEAR is more suitable for formalisms where primitives can better align to natural language.
In addition, while our approach is general for various semantic parsing tasks, the collection of semantic operations needs to be redesigned for each task. We need to ensure that these semantic operations are k-ary projections (as described in Section 2), and all the meaning representations are covered by the operations collection. This is tractable, but still requires some efforts from domain experts.

7 Related Work
7.1 Compositional Generalization
Recently, exploring compositional generalization (CG) on neural networks has attracted large attention in NLP community. For SCAN (Lake and Baroni, 2018), the ï¬rst benchmark to test CG on seq2seq models, many solutions have been proposed, which can be classiï¬ed into two tracks: data augmentation (Andreas, 2019; AkyuÂ¨rek et al., 2020; Guo et al., 2020a) and specialized architecture (Lake, 2019; Li et al., 2019; Gordon et al., 2020). However, most of these works only focus on lexical recombination. Some works on SCAN have stepped towards algebraic recombination (Liu et al., 2020; Chen et al., 2020), but they do not generalize well to other tasks such as CFQ (Keysers et al., 2019) and COGS (Kim and Linzen, 2020).
Before our work, there is no satisfactory solution on CFQ and COGS. Previous works on CFQ demonstrated that MLM pre-training (Furrer et al., 2020) and iterative back-translation (Guo et al., 2020d) can improve traditional seq2seq models. HPD (Guo et al., 2020b), the state-of-the-art solution before ours, was shown to be effective on CFQ, but still far from satisfactory. As for COGS, there is no solution to it to the best of our knowledge.
7.2 Compositional Semantic Parsing
In contrast to neural semantic parsing models which are mostly constructed under a fully seq2seq paradigm, compositional semantic parsing models predict partial meaning representations and compose them to produce a full meaning representation in a bottom-up manner (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012; Liang et al., 2013; Berant et al., 2013; Berant and Liang, 2015; Pasupat and Liang, 2015; Herzig and Berant, 2020). Our model takes the advantage of compositional semantic parsing, without requiring any handcraft lexicon or syntactic rule.
7.3 Unsupervised Parsing
Unsupervised parsing (or grammar induction) trains syntax-dependent models to produce syntactic trees of natural language expressions without direct syntactic annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Pate and Johnson, 2016; Shen et al., 2018; Kim et al., 2019; Drozdov et al., 2020). Comparing to them, our model learns both syntax and semantics jointly.

8 Conclusion
In this paper, we introduce LEAR, a novel end-toend neural model for compositional generalization in semantic parsing tasks. Our contribution is 4fold: (1) LEAR focuses on algebraic recombination, thus it exhibits stronger compositional generalization ability than previous methods that focus on simpler lexical recombination. (2) We model the semantic parsing task as a homomorphism between two partial algebras, thus encouraging algebraic recombination. (3) We propose the model architecture of LEAR, which consists of a Composer (to learn latent syntax) and an Interpreter (to learn operation assignments). (4) Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model.
Acknowledgments
The work was supported by the National Key Research and Development Program of China (No. 2019YFB1704003), the National Nature Science Foundation of China (No. 71690231), Tsinghua BNRist and Beijing Key Laboratory of Industrial Bigdata System and Application.
Ethical Consideration
The experiments in this paper are conducted on existing datasets. We describe the model architecture and training method in detail, and provide more explanations in the supplemental materials. All the data and code will be released with the paper. The resources required to reproduce the experiments is a Tesla P100 GPU, and for COGS benchmark even one CPU is sufï¬cient. Since the compositional generalization ability explored in this paper is a fundamental problem of artiï¬cial intelligence and has not yet involved real applications, there are no social consequences or ethical issues.
References
Ekin AkyuÂ¨rek, Afra Feyza AkyuÂ¨rek, and Jacob Andreas. 2020. Learning to recombine and resample data for compositional generalization.
Jens Allwood. 2003. Meaning potentials and context: Some consequences for the analysis of variation in meaning. Cognitive approaches to lexical semantics, pages 29â€“66.
Jacob Andreas. 2019. Good-enough compositional data augmentation. CoRR, abs/1904.09545.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.
Yoshua Bengio, JeÂ´roË†me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML â€™09, page 41â€“48, New York, NY, USA. Association for Computing Machinery.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533â€“1544.
Jonathan Berant and Percy Liang. 2015. Imitation learning of agenda-based semantic parsers. Transactions of the Association for Computational Linguistics, 3:545â€“558.
Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 865â€“872.
Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. 2020. Compositional generalization via neural-symbolic stack machines.
Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-speciï¬c tree structures. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 32.
Noam Chomsky. 1957. Syntactic structures (the hague: Mouton, 1957). Review of Verbal Behavior by BF Skinner, Language, 35:26â€“58.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Åukasz Kaiser. 2018. Universal transformers. arXiv preprint arXiv:1807.03819.
Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim Oâ€™Gorman, Mohit Iyyer, and Andrew McCallum. 2020. Unsupervised parsing with s-diora: Single tree encoding for deep inside-outside recursive autoencoders. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4832â€“4845.
Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-sql evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 351â€“360.
J. A. Fodor and Z. W. Pylyshyn. 1988. Connectionism and cognitive architecture - a critical analysis. Cognition, 28(1-2):3â€“71.

Jerry A Fodor and Ernest Lepore. 2002. The compositionality papers. Oxford University Press.
KaÂ¯rlis Freivalds, EmÂ¯Ä±ls OzolinÂ¸sË‡, and Agris SË‡ ostaks. 2019. Neural shufï¬‚e-exchange networksâ€“sequence processing in o (n log n) time. arXiv preprint arXiv:1907.07897.
Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael SchaÂ¨rli. 2020. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970.
Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. 2020. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations.
Demi Guo, Yoon Kim, and Alexander M. Rush. 2020a. Sequence-level mixed sample data augmentation.
Yinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei Zhang. 2020b. Hierarchical poset decoding for compositional generalization in language. arXiv preprint arXiv:2010.07792.
Yinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei Zhang. 2020c. Iterative utterance segmentation for neural semantic parsing. arXiv preprint arXiv:2012.07019.
Yinuo Guo, Hualei Zhu, Zeqi Lin, Bei Chen, JianGuang Lou, and Dongmei Zhang. 2020d. Revisiting iterative back-translation from the perspective of compositional generalization.
Serhii Havrylov, GermaÂ´n Kruszewski, and Armand Joulin. 2019. Cooperative learning of disjoint syntax and semantics. arXiv preprint arXiv:1902.09393.
Jonathan Herzig and Jonathan Berant. 2020. Spanbased semantic parsing for compositional generalization. arXiv preprint arXiv:2009.06040.
Sepp Hochreiter and JuÂ¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735â€“1780.
Rohit J Kate, Yuk Wah Wong, and Raymond J Mooney. 2005. Learning to transform natural to formal languages. In AAAI, volume 5, pages 1062â€“1068.
Daniel Keysers, Nathanael SchaÂ¨rli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Staï¬niak, Tibor Tihon, et al. 2019. Measuring compositional generalization: A comprehensive method on realistic data. arXiv preprint arXiv:1912.09713.
Najoung Kim and Tal Linzen. 2020. Cogs: A compositional generalization challenge based on semantic interpretation. arXiv preprint arXiv:2010.05465.
Yoon Kim, Chris Dyer, and Alexander M Rush. 2019. Compound probabilistic context-free grammars for grammar induction. arXiv preprint arXiv:1906.10225.

Dan Klein and Christopher D Manning. 2002. Natural language grammar induction using a constituentcontext model. Advances in neural information processing systems, 1:35â€“42.
Brenden M Lake. 2019. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural Information Processing Systems 32, pages 9791â€“9801. Curran Associates, Inc.
Brenden M. Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, StockholmsmaÂ¨ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2879â€“2888. PMLR.
Yuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hestness. 2019. Compositional generalization for primitive substitutions. arXiv preprint arXiv:1910.02612.
Percy Liang, Michael I Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389â€“446.
Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. 2020. Compositional generalization by learning analytical expressions.
Gary F Marcus. 2019. The algebraic mind: Integrating connectionism and cognitive science. MIT press.
Richard Montague. 1970. Universal grammar. 1974, pages 222â€“46.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19â€“51.
Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305.
John K Pate and Mark Johnson. 2016. Grammar induction from (lots of) words alone. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 23â€“32.
Francis Jeffry Pelletier. 2003. Context dependence and compositionality. Mind & Language, 18(2):148â€“ 161.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple unsupervised grammar induction from raw text with cascaded ï¬nite state models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1077â€“1086.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniï¬ed text-to-text transformer. arXiv preprint arXiv:1910.10683.

Jake Russin, Jason Jo, Randall C. Oâ€™Reilly, and Yoshua Bengio. 2019. Compositional generalization in a deep seq2seq model by separating syntax and semantics. CoRR, abs/1904.09708.
Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. 2018. Ordered neurons: Integrating tree structures into recurrent neural networks. arXiv preprint arXiv:1810.09536.
David So, Quoc Le, and Chen Liang. 2019. The evolved transformer. In International Conference on Machine Learning, pages 5877â€“5886. PMLR.
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and K. MuÂ¨ller, editors, Advances in Neural Information Processing Systems 12, pages 1057â€“1063. MIT Press.
ZoltaÂ´n Gendler SzaboÂ´. 2004. Compositionality.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1556â€“1566. The Association for Computer Linguistics.
Dmitry Tsarkov, Tibor Tihon, Nathan Scales, Nikola Momchev, Danila Sinopalnikov, and Nathanael SchaÂ¨rli. 2020. *-cfq: Analyzing the scalability of machine learning on a compositional task. arXiv preprint arXiv:2012.08266.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762.
Lex Weaver and Nigel Tao. 2001. The optimal reward baseline for gradient-based reinforcement learning. In UAI â€™01: Proceedings of the 17th Conference in Uncertainty in Artiï¬cial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, pages 538â€“545. Morgan Kaufmann.
Ronald J. Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Mach. Learn., 8:229â€“256.
Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701.
John M Zelle and Raymond J Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the national conference on artiï¬cial intelligence, pages 1050â€“1055.

Luke S Zettlemoyer and Michael Collins. 2012. Learning to map sentences to logical form: Structured classiï¬cation with probabilistic categorial grammars. arXiv preprint arXiv:1207.1420.
References
Ekin AkyuÂ¨rek, Afra Feyza AkyuÂ¨rek, and Jacob Andreas. 2020. Learning to recombine and resample data for compositional generalization.
Jens Allwood. 2003. Meaning potentials and context: Some consequences for the analysis of variation in meaning. Cognitive approaches to lexical semantics, pages 29â€“66.
Jacob Andreas. 2019. Good-enough compositional data augmentation. CoRR, abs/1904.09545.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.
Yoshua Bengio, JeÂ´roË†me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML â€™09, page 41â€“48, New York, NY, USA. Association for Computing Machinery.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533â€“1544.
Jonathan Berant and Percy Liang. 2015. Imitation learning of agenda-based semantic parsers. Transactions of the Association for Computational Linguistics, 3:545â€“558.
Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 865â€“872.
Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. 2020. Compositional generalization via neural-symbolic stack machines.
Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-speciï¬c tree structures. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 32.
Noam Chomsky. 1957. Syntactic structures (the hague: Mouton, 1957). Review of Verbal Behavior by BF Skinner, Language, 35:26â€“58.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Åukasz Kaiser. 2018. Universal transformers. arXiv preprint arXiv:1807.03819.

Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim Oâ€™Gorman, Mohit Iyyer, and Andrew McCallum. 2020. Unsupervised parsing with s-diora: Single tree encoding for deep inside-outside recursive autoencoders. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4832â€“4845.
Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-sql evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 351â€“360.
J. A. Fodor and Z. W. Pylyshyn. 1988. Connectionism and cognitive architecture - a critical analysis. Cognition, 28(1-2):3â€“71.
Jerry A Fodor and Ernest Lepore. 2002. The compositionality papers. Oxford University Press.
KaÂ¯rlis Freivalds, EmÂ¯Ä±ls OzolinÂ¸sË‡, and Agris SË‡ ostaks. 2019. Neural shufï¬‚e-exchange networksâ€“sequence processing in o (n log n) time. arXiv preprint arXiv:1907.07897.
Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael SchaÂ¨rli. 2020. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970.
Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. 2020. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations.
Demi Guo, Yoon Kim, and Alexander M. Rush. 2020a. Sequence-level mixed sample data augmentation.
Yinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei Zhang. 2020b. Hierarchical poset decoding for compositional generalization in language. arXiv preprint arXiv:2010.07792.
Yinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei Zhang. 2020c. Iterative utterance segmentation for neural semantic parsing. arXiv preprint arXiv:2012.07019.
Yinuo Guo, Hualei Zhu, Zeqi Lin, Bei Chen, JianGuang Lou, and Dongmei Zhang. 2020d. Revisiting iterative back-translation from the perspective of compositional generalization.
Serhii Havrylov, GermaÂ´n Kruszewski, and Armand Joulin. 2019. Cooperative learning of disjoint syntax and semantics. arXiv preprint arXiv:1902.09393.
Jonathan Herzig and Jonathan Berant. 2020. Spanbased semantic parsing for compositional generalization. arXiv preprint arXiv:2009.06040.

Sepp Hochreiter and JuÂ¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735â€“1780.
Rohit J Kate, Yuk Wah Wong, and Raymond J Mooney. 2005. Learning to transform natural to formal languages. In AAAI, volume 5, pages 1062â€“1068.
Daniel Keysers, Nathanael SchaÂ¨rli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Staï¬niak, Tibor Tihon, et al. 2019. Measuring compositional generalization: A comprehensive method on realistic data. arXiv preprint arXiv:1912.09713.
Najoung Kim and Tal Linzen. 2020. Cogs: A compositional generalization challenge based on semantic interpretation. arXiv preprint arXiv:2010.05465.
Yoon Kim, Chris Dyer, and Alexander M Rush. 2019. Compound probabilistic context-free grammars for grammar induction. arXiv preprint arXiv:1906.10225.
Dan Klein and Christopher D Manning. 2002. Natural language grammar induction using a constituentcontext model. Advances in neural information processing systems, 1:35â€“42.
Brenden M Lake. 2019. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural Information Processing Systems 32, pages 9791â€“9801. Curran Associates, Inc.
Brenden M. Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, StockholmsmaÂ¨ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2879â€“2888. PMLR.
Yuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hestness. 2019. Compositional generalization for primitive substitutions. arXiv preprint arXiv:1910.02612.
Percy Liang, Michael I Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389â€“446.
Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. 2020. Compositional generalization by learning analytical expressions.
Gary F Marcus. 2019. The algebraic mind: Integrating connectionism and cognitive science. MIT press.
Richard Montague. 1970. Universal grammar. 1974, pages 222â€“46.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19â€“51.

Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305.
John K Pate and Mark Johnson. 2016. Grammar induction from (lots of) words alone. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 23â€“32.
Francis Jeffry Pelletier. 2003. Context dependence and compositionality. Mind & Language, 18(2):148â€“ 161.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple unsupervised grammar induction from raw text with cascaded ï¬nite state models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1077â€“1086.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniï¬ed text-to-text transformer. arXiv preprint arXiv:1910.10683.
Jake Russin, Jason Jo, Randall C. Oâ€™Reilly, and Yoshua Bengio. 2019. Compositional generalization in a deep seq2seq model by separating syntax and semantics. CoRR, abs/1904.09708.
Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. 2018. Ordered neurons: Integrating tree structures into recurrent neural networks. arXiv preprint arXiv:1810.09536.
David So, Quoc Le, and Chen Liang. 2019. The evolved transformer. In International Conference on Machine Learning, pages 5877â€“5886. PMLR.
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and K. MuÂ¨ller, editors, Advances in Neural Information Processing Systems 12, pages 1057â€“1063. MIT Press.
ZoltaÂ´n Gendler SzaboÂ´. 2004. Compositionality.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1556â€“1566. The Association for Computer Linguistics.
Dmitry Tsarkov, Tibor Tihon, Nathan Scales, Nikola Momchev, Danila Sinopalnikov, and Nathanael SchaÂ¨rli. 2020. *-cfq: Analyzing the scalability of

machine learning on a compositional task. arXiv preprint arXiv:2012.08266.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762.
Lex Weaver and Nigel Tao. 2001. The optimal reward baseline for gradient-based reinforcement learning. In UAI â€™01: Proceedings of the 17th Conference in Uncertainty in Artiï¬cial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, pages 538â€“545. Morgan Kaufmann.
Ronald J. Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Mach. Learn., 8:229â€“256.
Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701.
John M Zelle and Raymond J Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the national conference on artiï¬cial intelligence, pages 1050â€“1055.
Luke S Zettlemoyer and Michael Collins. 2012. Learning to map sentences to logical form: Structured classiï¬cation with probabilistic categorial grammars. arXiv preprint arXiv:1207.1420.

This is the Appendix for the paper: â€œLearning Algebraic Recombination for Compositional Generalizationâ€.
A Semantic Operations in COGS
The semantic primitives used in COGS benchmark are entities (e.g., Emma and cat(x 1)), predicates (e.g., eat) and propositions (e.g., eat.agent(x 1, Emma)). The semantic operations in COGS are listed in Table 8.
The operations with â€œâˆ’1â€ (e.g., ONâˆ’1) are right-to-left operations (e.g., ONâˆ’1(cake, table)â†’table.ON.cake) while the operations without â€œ-1â€ represent the left-to-right operations (e.g., ON(cake, table)â†’cake.ON.table). For operation FillFrame, the entity in its arguments will be ï¬lled into predicate/proposition as an AGENT, THEME or RECIPIENT, which is decided by model.

B Semantic Operations in GEO and Post-process

The semantic primitives used in GEO benchmark are entities (e.g., var0), predicates (e.g., state()) and propositions (e.g., state(var0)). The semantic operations in GEO are listed in Table 9.
To ï¬t the FunQL formalism, we design two postprocessing rules for the ï¬nal semantics generated by the model. First, if the ï¬nal semantic is a predicate (not a proposition), it will be converted in to a proposition by ï¬lling the entity all. Second, the predicate most will be shifted forward two positions in the ï¬nal semantics.
C Policy Gradient and Differential Update

In this section, we will show more details about the formulation of our RL training based on policy gradient and how to use differential update strategy on it.
Denoting Ï„ = {z, g} as the trajectory of our model where z and g are actions (or called results) produced from Composer and Interpreter, respectively, and R(Ï„ ) as the reward of a trajectory Ï„ (elaborated in Sec. 4.1), the training objective of our model is to maximize the expectation of rewards as:

max J (Î¸, Ï†) = max EÏ„âˆ¼Ï€Î¸,Ï† R(Ï„ ), (13)

Î¸,Ï†

Î¸,Ï†

where Ï€Î¸,Ï† is the policy of the whole model Î¸ and Ï† are the parameters in Composer and Interpreter,

respectively. Applying the likelihood ratio trick, Î¸ and Ï† can be optimized by ascending the following gradient:
âˆ‡J (Î¸, Ï†) = EÏ„âˆ¼Ï€Î¸,Ï† R(Ï„ )âˆ‡ log Ï€Î¸,Ï† (Ï„ ) ,
which is same with Eq. 9. As described in Sec. 3 that the interpreting pro-
cess can be divided into two stages: interpreting lexical nodes and interpreting algebraic nodes, the action g can also be split as the semantic primitives of lexical nodes gl and the semantic operations of algebraic nodes ga. In our implement, we utilize two independent neural modules for interpreting lexical nodes and interpreting algebraic nodes, with parameters Ï†l and Ï†a respectively. Therefore, âˆ‡ log Ï€Î¸,Ï† (Ï„ ) in Eq. 9 can be expanded via the chain rule as:
âˆ‡ log Ï€Î¸,Ï† (Ï„ ) =âˆ‡ log Ï€Î¸ (z|x) + âˆ‡ log Ï€Ï†l (gl|x, z) + (14) âˆ‡ log Ï€Ï†a (ga|x, z, gl) .
With Eq. 14, we can set different learning rates:
Î¸ â† Î¸ + Î± Â· E R(Ï„ )âˆ‡ log Ï€Î¸ (z|x), Ï†l â† Ï†l + Î² Â· E R(Ï„ )âˆ‡ log Ï€Ï†l (gl|x, z), Ï†a â† Ï†a + Î³ Â· E R(Ï„ )âˆ‡ log Ï€Ï†a (ga|x, z, gl).
(15)
Furthermore, in our experiments, the AdaDelta optimizer (Zeiler, 2012) is employed to optimize our model.
D Phrase Table
The phrase table consists of lexical units (i.e., words and phrases) paired with semantic primitives that frequently co-occur with them. It can be obtained with statistical methods.
For CFQ, we leverage GIZA++4 (Och and Ney, 2003) toolkit to extract alignment pairs from training examples. We obtain 109 lexical units, each of which is paired with 1.7 candidate semantic primitives on average. Some examples in phrase table are shown in Table 10
As to COGS, for each possible lexical unit, we ï¬rst ï¬lter out the semantic primitives that exactly co-occur with it, and delete lexical units with no semantic primitive. Among the remaining lexical units, for those only contain one semantic primitive, we record their co-occurring semantic primitives
4https://github.com/moses-smt/giza-pp.git

Operation ON(t1, t2) IN(t1, t2) BESIDE(t1, t2) ONâˆ’1, INâˆ’1, BESIDEâˆ’1 REC-THE(t1, t2) THE-REC(t1, t2) AGE-THE, THE-AGE, REC-AGE, AGE-REC
FillFrame(t1, t2)
CCOMP(t1, t2) XCOMP(t1, t2) CCOMPâˆ’1, XCOMPâˆ’1

Arguments [t1: Entity, t2: Entity]
[t1: Entity, t2: Entity] [t1: Entity, t2: Pred/Prop] [t1: Pred/Prop, t2: Entity] [t1: Pred/Prop, t2: Pred/Prop]

Result Type Entity
Entity Proposition Proposition

Example Emma ate [the cake on a table] . A girl was awarded [a cake in a soup] . Amelia dusted [the girl beside a stage] .
NONE Lily gave [Emma a strawberry] . A girl offered [a rose to Isabella] .
-
A cat [disintegrated a girl] .
[Emma liked that a girl saw] . David [expected to cook] . NONE

Table 8: Semantic operations in COGS. â€œPredâ€ and â€œPropâ€ are abbreviations of â€œPredicateâ€ and â€œPropositionâ€, respectively. â€œAGEâ€, â€œTHEâ€ and â€œRECâ€ are abbreviations of â€œAGENTâ€, â€œTHEMEâ€ and â€œRECIPIENTâ€, respectively. â€œ-â€ omits similar examples. Some operations contain â€œNONEâ€ example, indicating that no example utilize these operations in dataset.

Operation UNION(t1, t2) INTER(t1, t2)
EXC(t1, t2) EXCâˆ’1(t1, t2) CONCAT(t1, t2) CONCATâˆ’1(t1, t2)
FillIn(t1, t2)

Arguments
[t1: Entity/Prop, t2: Entity/Prop]
[t1: Pred, t2: Pred] [t1: Entity/Prop, t2: Pred] [t1: Pred, t2: Entity/Prop]

Result Type Proposition
Pred Proposition

Example what is the population of [var0 var1] how many [cities named var0 in the usa] which [capitals are not major cities]
what is the [capital of var0]
how many [citizens in var0]

Table 9: Semantic operations in GEO. â€œPredâ€ and â€œPropâ€ are abbreviations of â€œPredicateâ€ and â€œPropositionâ€, respectively. â€œINTERâ€, â€œEXCâ€ and â€œCONCATâ€ are abbreviations of â€œINTERSECTIONâ€, â€œEXCLUDEâ€ and â€œCONCATENATIONâ€, respectively.

as ready semantic primitives. For lexical units with more than one semantic primitives, we delete the ready semantic primitives from their co-occurring semantic primitives. Finally, we obtain 731 lexical units and each lexical unit is paired with just one semantic primitive.
As GEO is quite small, we obtain its phrase table by handcraft.
E More Examples
We show more examples of generated treestructures and semantics in Figure 6.

Did m6â€˜ s star, costume designer, and director influence m0, m1, m2, and m3 and influence m4 and m5

^ ^

JOIN
^ ^

JOIN
^^^

JOIN
^

Entity: Predicate: Predicate: Predicate: Predicate: Entity: Entity: Entity: Entity: Predicate: Entity:

M6 ACTOR_FILM COSTU_FILM DIREC_FILM INFLU

M0 M1 M2 M3 INFLU

M4

Entity: M5

(a) An example of generated results in CFQ benchmark with the input â€œDid M6â€˜ s star, costume designer, and director inï¬‚uence M0, M1, M2, and M3 and inï¬‚uence M4 and M5 â€.

CCOMP

CCOMP

FillFrame: AGENT

FillFrame: AGENT

FillFrame: AGENT

FillFrame: RECIPIENT
FillFrame: THEME
BESIDE IN

Entity: Joshua

Predicate: Entity: Predicate: Entity:

like

Mason hope Amelia

Predicate: Entity: award hedgehog

Entity: stage

Entity: Entity: tent cat

(b) An example of generated results in COGS benchmark with the input â€œJoshua liked that Mason hoped that Amelia awarded the hedgehog beside the stage in the tent to a catâ€.

Figure 6: Examples of generated tree-structures and semantics in CFQ and COGS benchmarks.

Lexical Unit M0
executive producer
editor Italian

Semantic Primitive(s) M0
ï¬lm.ï¬lm.executive produced by ï¬lm.producer.ï¬lms executive produced
a ï¬lm.editor ï¬lm.editor.ï¬lm ï¬lm.ï¬lm.edited by people.person.nationality m 03rjj

Type Entity Predicate Predicate Attribute Predicate Predicate Attribute

Table 10: Some examples in CFQ phrase table.

