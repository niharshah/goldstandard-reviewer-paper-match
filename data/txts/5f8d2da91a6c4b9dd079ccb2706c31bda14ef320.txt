JOINT SPEECH RECOGNITION AND AUDIO CAPTIONING
Chaitanya Narisetty , Emiru Tsunoo‚Ä†, Xuankai Chang , Yosuke Kashiwagi‚Ä†, Michael Hentschel‚Ä†, Shinji Watanabe
Carnegie Mellon University, USA, ‚Ä† Sony Group Corporation, Japan

arXiv:2202.01405v1 [eess.AS] 3 Feb 2022

ABSTRACT
Speech samples recorded in both indoor and outdoor environments are often contaminated with secondary audio sources. Most endto-end monaural speech recognition systems either remove these background sounds using speech enhancement or train noise-robust models. For better model interpretability and holistic understanding, we aim to bring together the growing Ô¨Åeld of automated audio captioning (AAC) and the thoroughly studied automatic speech recognition (ASR). The goal of AAC is to generate natural language descriptions of contents in audio samples. We propose several approaches for end-to-end joint modeling of ASR and AAC tasks and demonstrate their advantages over traditional approaches, which model these tasks independently. A major hurdle in evaluating our proposed approach is the lack of labeled audio datasets with both speech transcriptions and audio captions. Therefore we also create a multi-task dataset by mixing the clean speech Wall Street Journal corpus with multiple levels of background noises chosen from the AudioCaps dataset. We also perform extensive experimental evaluation and show improvements of our proposed methods as compared to existing state-of-the-art ASR and AAC methods.
Index Terms‚Äî ASR, AAC, speech recognition, audio captioning, joint modeling
1. INTRODUCTION
Automatic speech recognition (ASR) is a well known task which decodes human speech as textual representations and has been a prominent research area for the past several decades [1]. End-to-end (E2E) ASR is a sequence-to-sequence task where a stream of acoustic features are converted into a sequence of words. These models typically use an encoder-decoder framework with connectionist temporal classiÔ¨Åcation (CTC) loss [2, 3] or use RNN-Transducers [4, 5]. The introduction of attention mechanisms enabled the decoder to focus on relevant parts of the audio signal when generating long word sequences [6, 7]. Transformer layers with self-attention in both the encoder and decoder, have shown signiÔ¨Åcant improvement in training speeds and modeling long-range dependencies [8].
Speech utterances captured in the real-world coexist with a wide variety of acoustic sources and are seldom pure. However most ASR systems focus only on the speech component of an audio signal and consider the non-speech sources as components to be disregarded during model learning [9, 10]. Synchronously modeling both speech and non-speech sources in a uniÔ¨Åed manner resembles the natural perception of the human auditory system. Such a uniÔ¨Åed model using Transformers was proposed by [11] to solve ASR, acoustic event detection (AED) and audio tagging (AT) tasks. Although tasks like AED and AT extract information of all constituent audio sources,

the outputted sequence of events and tags lacks proper structure for straightforward human understanding [12].
Automated audio captioning (AAC) aims to provide the information of constituent audio sources and events in a structured and easily comprehensible manner i.e., a natural language description of a given audio waveform [12, 13]. Recently, Transformer based encoder-decoder frameworks are being employed to model the temporal structure of audio events [14, 15] AAC is an emerging research area with several applications, such as enriching the raw textual information provided by ASR during television broadcasting and video streaming. Such integration of ASR and AAC tasks can potentially improve the viewing experience of the hearing impaired.
Performance of an ASR model in a noisy environment depends on its ability to adapt to various background sounds and robustly infer the speech components. This process makes the ASR and AAC tasks interdependent and motivates us to treat them in a uniÔ¨Åed manner. Also, given the similarity of these tasks in transforming input acoustic signals into output word sequences, we formulate them as a multi-task problem. Our main contributions are as follows:
1. We present the very Ô¨Årst attempt to jointly model ASR and AAC tasks, and propose various E2E Transformer frameworks to solve this multi-task problem.
2. Due to the lack of an audio dataset containing both transcript and caption labels, we prepare a synthetic multi-task dataset by combining clean speech samples and captioned non-speech samples.
3. We carefully evaluate the proposed jointly trained models on varying levels of background sounds and compare them with the independently trained ASR and AAC models.
2. INDEPENDENT MODELING
2.1. Automatic Speech Recognition
Popular E2E ASR systems are based on an encoder-decoder architecture, where the input acoustic features are decoded as speech transcripts. Let Xspec ‚àà RF √óT be an input spectrogram with frequency and time dimensions of F and T respectively, and WASR be the output word sequence. A neural network model with parameters ŒòASR, aims to model the posterior distribution P (WASR|Xspec, ŒòASR), as shown in Fig. 1(a). Some state-of-the-art models follow a Transformer encoder-decoder framework, optimized using both CTC loss and attention loss [16]. Typically, pretrained language models (LM) are integrated with the decoder during inference to form coherent word sequences [17, 18]. However, as the purpose of this work is to compare the fundamental capability of independent and joint modeling approaches, we will not use LMs during inference.

Independent modeling
a) ASR-only
NN
b) AAC-only
NN

Output word Input audio Neural-network sequence spectrogram parameters
speech transcript ùëÉ(ùëäASR|ùëãspec, ŒòASR)

audio caption

ùëÉ(ùëäAAC|ùëãspec, ŒòAAC)

Fig. 1. Typical independent modeling of E2E ASR and AAC tasks

Joint modeling
a) Cat-ASR-AAC and Cat-AAC-ASR

ùëÉ(ùëäASR, ùëäAAC|ùëãspec, ŒòCat)

NN

speech transcript <ùë†ùëíùëù. ùë°ùëúùëòùëíùëõ> audio caption

(or) audio caption <ùë†ùëíùëù. ùë°ùëúùëòùëíùëõ> speech transcript

b) Dual-decoder
NN

ùëÉ ùëäASR ùëãspec, ŒòDD
speech transcript audio caption

‚ãÖ ùëÉ(ùëäAAC|ùëãspec, ŒòDD)

Fig. 2. Joint modeling approaches of E2E ASR and AAC tasks

2.2. Automated Audio Captioning
In contrast to ASR systems, which primarily focus on speech contents, AAC systems must model all the acoustic sources in an input signal and output an intelligible textual description (caption) [12]. Neural network models for the AAC task aim to model the posterior distribution P (WAAC|Xspec, ŒòAAC), where WAAC denotes the output word sequence in an audio caption, as shown in Fig. 1(b). As both ASR and AAC systems output word sequences, state-of-theart AAC models also follow a Transformer based encoder-decoder framework, optimized using attention loss [15, 14]. Note that the CTC loss or RNN-Transducers are not particularly applicable to the AAC task, because the token sequence in a caption need not be temporally aligned with the input spectrogram frames. In this work, the transcripts and captions are tokenized as a sequence of characters.

3. MULTI-TASK DATASET SYNTHESIS

The inter-dependency of ASR and AAC tasks, in addition to their similar modeling techniques, motivates us to explore models that can be trained jointly on these two tasks. However an important challenge in realizing this joint modeling is the dearth of labeled realworld recordings with both a speech transcript and an audio caption. We overcome this hurdle by synthetically mixing clean speech samples from the Wall Street Journal (WSJ) corpus [19] with non-speech samples from AudioCaps dataset [12]. To support reproducible research, we will make our data generation scripts publicly available1.
WSJ corpus is a collective of the WSJ0 and WSJ1 datasets, totalling more than 37k clean speech samples and their transcripts, uttered by roughly 300 speakers. The AudioCaps dataset contains 46k audio samples paired with human-annotated captions, and is a subset of AudioSet (a large-scale collection of 10-second audio clips from YouTube) [20]. Our goal is to prepare a dataset where an audio sample contains speech from a primary speaker in the presence of background noise. Keeping with this goal, we identify and remove all the speech samples present in the AudioCaps dataset. This is done by Ô¨Åltering audio samples whose captions contain the substrings ‚Äòspeak‚Äô and ‚Äòtalk‚Äô (identifying words such as speaks, talking, etc.). The Ô¨Åltering process discarded approximately 20k speech samples and retained 26k non-speech audio samples.
We created a synthetic multi-task dataset by randomly mixing WSJ samples with non-speech AudioCaps samples. Given a speech signal x1 and a non-speech signal x2, we perform mixing as:

xmix = xÀÜ1 + Œ≥ ¬∑ xÀÜ2,

(1)

where (ÀÜ.) represents an amplitude normalization to [‚àí1, 1] range and Œ≥ denotes a scalar mixing weight. The duration of audio samples from WSJ can be up to 25 seconds, while those from AudioCaps are roughly 10 seconds. When xÀÜ1 is longer than xÀÜ2, we account for this
1https://chintu619.github.io/Joint-ASR-AAC/

duration mismatch during the mixing process in Eq. (1) by randomly choosing a segment of xÀÜ1, such that the duration of chosen segment is equal to the duration of xÀÜ2.
The transcript WASR of signal x1 and caption WAAC of signal x2 are stored as labels for the normalized mixture signal xÀÜmix. Although the addition of a clean speech event to the sounds present in x2 modiÔ¨Åes the textual description of xÀÜmix, we overlook this issue from a practical standpoint. For instance, if WAAC were to be ‚Äòa dog is barking‚Äô, then a human annotated caption for xÀÜmix would probably be of the form ‚Äòa person is speaking while a dog is barking‚Äô. Considering that all the mixture signals contain speech, their corresponding captions should ideally contain a phrase equivalent to ‚Äòa person is speaking‚Äô. This inconsistency is common across all the mixture samples and can therefore be overlooked for our synthetic multi-task mixture dataset.

4. PROPOSED JOINT MODELING
Given a set of noisy speech signals contaminated with various background sounds, we aim to train a single model capable of generating both the transcript and caption labels. Inferring two output sequences from such a jointly trained model resembles an E2E multispeaker ASR task [21, 22], without needing to accommodate for the permutation invariance among multiple speakers.

4.1. Concatenating Output Sequences

The independent ASR and AAC modeling frameworks discussed in Section 2 inherently follow a similar structure: acoustic feature encoding and word sequence decoding. Exploiting this similarity and the serialized output training proposed in [22], we propose an intuitive approach to our multi-task modeling: concatenating the word sequences WASR and WAAC as follows:

WCat = Concat( WASR, sep. token , WAAC )

(2)

WRev = Concat( WAAC, sep. token , WASR ),

(3)

where WCat and WRev denote the concatenated word sequences, and sep. token denotes a separation token. Eq. (2) signiÔ¨Åes the scenario where WASR precedes WAAC in the concatenated sequence, while Eq. (3) signiÔ¨Åes a sequence concatenated in the reverse order. For deÔ¨Ånitive detection of transition from WASR to WAAC or viceversa during model inference, we incorporate a special separation token sep. token during concatenation as shown in Fig. 2(a).
Given the input Xspec and model parameters ŒòCat, ŒòRev for WCat, WRev respectively, we formulate the posterior distribution as:

P (WCat|Xspec, ŒòCat) = P (WASR|Xspec, ŒòCat)¬∑ P (WAAC|WASR, Xspec, ŒòCat), (4)
P (WRev|Xspec, ŒòRev) = P (WAAC|Xspec, ŒòRev)¬∑ P (WASR|WAAC, Xspec, ŒòRev). (5)

We note that the formulations of Eq. (4) and Eq. (5) may appear as equivalent, but the evolving hidden state(s) of the neural network‚Äôs recurrent architecture increases their dissimilarity with each decoded token. Henceforth, we denote the frameworks modeling WCat and WRev as ‚ÄòCat-ASR-AAC‚Äô and ‚ÄòCat-AAC-ASR‚Äô respectively.
Although the proposed sequence concatenation approach aims to model the true joint distribution of WASR and WAAC, it suffers from several drawbacks. The decoding process is likely to output sub-optimal solutions due to a signiÔ¨Åcant increase in the length of output sequence. Also, the constraint of generating a transcript and caption as part of the same output sequence removes the temporal alignment between WCat or WRev and xÀÜmix. In-turn we lose the ability of optimizing our model using the CTC loss. Furthermore, the shared decoder is constrained to learn the uniÔ¨Åed token-transition probabilities within transcripts and captions, even though their individual token-transition probabilities need not be correlated.

4.2. Dual Output Decoding
Inspired by multi-task modeling in [11] and to alleviate the aforementioned drawbacks, we propose a shared encoder and dualdecoder framework to extract effective audio representations while separately decoding WASR and WAAC. This approach assumes conditional independence between the transcript and caption given the input Xspec and dual-decoder model parameters ŒòDD when modeling the joint probability distribution. SpeciÔ¨Åcally, we have:

P (WASR, WAAC|Xspec, ŒòDD) = P (WASR|Xspec, ŒòDD) ¬∑ P (WAAC|Xspec, ŒòDD). (6)

The Ô¨Çexibility of using two decoders allows the proposed model to individually learn the token transition distributions within transcripts and captions. The exponentially expanded search space of concatenated output model is now restored to output sequences of regular length. Finally, our proposed model also allows the use of the CTC loss when optimizing the ASR decoder as follows:

L = Œª ¬∑ Lctc + (1 ‚àí Œª) ¬∑ Latt,

(7)

where L, Lctc and Latt denote the overall, CTC and attention losses respectively, and Œª denotes the interpolation factor [23].

5. EXPERIMENTS
5.1. Dataset Preparation
Following the synthetic multi-task dataset preparation procedure detailed in Section 3, we randomly mix the 37k speech samples from WSJ with 26k non-speech AudioCaps samples using mixing weight Œ≥. To mix the 836 total samples in development and evaluation splits of WSJ, we proportionately select 600 samples at random from AudioCaps that are not seen during training. We choose 5 discrete values for Œ≥ ‚àà {0.1, 0.2, 0.4, 0.6, 0.8} to perform the above mixing process, aiming to evaluate our proposed methods at different levels of background sounds. This process results in 37k √ó 5 ‚âà 187k training samples and roughly 4k dev and eval samples.

5.2. Model Training and Evaluation
We trained the independent and proposed joint modeling frameworks using the ESPNet toolkit [16]. The models compared during our experiments are as follows: 1. ASR-only: trained to only output WASR

Method
ASR-only AAC-only Cat-ASR-AAC Cat-AAC-ASR Dual-decoder

CER WER

11.4 24.0

‚Äì

‚Äì

12.3 25.0

10.9 23.2

11.7 24.3

CIDEr
‚Äì 0.441 0.507 0.632 0.462

SPICE
‚Äì 0.140 0.153 0.171 0.134

SPIDEr
‚Äì 0.291 0.330 0.401 0.298

Table 1. Comparison of speech and captioning metrics scores for all models trained without CTC and evaluated over the combined eval split of all mixing weights.

2. AAC-only: trained to only output WAAC
3. Cat-ASR-AAC: outputs the word sequence WCat from Eq. (2)
4. Cat-AAC-ASR: outputs the word sequence WCat from Eq. (3)
5. Dual-decoder: trained to output both WASR and WAAC
All the models discussed in this work follow the same network architecture to facilitate consistency during evaluation. Our Transformer encoder has 12 layers with 2048 units each, and 4 attention heads of 256 dimension. On the other hand, the Transformer decoder has 6 layers with 2048 units each. The proposed dual-decoder model in Section 4.2 employs two decoders with the above speciÔ¨Åcations. Our batch-size for training is set to 64, dropout to 0.1 and label smoothing weight to 0.1. We use the Noam optimizer to train for 100 epochs and perform model parameter averaging over epochs with the best 10 validation scores.
Since CTC is not applicable to the concatenated output model, as discussed in Section 4.1, we only perform experiments with the CTC loss for ASR-only and Dual-decoder models. In this set of experiments, we set the interpolation factor Œª as 0.3 in Eq. (7). And during inference, we use beam-search with width 6 and CTC decoding weight of 0.3. When training without CTC, we observed that inference with a beam width of 1 gave the best results.
All the trained models are evaluated over the dev and eval mixture samples for all the 5 levels of background sounds. The generated speech transcripts are evaluated using traditional character and word error rates i.e., CER and WER. The captions are evaluated using captioning metrics: CIDEr [24], SPICE [25] and SPIDEr [26], where SPIDEr computes a simple average of CIDEr and SPICE scores. The n-gram cosine similarity between generated and target captions is computed by CIDEr, while their semantic similarity after lemmatization is computed by SPICE. Therefore, SPIDEr score balances the semantic and syntactic similarity during audio captioning.
6. RESULTS
This section evaluates the performance of models trained independently for ASR, AAC tasks and compares them with our proposed jointly trained models.
6.1. Independent vs. Joint Modeling
The overall scores of speech and captioning metrics for all mixture weights Œ≥ in Eq. (1), when training without the CTC is detailed in Table 1. Top and middle sections of Table 2 summarizes the performance comparison when training without CTC and evaluating over all 5 mixing weights. Expected trends include the increase of SPIDEr and CER scores with increasing mixture weight Œ≥ of non-speech audio signal. We also observe that CER for Cat-AAC-ASR model is

Method (without CTC)
AAC-only Cat-ASR-AAC Cat-AAC-ASR Dual-decoder Method (without CTC)
ASR-only Cat-ASR-AAC Cat-AAC-ASR Dual-decoder Method (with CTC)
ASR-only Dual-decoder

Œ≥ = 0.1 0.309 0.328 0.392 0.268
Œ≥ = 0.1 9.0 9.7 9.4 9.1
Œ≥ = 0.1 5.7 5.5

SPIDEr of dev-split Œ≥ = 0.2 Œ≥ = 0.4 Œ≥ = 0.6

0.322 0.311 0.397 0.289

0.292 0.343 0.406 0.320

0.283 0.348 0.423 0.327

CER of dev-split Œ≥ = 0.2 Œ≥ = 0.4 Œ≥ = 0.6

10.5

14.1

17.8

11.4

15.3

19.6

10.4

13.9

18.3

10.7

14.3

18.5

CER of dev-split Œ≥ = 0.2 Œ≥ = 0.4 Œ≥ = 0.6

6.5

9.3

12.6

6.4

9.2

12.4

Œ≥ = 0.8 0.279 0.354 0.440 0.327
Œ≥ = 0.8 22.2 23.8 22.8 22.5
Œ≥ = 0.8 16.0 16.3

Œ≥ = 0.1 0.292 0.297 0.337 0.229
Œ≥ = 0.1 7.1 7.3 6.4 7.0
Œ≥ = 0.1 4.2 4.2

SPIDEr of eval-split Œ≥ = 0.2 Œ≥ = 0.4 Œ≥ = 0.6

0.280 0.309 0.347 0.235

0.293 0.337 0.371 0.243

0.291 0.339 0.374 0.242

CER of eval-split Œ≥ = 0.2 Œ≥ = 0.4 Œ≥ = 0.6

8.0

10.8

13.8

8.6

11.9

15.3

7.2

9.9

13.4

7.9

11.3

14.4

CER of eval-split Œ≥ = 0.2 Œ≥ = 0.4 Œ≥ = 0.6

5.0

6.9

9.4

4.7

6.7

9.6

Œ≥ = 0.8 0.252 0.326 0.382 0.239
Œ≥ = 0.8 17.4 18.7 17.5 17.8
Œ≥ = 0.8 12.3 12.4

Table 2. Comparison of models are trained without CTC (top, middle) and with CTC (bottom) and evaluated using CER (lower is better) and SPIDEr (higher is better) over the dev and eval splits for various mixing weights Œ≥ ‚àà {0.1, 0.2, 0.4, 0.6, 0.8}.

better as compared to Cat-ASR-AAC. The latter infers a speech transcript after generating the caption, thereby demonstrating the ability of Cat-AAC-ASR model to understand and utilize the background audio information when performing ASR. Bottom section of Table 2 shows signiÔ¨Åcant improvements when using CTC for speech transcription. It also empirically demonstrates the advantage of our joint modeling framework towards better speech recognition capability.
6.2. Testing with Real-world Speech Recordings
We acknowledge that it would be premature to conclusively state the superiority of our multi-task modeling based solely on improvements seen with synthetic mixtures. Therefore we additionally compare the model outputs over a few speech samples recorded in the presence of background sounds. These recordings were chosen from the speech samples present in AudioCaps dataset, which were Ô¨Åltered-out during mixing process.
For an audio sample captured during a duck hunt gameplay2, the transcripts and captions generated from models trained without the CTC loss are compared with human annotations in Table 3. The 10 second audio sample actually starts with some gun shots, followed by a person speaking and more gun shots, while ducks quack in the background. Although the human annotated caption is short, our dual-decoder model is able to capture all the constituent audio sources. On the other hand, all the ASR outputs seem mostly similar, with our proposed Cat-AAC-ASR and dual-decoder models slightly outperforming the ASR-only model.
7. CONCLUSIONS
ASR and AAC tasks generate speech transcripts and descriptive audio captions respectively, but share a commonality of outputting coherent word sequences from a given audio signal. As existing state-of-the-art models proposed for these tasks follow a similar Transformer based encoder-decoder framework, we explore the Ô¨Årst known attempt at modeling ASR and AAC as a multi-task problem.
2https://youtu.be/8hSarhQXJbg?start=30&end=40

Method ASR-only AAC-only Cat-AAC-ASR Dual-decoder
Human

Generated Output(s)
n. e. scale now that‚Äôs a break job a train approaches and blows a horn nice giel now that‚Äôs a break job a gun Ô¨Åres and a person whistles nise feel now that‚Äôs a break job gunshots Ô¨Åre and male voices with gunshots and blowing while a duck quacks in the background nice kill, now that‚Äôs a great shot a man speaking and gunshots ringing out

Table 3. Comparison of transcripts (blue) and captions (pink) between human annotations and generations from independently and jointly trained models for a speech sample recorded in real-world.

Our proposed method takes advantage of the common Transformer frameworks and propose several joint modeling approaches. As there are no currently known audio datasets which are labeled with both speech transcripts and audio captions, we prepare a multi-task synthetic dataset by mixing the clean speech samples with nonspeech captioned audio samples. Our extensive experiments over noisy speech samples with varying levels of background sounds, demonstrate the capability of our proposed multi-task models to match or even surpass the performance of independently trained ASR and AAC models.
8. ACKNOWLEDGEMENT
This work was supported in part by Sony Group Corporation and used the Extreme Science and Engineering Discovery Environment (XSEDE) [27], which is supported by National Science Foundation grant number ACI-1548562. SpeciÔ¨Åcally, it used the Bridges system [28], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).

9. REFERENCES
[1] Dong Yu and Li Deng, Automatic Speech Recognition., Springer, 2016.
[2] Alex Graves and Navdeep Jaitly, ‚ÄúTowards end-to-end speech recognition with recurrent neural networks,‚Äù in International conference on machine learning. PMLR, 2014, pp. 1764‚Äì1772.
[3] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al., ‚ÄúDeep speech 2: End-to-end speech recognition in english and mandarin,‚Äù in International conference on machine learning. PMLR, 2016, pp. 173‚Äì182.
[4] Alex Graves, ‚ÄúSequence transduction with recurrent neural networks,‚Äù arXiv preprint arXiv:1211.3711, 2012.
[5] Jinyu Li, Rui Zhao, Zhong Meng, Yanqing Liu, Wenning Wei, Sarangarajan Parthasarathy, Vadim Mazalov, Zhenghao Wang, Lei He, Sheng Zhao, et al., ‚ÄúDeveloping RNN-T models surpassing high-performance hybrid models with customization capability,‚Äù arXiv preprint arXiv:2007.15188, 2020.
[6] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, ‚ÄúAttention-based models for speech recognition,‚Äù arXiv preprint arXiv:1506.07503, 2015.
[7] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, ‚ÄúListen, attend and spell: A neural network for large vocabulary conversational speech recognition,‚Äù in Proc. of ICASSP. IEEE, 2016, pp. 4960‚Äì4964.
[8] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al., ‚ÄúA comparative study on Transformer vs RNN in speech applications,‚Äù in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 449‚Äì456.
[9] Jinyu Li, Li Deng, Yifan Gong, and Reinhold Haeb-Umbach, ‚ÄúAn overview of noise-robust automatic speech recognition,‚Äù IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745‚Äì777, 2014.
[10] Reinhold Haeb-Umbach, Shinji Watanabe, Tomohiro Nakatani, Michiel Bacchiani, Bjorn Hoffmeister, Michael L Seltzer, Heiga Zen, and Mehrez Souden, ‚ÄúSpeech processing for digital home assistants: Combining signal processing with deep-learning techniques,‚Äù IEEE Signal processing magazine, vol. 36, no. 6, pp. 111‚Äì124, 2019.
[11] Niko Moritz, Gordon Wichern, Takaaki Hori, and Jonathan Le Roux, ‚ÄúAll-in-one transformer: Unifying speech recognition, audio tagging, and event detection.,‚Äù in INTERSPEECH, 2020, pp. 3112‚Äì3116.
[12] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim, ‚ÄúAudioCaps: Generating captions for audios in the wild,‚Äù in Proc. of NAACL-HLT, 2019, pp. 119‚Äì132.
[13] Konstantinos Drossos, Sharath Adavanne, and Tuomas Virtanen, ‚ÄúAutomated audio captioning with recurrent neural networks,‚Äù in Proc. of WASPAA. IEEE, 2017.
[14] Chaitanya Narisetty, Tomoki Hayashi, Ryunosuke Ishizaki, Shinji Watanabe, and Kazuya Takeda, ‚ÄúLeveraging state-ofthe-art ASR techniques to audio captioning,‚Äù Tech. Rep., DCASE2021 Challenge, July 2021.

[15] Xinhao Mei, Xubo Liu, Qiushi Huang, Mark D Plumbley, and Wenwu Wang, ‚ÄúAudio captioning transformer,‚Äù arXiv preprint arXiv:2107.09817, 2021.
[16] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al., ‚ÄúEspnet: End-to-end speech processing toolkit,‚Äù arXiv preprint arXiv:1804.00015, 2018.
[17] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan, ‚ÄúAdvances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM,‚Äù arXiv preprint arXiv:1706.02737, 2017.
[18] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng Chen, and Rohit Prabhavalkar, ‚ÄúAn analysis of incorporating an external language model into a sequence-tosequence model,‚Äù in Proc. of ICASSP. IEEE, 2018, pp. 1‚Äì5828.
[19] Douglas B Paul and Janet Baker, ‚ÄúThe design for the wall street journal-based CSR corpus,‚Äù in Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992, 1992.
[20] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter, ‚ÄúAudio set: An ontology and human-labeled dataset for audio events,‚Äù in Proc. of ICASSP. IEEE, 2017, pp. 776‚Äì780.
[21] Xuankai Chang, Yanmin Qian, Kai Yu, and Shinji Watanabe, ‚ÄúEnd-to-end monaural multi-speaker ASR system without pretraining,‚Äù in Proc. of ICASSP. IEEE, 2019, pp. 6256‚Äì6260.
[22] Naoyuki Kanda, Yashesh Gaur, Xiaofei Wang, Zhong Meng, and Takuya Yoshioka, ‚ÄúSerialized output training for endto-end overlapped speech recognition,‚Äù arXiv preprint arXiv:2003.12687, 2020.
[23] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi, ‚ÄúHybrid CTC/attention architecture for end-to-end speech recognition,‚Äù IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, 2017.
[24] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh, ‚ÄúCider: Consensus-based image description evaluation,‚Äù in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4566‚Äì4575.
[25] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould, ‚ÄúSpice: Semantic propositional image caption evaluation,‚Äù in European conference on computer vision. Springer, 2016, pp. 382‚Äì398.
[26] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy, ‚ÄúImproved image captioning via policy gradient optimization of spider,‚Äù in Proceedings of the IEEE international conference on computer vision, 2017, pp. 873‚Äì881.
[27] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood, S. Lathrop, D. Lifka, G. D. Peterson, R. Roskies, J. R. Scott, and N. Wilkins-Diehr, ‚ÄúXsede: Accelerating scientiÔ¨Åc discovery,‚Äù Computing in Science & Engineering, vol. 16, no. 5, pp. 62‚Äì74, Sept.-Oct. 2014.
[28] Nicholas A Nystrom, Michael J Levine, Ralph Z Roskies, and J Ray Scott, ‚ÄúBridges: a uniquely Ô¨Çexible HPC resource for new communities and data analytics,‚Äù in Proceedings of the 2015 XSEDE Conference: ScientiÔ¨Åc Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1‚Äì8.

