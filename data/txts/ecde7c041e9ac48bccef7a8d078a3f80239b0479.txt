arXiv:1607.04648v2 [cs.CV] 19 Jul 2016

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

1

Context Matters: Reﬁning Object Detection in Video with Recurrent Neural Networks

Subarna Tripathi1 stripathi@ucsd.edu Zachary C. Lipton1 zlipton@cs.ucsd.edu Serge Belongie2, 3 sjb344@cornell.edu Truong Nguyen1 tqn001@eng.ucsd.edu

1 University of California San Diego La Jolla, CA, USA
2 Cornell University Ithaca, NY, USA
3 Cornell Tech New York, NY, USA

Abstract
Given the vast amounts of video available online, and recent breakthroughs in object detection with static images, object detection in video offers a promising new frontier. However, motion blur and compression artifacts cause substantial frame-level variability, even in videos that appear smooth to the eye. Additionally, video datasets tend to have sparsely annotated frames. We present a new framework for improving object detection in videos that captures temporal context and encourages consistency of predictions. First, we train a pseudo-labeler, that is, a domain-adapted convolutional neural network for object detection. The pseudo-labeler is ﬁrst trained individually on the subset of labeled frames, and then subsequently applied to all frames. Then we train a recurrent neural network that takes as input sequences of pseudo-labeled frames and optimizes an objective that encourages both accuracy on the target frame and consistency across consecutive frames. The approach incorporates strong supervision of target frames, weaksupervision on context frames, and regularization via a smoothness penalty. Our approach achieves mean Average Precision (mAP) of 68.73, an improvement of 7.1 over the strongest image-based baselines for the Youtube-Video Objects dataset. Our experiments demonstrate that neighboring frames can provide valuable information, even absent labels.
1 Introduction
Despite the immense popularity and availability of online video content via outlets such as Youtube and Facebook, most work on object detection focuses on static images. Given the breakthroughs of deep convolutional neural networks for detecting objects in static images, the application of these methods to video might seem straightforward. However, motion blur and compression artifacts cause substantial frame-to-frame variability, even in videos that appear smooth to the eye. These attributes complicate prediction tasks like classiﬁcation and localization. Object-detection models trained on images tend not to perform competitively on videos owing to domain shift factors [12]. Moreover, object-level annotations in popular
c 2016. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.

2

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

video data-sets can be extremely sparse, impeding the development of better video-based object detection models.
Girshik et al. [9] demonstrate that even given scarce labeled training data, high-capacity convolutional neural networks can achieve state of the art detection performance if ﬁrst pretrained on a related task with abundant training data, such as 1000-way ImageNet classiﬁcation. Followed the pretraining, the networks can be ﬁne-tuned to a related but distinct domain. Also relevant to our work, the recently introduced models Faster R-CNN [21] and You Look Only Once (YOLO) [20] unify the tasks of classiﬁcation and localization. These methods, which are accurate and efﬁcient, propose to solve both tasks through a single model, bypassing the separate object proposal methods used by R-CNN [9].
In this paper, we introduce a method to extend uniﬁed object recognition and localization to the video domain. Our approach applies transfer learning from the image domain to video frames. Additionally, we present a novel recurrent neural network (RNN) method that reﬁnes predictions by exploiting contextual information in neighboring frames. In summary, we contribute the following:

• A new method for reﬁning a video-based object detection consisting of two parts: (i) a pseudo-labeler, which assigns provisional labels to all available video frames. (ii) A recurrent neural network, which reads in a sequence of provisionally labeled frames, using the contextual information to output reﬁned predictions.
• An effective training strategy utilizing (i) category-level weak-supervision at every time-step, (ii) localization-level strong supervision at ﬁnal time-step (iii) a penalty encouraging prediction smoothness at consecutive time-steps, and (iv) similarity constraints between pseudo-labels and prediction output at every time-step.
• An extensive empirical investigation demonstrating that on the YouTube Objects [19] dataset, our framework achieves mean average precision (mAP) of 68.73 on test data, compared to a best published result of 37.41 [26] and 61.66 for a domain adapted YOLO network [20].

2 Methods
In this work, we aim to reﬁne object detection in video by utilizing contextual information from neighboring video frames. We accomplish this through a two-stage process. First, we train a pseudo-labeler, that is, a domain-adapted convolutional neural network for object detection, trained individually on the labeled video frames. Speciﬁcally, we ﬁne-tune the YOLO object detection network [20], which was originally trained for the 20-class PASCAL VOC [5] dataset, to the Youtube-Video [19] dataset.
When ﬁne-tuning to the 10 sub-categories present in the video dataset, our objective is to minimize the weighted squared detection loss (equation 3) as speciﬁed in YOLO [20]. While ﬁne-tuning, we learn only the parameters of the top-most fully-connected layers, keeping the 24 convolutional layers and 4 max-pooling layers unchanged. The training takes roughly 50 epochs to converge, using the RMSProp [25] optimizer with momentum of 0.9 and a minibatch size of 128.
As with YOLO [20], our ﬁne-tuned pseudo − labeler takes 448 × 448 frames as input and regresses on category types and locations of possible objects at each one of S × S nonoverlapping grid cells. For each grid cell, the model outputs class conditional probabilities

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

3

as well as B bounding boxes and their associated conﬁdence scores. As in YOLO, we consider a responsible bounding box for a grid cell to be the one among the B boxes for which
the predicted area and the ground truth area shares the maximum Intersection Over Union.
During training, we simultaneously optimize classiﬁcation and localization error (equation 3). For each grid cell, we minimize the localization error for the responsible bounding box
with respect to the ground truth only when an object appears in that cell.
Next, we train a Recurrent Neural Network (RNN), with Gated Recurrent Units (GRUs) [2]. This net takes as input sequences of pseudo-labels, optimizing an objective that encour-
ages both accuracy on the target frame and consistency across consecutive frames. Given a series of pseudo-labels x(1), ..., x(T), we train the RNN to generate improved predictions yˆ(1), ..., yˆ(T) with respect to the ground truth y(T) available only at the ﬁnal step in each sequence. Here, t indexes sequence steps and T denotes the length of the sequence. As output,
we use a fully-connected layer with a linear activation function, as our problem is regression.
In our ﬁnal experiments, we use a 2-layer GRU with 150 nodes per layer, hyper-parameters
determined on validation data. The following equations deﬁne the forward pass through a GRU layer, where h(lt) denotes
the layer’s output at the current time step, and h(l−t)1 denotes the previous layer’s output at the same sequence step:

rl(t) = σ (hl(−t)1Wlxr + h(lt−1)Wlhr + brl ) ul(t) = σ (hl(−t)1Wlxu + h(lt−1)Wlhu + bul )
(1) cl(t) = σ (hl(−t)1Wlxc + rt (h(lt−1)Wlhc) + bcl ) hl(t) = (1 − ul(t)) h(lt−1) + ul(t) c(lt)

Here, σ denotes an element-wise logistic function and is the (element-wise) Hadamard
product. The reset gate, update gate, and candidate hidden state are denoted by r, u, and c respectively. For S = 7 and B = 2, the pseudo-labels x(t) and prediction yˆ(t) both lie in R1470.

2.1 Training
We design an objective function (Equation 2) that accounts for both accuracy at the target frame and consistency of predictions across adjacent time steps in the following ways:

loss = d_loss + α · s_loss + β · c_loss + γ · pc_loss

(2)

Here, d_loss, s_loss, c_loss and pc_loss stand for detection_loss, similarity_loss, category_loss and prediction_consistency_loss described in the following sections. The values of the hyper-parameters α = 0.2, β = 0.2 and γ = 0.1 are chosen based on the detection performance on the validation set. The training converges in 80 epochs for parameter updates using RMSProp [25] and momentum 0.9. During training we use a mini-batch size of 128 and sequences of length 30.

2.1.1 Strong Supervision at Target Frame
On the ﬁnal output, for which the ground truth classiﬁcation and localization is available, we apply a multi-part object detection loss as described in YOLO [20].

4

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

∑ ∑ S2 B

detection_loss = λcoord

1oi jb j xi(T ) − xˆi(T ) 2 + y(iT ) − yˆ(iT ) 2

i=0 j=0

∑ ∑ + λcoord S2 B 1oi jb j √wi(T ) − i=0 j=0

wˆ (iT ) 2 +

hi(T ) − hˆ (iT ) 2

∑ ∑ S2 B

+

1oi jb j(Ci − Cˆi)2

(3)

i=0 j=0

∑ ∑ 1 S2 B
+ λnoob j

ni joob j Ci(T ) − Cˆ i(T ) 2

i=0 j=0

∑ ∑ S2
+ 1oi b j

p(iT )(c) − pˆi(T )(c) 2

i=0

c∈classes

where 1oi b j denotes if the object appears in cell i and 1oi jb j denotes that jth bounding box
predictor in cell i is responsible for that prediction. The loss function penalizes classiﬁcation
and localization error differently based on presence or absence of an object in that grid
cell. xi, yi, wi, hi corresponds to the ground truth bounding box center coordinates, width and height for objects in grid cell (if it exists) and xˆi, yˆi, wˆi, hˆi stand for the corresponding predictions. Ci and Cˆi denote conﬁdence score of objectness at grid cell i for ground truth and prediction. pi(c) and pˆi(c) stand for conditional probability for object class c at cell index i for ground truth and prediction respectively. We use similar settings for YOLO’s
object detection loss minimization and use values of λcoord = 5 and λnoob j = 0.5.

2.1.2 Similarity between Pseudo-labels and Predictions

Our objective function also includes a regularizer that penalizes the dissimilarity between

pseudo-labels and the prediction at each time frame t.

∑ ∑ T S2

2

similarity_loss =

Cˆi(t) x(it) − yˆi(t)

(4)

t=0 i=0

Here, x(it) and yˆi(t) denote the pseudo-labels and predictions corresponding to the i-th grid cell at t-th time step respectively. We perform minimization of the square loss weighted by
the predicted conﬁdence score at the corresponding cell.

2.1.3 Object Category-level Weak-Supervision
Replication of the static target at each sequential step has been shown to be effective in [3, 16, 28]. Of course, with video data, different objects may move in different directions and speeds. Yet, within a short time duration, we could expect all objects to be present. Thus we employ target replication for classiﬁcation but not localization objectives.
We minimize the square loss between the categories aggregated over all grid cells in the ground truth y(T) at ﬁnal time step T and predictions yˆ(t) at all time steps t. Aggregated category from the ground truth considers only the cell indices where an object is present. For predictions, contribution of cell i is weighted by its predicted conﬁdence score Cˆi(t). Note that cell indices with positive detection are sparse. Thus, we consider the conﬁdence score of each cell while minimizing the aggregated category loss.

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

5

∑ ∑ ∑ ∑ T
category_loss =

S2

S2

2

Cˆi(t) pˆi(t)(c) −

1ob j(T ) i

p(iT )(c)

(5)

t=0 c∈classes i=0

i=0

2.1.4 Consecutive Prediction Smoothness

Additionally, we regularize the model by encouraging smoothness of predictions across con-

secutive time-steps. This makes sense intuitively because we assume that objects rarely move

rapidly from one frame to another.

∑T −1

2

prediction_consistency_loss =

yˆi(t) − yˆi(t+1)

(6)

t=0

2.2 Inference
The recurrent neural network predicts output at every time-step. The network predicts 98 bounding boxes per video frame and class probabilities for each of the 49 grid cells. We note that for every cell, the net predicts class conditional probabilities for each one of the C categories and B bounding boxes. Each one of the B predicted bounding boxes per cell has an associated objectness conﬁdence score. The predicted conﬁdence score at that grid is the maximum among the boxes. The bounding box with the highest score becomes the responsible prediction for that grid cell i.
The product of class conditional probability pˆ(it)(c) for category type c and objectness conﬁdence score Cˆi(t) at grid cell i, if above a threshold, infers a detection. In order for an object of category type c to be detected for i-th cell at time-step t, both the class conditional probability pˆ(it)(c) and objectness score Cˆi(t) must be reasonably high.
Additionally, we employ Non-Maximum Suppression (NMS) to winnow multiple high scoring bounding boxes around an object instance and produce a single detection for an instance. By virtue of YOLO-style prediction, NMS is not critical.

3 Experimental Results
In this section, we empirically evaluate our model on the popular Youtube-Objects dataset, providing both quantitative results (as measured by mean Average Precision) and subjective evaluations of the model’s performance, considering both successful predictions and failure cases.
The Youtube-Objects dataset[19] is composed of videos collected from Youtube by querying for the names of 10 object classes of the PASCAL VOC Challenge. It contains 155 videos in total and between 9 and 24 videos for each class. The duration of each video varies between 30 seconds and 3 minutes. However, only 6087 frames are annotated with 6975 bounding-box instances. The training and test split is provided.
3.1 Experimental Setup
We implement the domain-adaption of YOLO and the proposed RNN model using Theano [24]. Our best performing RNN model uses two GRU layers of 150 hidden units each and dropout of probability 0.5 between layers, signiﬁcantly outperforming domain-adapted YOLO alone. While we can only objectively evaluate prediction quality on the labeled frames, we present subjective evaluations on sequences.

6

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

Average Precision on 10-categories

Methods

airplane bird boat car

cat

cow dog horse mbike train

DPM[6] VOP[26] YOLO[20] DA YOLO

28.42 29.77 76.67 83.89

48.14 28.82 89.51 91.98

25.50 35.34 57.66 59.91

48.99 41.00 65.52 81.95

1.69 33.7 43.03 46.67

19.24 57.56 53.48 56.78

15.84 34.42 55.81 53.49

35.10 54.52 36.96 42.53

31.61 29.77 24.62 32.31

39.58 29.23 62.03 67.09

RNN-IOS RNN-WS RNN-PS

82.78 77.78 76.11

89.51 89.51 87.65

68.02 69.40 62.16

82.67 78.16 80.69

47.88 51.52 62.42

70.33 78.39 78.02

52.33 47.09 58.72

61.52 81.52 81.77

27.69 36.92 41.54

67.72 62.03 58.23

Table 1: Per-category object detection results for the Deformable Parts Model (DPM), Video

Object Proposal based AlexNet (VOP), image-trained YOLO (YOLO), domain-adapted

YOLO (DA-YOLO). RNN-IOS regularizes on input-output similarity, to which RNN-WS

adds category-level weak-supervision, to which RNN-PS adds a regularizer encouraging

prediction smoothness.

3.2 Objective Evaluation
We compare our approach with other methods evaluated on the Youtube-Objects dataset. As shown in Table 3.2 and Table 3.2, Deformable Parts Model (DPM) [6])-based detector reports [12] mean average precision below 30, with especially poor performance in some categories such as cat. The method of Tripathi et al. (VPO) [26] uses consistent video object proposals followed by a domain-adapted AlexNet classiﬁer (5 convolutional layer, 3 fully connected) [14] in an R-CNN [9]-like framework, achieving mAP of 37.41. We also compare against YOLO (24 convolutional layers, 2 fully connected layers), which uniﬁes the classiﬁcation and localization tasks, and achieves mean Average Precision over 55.
In our method, we adapt YOLO to generate pseudo-labels for all video frames, feeding them as inputs to the reﬁnement RNN. We choose YOLO as the pseudo-labeler because it is the most accurate among feasibly fast image-level detectors. The domain-adaptation improves YOLO’s performance, achieving mAP of 61.66.
Our model with RNN-based prediction reﬁnement, achieves superior aggregate mAP to all baselines. The RNN reﬁnement model using both input-output similarity, categorylevel weak-supervision, and prediction smoothness performs best, achieving 68.73 mAP. This amounts to a relative improvement of 11.5% over the best baselines. Additionally, the RNN improves detection accuracy on most individual categories (Table 3.2).

mean Average Precision on all categories

Methods DPM VOP YOLO DA YOLO RNN-IOS RNN-WS RNN-PS

mAP

29.41 37.41 56.53 61.66

65.04

67.23

68.73

Table 2: Overall detection results on Youtube-Objects dataset. Our best model (RNN-PS) provides 7% improvements over DA-YOLO baseline.

3.3 Subjective Evaluation
We provide a subjective evaluation of the proposed RNN model in Figure 1. Top and bottom rows in every pair of sequences correspond to pseudo-labels and results from our approach respectively. While only the last frame in each sequence has associated ground truth, we can observe that the RNN produces more accurate and more consistent predictions across

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

7

time frames. The predictions are consistent with respect to classiﬁcation, localization and conﬁdence scores.
In the ﬁrst example, the RNN consistently detects the dog throughout the sequence, even though the pseudo-labels for the ﬁrst two frames were wrong (bird). In the second example, pseudo-labels were motorbike, person, bicycle and even none at different time-steps. However, our approach consistently predicted motorbike. The third example shows that the RNN consistently predicts both of the cars while the pseudo-labeler detects only the smaller car in two frames within the sequence. The last two examples show how the RNN increases its conﬁdence scores, bringing out the positive detection for cat and car respectively both of which fell below the detection threshold of the pseudo-labeler.

3.4 Areas For Improvement
The YOLO scheme for unifying classiﬁcation and localization [20] imposes strong spatial constraints on bounding box predictions since each grid cell can have only one class. This restricts the set of possible predictions, which may be undesirable in the case where many objects are in close proximity. Additionally, the rigidity of the YOLO model may present problems for the reﬁnement RNN, which encourages smoothness of predictions across the sequence of frames. Consider, for example, an object which moves slightly but transits from one grid cell to another. Here smoothness of predictions seems undesirable.
Figure 2 shows some failure cases. In the ﬁrst case, the pseudo-labeler classiﬁes the instances as dogs and even as birds in two frames whereas the ground truth instances are horses. The RNN cannot recover from the incorrect pseudo-labels. Strangely, the model increases the conﬁdence score marginally for a different wrong category cow. In the second case, possibly owing to motion and close proximity of multiple instances of the same object category, the RNN predicts the correct category but fails on localization. These point to future work to make the framework robust to motion.
The category-level weak supervision in the current scheme assumes the presence of all objects in nearby frames. While for short snippets of video this assumption generally holds, it may be violated in case of occlusions, or sudden arrival or departure of objects. In addition, our assumptions regarding the desirability of prediction smoothness can be violated in the case of rapidly moving objects.
4 Related Work
Our work builds upon a rich literature in both image-level object detection,video analysis, and recurrent neural networks. Several papers propose ways of using deep convolutional networks for detecting objects [1, 8, 9, 10, 17, 20, 21, 22, 23, 27]. Some approaches classify the proposal regions [9, 10] into object categories and some other recent methods [20, 21] unify the localization and classiﬁcation stages. Kalogeiton et al. [12] identiﬁes domain shift factors between still images and videos, necessitating video-speciﬁc object detectors. To deal with shift factors and sparse object-level annotations in video, researchers have proposed several strategies. Recently, [26] proposed both transfer learning from the image domain to video frames and optimizing for temporally consistent object proposals. Their approach is capable of detecting both moving and static objects. However, the object proposal generation step that precedes classiﬁcation is slow.
Prest et al. [18], utilize weak supervision for object detection in videos via category-level annotations of frames, absent localization ground truth. This method assumes that the target

8

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

Figure 1: Object detection results from the ﬁnal eight frames of ﬁve different test-set sequences. In each pair of rows, the top row shows the pseudo-labeler and the bottom row shows the RNN. In the ﬁrst two examples, the RNN consistently predicts correct categories dog and motorbike, in contrast to the inconsistent baseline. In the third sequence, the RNN correctly predicts multiple instances while the pseudo-labeler misses one. For the last two sequences, the RNN increases the conﬁdence score, detecting objects missed by the baseline.
Figure 2: Failure cases for the proposed model. Left: the RNN cannot recover from incorrect pseudo-labels. Right: RNN localization performs worse than pseudo-labels possibly owing to multiple instances of the same object.

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

9

object is moving, outputting a spatio-temporal tube that captures this most salient moving object. This paper, however, does not consider context within video for detecting multiple objects.
A few recent papers [1, 17] identify the important role of context in visual recognition. For object detection in images, Bell et al. [1] use spatial RNNs to harness contextual information, showing large improvements on PASCAL VOC [5] and Microsoft COCO [15] object detection datasets. Their approach adopts proposal generation followed by classiﬁcation framework. This paper exploits spatial, but not temporal context.
Recently, Kang et al. [13] introduced tubelets with convolutional neural networks (TCNN) for detecting objects in video. T-CNN uses spatio-temporal tubelet proposal generation followed by the classiﬁcation and re-scoring, incorporating temporal and contextual information from tubelets obtained in videos. T-CNN won the recently introduced ImageNet object-detection-from-video (VID) task with provided densely annotated video clips. Although the method is effective for densely annotated training data, it’s behavior for sparsely labeled data is not evaluated.
By modeling video as a time series, especially via GRU [2] or LSTM RNNs[11], several papers demonstrate improvement on visual tasks including video classiﬁcation [28], activity recognition [4], and human dynamics [7]. These models generally aggregate CNN features over tens of seconds, which forms the input to an RNN. They perform well for global description tasks such as classiﬁcation [4, 28] but require large annotated datasets. Yet, detecting multiple generic objects by explicitly modeling video as an ordered sequence remains less explored.
Our work differs from the prior art in a few distinct ways. First, this work is the ﬁrst, to our knowledge, to demonstrate the capacity of RNNs to improve localized object detection in videos. The approach may also be the ﬁrst to reﬁne the object predictions of frame-level models. Notably, our model produces signiﬁcant improvements even on a small dataset with sparse annotations.
5 Conclusion

We introduce a framework for reﬁning object detection in video. Our approach extracts contextual information from neighboring frames, generating predictions with state of the art accuracy that are also temporally consistent. Importantly, our model beneﬁts from context frames even when they lack ground truth annotations.
For the recurrent model, we demonstrate an efﬁcient and effective training strategy that simultaneously employs localization-level strong supervision, category-level weak-supervision, and a penalty encouraging smoothness of predictions across adjacent frames. On a video dataset with sparse object-level annotation, our framework proves effective, as validated by extensive experiments. A subjective analysis of failure cases suggests that the current approach may struggle most on cases when multiple rapidly moving objects are in close proximity. Likely, the sequential smoothness penalty is not optimal for such complex dynamics.
Our results point to several promising directions for future work. First, recent state of the art results for video classiﬁcation show that longer sequences help in global inference. However, the use of longer sequences for localization remains unexplored. We also plan to explore methods to better model local motion information with the goal of improving localization of multiple objects in close proximity. Another promising direction, we would like to experiment with loss functions to incorporate specialized handling of classiﬁcation and localization objectives.

10
References

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

[1] Sean Bell, C. Lawrence Zitnick, Kavita Bala, and Ross B. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. to appear in CVPR 2016, abs/1512.04143, 2015. URL http://arxiv.org/abs/ 1512.04143.
[2] KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In Proc. Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.
[3] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, pages 3061–3069, 2015.
[4] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. CoRR, abs/1411.4389, 2014. URL http://arxiv.org/abs/1411.4389.
[5] Mark Everingham, Luc Gool, Christopher K. Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. Int. J. Comput. Vision, 88 (2):303–338, June 2010. ISSN 0920-5691. doi: 10.1007/s11263-009-0275-4. URL http://dx.doi.org/10.1007/s11263-009-0275-4.
[6] Pedro Felzenszwalb, David McAllester, and Deva Ramanan. A discriminatively trained, multiscale, deformable part model. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008.
[7] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. Recurrent network models for human dynamics. In The IEEE International Conference on Computer Vision (ICCV), December 2015.
[8] Spyros Gidaris and Nikos Komodakis. Object detection via a multi-region and semantic segmentation-aware cnn model. In The IEEE International Conference on Computer Vision (ICCV), December 2015.
[9] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. CVPR, 2014.
[10] Ross B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. URL http: //arxiv.org/abs/1504.08083.
[11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, November 1997.
[12] Vicky Kalogeiton, Vittorio Ferrari, and Cordelia Schmid. Analysing domain shift factors between videos and images for object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.
[13] Kai Kang, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. Object detection from video tubelets with convolutional neural networks. to appear in CVPR, 2016. URL http://www.ee.cuhk.edu.hk/~wlouyang/Papers/ KangVideoDet_CVPR16.pdf.

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

11

[14] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NIPS, 2012.

[15] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.

[16] Zachary Chase Lipton, David C. Kale, Charles Elkan, and Randall Wetzell. Learning to diagnose with LSTM recurrent neural networks. ICLR 2016, 2016. URL http: //arxiv.org/abs/1511.03677.

[17] Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, and Xiaoou Tang. Deepidnet: Deformable deep convolutional neural networks for object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.

[18] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio Ferrari. Learning object class detectors from weakly annotated video. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012, pages 3282–3289, 2012.

[19] Alessandro Prest, Vicky Kalogeiton, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio Ferrari. Youtube-objects dataset v2.0, 2014. URL calvin. inf.ed.ac.uk/datasets/youtube-objects-dataset. University of Edinburgh (CALVIN), INRIA Grenoble (LEAR), ETH Zurich (CALVIN).

[20] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object detection. to appear in CVPR 2016, abs/1506.02640, 2015. URL http://arxiv.org/abs/1506.02640.

[21] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In NIPS, pages 91–99, 2015.

[22] Pierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013. URL http://arxiv.org/abs/ 1312.6229.

[23] Christian Szegedy, Scott E. Reed, Dumitru Erhan, and Dragomir Anguelov. Scalable, high-quality object detection. CoRR, abs/1412.1441, 2014. URL http://arxiv. org/abs/1412.1441.

[24] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016. URL http: //arxiv.org/abs/1605.02688.

[25] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude, 2012.

[26] Subarna Tripathi, Serge J. Belongie, Youngbae Hwang, and Truong Q. Nguyen. Detecting temporally consistent objects in videos through object class label propagation. WACV, 2016.

12

TRIPATHI ET AL.: REFINING VIDEO OBJECT DETECTION WITH RNN

[27] Bin Yang, Junjie Yan, Zhen Lei, and Stan Z. Li. Craft objects from images. to appear in CVPR, 2016. URL http://arxiv.org/pdf/1604.03239v1.pdf.

[28] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George Toderici. Beyond short snippets: Deep networks for video classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4694–4702, 2015.

