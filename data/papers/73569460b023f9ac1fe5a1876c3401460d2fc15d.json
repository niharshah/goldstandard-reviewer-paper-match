{"title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding", "abstract": "With the great success of pre-trained models, the pretrain-thenfinetune paradigm has been widely adopted on downstream tasks for source code understanding. However, compared to costly training a large-scale model from scratch, how to effectively adapt pretrained models to a new task has not been fully explored. In this paper, we propose an approach to bridge pre-trained models and code-related tasks. We exploit semantic-preserving transformation to enrich downstream data diversity, and help pre-trained models learn semantic features invariant to these semantically equivalent transformations. Further, we introduce curriculum learning to organize the transformed data in an easy-to-hard manner to fine-tune existing pre-trained models. We apply our approach to a range of pre-trained models, and they significantly outperform the state-of-the-art models on tasks for source code understanding, such as algorithm classification, code clone detection, and code search. Our experiments even show that without heavy pre-training on code data, natural language pretrained model RoBERTa fine-tuned with our lightweight approach could outperform or rival existing code pre-trained models finetuned on the above tasks, such as CodeBERT and GraphCodeBERT. This finding suggests that there is still much room for improvement in code pre-trained models. \u2217Zhouyang Jia and Shanshan Li are the corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICSE \u201922, May 21\u201329, 2022, Pittsburgh, PA, USA \u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00 https://doi.org/10.1145/3510003.3510062 CCS CONCEPTS \u2022 Computing methodologies \u2192 Supervised learning; Artificial intelligence.", "year": 2021, "ssId": "73569460b023f9ac1fe5a1876c3401460d2fc15d", "arXivId": "2112.02268", "link": "https://arxiv.org/pdf/2112.02268.pdf", "openAccess": true, "authors": ["Deze Wang", "Zhouyang Jia", "Shanshan Li", "Yue Yu", "Yun Xiong", "Wei Dong", "Xiangke Liao"]}