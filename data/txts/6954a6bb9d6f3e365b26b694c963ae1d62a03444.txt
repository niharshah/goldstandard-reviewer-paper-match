Fastformer: Additive Attention Can Be All You Need
Chuhan Wuâ€  Fangzhao Wuâ€¡ Tao Qiâ€  Yongfeng Huangâ€  Xing Xieâ€¡ â€ Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084, China
â€¡Microsoft Research Asia, Beijing 100080, China {wuchuhan15, wufangzhao, taoqi.qt}@gmail.com yfhuang@tsinghua.edu.cn, xingx@microsoft.com

arXiv:2108.09084v6 [cs.CL] 5 Sep 2021

Abstract
Transformer is a powerful model for text understanding. However, it is inefï¬cient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefï¬cient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efï¬cient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise intractions between tokens, we ï¬rst use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on ï¬ve datasets show that Fastformer is much more efï¬cient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.
1 Introduction
Transformer (Vaswani et al., 2017) and their variants have achieved great success in many ï¬elds. For example, Transformer is the backbone architecture of many state-of-the-art pre-trained language models in NLP, such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2019). Transformer also shows great promises in vision-related tasks (Dosovitskiy et al., 2021). The core of a Transformer model is self-attention mechanism, which allows the Transformer to model the contexts within an input sequence (Parikh et al., 2016). However, since self-attention computes the dot-product between the input representations at each pair of positions, its complexity is quadratic to the input sequence length (Vaswani et al., 2017). Thus, it is difï¬cult for standard Transformer models to efï¬ciently handle long input sequences (Tay et al., 2020).

There are many methods to accelerate the Transformer model (Beltagy et al., 2020; Zaheer et al., 2020; Wang et al., 2020b; Kitaev et al., 2020; Tay et al., 2021). For example, BigBird (Zaheer et al., 2020) computes sparse attention instead of a dense one. It uses a combination of local attention, global attention at certain positions and random attention between a certain number of tokens. However, sparse attention usually cannot fully model the global context (Wu et al., 2021b). Linformer (Wang et al., 2020b) exploits the low-rank characteristic of the self-attention matrix by computing approximated ones. It projects attention key and value into low-dimensional matrices that are independent of the sequence length. However, the approximation is in fact context-agnostic, which may weaken the context modeling ability of Transformer. In addition, these methods are not efï¬cient enough when the input sequence length is very long.
In this paper we propose Fastformer1, which is an efï¬cient Transformer variant based on additive attention that can achieve effective context modeling in linear complexity. In Fastformer, we ï¬rst use additive attention mechanism to summarize the input attention query matrix into a global query vector. Next, we model the interaction between attention key and the global query vector via element-wise product to learn global contextaware key matrix, and we further summarize it into a global key vector via additive attention. Then we use element-wise product to aggregate the global key and attention value, which are further processed by a linear transformation to compute the global context-aware attention value. Finally, we add together the original attention query and the global context-aware attention value to form the ï¬nal output. We conduct extensive experiments on ï¬ve benchmark datasets in various tasks, including
1A pytorch version of Fastformer using the huggingface style is available at https://github.com/wuch15/Fastformer.

sentiment classiï¬cation, topic prediction, news recommendation and text summarization. The results demonstrate that Fastformer is much more efï¬cient than many Transformer models and can achieve quite competitive results in long text modeling.
The contributions of this paper are summarized as follows:
â€¢ We propose an additive attention based Transformer named Fastformer. To the best of our knowledge, Fastformer is the most efï¬cient Transformer architecture.
â€¢ We propose to model the interaction between global contexts and token representations via element-wise product, which can help fully model context information in an efï¬cient way.
â€¢ Extensive experiments on ï¬ve datasets show that Fastformer is much more efï¬cient than many Transformer models and can achieve competitive performance.
2 Related Work
2.1 Transformer and Self-Attention
The Transformer model is built upon multi-head self-attention, which can effectively model the contexts within a sequence by capturing the interactions between the inputs at each pair of positions (Vaswani et al., 2017). An h-head selfattention mechanism can be formulated as follows:

MultiHead(Q, K, V) = Concat(head1, head2, ..., headh)WO, (1)

where Q, K, V âˆˆ RNÃ—d are the input query, key and value matrices, N is the sequence length, d is the hidden dimension in each attention head, and WO âˆˆ RhdÃ—d is a linear transformation parameter matrix. The representation learned by each attention head is formulated as follows:

headi = Attention(QWiQ, KWiK, VWiV)

QWiQ(KWiK)T

V

= softmax(

âˆš

)VWi ,

d

(2)

where WiQ, WiK , WiV âˆˆ RdÃ—d are learnable pa-

rameters. From this formula, we can see that the

computational complexity is quadratic to the se-

quence length N . It has become a bottleneck for

Transformers to handle long sequences.

2.2 Efï¬cient Transformer
In recent years, there are many approaches to improving the efï¬ciency of Transformer architecture (Tay et al., 2020). Some methods use sparse attention mechanisms to reduce the complexity of self-attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Zhang et al., 2021). For example, Longformer (Beltagy et al., 2020) uses sliding window attention to attend local contexts and uses global attention on a few pre-selected input locations to capture global contexts. BigBird (Zaheer et al., 2020) combines local attention, global attention at certain positions and random attention on several randomly selected token pairs. However, these methods may need a relative larger number of attended tokens to reduce the performance degradation on longer sequences, which usually leads to a limited speed-up ratio.
Another way is using hashing technique to accelerate self-attention computation. For example, Reformer (Kitaev et al., 2020) uses a multiround hashing scheme to put similar representations into same buckets when computing selfattention, which can theoretically reduce the complexity to O(N log(N )). However, the computational complexity constant of Reformer is quite large, making it inefï¬cient in processing common sequences with rather limited lengths.
There are also several methods that aim to reduce the computational cost by computing approximated self-attention (Choromanski et al., 2020; Wang et al., 2020b; Tay et al., 2021). For instance, Linformer (Wang et al., 2020b) projects the attention key and value into low-rank matrices to approximate the self-attention mechanism. Linear Transformer (Katharopoulos et al., 2020) uses a kernelbased formulation of self-attention and the associative property of matrix multiplication to approximate the dot-product attention. However, these methods approximate self-attention in a contextagnostic manner, which may not be optimal for text modeling. In addition, they still bring heavy computational cost when the sequence length is very long. Different from the aforementioned methods, Fastformer uses additive attention to model global contexts and uses element-wise product to model the interaction between each input representation and global contexts, which can greatly reduce the computational cost and meanwhile effectively capture contextual information.

Fastformer

Output +

ğ’“ğ’“1 ğ’“ğ’“2 ğ’“ğ’“ğ‘ğ‘âˆ’ğŸğŸ ğ’“ğ’“ğ‘ğ‘
â€¦

ğ’’ğ’’
+ ** **
ğ›¼ğ›¼1 ğ›¼ğ›¼2 ğ›¼ğ›¼ğ‘ğ‘âˆ’ğŸğŸ ğ›¼ğ›¼ğ‘ğ‘
â€¦
ğ’’ğ’’1 ğ’’ğ’’2 ğ’’ğ’’ğ‘ğ‘âˆ’ğŸğŸ ğ’’ğ’’ğ‘ğ‘
Query Transformation

ğ’Œğ’Œ

+

**

**

ğ›½ğ›½1 ğ›½ğ›½2 ğ›½ğ›½ğ‘ğ‘âˆ’ğŸğŸ ğ›½ğ›½ğ‘ğ‘

ğ’‘ğ’‘1 ğ’‘ğ’‘2

â€¦
ğ’‘ğ’‘ğ‘ğ‘âˆ’ğŸğŸ ğ’‘ğ’‘ğ‘ğ‘

* *â€¦* *
â€¦
ğ’Œğ’Œ1 ğ’Œğ’Œ2 ğ’Œğ’Œğ‘ğ‘âˆ’ğŸğŸ ğ’Œğ’Œğ‘ğ‘ Key
Transformation

Transformation

ğ’–ğ’–1 ğ’–ğ’–2

â€¦
ğ’–ğ’–ğ‘ğ‘âˆ’ğŸğŸ ğ’–ğ’–ğ‘ğ‘

* *â€¦* *

â€¦
ğ’—ğ’—1 ğ’—ğ’—2 ğ’—ğ’—ğ‘ğ‘âˆ’ğŸğŸ ğ’—ğ’—ğ‘ğ‘
Value Transformation

â€¦
ğ’†ğ’†1 ğ’†ğ’†2 ğ’†ğ’†ğ‘ğ‘âˆ’ğŸğŸ ğ’†ğ’†ğ‘ğ‘
Input
Figure 1: The architecture of Fastformer.

3 Fastformer
In this section, we introduce our Fastformer approach based on additive attention. The architecture of Fastformer is shown in Fig. 1. It ï¬rst uses additive attention mechanism to summarize the query sequence into a global query vector, next models the interaction between the global query vector and attention keys with element-wise product and summarize keys into a global key vector via additive attention, then models the interactions between global key and attention values via elementwise product and uses a linear transformation to learn global context-aware attention values, and ï¬nally adds them with the attention query to form the ï¬nal output. In this way, the computational complexity can be reduced to linearity, and the contextual information in the input sequence can be effectively captured. Next, we introduce the details of Fastformer in the following section.
3.1 Architecture
The Fastformer model ï¬rst transforms the input embedding matrix into the query, key and value sequences. The input matrix is denoted as E âˆˆ

RNÃ—d, where N is the sequence length and d is the hidden dimension. Its subordinate vectors are denoted as [e1, e2, ..., eN ]. Following the standard Transformer, in each attention head2 we use three independent linear transformation layer to transform the input into the attention query, key and value matrices Q, K, V âˆˆ RNÃ—d, which are written as Q = [q1, q2, ..., qN ], K = [k1, k2, ..., kN ] and V = [v1, v2, ..., vN ], respectively.
Next, modeling the contextual information of the input sequence based on the interactions among attention query, key and value is a critical problem for Transformer-like architectures. In the vanilla Transformer, dot-product attention mechanism is used to fully model the interaction between query and key. Unfortunately, its quadratic complexity makes it inefï¬cient in long sequence modeling. A potential way to reduce the computational complexity is to summarize the attention matrices (e.g., query) before modeling their interactions. Additive attention is a form of attention mechanism that can efï¬ciently summarize important information
2Different attention heads use the same formulation but different parameters.

within a sequence in linear complexity. Thus, we

ï¬rst use additive attention to summarize the query matrix into a global query vector q âˆˆ Rd, which

condenses the global contextual information in the

attention query. More speciï¬cally, the attention

weight Î±i of the i-th query vector is computed as

follows:

âˆš

exp(wqT qi/ d)

Î±i =

N

âˆš, exp(wT q / d)

(3)

j=1

qj

where wq âˆˆ Rd is a learnable parameter vector.

The global attention query vector is computed as

follows:
N

q = Î±iqi.

(4)

i=1

Then, a core problem in Fastformer is how to

model the interaction between the summarized

global query vector and the key matrix. There are

several intuitive options, such as adding or concate-

nating the global query to each vector in the key

matrix. However, they cannot differ the inï¬‚uence

of the global query on different keys, which is not

beneï¬cial for context understanding. Element-wise

product is an effective operation to model the non-

linear relations between two vectors (Wang et al.,

2017). Thus, we use the element-wise product be-

tween the global query vector and each key vector

to model their interactions and combine them into

a global context-aware key matrix. We denote the

i-th vector in this matrix as pi, which is formulated

as pi = q âˆ— ki (the symbol âˆ— means element-wise

product). In a similar way, we use additive attention

mechanism to summarize the global context-aware

key matrix due to efï¬ciency reasons. The additive

attention weight of its i-th vector is computed as

follows:

âˆš

Î²=

exp(wkT pi/

d) âˆš

,

(5)

i

N j=1

exp(wkT

pj

/

d)

where wk âˆˆ Rd is the attention parameter vector. The global key vector k âˆˆ Rd is further computed

as follows:
N

k = Î²ipi.

(6)

i=1

Finally, we model the interaction between attention value matrix and the global key vector for better context modeling. Similar with the query-key interaction modeling, we also perform elementwise product between the global key and each

value vector to compute a key-value interaction vector ui, which is formulated as ui = k âˆ— vi. Motivated by the vanilla Transformer, we apply a linear transformation layer to each key-value interaction vector to learn its hidden representation. The output matrix from this layer is denoted as R = [r1, r2, ..., rN ] âˆˆ RNÃ—d. This matrix is further added together with the query matrix to form the ï¬nal output of Fastformer. Note that the output matrix from each attention head is concatenated along the hidden dimension axis.
We can see that in Fastformer, each key and value vector can interact with the global query or key vector to learn contextualized representations. By stacking multiple Fastformer layers, we can fully model contextual information. Motivated by the weight sharing techniques used in (Wang et al., 2020b), we share the value and query transformation parameters to reduce the memory cost. In addition, we share the parameters across different Fastformer layers to further reduce the parameter size and mitigate the risk of overï¬tting.
3.2 Complexity Analysis
In this section, we analyze the computational complexity of Fastformer. For the additive attention networks to learn global query and key vectors, their time and memory cost are both O(N Â· d), and their total number of additional parameters is 2hd (h is the attention head number). In addition, the time cost and memory cost of element-wise product is also O(N Â· d), the total complexity is O(N Â· d), which is much more efï¬cient than the standard Transformer with O(N 2 Â· d) complexity.3 If the weight sharing technique is used, the total parameter size of Fastformer per layer is 3hd2 + 2hd. Compared with Transformer with at least 4hd2 parameters4, Fastformer also uses fewer parameters. These analysis results demonstrate the theoretical efï¬ciency of Fastformer.
4 Experiments
4.1 Datasets and Experimental Settings
We conduct extensive experiments on ï¬ve benchmark datasets for different tasks. Their details
3Note that the complexity of linear transformations is not taken into account by many prior works, and we also do not consider their effects on computational costs.
4Including the query, key, value and output transformation matrices. The parameters in the bias term, feed-forward network and layer normalization are not counted.

Dataset Amazon IMDB MIND

#Train 40.0k 108.5k 128.8k

#Val 5.0k 13.6k 16.1k

#Test 5.0k 13.6k 16.1k

Avg. len. 133.4 385.7 505.4

#Class 5 10 18

Table 1: Statistics of sentiment and news topic classiï¬cation datasets.

#News #Train impression #Test impression

161,013 2,232,748 2,370,727

#Users #Val impression Avg. click his. len.

1,000,000 376,471
37.1

Table 2: Statistics of MIND dataset for the news recommendation task.

Dataset CNN/DailyMail
PubMed

#Train 287.1k 108.5k

#Val 13.4k 13.6k

#Test 11.5k 13.6k

Avg. doc/summary len. 781/56 3016/203

Table 3: Statistics of the text summarization datasets.

from news titles and then learn user embeddings from the embeddings of historical clicked news. We use Adam (Bengio and LeCun, 2015) for model optimization. More detailed experimental settings are included in Appendix. We run our experiments on an Nvidia Tesla V100 GPU with 32GB memory. We repeat each experiment 5 times and report the average performance as well as the standard deviations. On the classiï¬cation tasks, we use accuracy and macro-F scores as performance metrics. On the news recommendation task, following (Wu et al., 2020) we use AUC, MRR, nDCG@5 and nDCG@10 as the metrics. On the text summarization tasks, we use the ROUGE-1, ROUGE-2 and ROUGE-L metrics (denoted as R-1, R-2 and R-L) to evaluate the generated summaries.

are introduced as follows. The ï¬rst one is Amazon (He and McAuley, 2016) (we use the Electronics domain)5, which is a widely used dataset for review rating prediction. The second one is IMDB (Diao et al., 2014).6 It is a benchmark dataset for movie review rating prediction. The third one is MIND (Wu et al., 2020)7, which is a large-scale English dataset for news recommendation and intelligence. We perform two tasks on this dataset, i.e., the news topic classiï¬cation task based on news body and personalized news recommendation task based on the relevance between candidate news and user interests inferred from historical clicked news. The fourth one is CNN/DailyMail dataset (Hermann et al., 2015) (denoted as CNN/DM), which is a widely used benchmark dataset for text summarization. The ï¬fth one is PubMed (Cohan et al., 2018), which is another benchmark text summarization dataset with much longer documents lengths. The detailed statistical information of the datasets introduced above are shown in Tables 1, 2 and 3.
In our experiments, we use Glove (Pennington et al., 2014) embeddings to initialize token embedding matrix. To obtain the embeddings in the classiï¬cation and news recommendation tasks, we apply an additive attention network to convert the matrix output by Fastformer into an embedding. In addition, in the news recommendation task, following (Wu et al., 2019) we use Fastformer in a hierarchical way to ï¬rst learn news embeddings
5https://jmcauley.ucsd.edu/data/amazon/ 6https://github.com/nihalb/JMARS 7https://msnews.github.io/

4.2 Effectiveness Comparison
First, we compare the performance of Fastformer with many baseline methods, including: (1) Transformer (Vaswani et al., 2017), the vanilla Transformer; (2) Longformer (Beltagy et al., 2020), a Transformer variant with sparse attention. It combines sliding window attention and global attention to model local and global contexts; (3) BigBird (Zaheer et al., 2020), an extension of Longformer, which incorporates sparse random attention mechanism; (4) Linformer (Wang et al., 2020b), a Transformer variant with linear complexity, which use low-dimensional key and value matrices to compute approximated self-attention; (5) Linear Transformer (Katharopoulos et al., 2020), another linear complexity Transformer using kernel functions to approximate self-attention mechanism; (6) Poolingformer (Zhang et al., 2021), a hierarchical architecture that ï¬rst uses sliding window self-attention to capture short-range contexts and then uses pooling self-attention to capture long-range contexts.
The performance of these methods on the three classiï¬cation datasets are compared in Table 4. From the results, we ï¬nd that efï¬cient Transformer variants usually outperform the standard Transformer model. This is because the quadratic computational cost of vanilla Transformer limits the maximum sequence length can be handled, and many useful contexts are lost when truncating the input text sequence. Fastformer can achieve competitive or better performance than other efï¬cient Transformer variants in both long and short text modeling. This is because Fastformer can effectively model global contexts and their relationship

Methods
Transformer Longformer BigBird Linformer Linear Transformers Poolingformer Fastformer

Amazon Accuracy Macro-F 65.32Â±0.35 42.31Â±0.33 65.45Â±0.39 42.48Â±0.44 66.14Â±0.42 42.96Â±0.40 66.20Â±0.49 43.13Â±0.48 66.12Â±0.42 43.04Â±0.44 66.05Â±0.44 43.00Â±0.45 66.13Â±0.29 43.23Â±0.30

IMDB Accuracy Macro-F 52.04Â±0.50 42.69Â±0.47 52.21Â±0.36 43.36Â±0.38 53.23Â±0.46 44.03Â±0.44 53.17Â±0.59 44.34Â±0.57 53.09Â±0.47 44.30Â±0.49 53.78Â±0.51 44.52Â±0.50 54.10Â±0.42 44.65Â±0.44

MIND Accuracy Macro-F 80.90Â±0.20 60.02Â±0.21 81.36Â±0.21 62.59Â±0.23 81.93Â±0.24 63.58Â±0.26 82.16Â±0.28 63.77Â±0.30 82.25Â±0.23 63.81Â±0.22 82.46Â±0.24 64.10Â±0.26 82.34Â±0.19 63.89Â±0.20

Table 4: The results of different methods in the sentiment and topic classiï¬cation tasks. Best average scores are highlighted.

Methods NRMS FIM PLM-NR Transformer Longformer BigBird Linformer Linear Transformers Poolingformer Fastformer Fastformer+PLM-NR Fastformer+PLM-NR*

AUC 68.18 68.31 70.64 68.22 67.98 68.14 68.02 67.76 68.54 69.11 71.04 72.68

MRR 33.29 33.42 35.39 33.32 33.04 33.28 33.19 32.94 33.60 34.25 35.91 37.45

nDCG@5
36.31 36.47 38.71 36.35 36.18 36.30 36.22 36.16 36.69 37.26 39.16 41.51

nDCG@10
42.20 42.35 44.38 42.23 42.06 42.18 42.10 41.97 42.60 43.38 45.03 46.84

Table 5: Performance of different methods in the news recommendation task. *Ensemble of ï¬ve independently trained models, which is the 1st ranked result on the MIND leaderboard.

to different tokens, which can help understand context information accurately.
We also compare the performance of different methods in the news recommendation task. We add three recent news recommendation methods to the comparison, including: (1) NRMS (Wu et al., 2019), which uses multi-head self-attention networks to learn news and user representations; (2) FIM (Wang et al., 2020a), a ï¬ne-grained interest matching method for personalized news recommendation; (3) PLM-NR (Wu et al., 2021a), empowering news recommendation with pre-trained language models. In PLM-NR, we use the best performed UniLM (Bao et al., 2020) empowered model. In addition, we explore to replace the user encoder in PLM-NR with Fastformer. The results are shown in Table 5. We can see that among different Transformer architectures, Fastformer achieves the best performance, and it also outperforms its basic NRMS model. In addition, Fastformer can further improve the performance of PLM-NR, and the ensemble model achieves the best results on

Method
Transformer Longformer BigBird Linformer Linear Transformer Poolingformer Fastformer

CNN/DailyMail R-1 R-2 R-L 38.52 16.04 35.87 37.89 15.46 35.19 38.31 15.78 35.60 37.96 15.58 35.34 37.24 14.87 34.64 38.58 16.16 36.17 38.54 16.22 36.21

R-1 34.26 36.92 37.73 37.22 36.43 37.82 38.09

PubMed R-2 11.88 14.34 14.99 14.48 13.80 15.15 15.44

R-L 31.64 33.75 34.51 34.02 33.21 34.63 34.81

Table 6: Performance of different methods in the text summarization task. Best scores are highlighted.

the MIND leaderboard8. These results show that Fastformer is not only effective in text modeling, but also effective in understanding user interest.
We further conduct experiments on the text summarization tasks to verify the effectiveness of Fastformer in natural language generation. The results are shown in Table 6. We ï¬nd that on the CNN/DM dataset, many efï¬cient Transformer variants (except Poolingformer and Fastformer) are inferior to the vanilla Transformer. This is because sparse attention based method such as Longformer and BigBird cannot fully model the document contexts, and approximated self-attention based methods such as Linformer and Linear Transformer cannot effectively consider context information in the approximation. Since the summaries in the CNN/DM dataset have short lengths, they may be less effective than vanilla Transformer when the same sequence length is used. Fastformer can achieve the best performance in most metrics, which shows the advantage of Fastformer in natural language generation.
4.3 Efï¬ciency Comparison
In this section, we evaluate the efï¬ciency of different methods. We ï¬rst compare the theoretical computational complexity of these methods in Ta-
8https://msnews.github.io/

Inference Time Per Layer (s) Training Time Per Layer (s)

Transformer

Longformer

BigBird

103

Linformer

Linear Transformer Poolingformer Fastformer

Transformer

Longformer

104

BigBird

Linformer

Linear Transformer Poolingformer Fastformer

102

101

128/512

256/256

512/128 1024/64 2048/32 4096/16 8192/8 16384/4
Sequence Length/Batch Size

32768/2

65536/1

(a) Inference.

103
102
128/512 256/256 512/128 1024/64 2048/32 4096/16 8192/8 16384/4 32768/2 65536/1
Sequence Length/Batch Size
(b) Training.

Figure 2: Training and inference speed of different methods. The y-axis (time) is in logarithmic scale.

Method Transformer Longformer BigBird Linformer Linear Transformer Poolingformer Fastformer

Complexity O(N 2 Â· d)
O(N Â· K Â· d)
O(N Â· K Â· d) O(N Â· d/ 2) O(N Â· d2)
O(N Â· d Â· w)
O(N Â· d)

Table 7: Asymptotic computational complexity of different methods. N is the sequence length, K is the average number of tokens to be attended by per token, d is the hidden dimension, is the attention matrix approximation error in Linformer, and w is the window size in Poolingformer.

ble 7.9 The complexity of vanilla Transformer is O(N 2 Â· d), while other compared methods have linear complexity with respect to the input sequence length. However, the complexity of Longformer and BigBird depends on the average number of tokens to be attended by each token, Linformer has a complexity depending on the square of the approximation error, the complexity of Linear Transformer has a quadratic term of hidden dimension, and the complexity of Poolingformer depends on its window size. Different from them, the complexity of Fastformer only depends on the sequence length and the hidden dimension, and it has the least complexity among compared methods. This result shows that Fastformer is efï¬cient in theory.
Then, we conduct experiments to measure the real training and inference cost of different meth-
9We exclude the complexity of the linear Transformation applied after the input and before the output.

ods.10 Motivated by prior works (Katharopoulos et al., 2020; Wang et al., 2020b), we vary the sequence length from 128 to 65535 and scale the batch size inversely with the sequence length. We generate pseudo samples with random tokens and ï¬x the token embeddings to better measure the computational cost of different methods. The results are shown in Fig. 2.11 We ï¬nd that Transformer is inefï¬cient when the sequence length is relatively long (e.g., 512). In addition, we ï¬nd that although Poolingformer has linear complexity in theory, it is inefï¬cient in practice. This is because it uses a large window size (e.g., 256) to compute pooling weight in a convolution-like way, which leads to a very large constant term of the computational cost. Besides, Fastformer is much more efï¬cient than other linear complexity Transformer variants in terms of both training and inference time. These results verify the efï¬ciency of Fastformer.
4.4 Inï¬‚uence of Interaction Function
Next, we study the inï¬‚uence of using different functions to model the interactions among query, key and value in Fastformer. We compare the performance of Fastformer and its variants using the add or concatenation functions to combine the global query/key vector with the vectors in the key/value matrix. The results are shown in Fig. 3. We ï¬nd concatenating is not a good option for Fastformer. This is because simply concatenating two vectors cannot consider the interactions between them. In addition, we ï¬nd adding is not optimal. This is
10We use the model conï¬gurations in the news topic classiï¬cation task.
11The maximum sequence length of Transformer is limited by the GPU memory.

Macro-F

Accuracy

84.0 82.0 80.0

80.57 81.34 82.34

63.89 62.12 60.46

65.0 63.0 61.0

78.0 76.0 74.0

Concatenation Add Element-wise Product

Accuracy

Macro-F

59.0 57.0 55.0

(a) Amazon.

Macro-F Accuracy

83.0 82.0 81.0 80.0 79.0 78.0

82.13 82.10

82.34

81.76

63.52 63.57

63.89

62.32
w/o Sharing Query-Value Sharing Query-Value+Head-wise Sharing Query-Value+Layer-wise Sharing

Accuracy

Macro-F

65.0 64.0 63.0 62.0 61.0 60.0

(a) Amazon.

Macro-F

Accuracy

68.0 66.0 64.0

64.47 65.22 66.13

43.23 41.89 40.87

45.0 43.0 41.0

62.0 60.0 58.0

Concatenation Add Element-wise Product

Accuracy

Macro-F

39.0 37.0 35.0

(b) IMDB.

Macro-F Accuracy

67.0 66.0

66.13 65.87 65.92
65.63

65.0

42.98 42.76

43.23

42.43

44.0 43.0 42.0

64.0 63.0 62.0

w/o Sharing Query-Value Sharing Query-Value+Head-wise Sharing Query-Value+Layer-wise Sharing

Accuracy

Macro-F

41.0 40.0 39.0

(b) IMDB.

Macro-F

Accuracy

55.0 54.0 53.0 52.0 51.0 50.0

54.10

44.65

52.99 52.44

42.99 42.33

Concatenation Add Element-wise Product

Accuracy

Macro-F

45.0 44.0 43.0 42.0 41.0 40.0

(c) MIND.

Macro-F Accuracy

55.0 54.0 53.0

54.10

53.77

53.56

53.42

44.13 44.25

44.65

43.67

45.0 44.0

43.0

52.0 51.0 50.0

w/o Sharing Query-Value Sharing Query-Value+Head-wise Sharing Query-Value+Layer-wise Sharing

Accuracy

Macro-F

42.0 41.0 40.0

(c) MIND.

Figure 3: Inï¬‚uence of different combination functions.
because the add function can only model the linear interactions between two vectors, which may also be insufï¬cient to learn accurate context representations. Different from concatenation and add, element-wise product can model the non-linear interactions between two variables, which may help model the complex contexts in long sequences.
4.5 Inï¬‚uence of Parameter Sharing
Then, we study the inï¬‚uence of different parameter sharing techniques on the performance of Fastformer, including sharing query and value transformation matrices, sharing the parameters across different attention heads, and sharing parameters across different layers.12 The results are shown
12We do not observe signiï¬cant differences in terms of time cost when using different parameter sharing methods, and we only compare the performance here.

Figure 4: Inï¬‚uence of different parameter sharing strategies.
in Fig. 4. We ï¬nd that using query-value parameter sharing can achieve similar or slightly better performance than the Fastformer model without any parameter sharing techniques. Thus, it is favorable to reduce the parameter size by sharing the query and value transformation matrix. In addition, we ï¬nd head-wise parameter sharing will lead to notable performance drops. This is because different attention heads are expected to capture different patterns of contexts, and sharing their parameters is not beneï¬cial for context modeling. Moreover, we ï¬nd incorporating layer-wise sharing method can further improve the model performance. This is because parameter sharing among different layers can mitigate the risk of overï¬tting (Lan et al., 2020). Thus, in Fastformer we incorporate both queryvalue sharing and layer-wise sharing strategies to

improve the model performance and meanwhile reduce the model parameter size.

4.6 Applications of Fastformer
We further apply Fastformer to a downstream Ad CVR prediction task. We conduct experiments on a large-scale Ad CVR prediction data collected from Bing Ads, which contains user behaviors such as search query, webpage browsing and the conversion of clicked Ads. The task is a binary classiï¬cation task by predicting whether a clicked Ad leads to a conversion. The dataset contains 52.2m training samples, 387k validation samples and 611k test samples (logs in the last week are for test, and the rest are for training and validation). Similar to news recommendation, we ï¬rst use a Transformer or Fastformer to learn ad/behavior embedding, and then use another Transformer or Fastformer to learn user embedding from behavior embeddings. The prediction AUC, training memory utilization and local inference latency of Transformer and Fastformer based methods are shown in Table 8. The results show the effectiveness and efï¬ciency of Fastformer in Ad CVR prediction.13

Transformer Fastformer

AUC 0.7299 0.7394(+1.3%)

Memory 30GB 25GB(-16.7%)

Latency 176ms 163ms(-7.2%)

Table 8: Accuracy, memory cost and inference speed comparison.

5 Conclusion and Future Work
In this paper, we propose Fastformer, which is a Transformer variant based on additive attention that can handle long sequences efï¬ciently with linear complexity. In Fastformer, we ï¬rst use additive attention to summarize the query matrix into a global query vector. Next, we combine it with each key vector via element-wise product to learn global context-aware key matrix, and we further summarize it into a global key vector via additive attention. We then model the interactions between global context-aware key and the value to learn global context-aware attention value, which is further combined with the query to form the ï¬nal output. Extensive experiments on ï¬ve benchmark datasets show that Fastformer is much more efï¬cient than many existing Transformer models and
13The latency improvement scale is relatively smaller than in Fig. 2 because the sequences are much shorter and there are multiple additional MLPs in the model.

meanwhile can achieve competitive or even better performance in long text modeling.
In our future work, we plan to pre-train Fastformer-based language models to better empower NLP tasks with long document modeling. In addition, we will explore applying Fastformer to other scenarios such as e-commerce recommendation and Ads CTR prediction to improve user modeling based on long user behavior sequences.
References
Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudomasked language models for uniï¬ed language model pre-training. In ICML, pages 642â€“652. PMLR.
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.
Yoshua Bengio and Yann LeCun. 2015. Adam: A method for stochastic optimization. In ICLR.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, et al. 2020. Masked language modeling for proteins via linearly scalable long-context transformers. arXiv preprint arXiv:2006.03555.
Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In NAACL-HLT, pages 615â€“621.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171â€“4186.
Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars). In KDD, pages 193â€“202.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR.

Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative ï¬ltering. In WWW, pages 507â€“517.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. NIPS, 28:1693â€“1701.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FrancÂ¸ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, pages 5156â€“5165.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efï¬cient transformer. In ICLR.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In ICLR.
Ankur Parikh, Oscar TaÂ¨ckstroÂ¨m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP, pages 2249â€“2255.
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In EMNLP, pages 1532â€“1543.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021. Synthesizer: Rethinking self-attention for transformer models. In ICML, pages 10183â€“10192.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efï¬cient transformers: A survey. arXiv preprint arXiv:2009.06732.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998â€“6008.
Heyuan Wang, Fangzhao Wu, Zheng Liu, and Xing Xie. 2020a. Fine-grained interest matching for neural news recommendation. In ACL, pages 836â€“845.
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. 2020b. Linformer: Selfattention with linear complexity. arXiv preprint arXiv:2006.04768.
Xiang Wang, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2017. Item silk road: Recommending items from information domains to social users. In SIGIR, pages 185â€“194.

Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. 2019. Neural news recommendation with multi-head self-attention. In EMNLP, pages 6390â€“6395.
Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021a. Empowering news recommendation with pre-trained language models. In SIGIR, pages 1652â€“1656. ACM.
Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021b. Hi-transformer: Hierarchical interactive transformer for efï¬cient and effective long document modeling. In ACL.
Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A large-scale dataset for news recommendation. In ACL, pages 3597â€“3606.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago OntanËœoÂ´n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In NeurIPS.
Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Poolingformer: Long document modeling with pooling attention. In ICML, volume 139, pages 12437â€“ 12446.
A Appendix
A.1 Experimental Environment
Our experiments are conducted on a cloud Linux server with Ubuntu 16.04 operating system. The codes are written in Python 3.6 using the Keras library 2.2.4 with Tensorï¬‚ow 1.12 backend. The GPU type is Nvidia Tesla V100 with 32GB GPU memory. Each experiment is run by a single thread.
A.2 Preprocessing
In our experiments, we use the NLTK tool to preprocess the texts. We use the word tokenize and function to convert the input texts into token sequences. The word embeddings of out-ofvocabulary words are ï¬lled with random vectors that have the same mean and co-variation values as other words.
A.3 Hyperparameter Settings
The detailed hyperparameter settings on each dataset used in this paper are listed in Table 9.

Method
# Encoder Layer # Decoder Layer # Global Token # Random Token Window size (Longformer, BigBird) Window size (Poolingformer) Block length (BigBird) Projection dimension (Linformer) # Attention Head Beam Size Maximum Text Length (Transformer) Maximum Text Length (others) Maximum User Behaviors Hidden dimension Loss Batch Size Optimizer
Learning Rate
Max Epochs Dropout

Amazon
2 8 8 8 64 4 16 16 512 512 256 Crossentropy 64 Adam
1e-3
3 0.2

IMDB
2 8 8 8 64 4 16 16 512 1,024 256 Crossentropy 64 Adam
1e-3
3 0.2

MIND (classiï¬cation)
2 16 16 16 256 8 16 16 512 2,048 256 Crossentropy 64 Adam
1e-3
3 0.2

MIND (recommendation)
1 8 8 8 64 4 16 16 512 48 50 256 Crossentropy 64 Adam 3e-6 for PLM-NR 1e-4 for others 3 0.2

CNN/DailyMail
4 4 64 64 64 256 32 64 16 5 512 2,048 256 Crossentropy 32 Adam
1e-4
12 0.2

PubMed
2 4 64 64 64 256 32 64 16 5 512 4,096 256 Crossentropy 32 Adam
1e-4
15 0.2

Table 9: Detailed hyperparameter settings on each dataset.

