A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution

Valts Blukis1,2, Chris Paxton1, Dieter Fox1,3, Animesh Garg1,4, Yoav Artzi2

1NVIDIA

2Cornell University

3University of Washington

4University of Toronto, Vector Institute

arXiv:2107.05612v3 [cs.RO] 28 Nov 2021

Abstract: Natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. However, non-experts are likely to specify such tasks with high-level instructions, which abstract over speciÔ¨Åc robot actions through several layers of abstraction. We propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. We propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. We evaluate our approach on the ALFRED benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions. https://hlsm-alfred. github.io/
Keywords: vision and language, spatial representations
1 Introduction
Mobile manipulation in a home environment requires addressing multiple challenges, including exploration and making long-term inference about actions to perform. In addition to reasoning, robots require an accessible, yet sufÔ¨Åciently expressive interface to specify their tasks. Natural Language provides an intuitive mechanism for task speciÔ¨Åcation, and coupled with advances in automated language understanding, is increasingly applied to embodied agents [e.g., 1‚Äì11].
In this paper, we study the problem of learning to map high-level natural language instructions to low-level mobile manipulation actions in an interactive 3D environment [12]. Existing work largely studies language tightly aligned to the robot actions, either using single-sentence instructions [e.g., 1, 2, 5, 9] or sequences of instructions [13‚Äì18]. In contrast, we focus on high-level instructions, which provide more efÔ¨Åcient human-robot communication, but require long-horizon reasoning across layers of abstraction to generate actions not explicitly speciÔ¨Åed in the instruction.
Robust reasoning about manipulation goals from unrestricted high-level natural language instructions has a variety of open challenges. Consider the instruction secure two discs in a bedroom safe (Figure 1). The robot must Ô¨Årst locate the safe in the bedroom. It then needs to distribute the actions entailed by secure to two objects (two discs), each requiring a distinct sequence of actions, but targeting the same safe. It is also required to map the verb secure to its action space. In parallel, the robot must address mobile manipulation challenges, and often can only identify required actions as it observes and manipulates the world (e.g., if the safe needs to be opened).
We propose to construct and continually update a spatial semantic representation of the world from robot observations (Figure 2). Similar to widely used map representations [19‚Äì22], we retain the spatial properties of the environment, allowing the robot to navigate and reason about relations between objects, as required to accomplish its task. We propose the Hierarchical Language-conditioned Spatial Model (HLSM), a hierarchical approach that uses our spatial representation as a long-term memory to solve long-horizon tasks. HLSM consists of a high-level controller that generates subgoals, and a low-level controller that generates sequences of actions to accomplish them. In our example (Figure 1), the sequence of subgoals is pick up a CD, open the safe, put the CD in the safe, . . . , each requiring a sequence of actions. The spatial representation allows selecting subgoals that use previously observed objects outside of the agent‚Äôs view, or to decide about needed exploration.
We evaluate our approach on the ALFRED [12] benchmark and achieve state-of-the-art results without using the low-level instructions used by previous work [16‚Äì18, 23], neither during training

Time

Egocentric RGB
Observations

Subgoals Actions

ROTATELEFT

...

...

...

...

......

PICKUPOBJECT(CD)

MOVEFORWARD

ROTATERIGHT

PICKUPOBJECT

ROTATELEFT

OPENOBJECT(SAFE) LOOKDOWN

OPENOBJECT

PUTOBJECT(SAFE)
PUTOBJECT

Task: Secure two discs in a bedroom safe
Figure 1: Illustration of the task and our hierarchical formulation. The agent receives a high-level task in natural language. It needs to map RGB images to navigation and manipulation actions to complete the task.
nor at test-time. This paper makes three key contributions: (a) a modular representation learning approach for the problem of mapping high-level natural language task descriptions to actions in a 3D environment; (b) a method for utilizing a spatial semantic representation within a hierarchical model for solving mobile manipulation tasks; and (c) state-of-the-art performance on the ALFRED benchmark, even outperforming all approaches that use detailed sequential instructions.

2 Related Work
Natural language has been extensively studied in robotics research, including with focus on instruction [1, 24], reference resolution [25], question generation [26‚Äì28], and dialogue [4, 29, 30]. Most work in this area has considered either synthetic instructions of relatively simple goals [7, 31‚Äì33], or natural language instructions where all intermediate steps are explained in detail [5, 12‚Äì14, 34‚Äì38]. In contrast, we focus on high-level instructions, which are more likely in home environments [39].
Representation of world state, action history, and language semantics plays a central role in robot systems and their algorithm design. Symbolic representations have been extensively studied for instruction following agents [1‚Äì4, 19, 20, 39‚Äì45]. While they simplify the symbol grounding problem and enable robustness, the ontologies on which they rely on are laborious to scale to new, unstructured environments and language. Representation learning presents an alternative by learning to map observations and language directly to actions [5, 8, 9, 11, 13, 34]. World state and language semantics are represented with vectors [13] or by memorizing past observations [8, 17]. Modelling improvements have enabled these approaches to achieve good performance on complex navigation tasks [7, 9, 11, 13, 14, 37], a success that has not yet translated to mobile manipulation [12, 46, 47].
We propose integrating a semantic voxel map state representation within a hierarchical representation learning system. Similar semantic 2D maps have been successfully used in navigation [7, 8, 48, 49] and more recently even in mobile manipulation instruction-following tasks [23]. We extend these maps to 3D and show state-of-the-art results on a challenging mobile manipulation benchmark. Our map design is related to sparse metric, topological and semantic maps [10, 19‚Äì 21, 50] that have enabled grounding symbolic instruction representations. Our map does not impose a topological structure or require reasoning about object instances, instead modelling a distribution over semantic classes for every voxel.

3 Problem DeÔ¨Ånition

Let A be the set of agent actions, and S the set of world states. Given a natural language
instruction L and an initial state s0 ‚àà S, the agent‚Äôs goal is to generate an execution Œû = s0, a0, s1, a1, . . . , sT , aT , where at ‚àà A is an action taken by the agent at time t, st ‚àà S is the state before taking at, and st+1 = T (st, at) under environment dynamics T : S √ó A ‚Üí S. The state st is deÔ¨Åned by the environment layout and the poses and states of all objects and the agent. The agent does not have access to the state st, but only to an observation ot. An observation ot = (It, Pt, vtS, L) includes a Ô¨Årst-person RGB camera image It, the agent‚Äôs pose Pt, a one-hot encoding of the object class the agent is holding vtS, and the instruction L.The task is considered successful if all goal-conditions corresponding to the task L are true at the Ô¨Ånal state sT . Partial success is measured as the fraction of goal-conditions that have been achieved.

The ALFRED dataset includes sets of seen and unseen environments. The set

of actions A = Anav ‚à™ Aint includes parameter-free navigation actions Anav =

{MOVEAHEAD, ROTATELEFT, ROTATERIGHT} and interaction actions Aint

=

2

Current RGB Observation
ùêºùë°
Observation Model ùêπ

Environment

Safe Desk

Mirror Floor Wall

Laundry Hamper

Pillow Bed

Action ùëéùë°
MOVEAHEAD
ùúãùêø
Subgoal ùëîùëò
OPENOBJECT(SAFE)

Semantic Segmentation

ùêºùë°ùëÜ

Depth ùêºùë°ùê∑

Semantic Voxel Map

ùëâùë°ùëÜ

Agent Position

ùëÉùë°

ùúãùêª

Instruction ùêø Figure 2: Model architecture consisting of an observation model, high-level controller (œÄH ), and low-level controller (œÄL). The observation model updates the semantic voxel map state representation from RGB observations. œÄH predicts the next subgoal given the instruction and the map. œÄL outputs a sequence of actions to achieve the subgoal. The semantic voxel map is visualized in the middle with agent position illustrated as a black pillar, ans the current sugoal argument mask in yellow. Other colors are different segmentation classes. Saturated voxels are observed in the current timestep.
{PICKUP, PUT, TOGGLEON, TOGGLEOFF, OPEN, CLOSE, SLICE} parameterized by a binary mask that identiÔ¨Åes the object of the interaction in the agent‚Äôs current Ô¨Årst-person view. We compute Pt and vtS using dead-reckoning from RGB observations and actions.

4 Hierarchical Model with a Persistent Spatial Semantic Representation

We model the agent behavior with a policy œÄ that maps an instruction L and the observation ot at time t to an action at. The policy œÄ is made of an observation model F and two controllers: a high-level controller œÄH and a low-level controller œÄL. The observation model builds a spatial state representation sÀÜt that captures the cumulative agent knowledge of the world at time t. sÀÜt is used by both œÄH for high-level long-horizon task planning, and œÄL for near-term reasoning, such as object search, navigation, collision avoidance, and manipulation. Figure 2 illustrates the policy.
The high-level controller œÄH computes a probability over subgoals. A subgoal g is a tuple (type, argC , argM ), where type ‚àà Aint is an interaction type (e.g., OPEN, PICKUP), argC is the semantic class of the interaction argument (e.g., SAFE, CD), and argM is a 3D mask identifying the location of the argument instance. In ALFRED, each interaction action in the set Aint corresponds to a subgoal type. When predicting the k-th subgoal at time t, œÄH considers the instruction L, the current state representation sÀÜt, and the sequence of past subgoals gi, i<k. During inference, we sample from œÄH . Unlike arg max, sampling allows the agent to re-try the same or different subgoal incase of a potentially random failure (e.g., if a MUG was not found, pick up a CUP).
The low-level controller œÄL is given the subgoal gk as its goal speciÔ¨Åcation at time t. At every timestep j > t, œÄL maps the state representation sÀÜj and subgoal gk to an action aj, until it outputs one of the stop actions: aPASS or aFAIL to indicate successful or failed subgoal completion.
The execution Ô¨Çow is as follows. At time t = 0 the initial observation o0 is received. At each timestep, we update the state representation sÀÜt using the observation model. If there is no currently active subgoal, we sample a new subgoal gk from œÄH , and then sample an action at from œÄL. If at is aPASS, we increment subgoal counter k. If it is aFAIL, we discard the current subgoal k. We repeat sampling subgoals and actions until an executable action at is sampled. We execute at, increment the timestep t, and receive the next observation ot. The episode ends when the subgoal gSTOP is sampled or the horizon Tmax is exceeded. Algorithm 1 in Appendix A.4 describes this process.

4.1 State Representation
The state representation sÀÜt at time t captures the agent‚Äôs current understanding of the state of the world, including the locations of objects observed and the agent‚Äôs relation to them. The state representation is a tuple (VtS, VtO, vtS, Pt). The semantic map VtS ‚àà [0, 1]X√óY √óZ√óC is a 3D voxel map that for every position indicates which of the c ‚àà [1, C] object classes are present in the voxel. The

3

observability map VtO ‚àà {0, 1}X√óY √óZ is a 3D voxel map that indicates whether the corresponding position has been observed. The inventory vector vtS ‚àà {0, 1}C indicates which of the C object classes the agent is currently holding. The agent pose Pt = (x, y, œâp, œây) is speciÔ¨Åed by the 2D position (x, y), pitch angle œâp, and yaw angle œây.
We also compute 2D state affordance features AFFORD(sÀÜt) ‚àà [0, 1]7√óX√óY in a top-down view that represent each position with one or more of seven affordance classes {pickable, receptacle, togglable, openable, ground, obstacle, observed}. Each [AFFORD(sÀÜt)](œÑ,x,y) = 1.0 if at least one of the voxels at position (x, y) has affordance class œÑ , otherwise it is zero. AFFORD(sÀÜt) is suited for object class agnostic reasoning, for example predicting a pose to pick up an object. 1

4.2 Observation Model

The observation model F (sÀÜt‚àí1, ot, gk) updates the state representation with new observations. It considers the current subgoal gk to actively acquire information relevant to gk. The computation of F consists of three steps: perception, projection, accumulation.

Perception Step We predict semantic segmentation ItS and depth map ItD from the RGB observation It. We use neural networks pre-trained in the ALFRED environment. The semantic segmentation [ItS](u,v) is a distribution over C object classes at pixel (u, v). The depth map [ItD](u,v) is a binned distribution over B bins.2 We also heuristically compute a binary mask MtD that indicates which pixels have conÔ¨Ådent depth readings. We allow more conÔ¨Ådence slack in pixels that correspond to the current subgoal argument argCt according to ItS. Appendix A.3 provides further details. We use perception models based on the U-Net [51] architecture, but our framework supports other,
potentially more powerful models as well (e.g. [52, 53]).

Projection Step We use a pinhole camera model to convert depth ItD and segmentation ItS to a

point cloud that represents each image pixel (u, v) with a 3D position (x, y, z) ‚àà RX√óY √óZ and

a

semantic

distribution

[ItS ](u,v).

We

use

arg

m

axB

(I

D t

)

to

compute

the

3D

positions,

and

discard

points at pixels (u, v) when the binary mask value is [MtD](u,v) = 0. We construct a discrete

semantic voxel map VÀÜtS ‚àà [0, 1]X√óY √óZ√óC , where X, Y , and Z are the width, height, and length. The value at each voxel [VÀÜtS](x,y,z) is the element-wise maximum of the segmentation distributions

[ItS](u,v) across all points (u, v) within the voxel. We additionally compute a binary observability

map VÀÜtO ‚àà {0, 1}X√óY √óZ that indicates the voxels observed at time t. A voxel is observed if it

contains points, or if a ray cast from the camera through the voxel centroid has expected depth

greater than the distance from the camera to the centroid.

Accumulation Step We integrate VÀÜtS and VÀÜtO into a persistent state representation:

VtS = VÀÜtS √ó VÀÜtO + VtS‚àí1 √ó (1 ‚àí VÀÜtO)

VtO = max(VtO‚àí1, VÀÜtO) .

(1)

This operation updates each voxel with the most recent semantic distribution, while retaining the
values of all voxels not visible at time t. The output of the observation model is the spatial state representation sÀÜt = (VtS, VtO, vtS, Pt). The inventory vtS and pose Pt are taken directly from ot.

4.3 High-level Controller (œÄH )

At timestep t, when invoked for the k-th time, the input to œÄH is the instruction L, the sequence of past subgoals gi i<k, and the current state representation sÀÜt. The output is the next subgoal gk = (typek, argCk , argM k ). Figure 3 illustrates the high-level controller architecture.
Input Encoding We encode the text L using a pre-trained BERT [54] model that we Ô¨Åne-tune during training. We use the CLS token embedding as the task embedding œÜL. We encode the state representation sÀÜt to account for classes of all observed objects, and the object that the agent is holding: œÜs(sÀÜt) = [vtS; max(x,y,z)(VtS)], where max(x,y,z) is a max-pooling operation over spatial dimensions and [¬∑; ¬∑] denotes concatenation. We compute the representations of previous subgoals as REPR(gi) ki=‚àí01, where REPR(gi) is the sum of a sinusoidal positional encoding [55] of index i and
1We assume a known mapping between object semantic classes and affordance classes. 2We use B uniformly spaced depth bins {0, ‚àÜD, 2‚àÜD, . . . , (B ‚àí 1)‚àÜD}, where ‚àÜD is a depth resolution. We suggest ‚àÜD should be less than 50% of the voxel size. We used voxels with edge length 0.25m.

4

Semantic Map ùëâùë°ùë† Inventory ùë£ùë°ùë†
Language ùêø
Subgoal History (typeùëñùê∂, argùëñùê∂) ùëñùëò=‚àí01
ùêáùëò‚àí1

Spatial MaxPool ùúôùë°ùë†
State Encoder
ùúôùêø BERT CLS
Language Encoder

ùêáùëò‚àí1 Subgoal history tensor

AFFORD State features AFFORD(ùë†‡∑ùùë°)

Slice

Argument class

[ùëâùë°ùë†](ùëéùëüùëîùê∂) ùëò

mask

‚®Ä

Birds-eye view representation
ùê±ùëò EGOCENTRIC
TRANSFORM
REFINER

Transformer Encoder
Subgoal History Encoder

ùúôùëòùëî‚àí1

Dense MLP

Sample
ùëÉ(typeùë°ùêª| ‚ãÖ) ùëÉ(argùë°ùêª|typeùë°ùêª,‚ãÖ)

Subgoal Predictor

Figure 3: Illustration of the high-level controller œÄH (Section 4.3).

Subgoal ùëîùëò = argùëòùëÄ argùëòùê∂
typeùëò

learned embeddings for typei and argCi . We process this sequence with a two-layer Transformer autoregressive encoder [55] to compute œÜgi ki=‚àí01. We take œÜgk‚àí1 as the subgoal history embedding

vector. We additionally encode the argument mask information argM i from the subgoal history in
an integer-valued subgoal history tensor Hk‚àí1 ‚àà NK√óX√óY where [Hk‚àí1](œÑ,x,y) is the number of

times an interaction action type œÑ was performed at 2D position (x, y) in the birds-eye view:

k‚àí1

[Hk‚àí1](œÑ,x,y) =

max([argM i ](x,y,z)) .

(2)

z

i=0...k‚àí1

argCi =œÑ

Subgoal Prediction We concatenate the three representations h(t,k)

=

[

œÜ

L

;

œÜ

s t

;

œÜ

g k

‚àí

1

].

We use a

densely connected multi-layer perceptron [56] to predict two distributions P (typek | h(t,k)) and

P (argCk | typek, h(t,k)), from which we sample a subgoal type typek and argument class argCk .

The remaining component of the subgoal is the action argument mask argM k . Let [VtS](argC) be a k
voxel map that only retains the object information for objects of class argCk in the semantic map VtS. We reÔ¨Åne it to identify a single object instance. We compute a birds-eye view representation:

xt = [AFFORD(sÀÜt); Hk‚àí1; max([VtS ](argC )) ‚äó 1typek ]

(3)

z

k

where AFFORD(sÀÜt) is a birds-eye view state affordance feature map (Section 4.1) and 1typek is a
one-hot encoding of typek.3 Finally, we compute the 3D argument mask argM k ‚àà [0, 1]X√óY √óZ :

argM k = REFINER(EGOTRANSFORM(xt, Pt), œÜL) ,

(4)

where EGOTRANSFORM(x, Pt) transforms the map x to the agent egocentric pose Pt, REFINER is a neural network based on the LingUNet architecture [36], and œÜL is the language embedding. The reÔ¨Åned argM k is a [0, 1]-valued 3D mask that identiÔ¨Åes the instance of the interaction argument object. If the object is believed to be unobserved, then argM k contains all zeroes. The controller output is the subgoal gk = (typek, argCk , argM k ).
4.4 Low-level Controller (œÄL)

The low-level controller œÄL is conditioned on the most recent subgoal gk = (typek, argCk , argM k ). At time t, it maps the state representation sÀÜt to an action at. It combines engineered and learned components. Appendix A.6 provides the implementation details. The controller œÄL invokes a set of procedures: NavigateTo, SampleExplorationPosition, SampleInteractionPose, and InteractMask. Their invokation follows a pre-speciÔ¨Åed execution Ô¨Çow across multiple timesteps. First, we perform a 360¬∞ rotation to observe the nearby environment. If no objects of type argCk are observed, we explore the environment by sampling a position (x, y) = SampleExplorationPosition(sÀÜt), navigating there using the procedure NavigateTo(x, y, sÀÜt), and performing a 360¬∞ rotation. We repeat exploration until a voxel in VtS contains the class argCk with >50% probability. To interact with an object, we sample an interaction pose (x, y, œây, œâp) = SampleInteractionPose(sÀÜt, gk), invoke NavigateTo(x, y, sÀÜt) to reach the position (x,y), and then rotate according to yaw and pitch angles (œây, œâp). Finally, we generate the egocentric interaction mask maskt = InteractMask(sÀÜt, argM k ), and output the interaction action (typek, maskt).
3‚äó denotes multiplication of a X √ó Y tensor with a K-dimensional vector to obtain a K √ó X √ó Y tensor. [¬∑; ¬∑; ¬∑] denotes channel-wise concatenation.

5

All procedures use the spatial representation sÀÜt. NavigateTo navigates to a goal position using a value iteration network (VIN) [57] that reasons over obstacle and observability maps from sÀÜt. SampleExplorationPosition samples positions on the boundary of observed space in sÀÜt. SampleInteractionPose uses a learned neural network NAVMODEL to predict a distributon of poses from which the interaction gk will likely succeed. InteractMask uses the segmentation image ItS and the 3D argument mask argM t to compute the Ô¨Årst-person mask of the target object.

5 Learning

The policy contains four learned models: the segmentation and depth networks, œÄH , and the navigation model NAVMODEL used by œÄL. We train all four networks independently using supervised learning. We assume access to a training dataset D = {(L(j), Œû(j))}Nj=D1 of high-level natural lan-
guage instructions L(j) paired with demonstration execution Œû(j) in a set of seen environments.
Each execution Œû(j) is a sequence of states and actions s(0j), a(0j), . . . , s(Tj), a(Tj) . We denote NP the total number of states in dataset D, and NG the total number of subgoals.

We process D into three datasets.

The perception dataset DP

=

{([I

]

(i

)

,

[

I

D

]

(i

)

,

[

I

S

]

(i

)

}

NP i=1

in-

cludes RGB images [I](i) with ground truth depth [ID](i) and segmentation [IS](i). The subgoal

dataset Dg = {(L(i), sÀÜ(ti), gj(i) kj=0)}Ni=G1 contains natural language instructions L(i), state repre-

sentations sÀÜ(ti) at the start of k-th subgoal execution, and sequences of the Ô¨Årst k subgoals

gj(i)

k j=0

extracted from Œû(j). The navigation dataset DN = {(sÀÜ(i), g(i), P (i))}Ni=P1 consists of state representations sÀÜ(i), subgoals g(i), and agent poses P (i) at the time of taking the interaction action

corresponding to subgoal g(i). The state representations sÀÜ(¬∑) in datasets Dg and DN are constructed

using the observation model (Section 4.2), but using ground-truth depth and segmentation images.

We train the perception models on DP and the œÄH on Dg to predict the k-th subgoal by optimizing cross-entropy losses. We use DN to train the navigation model NAVMODEL by optimizing a cross-
entropy loss for positions and yaw angles, and an L2 loss for the pitch angle.

6 Experimental Setup
Environment, Data, and Evaluation We evaluate our approach on the ALFRED [12] benchmark. It contains 108 training scenes, 88/4 validation seen/unseen scenes, and 107/8 test seen/unseen scenes. There are 21,023 training tasks, 820/821 validation seen/unseen tasks, and 1533/1529 test seen/unseen tasks. Each task is speciÔ¨Åed with a high-level natural language instruction. The goal of the agent is to map raw RGB observations to actions to complete the task. ALFRED also provides detailed low-level step-by-step instructions, which simplify the reasoning process. We do not use these instructions for training or evaluation. We collect a training dataset of languagedemonstration pairs for learning (Section 5). To extract subgoal sequences, we label each interaction action at = (typet, maskt) and any preceding navigation actions with a single subgoal of type = typet. We compute the subgoal argument class argC and 3D mask argM labels from the Ô¨Årst-person mask maskt, and ground truth segmentation and depth. Completing a task requires satisfying several goal conditions. Following the common evaluation [58, 59], we report two metrics. Success rate (SR) is the fraction of tasks for which all goal conditions were satisÔ¨Åed. Goal condition rate (GC) is the fraction of goal-conditions satisÔ¨Åed across all tasks.
Systems We compare our approach, the Hierarchical Language-conditioned Spatial Model (HLSM) to others on the ALFRED leaderboard that only use the high-level instructions. At the time of writing, the only such published approach is HiTUT [47], an approach that uses a Ô¨Çat BERT [54] architecture to model a hierarchical task structure without using a spatial representation. See Appendix A.2 for a detailed comparison. We also compare to approaches that use the step-by-step instructions, which puts our method at a disadvantage. Of these, LAV [60] also imposes a hierarchical task structure and uses pre-trained depth and segmentation models, but without using a spatial state representation.
Additionally, we perform ablations and study sensory oracles. To study the observation model, we compare to using sensory oracles for ground truth depth, ground truth segmentation, and both. We report high-level controller ablations that remove the subgoal encoder, language encoder, and state

6

Success

‚úî

‚úî

‚úî

‚úî

‚úî

Task: Secure two discs in the bedroom safe

‚úî

‚úî

‚úî

12
PICKUPOBJECT

51
OPENOBJECT

61
PUTOBJECT

72
CLOSEOBJECT

232
PICKUPOBJECT

272
OPENOBJECT

Correctly identified the safe, but failed to open it due to agent blocking the safe door

286
OPENOBJECT

295
PUTOBJECT

308
CLOSEOBJECT

2nd attempt succeeded from a different agent pose

Perception failure

RGB Image Input

Predicted Segmentation

Predicted Depth

Task: Hold the clock and turn on a lamp

Semantic Voxel Map (3D)

Navigation Value Function (2D)

Non-fatal perception error

RGB Image Input

Semantic Voxel Map (3D)

Unfamiliar alarm clock and room

Alarm clock mistaken as ‚Äúbed‚Äù

Depth prediction error

Agent is ‚Äúblocked‚Äù from moving to its goal by the imagined obstacle

186

Mirror Agent
Wall

Agent is facing a mirror, but interprets it as another room behind the wall

Grounding failure

Task: Put a washed bowl away in a kitchen cabinet
‚úî

Subgoal prediction failure

Task: Put an egg in a bowl on the counter

1

161

‚úî

PICKUPOBJECT (BOWL)

Agent wrongly picks up a cup instead of a bowl

162
PUTOBJECT (SINK)

175

176
TOGGLEON (FAUCET)

Continues unaware of the mistake

1
PICKUPOBJECT (CD)

108

109
PUTOBJECT (BOWL)

Looks for a CD instead of Egg. Thinks vase base is a CD.

Figure 4: Qualitative results showcasing successes and failures of our approach. Top row: snapshots of every interaction action taken during a successful task. Action argument masks are overlaid in red over the RGB images. The white numbers are timesteps. Middle-right: illustration of a non-fatal perception error. Middleleft: illustration of a fatal perception error. The agent incorrectly interprets the reÔ¨Çection on the alarm clock as an obstacle, causing the agent (blue star) to believe that the path to the goal (green star) is blocked off. This is reÔ¨Çected in the navigation value function computed by the value iteration network (VIN) [57], where black cells are obstacles with value ‚àí1. White cell is the goal with value 1. Bottom-left: grounding failure. The agent wrongly picks up the cup instead of a bowl. Predicted subgoals are shown in green. Bottom-right: high-level controller and percepton failure. œÄH predicts the wrong subgoal argument class (CD instead of EGG). The segmentation model then mistakes the vase for a CD.

representation encoder as used for predicting subgoal type typek and argument class argCk , while still using the state representation sÀÜt to predict the subgoal argument mask argM k . We also study a low-level controller ablation that removes the exploration procedure.

7 Results

Table 1 shows test and validation results. Our approach achieves state-of-the-art performance across both seen and unseen environments in the setting with only high-level instructions. We achieve 10.04% absolute (98.1% relative) improvement in SR on the test unseen split, and 11.53% absolute (62.6% relative) improvement in SR on the test seen split compared to HiTUT G-only.

Our approach performs competitively even when compared to approaches that also use the low-level step-by-step instructions. We achieve 4.84% absolute (31.4% relative) improvement in SR on the test unseen split compared to ABP [61]. On the test seen split, our approach performs reasonably well, however ABP [61] and LWIT [18] perform better, reÔ¨Çecting potentially stronger scene overÔ¨Åtting.

Tables 2 and 3 show development results. We performed Ô¨Åve runs of the full HLSM model on the validation unseen data and found the sample standard deviation of the success rate is 1.1% (absolute). All other results are from a single-evaluation runs. Ground truth depth alone (+ gt depth) does not signiÔ¨Åcantly affect performance. Ground truth segmentation (+ gt seg) provides 6.6%/16.4% absolute improvement in seen/unseen scenes. Using both (+ gt depth, gt seg) provides 11.1%/21.9% absolute improvement and narrows the seen/unseen gap from 11.3% to 0.5%. This points to perception being the main bottleneck in generalization to unseen scenes.
We report high-level controller œÄH input encoder ablations. The poor performance without the language encoder reÔ¨Çects task difÔ¨Åculty. Zeroing the input to the subgoal history encoder (but keeping position encodings) does not signiÔ¨Åcantly affect performance, showing that knowing the index of the current subgoal in addition to the state representation is often sufÔ¨Åcient. Not using the state representation for predicting subgoal type and argument class gives mixed results in seen

7

Method

Test

Seen

Unseen

SR GC

SR GC

Low-level Sequential Instructions + High-level Goal Instruction

Validation

Seen

Unseen

SR GC

SR GC

SEQ2SEQ [12] MOCA [46] E.T. [17] E.T. + synth. data [17] LWIT [62] HITUT[47] ABP [61]

3.98 22.05 28.77 38.42 30.92 21.27 44.55

9.42 28.29 36.47 45.44 45.44 29.97 51.13

0.39 5.30 5.04 8.57 9.42 13.87 15.43

7.03 14.28 15.01 18.6 20.91 20.31 24.76

3.70 19.15 33.78 46.59 33.70 25.24 42.93

10.00 28.5 42.48 52.82 43.10 34.85 50.45

0.00 3.78 3.17 7.32 9.70 12.44 12.55

6.90 13.4 13.12 20.87 23.10 23.71 25.19

High-level Goal Instruction Only

HITUT G-only[47] LAV [60] HLSM (Ours)

18.41 13.35 29.94

25.27 23.21 41.21

10.23 6.38 20.27

20.27 17.27 30.31

13.63 12.7 29.63

21.11 23.4 38.74

11.12 -
18.28

17.89 -
31.24

Table 1: Test results. Test seen/unseen and validation seen/unseen splits. Top section approaches use sequential step-by-step instructions. The bottom section uses only high-level instructions. Best results using only highlevel instructions and using both types of instructions are highlighted.

Method HLSM

Validation

Seen

Unseen

SR GC

SR GC

29.6 38.8

18.3 31.2

+ gt depth + gt depth, gt seg. + gt seg.

29.6 40.5 40.7 50.4 36.2 47.0

20.1 33.7 40.2 52.2 34.7 47.8

w/o language enc. 0.9 8.6 w/o subg. hist. enc. 29.4 38.5 w/o state repr enc. 30.0 40.6

0.2 7.5 16.6 29.2 18.9 30.8

w/o exploration

32.2 42.4

18.1 31.3

Table 2: Development results on validation split. Per-
formance of our full approach, with perception oracles, a perception ablation, œÄH ablations, and œÄL ablations

Task Type
Overall
Examine Pick & Place Stack & Place Clean & Place Cool & Place Heat & Place Pick 2 & Place

Validation

Seen

Unseen

SR GC

SR GC

29.6 38.7

18.3 31.2

46.8 59.0 57.0 57.0 13.0 27.0 25.0 39.5 17.5 33.8
9.3 29.1 34.7 51.9

36.6 59.9 34.8 34.8
4.4 14.3 11.3 25.8 14.8 39.6 0.0 17.0 18.0 34.7

Table 3: Performance breakdown per task type on the validation split.

and unseen scenes, but without a signiÔ¨Åcant difference in performance. Therefore, predicting the
sequence of subgoal types and argument classes (i.e., what to do) is at times possible without spatial
reasoning, while grounding the subgoal (i.e., where to do it) requires spatial information. Removing random exploration from œÄL does not signiÔ¨Åcantly affect unseen performance.

Figure 4 illustrates the model behavior, showing both successes and common failures. The main failures in valid unseen scenes are due to (1) perception errors that result in missing or extraneous obstacles or picking up wrong objects; (2) insufÔ¨Åciency of random exploration (e.g., not searching inside cabinets); (3) navigation model errors (e.g., blocking objects from opening); (4) subgoal prediction errors (e.g., picking up wrong objects); and (5) lack of state-aware multi-step planning and backtracking. More qualitative results are available in Appendix A.10.

8 Discussion and Limitations
We showed that a persistent spatial semantic representation enables a hierarchical model to achieve state-of-the-art performance on a challenging instruction-following mobile manipulation task. The main performance bottlenecks include long-horizon exploration, perception generalization to unseen environments, and low-level motion planning for continuous collision avoidance. In terms of learning, incorporating reinforcement learning to train œÄH , œÄL, and observation model F jointly could improve robustness. We deÔ¨Åned the interface to œÄL to be faithful to skills available on physical robots, but the exact implementation of œÄL is not the focus of our work. Physical deployment would require changes to œÄL, and study on robustness to errors in continuous environments, such as localization or motion uncertainty.

8

9 Acknowledgements
This research was supported by ARO W911NF-21-1-0106, a Google Focused Award, and NSF under grant No. 1750499. Animesh Garg is supported in part by CIFAR AI Chair and NSERC Discovery Grant. A signiÔ¨Åcant part of the work was done during the Ô¨Årst author‚Äôs internship at Nvidia. We thank the authors of ALFRED for maintaining the benchmark. We thank Mohit Shridhar and Jesse Thomason for their help answering our questions, and the anonymous reviewers for their helpful comments.
References
[1] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. Gopal Banerjee, S. Teller, and N. Roy. ‚ÄùApproaching the Symbol Grounding Problem with Probabilistic Graphical Models. AI Magazine, 2011.
[2] C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. Learning to parse natural language commands to a robot control system. In ISER, 2012.
[3] D. K. Misra, J. Sung, K. Lee, and A. Saxena. Tell me dave: Context-sensitive grounding of natural language to mobile manipulation instructions. In RSS, 2014.
[4] J. Thomason, S. Zhang, R. J. Mooney, and P. Stone. Learning to interpret natural language commands through human-robot dialog. In IJCAI, 2015.
[5] D. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017.
[6] D. Nyga, S. Roy, R. Paul, D. Park, M. Pomarlan, M. Beetz, and N. Roy. Grounding robot plans from natural language instructions with incomplete world knowledge. In CoRL, 2018.
[7] V. Blukis, N. Brukhim, A. Bennet, R. Knepper, and Y. Artzi. Following high-level navigation instructions on a simulated quadcopter with imitation learning. In RSS, 2018.
[8] V. Blukis, D. Misra, R. A. Knepper, and Y. Artzi. Mapping navigation instructions to continuous control actions with position-visitation prediction. In CoRL, 2018.
[9] V. Blukis, Y. Terme, E. Niklasson, R. A. Knepper, and Y. Artzi. Learning to map natural language instructions to physical quadcopter control using simulated Ô¨Çight. In CoRL, 2019.
[10] S. Patki, E. Fahnestock, T. M. Howard, and M. R. Walter. Language-guided semantic mapping and mobile manipulation in partially observable environments. In CoRL, 2019.
[11] V. Blukis, R. A. Knepper, and Y. Artzi. Few-shot object grounding and mapping for natural language robot instruction following. In CoRL, 2020.
[12] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In CVPR, 2020.
[13] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S√ºnderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018.
[14] H. Tan, L. Yu, and M. Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In NAACL-HLT, 2019.
[15] P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee. Simto-real transfer for vision-and-language navigation. In CoRL, 2020.
[16] B. Kim, S. Bhambri, K. P. Singh, R. Mottaghi, and J. Choi. Abp, alfred leaderboard, may 10th 2021. https://leaderboard.allenai.org/alfred/submission/ c2t70j37q4q5ci4so89g. Accessed: June 16th, 2021.
9

[17] A. Pashevich, C. Schmid, and C. Sun. Episodic Transformer for Vision-and-Language Navigation, 2021.
[18] Anonymous. Lwit, alfred leaderboard, may 10th 2021. https://leaderboard.allenai. org/alfred/submission/bvppcin94ro4j7j0jqlg. Accessed: May 25th, 2021.
[19] M. R. Walter, S. Hemachandra, B. Homberg, S. Tellex, and S. Teller. Learning Semantic Maps from Natural Language Descriptions. In RSS, 2013.
[20] S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter. Learning models for following natural language directions in unknown environments. In ICRA, 2015.
[21] S. Patki, A. F. Daniele, M. R. Walter, and T. M. Howard. Inferring compact representations for efÔ¨Åcient natural language understanding of robot instructions. In 2019 International Conference on Robotics and Automation (ICRA), pages 6926‚Äì6933. IEEE, 2019.
[22] I. Kostavelis and A. Gasteratos. Semantic mapping for mobile robotics tasks: A survey. Robotics and Autonomous Systems, 2015.
[23] H. Saha, F. Fotouhi, Q. Liu, and S. Sarkar. A modular vision language navigation and manipulation framework for long horizon compositional tasks in indoor environment. arXiv preprint arXiv:2101.07891, 2021.
[24] T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In HRI, 2010.
[25] C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. A Joint Model of Language and Perception for Grounded Attribute Learning. In ICML, 2012.
[26] S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy. Asking for help using inverse semantics. In RSS, 2014.
[27] R. A. Knepper, S. Tellex, A. Li, N. Roy, and D. Rus. Recovering from Failure by Asking for Help. Autonomous Robots, 2015.
[28] Z. Gong and Y. Zhang. Temporal spatial inverse semantics for robots communicating with humans. In ICRA, 2018.
[29] T. Brick and M. Scheutz. Incremental natural language processing for hri. In HRI, 2007.
[30] S. Tellex, P. Thaker, R. Deits, D. Simeonov, T. Kollar, and N. Roy. Toward information theoretic human-robot dialog. Robotics, 2013.
[31] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017.
[32] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov. Gated-attention architectures for task-oriented language grounding. AAAI, 2018.
[33] C. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Fox. Prospection: Interpretable plans from language by predicting the future. In ICRA, 2019.
[34] A. Suhr and Y. Artzi. Situated mapping of sequential instructions to actions with single-step reward observation. In ACL, 2018.
[35] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navigation. In NeurIPS, 2018.
[36] D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkin, and Y. Artzi. Mapping instructions to actions in 3D environments with visual goal prediction. In EMNLP, 2018.
[37] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong. Self-monitoring navigation agent via auxiliary progress estimation. In ICLR, 2019.
10

[38] H. Chen, A. Suhr, D. Misra, and Y. Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In CVPR, 2019.
[39] D. K. Misra, K. Tao, P. Liang, and A. Saxena. Environment-driven lexicon induction for highlevel instructions. In ACL, 2015.
[40] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.
[41] S. R. K. Branavan, L. S. Zettlemoyer, and R. Barzilay. Reading between the lines: Learning to map high-level instructions to commands. In ACL, 2010.
[42] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011.
[43] F. Duvallet, T. Kollar, and A. Stentz. Imitation learning for natural language direction following through unknown environments. In ICRA, 2013.
[44] Y. Artzi and L. Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. TACL, 2013.
[45] E. C. Williams, N. Gopalan, M. Rhee, and S. Tellex. Learning to parse natural language to grounded reward functions with weak supervision. In ICRA, 2018.
[46] K. P. Singh, S. Bhambri, B. Kim, R. Mottaghi, and J. Choi. Moca: A modular object-centric approach for interactive instruction following. arXiv preprint arXiv:2012.03208, 2020.
[47] Y. Zhang and J. Chai. Hierarchical task learning from language instructions with uniÔ¨Åed transformers and self-monitoring. ACL Findings, 2021.
[48] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2018.
[49] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee. Chasing ghosts: Instruction following as bayesian state tracking. In NeurIPS, 2019.
[50] S. Hemachandra, M. R. Walter, S. Tellex, and S. Teller. Learning spatial-semantic representations from natural language descriptions and scene classiÔ¨Åcations. In ICRA, 2014.
[51] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015.
[52] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison. Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation? In ICCV, 2017.
[53] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Girshick. Detectron2. https://github.com/ facebookresearch/detectron2, 2019.
[54] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.
[55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS, 2017.
[56] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.
[57] A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel. Value iteration networks. In NeurIPS, 2016.
[58] M. Shridhar and D. Hsu. Interactive visual grounding of referring expressions for human-robot interaction. In RSS, 2018.
[59] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred leaderboard. https://leaderboard.allenai.org/alfred/. Accessed: June 16th, 2021.
11

[60] K. Nottingham, L. Liang, D. Shin, C. C. Fowlkes, R. Fox, and S. Singh. Modular framework for visuomotor language grounding. arXiv preprint arXiv:2109.02161, 2021.
[61] B. Kim, S. Bhambri, K. P. Singh, R. Mottaghi, and J. Choi. Agent with the big picture: Perceiving surroundings for interactive instruction following. In Embodied AI Workshop CVPR, 2021.
[62] V.-Q. Nguyen, M. Suganuma, and T. Okatani. Look wide and interpret twice: Improving performance on interactive instruction-following tasks. In IJCAI, 2021.
[63] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis. Learning navigation behaviors end-to-end with autorl. RA-L, 2019.
[64] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox. Contact-graspnet: EfÔ¨Åcient 6-dof grasp generation in cluttered scenes. arXiv preprint arXiv:2103.14127, 2021.
[65] D. Meagher. Geometric modeling using octree encoding. Computer graphics and image processing, 1982.
[66] T. Takikawa, J. Litalien, K. Yin, K. Kreis, C. Loop, D. Nowrouzezahrai, A. Jacobson, M. McGuire, and S. Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In CVPR, 2021.
[67] H. P. Grice. Logic and conversation. In Speech acts. 1975.
[68] J. Roh, C. Paxton, A. Pronobis, A. Farhadi, and D. Fox. Conditional driving from natural language instructions. In CoRL, pages 540‚Äì551, 2020.
[69] C. Matuszek, D. Fox, and K. Koscher. Following directions using statistical machine translation. In HRI, 2010.
[70] N. Gopalan, D. Arumugam, L. L. Wong, and S. Tellex. Sequence-to-sequence language grounding of non-markovian task speciÔ¨Åcations. In RSS, 2018.
[71] D. Bahdanau, F. Hill, J. Leike, E. Hughes, A. Hosseini, P. Kohli, and E. Grefenstette. Learning to understand goal speciÔ¨Åcations by modelling reward. In ICLR, 2018.
[72] P. Goyal, S. Niekum, and R. J. Mooney. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. CoRL, 2020.
[73] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Visionand-language navigation in continuous environments. arXiv preprint arXiv:2004.02857, 2020.
[74] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge. Stay on the path: Instruction Ô¨Ådelity in vision-and-language navigation. In ACL, 2019.
[75] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual visionand-language navigation with dense spatiotemporal grounding. In EMNLP, 2020.
[76] M. Persson, T. Duckett, C. Valgren, and A. Lilienthal. Probabilistic semantic mapping with a virtual sensor for building/nature detection. In Computational Intelligence in Robotics and Automation, 2007.
[77] H. Zender, O. M. Mozos, P. Jensfelt, G.-J. Kruijff, and W. Burgard. Conceptual spatial representations for indoor mobile robots. Robotics and Autonomous Systems, 2008.
[78] A. Pronobis, O. Martinez Mozos, B. Caputo, and P. Jensfelt. Multi-modal semantic place classiÔ¨Åcation. IJRR, 2010.
[79] A. Pronobis. Semantic mapping with mobile robots. PhD thesis, KTH Royal Institute of Technology, 2011.
12

A Appendix
A.1 Frequently Asked Questions
‚Ä¢ Are the ALFRED sequential instructions needed during training? The sequential stepby-step instructions are not needed neither during training, nor at test-time.
‚Ä¢ What has to be done to apply this approach to a real robot? The observation model, high-level controller, state representation, and the interface to the low-level controller together constitute our contribution and are intended to generalize to physical robots. Deployment on a real robot would require an implementation of the low-level controller designed for continuous motion in cluttered environments, and an implementation of the ALFRED interface to enable execution of manipulation actions such as PICKUP and TOGGLEON. Such physical robot capabilities are subject of ongoing research [63, 64].
‚Ä¢ Does this simulated environment result constitute progress towards real-world capabilities? Real-robot operation is the long-term motivation of this work and has been carefully considered in the design of the representation and the approach. However, we do not claim to execute high-level natural language mobile manipulation instructions from raw vision on real robots in unseen environments. To date, such capabilities haven‚Äôt been demonstrated even in simulated environments, such as ALFRED. Even in this scenario, though our method achieves better results than existing work, it can still only solve 18.28% of problems in unseen environments.
‚Ä¢ Would the system scale to physically larger environments? The main bottleneck towards scaling to larger environments is the memory constraint of the semantic memory. While our implementation is likely restricted to interior scenes when using commodity hardware, follow-up work could address this, perhaps using multi-scale representations such as Octress [65, 66].
‚Ä¢ How are the state dynamics modeled? Are they assumed to be known or are they learned? The GOTO procedure in the low-level controller is based on a value-iteration network that utilizes a deterministic grid-navigation dynamics model on the internal representation, which is a crude approximation of the dynamics of the ROTATELEFT, ROTATERIGHT, MOVEAHEAD navigation commands. Other than that, the dynamics of the environment are assumed to be completely unknown to the agent, and are not explicitly learned or modeled.
‚Ä¢ How would localization uncertainty affect the approach? Our representation approach assumes a reliable robot pose estimate. Precisely studying the effects of pose errors would require integration into a system for continuous environments. Intuitively, voxels further away are affected by pose errors more, but may better tolerate it due to being used mainly to decide navigation goals. Voxels close to the agent require more precision as they are used for object instance mask generation, but would be less affected by pose errors. Our voxel map uses a relatively coarse 25cm resolution.
‚Ä¢ Which model was used to obtain test results? The full HLSM model was evaluated on the test set, even though the model without state representation encoding input to the high-level controller performed better in unseen environments on the validation set.
‚Ä¢ Why does the ablation without state representation encodings perform better in unseen environments? In unseen environments, the semantic segmentation is erroneous due to the generalization gap, resulting in state encodings that contain errors. This may affect perfromance of the high-level controller that was trained on data with perfect segmentation, and thus with perfect state encodings.
‚Ä¢ What is the beneÔ¨Åt of sampling the subgoals instead of attempting execution from most to least likely in order? There are two types of subgoal execution failures: systematic and random. An example of a systematic failure is the selection of an incorrect subgoal. For example, TOGGLEON(FLOORLAMP) would fail if a FLOORLAMP does not exist in the environment. An example of a random failure is the low-level controller sampling an interaction pose for which the interaction fails (e.g., Figure 4, row 1, timestep 272). A next-best approach would alleviate a systematic failure, but a sampling approach alleviates both: the systematic failures by trying different subgoals, and random failures by potentially sampling the same subgoal multiple times.
13

A.2 Extended Related Work
Grounding High-level Language to Actions in Robotics In order for natural language humanrobot interfaces to be useful and widely adopted in practice, they should support instructions that are as brief as possible while still being informative of the task, i.e., that adhere to Grice‚Äôs maxim of quantity [67]. Following such high-level instructions requires bridging the gap from high-level language to long sequences of low-level actions. This is commonly achieved using temporal abstraction, where subgoals or options abstract over sequences of low-level actions, reducing the effective time horizon of the problem. Most work on instruction following in robotics utilizes temporal abstraction [1, 3, 10, 20, 21, 33, 36, 39, 42, 44, 68].
Various methods explicitly model correspondences between linguistic constituents in a symbolic instruction representation, environment percepts in the world model, and subgoals (behavior primitives) [1, 3, 20, 24, 39, 69]. This requires the instruction to at least mention each subgoal, and precludes instructions that omit intermediate goals that are expected to be inferred. This limitation can be overcome by directly mapping from language to reward speciÔ¨Åcations [45, 70‚Äì72] or postconditions [39], and then using a planner [39] or learning a task-speciÔ¨Åc policy [71, 72] to solve for the sequence of actions. Both are difÔ¨Åcult in practice. Planning requires a compact, symbolic environment representation with an underlying ontology that is hard to construct for unstructured environments, such as the household environment studied in this work. Policy learning is computationally expensive, and poorly adapts to novel tasks speciÔ¨Åed in natural language in real-time.
Recently, methods that map language and observations directly to actions using neural networks have seen rising popularity and success on simulated [8, 13, 14, 38, 73‚Äì75] and real-robot [9, 11, 15] navigation, as well as simulated manipulation [5] tasks. Simulated mobile manipulation is a promising next frontier [12, 46, 47]. Representation learning approaches avoid planning, by using a direct sequence-to-sequence formulation and a data-driven approach that theoretically permits mapping arbitrarily terse input text to arbitrarily long action sequences that potentially include any necessary intermediate steps not explicitly mentioned in the text. In practice, however, most research has focuses on relatively detailed step-by-step instructions, sometimes using modelling tools such as attention [13, 17] and progress monitoring [37] to leverage the sequential nature of the instructions.
We learn to follow high-level instructions in an interactive mobile manipulation 3D environment. To bridge the gap between language and actions, we use temporal abstraction, where the high-level controller predicts subgoals that abstract over sequences of actions, and the low-level controller generates actions to fulÔ¨Ål each subgoal. The controllers rely on a spatial-semantic state representation to enable reasoning about what subgoals make progress towards the high-level task, and what actions make progress towards the speciÔ¨Åc subgoal, given all past sensory observations. The persistent representation enables operation over long time horizons. Using a shared world representation for both the high- and low-level controllers reduces representation engineering effort and error accumulation typically associated with pipeline approaches.
Semantic Maps for Language Grounding in Robotics The idea of building maps that combine spatial and semantic information [19, 50, 76‚Äì79] and using them for following natural language instructions [7, 9, 10, 20, 21, 49] has a long history in robotics. Common approaches can be classiÔ¨Åed into sparse topological and dense grid-based maps.
Walter et al. [19] introduced a sparse semantic graph that combines pose, semantic, and topological information, extracted from sensory observations and speech descriptions along a route. Hemachandra et al. [50] added a spatial map layer, and fused language with other sensory modalities. Hemachandra et al. [20] used these representations for grounding natural language route instructions. More recently, Patki et al. [21] extended this framework to build compact world models speciÔ¨Åc to the input instruction, and Patki et al. [10] enabled supporting previously unseen environments. This class of sparse topological maps are well suited for probabilistic language grounding from symbolic representations.
Dense grid-based 2D semantic maps are suited for downstream processing using learned neural network modules, and have been used in modular neural network approaches for language grounding [8, 9, 23, 48, 49]. Saha et al. [23] used a grid-based spatial representation and a map Ô¨Åltering method, showing promising early results on a subset of the ALFRED dataset. We extend this line of work to 3D voxel maps, add explicit tracking of occupancy and observability, and maintain the representation through time to facilitate grounding high-level language over long time horizons. Our
14

dense representation has a number of advantages. First, it is easy to build in real-time from RGBD data using segmentation models and geometric operations. Second, it captures structures found in indoor environments, such as L-shaped countertops or kitchen islands with sinks that are hard to represent topologically. Third, it encodes spatial object relationships without requiring an ontology of spatial relations, or even tracking of object instances. The main limitation of our approach is a memory footprint that scales with the physical size of the environment, making it less suited for outdoor or Ô¨Åeld applications. Follow-up work could address this limitation, for example by using multi-scale representations such as octrees [65, 66].
Detailed comparison to HiTUT We provide a detailed technical comparison between our approach and HiTUT [Hierarchical Task Learning from Language Instructions with UniÔ¨Åed Transformers and Self-Monitoring; 47], our main point of comparison.
Both our approach and HiTUT use a hierarchical task decomposition of goals into sequences of subgoals, and subgoals into sequences of actions. The set of subgoals assumed by our approach and HiTUT have differences. HiTUT has an additional subgoal GOTO(LOCATION), while we view any navigation as a means to an end of a manipulation subgoal, and therefore do not have an explicit GOTO subgoal. HiTUT additionally has subgoals for CLEAN and HEAT, (e.g., CLEAN(OBJ) usually abstracts over the sequence PUT(SINK), TOGGLEON(FAUCET), TOGGLEOFF(FAUCET), PICKUP(OBJ)), while our high-level policy would have to predict this entire sequence.
In terms of the model architecture, we use a hierarchical model with high-level and low-level controllers to mimic the task structure. In contrast, HiTUT uses a Ô¨Çat transformer model to jointly solve high-level subgoal planning and low-level action prediction. One of their main contributions is showing how a Ô¨Çat transformer model can be used to model a hierarchical task structure. The beneÔ¨Åt of our hierarchical model decomposition in combination with a shared spatial state representation is its ability to solve low-level navigation and manipulation problems with specialized modules, while avoiding the representational error accumulation and representation engineering issues typically associated with modular pipeline approaches.
In terms of inference, HiTUT and our approach both sample subgoals one at a time, dynamically responding to changes in environment and execution. Both approaches perform backtracking to previous subgoals upon subgoal failure.
In terms of perception, our approach requires a pre-trained segmentation model, while HiTUT requires a pre-trained object detection model to generate object entity information that is fed into the transformer.
A.3 Observation Model Details

ùêºùë°
RGB Input

ùêºùë°ùê∑ /ùêºùë°ùëÜ
Depth / Segmentation Output

Legend: Conv 3x3
Conv 3x3 Stride=2
LeakyReLU
InstanceNorm2D Upscale 2x Softmax
Figure 5: Illustration of the U-Net architecture used in the depth and segmentation networks.

At time t, during the perception step, we predict Ô¨Årst-person semantic segmentation ItS and depth ItD from the observation ot = (It, Pt, vtS, L), from the RGB image It with neural network models pre-trained in the ALFRED environment. Each pixel [ItS](u,v) at coordinates (u, v) is a distribution over C object classes. Likewise, [ItD](u,v) is a distribution over B uniformly spaced depth bins
{0, ‚àÜD, 2‚àÜD, . . . , (B ‚àí1)‚àÜD}, where ‚àÜD is a depth resolution. In early experiments, we observed
that ‚àÜD should be less than 50% of the voxel size. We use ‚àÜD = 0.1m, B = 50, and voxel size of 0.25m. We also heuristically compute a binary mask MtD that indicates which pixels have conÔ¨Ådent depth readings. We allow more conÔ¨Ådence slack in pixels that correspond to the current subgoal

15

argument argCt according to ItS. The mask MtD is used in the projection step to discard points (x, y, z) that correspond to pixels (u, v) for which [M D](u,v) = 0. The mask computation is:

MtD = (W90[ItD] < c1 E[ItD]) ‚à® ((W90[ItD] < c2 E[ItD]) ‚àß ([ItS]argC > 0.5)) ,

(5)

t

where W90[ItD] is the width of the 90% conÔ¨Ådence interval at each pixel, E[ItD] is the expected depth at each pixel, and [ItS]argC is a 0-1 valued segmentation mask of the class of the current subgoal
t
argument. We set the hyperparameters c1 = 0.3 and c2 = 1.0 to allow higher depth uncertainty for points corresponding to the subgoal argument.

If the agent is currently holding an object (i.e. i[[vtS](i)] > 0), we also discard points closer than 0.7m to the camera to make sure that the object in the agent inventory does not get added to the voxel map.

We use custom models based on the U-Net architecture [51] for depth and segmentation networks. The architecture is illustrated in Figure 5. It consists of a cascade of Ô¨Åve downscale blocks followed by Ô¨Åve upscale blocks with skip-connections. Each block includes two convolutions, two leakyReLU activations, and an instance normalization layer. The upscale blocks contain a 2x spatial upscaling operation. We found that training a separate network for depth and segmentation worked better than sharing one network for both tasks.

A.4 Model Execution Flow

Algorithm 1 Execution Flow

Input: Instr. L, Horizon Tmax.

1: g0,1,2..., sÀÜ‚àí1 ‚Üê null

2: k ‚Üê 1

3: Observe initial o0.

4: for t = 0, 1, 2, . . . H do

5: sÀÜt ‚Üê F (sÀÜt‚àí1, ot, gk)

6: do

7: if gk = null then

8:

gk ‚àº œÄH (L, sÀÜt, gi i<k)

9:

if gk = gSTOP then

10:

End episode

11: at ‚àº œÄL(gk, sÀÜt)

12: if at = aPASS then

13: k ‚Üê k + 1

14: if at = aFAIL then 15: gk ‚Üê null

16: while at ‚àà {aFAIL, aPASS} 17: Perform at, observe ot+1

18: End episode

Algorithm 1 describes the execution Ô¨Çow. At time t = 0 the initial observation o0 is received. At each timesep, we update the state representation sÀÜt (Line 5). If needed, we sample a new subgoal gk from œÄH (Line 8), and then sample an action at from œÄL. If at is aPASS, we increment subgoal counter k (Line 13). If it is aFAIL, we discard the current subgoal k (Line 15). We repeat Lines 8‚Äì15 until an executable action at is sampled. We execute at, receive the next observation (Line 17), and proceed to the next timestep. The episode ends when the subgoal gSTOP is sampled (Line 10) or the horizon Tmax is exceeded (Line 18).
A.5 High-Level Controller Details
Subgoals are predicted periodically. Let gk = (typek, argCk , argM k ) be the k-th subgoal predicted at time t. Predicting the subgoal type typek and the argument class argCk is described in the main paper (Section 4.3). This section provides further details of REFINER, the model we use to generate argM k . The mask reÔ¨Åner REFINER has four inputs:4 (a) a spatial feature map xetgo ‚àà [0, 1]N√óW √óL oriented in the agent egocentric reference frame; (b) [VtS](argC) ‚àà [0, 1]W √óL√óH , a
k
4Errata: Equation 4 in the main paper is missing [VtS](argC) and Pt arguments to the REFINER. k

16

Input Context Vector

Legend: Linear
Conditional Conv 1x1

Input Image

Output Image

Conv 3x3

Conv 3x3 Stride=2
LeakyReLU
InstanceNorm2D

Upscale 2x
Figure 6: Illustration of the LingUNet architecture used for as part of REFINER within the high-level controller œÄH , and as part of the navigation model NAVMODEL within the low-level controller. The conditional
convolutions parameters are computed during the network forward pass.

3D mask indicating all voxels that contain objects of class argCk in the voxel map VtS; (c) the agent‚Äôs pose Pt; and (d) a vector representation of the instruction œÜL. It outputs a 3D mask argM k ‚àà [0, 1]W √óL√óH that identiÔ¨Åes the subgoal argument object. Formally, the computation is:5

REFINER(xetgo, [VtS ](argC ), Pt, œÜL) =

(6)

k

ALLOTRANSFORM(LINGUNETm(xetgo, œÜL), Pt) ‚äó [VtS ](argC) , k

where ALLOTRANSFORM transforms a spatial 2D map from an egocentric to the global reference frame, and LINGUNETm is a language-conditioned image-to-image encoder-decoder [36]. The architecture of LINGUNETm is illustrated in Figure 6.

A.6 Low-Level Controller Details

We describe the implementation of each of the low-level controller procedures. This implementation is not the focus of this paper, and could be improved or replaced with other algorithms. Some of the procedures cause actions in the AI2Thor environment, others simply process data to pass between procedures.
The procedures are NavigateTo, SampleExplorationPosition, SampleInteractionPose, and InteractMask. The low-level controller receives the subgoal gk, and follows a pre-speciÔ¨Åed execution Ô¨Çow across multiple timesteps to complete it. The execution Ô¨Çow (Figure 7) consists of an exploration and interaction phase. In the exploration phase, we perform a 360¬∞ rotation by generating a sequence of three ROTATELEFT actions to observe the environment and add information to the semantic map. If the semantic map indicates that no object of type argCk , the action argument, is observed, we explore the environment by sampling a position (x, y) = SampleExplorationPosition(sÀÜt), navigating there using NavigateTo(x, y, sÀÜt), and performing another 360¬∞ rotation. We repeat this process until a voxel in VtS contains the class argCk with >50% probability, at which point we move on to the interaction phase. In the interaction phase, we sample an interaction pose (x, y, œây, œâp) = SampleInteractionPose(sÀÜt, gk), invoke NavigateTo(x, y, sÀÜt) to reach the position (x,y), and rotate according to yaw and pitch angles (œây, œâp). Finally, we generate the egocentric interaction action mask maskt = InteractMask(sÀÜt, argM k ), and execute the interaction action (typek, maskt) in the ALFRED environment. We output aPASS or aFAIL depending if the interaction action has succeeded, and pass control back to the high-level controller to sample the next subgoal.

A.6.1 NavigateTo Procedure
At time t, the NavigateTo procedure maps a 2D navigation goal position (x, y) and the state representation sÀÜt to one of the actions: {ROTATELEFT, ROTATERIGHT, MOVEAHEAD, aSTOP}. We implement it with a Value Iteration Network [VIN; 57] that solves a 2D grid-MDP to predict navigation actions using fast GPU-accelerated convolution and max-pooling operations. The VIN parameters are pre-deÔ¨Åned, and not learned. Other motion planners such as A‚àó could be used as well.
5‚äó is an operation that multiplies a W √ó L matrix by a W √ó L √ó H tensor to obtain a W √ó L √ó H tensor

17

Start

RotateInPlace

Yes

ùíÇùíìùíàùíåùë™ found in ùëΩùíïùë∫

No

Sample

Exploration

Position

Sample Interaction
Pose

Interaction Mask

INTERACTION SUCCESS?

Yes No

END
ùëéùëùùëéùë†ùë†

Exploration Phase

NavigateTo

NavigateTo

Interaction Phase

END
ùëéùëìùëéùëñùëô

Figure 7: Illustration of the low-level controller execution Ô¨Çow, showing the order in which procedures are used to complete a subgoal gk.

Current Heading North East South West

VIN Action
WEST NORTH EAST or SOUTH
NORTH EAST SOUTH or WEST
EAST SOUTH WEST or NORTH
SOUTH WEST NORTH or EAST

AI2Thor Action
ROTATELEFT MOVEAHEAD ROTATERIGHT
ROTATELEFT MOVEAHEAD ROTATERIGHT
ROTATELEFT MOVEAHEAD ROTATERIGHT
ROTATELEFT MOVEAHEAD ROTATERIGHT

Table 4: Mapping from VIN actions to AI2Thor actions.

The VIN is deÔ¨Åned by a state-space Svin, action space Avin, transition function T vin : Svin √ó Avin ‚Üí Svin, a reward function Rvin : Svin √ó Avin ‚Üí Avin, and terminal state set Mvin. At
each timestep t, VIN performs value iteration to compute the Q-function Qvin : Svin √ó Avin ‚Üí R
that estimates the expected sum of future discounted rewards for taking action avt in ‚àà Avin in state svt in ‚àà Svin, and thereafter following a greedy policy: avi in = arg maxavin‚ààAvin Q(svi in, avin), i > t. We implement the state space Svin as a 2D grid of shape W vin √ó Hvin. Each state svin is tagged with three 0-1 valued attributes: OBSTACLE, UNOBSERVED, GOAL. At each timestep t, we set the values of state attributes according to the most recent state representation sÀÜt and current navigation goal (x, y). States svin with occupied voxels in the height range [0, 1.75m] are tagged OBSTACLE(svin) = 1, otherwise OBSTACLE(svin) = 0. States with all voxels unobserved are tagged UNOBSERVED(svin) = 1, otherwise UNOBSERVED(svin) = 0. The state at the goal position is tagged GOAL(svin) = 1, for all others GOAL(svin) = 0. The action space is Avin = {NORTH, EAST, SOUTH, WEST, STOP}. The transition function encodes epsilon-
greedy grid navigation dynamics: (1) the action NORTH moves the agent one state north and likewise for other actions, and (2) with probability = 8% a random transition to a neighboring state occurs. Visiting any terminal state svin ‚àà Mvin or executing the action STOP ter-
minates the episode. Terminal states are all states tagged with attributes OBSTACLE and GOAL, Mvin = {svin ‚àà Svin | (OBSTACLE(svin) > 0.5) ‚àß (GOAL(svin) > 0.5)}.
The reward function assigns different rewards for visiting states with different attributes:

Rvin(svin, avin) = ‚àí0.9 ¬∑ OBSTACLE(svin) + 1.0 ¬∑ GOAL(svin)
‚àí 0.02 ¬∑ UNOBSERVED(svin) + 0.001 ¬∑ 1avin=STOP . (7)
OBSTACLE states receive reward ‚àí0.9, GOAL states receive reward 1.0, and UNOBSERVED states receive reward ‚àí0.02. Taking the STOP action in any state gives reward 0.001, which has the effect of the agent stopping in unsolvable cases. We use the VIN iteratively for N vin iterations, and predict an action avin = arg maxavin‚ààAvin (Qvin(svt in, avin). We map from the VIN action avin to a single valid AI2Thor navigation action using a deterministic mapping (Table 4).

18

A.6.2 SampleExplorationPosition
The SampleExplorationPosition procedure maps a state representation sÀÜt to a discrete 2D position pexplore = (x, y). Let Ps be the set of 2D positions corresponding to voxel centroids in the voxel map along the horizontal axes, and the ground set Pg as the set of all unoccupied positions that have the class FLOOR or RUG in at least one voxel. A position is unoccupied if all voxels in the height range [0, 1.75m] are free of obstacles. We deÔ¨Åne a frontier set Pf as the set of all positions Pg for which at least one immediately neighboring position contains zero observed voxels. If Pf is non-empty, we sample the position pexplore uniformly at random from Pf . Otherwise, we sample pexplore uniformly at random from Pg.

A.6.3 SampleInteractionPose

The SampleInteractionPose procedure maps the state representation sÀÜt and subgoal gk = (typek, argCk , argM k ) to a pose P = (x, y, œây, œâp), where (x, y) is a discrete 2D position, œây is the agent yaw angle, and œâp is the agent camera pitch angle. The pose is predicted such that upon reaching it, the interaction action of type typek is likely to succeed on the object of class argCk at location identiÔ¨Åed by the mask argM k .
We use a neural network model NAVMODEL to predict expected pitch E(œâp|x, y; gk, sÀÜt) and a
distribution P (x, y, œây|gk, sÀÜt), factored as:

P (x, y, œây|gk, sÀÜt) = P (œây|x, y; gk, sÀÜt)P (x, y|gk, sÀÜt)

(8)

The network NAVMODEL is based on the LingUNet architecture (Figure 6):

NAVMODEL(sÀÜt, gk) = LINGUNET(AFFORD(sÀÜt), LINEAR([LUTT (typek); LUTC (argCk )])) , (9)
where AFFORD is an affordance feature map (Section 4.1), LINEAR is a linear layer with bias, LUTT and LUTC are embedding lookup tables, and [¬∑; ¬∑] is a vector concatenation.
To sample a pose P , we Ô¨Årst sample a position (x, y) ‚àº P (x, y|gk, sÀÜt), then sample a yaw angle
œây ‚àº P (œây|x, y; gk, sÀÜt), and Ô¨Ånally lookup a pitch angle œâp = E(œâp|x, y; gk, sÀÜt).

A.6.4 InteractionMask

The

InteractionMask

procedure

maps

a

state

representation

sÀÜt

=

(

V

S t

,

VtO

,

vtS

,

Pt

)

,

the

most

recent RGB observation It, the most recent predicted segmentation ItS, and a subgoal gk =

(typek, argCk , argM k ) to a 0-1 valued mask maskt ‚àà [0, 1]H√óW that identiÔ¨Åes the interaction ob-

ject in the Ô¨Årst-person view observation. The interaction mask maskt is in the format expected by

ALFRED. Formally, it is computed in three steps:

maskAt = [ItS ]argC

(10)

k

maskBt = PINHOLECAM(argM k , Pt)

(11)

maskt = maskAt ¬∑ maskBt ,

(12)

where PINHOLECAM projects the 0-1 valued 3D voxel map argM k to the agent‚Äôs camera plane according to the pose Pt. The mask maskAt is an egocentric 0-1 valued mask that identiÔ¨Åes all objects of class argCk in the image It. The maskBt is an egocentric 0-1 valued mask that identiÔ¨Åes the voxels argM k . For each pixel (u, v), the value [maskBt ](u,v) is the maximum of all values [argM k ](x,y,z) over voxels with coordinates (x, y, z) that the ray cast from the camera through the pixel (u, v) intersects
with. The Ô¨Ånal mask maskt is a 0-1 valued mask that identiÔ¨Åes not only the correct object class, but also the correct instance according to the voxel mask argM k .

A.7 Additional Learning Details

A.7.1 Observation Model Learning

Data As described in Section 5, we use a perception dataset DP for training depth and segmentation models. The dataset DP = {([I](i), [ID](i), [IS](i)}Ni=P1 includes RGB images [I](i) with ground

19

Figure 8: Examples of images produced with our AUGMENT procedure. The top row shows raw RGB images from ALFRED. The bottom row shows images generated by our segmentation-aware data augmentation method. Objects like walls, sinks, Ô¨Çoors, and furniture randomly change color, while the apple and the spoons do not.
truth depth [ID](i) and segmentation [IS](i). The ground truth depth [ID](i) at each pixel (u, v) is a distribution [ID](((i)u,v)) over B depth bins, where 100% of the probability mass is assigned to the bin containing the reference depth value. The ground truth segmentation [IS](i) is likewise at each pixel (u, v) a one-hot vector indicating the object class that pixel belongs to.
Data Augmentation The ALFRED dataset consists of 108 different training scenes, where each scene has a Ô¨Åxed furniture and light Ô¨Åxtures. Observations are highly correlated within each scene, which greatly reduces the effective size of the perception dataset and hurts generalization to unseen scenes. We use a custom segmentation-aware data augmentation strategy that increases the diversity of RGB observations. We compute an augmented RGB image IÀú = AUGMENT(I, IS, Ov) that maps the image I, groundtruth segmentation IS, and a set of semantic classes Ov to a new image IÀú. Ov is the set of object classes that are likely to appear in different colors. Ov includes classes like FLOOR, COUNTERTOP, CABINET, VASE, SOAPBOTTLE, ALARMCLOCK that come in different designs and colors, but not classes like BANANA, APPLE, SPOON that tend to have even appearance. Algorithm 2 shows the implementation of AUGMENT. It emulates more diverse environments by applying a different random color offset to each object class in the RGB image. Figure 8 shows examples of images produced with this augmentation procedure.
During training, we apply AUGMENT with 50% probability to each training example. Additionally, with 50% probability we perform a horizontal Ô¨Çip.
A.8 Additional Experimental Details
We collect a training dataset of language-demonstration pairs as described in Section 5. The demonstrations in ALFRED typically navigate while looking down at the Ô¨Çoor, likely a side-effect of the PDDL planner that had access to the world state during data generation, and as such has no need to explore or observe the visual environment. We modify the demonstration trajectories to get more informative Ô¨Årst-person observations. First, we insert four ROTATELEFT actions at the start of each trajectory. Second, we maintain a nominal camera pitch angle of 30¬∞during navigation, by inserting LOOKDOWN and LOOKUP actions before and after every interaction action. We discard trajectories for which these modiÔ¨Åcations cause failures. These modiÔ¨Åcations result in observations that are more useful for learning and constructing our persistent spatial representation.
A.9 Hyperparameters
Table 5 shows hyperparameter values. The hyperparameters were hand-tuned on the validation unseen split.
A.10 Additional Results
Additional qualitative results are available at: https://hlsm-alfred.github.io/.
20

Algorithm 2 AUGMENT

Input: RGB Image I, ground truth segmentation IS, set of object classes Ov. 1: IÀú ‚Üê I
2: for c ‚àà Ov do 3: ¬ª Extract a binary mask corresponding to object class c 4: Mc ‚Üê [IS ](c) 5: ¬ª Apply modiÔ¨Åcations to the image, masked by the segmentation mask Mc 6: ¬ª multiplies a N -dimensional vector with a H √ó W tensor to compute a N √ó H √ó W tensor
7: ¬ª ¬∑ is an elementwise multiplication
8: if randomBernoulli(0.5) then 9: ¬ª Sample an additive color offset for class c from a normal distribution.
10: ¬ª I3 is the 3 √ó 3 identity matrix. ‚àí‚Üí
11: a ‚àº N ( 0 , œÉaI3) 12: IÀú ‚Üê IÀú + a Mc

13: if randomBernoulli(0.5) then

14: ¬ª Sample additive gaussian noise for each pixel (u, v) for class c

15: for each pixel (u, v) do

16:

gu,v ‚àº N (1, œÉg)

17:

[IÀú](u,v) ‚Üê [IÀú](u,v) + gu,v ¬∑ [Mc](u,v)

18: if randomBernoulli(0.5) then 19: ¬ª Sample an multiplicative color offset for class c from a normal distribution
‚àí‚Üí 20: m ‚àº N ( 0 , œÉmI3) 21: IÀú ‚Üê IÀú ¬∑ (m Mc)
22: clamp image IÀú within 0-1 bounds 23: return IÀú

A successful example of task execution is available at: https://drive.google.com/file/d/ 1APKe3cR_-vliyU2elT5Un30w7PvkEdYs/view?usp=sharing

A failed example of task execution is available at: https://drive.google.com/file/d/1j8BJ_ ALoXGyf8a-IOkmQAg38awSWYt6f/view?usp=sharing

21

Hyperparameter

Value

Observation Model

Number of depth bins B Depth resolution ‚àÜD

50 0.1m

Spatial State Representation

Voxel Size Voxel Map Dimensions in Voxels Voxel Map Dimensions in Meters
Number of semantic classes C

0.25m 61 √ó 61 √ó 10 15.25m √ó 15.25m √ó 2.5m
123

High-level Controller

Subgoal history encoder hidden dimension

128

Subgoal history encoder transformer layers

2

Subgoal predictor dense MLP layers

3

Subgoal predictor dense MLP hidden dimension

128

Low-level Controller
Number of VIN iterations N vin VIN state space size

122 61 √ó 61

Development Environment

Programming Language ML and Math Library
Table 5: Hyperparameter values.

Python PyTorch

22

