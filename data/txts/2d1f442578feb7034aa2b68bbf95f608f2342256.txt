Group Fairness in Bandits with Biased Feedback

Candice Schumann
University of Maryland College Park, USA
candiceschumann@gmail.com

Zhi Lang
University of Maryland College Park, USA
willylang1996@gmail.com

arXiv:1912.03802v3 [cs.LG] 15 Feb 2022

Nicholas Mattei
Tulane University New Orleans, USA nsmattei@gmail.com
ABSTRACT
We propose a novel formulation of group fairness with biased feedback in the contextual multi-armed bandit (CMAB) setting. In the CMAB setting, a sequential decision maker must, at each time step, choose an arm to pull from a finite set of arms after observing some context for each of the potential arm pulls. In our model, arms are partitioned into two or more sensitive groups based on some protected feature(s) (e.g., age, race, or socio-economic status). Initial rewards received from pulling an arm may be distorted due to some unknown societal or measurement bias. We assume that in reality these groups are equal despite the biased feedback received by the agent. To alleviate this, we learn a societal bias term which can be used to both find the source of bias and to potentially fix the problem outside of the algorithm. We provide a novel algorithm that can accommodate this notion of fairness for an arbitrary number of groups, and provide a theoretical bound on the regret for our algorithm. We validate our algorithm using synthetic data and two real-world datasets for intervention settings wherein we want to allocate resources fairly across groups.
KEYWORDS
Group fairness; fair bandits; contextual bandits; human collaboration
ACM Reference Format: Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson. 2022. Group Fairness in Bandits with Biased Feedback. In Proc. of the 21st International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2022), Online, May 9â€“13, 2022, IFAAMAS, 16 pages.
Knowing that one may be subject to bias is one thing; being able to correct it is another.
Jon Elster
1 INTRODUCTION
In many online settings, a computational or human agent must sequentially select an item from a slate, receive feedback on that selection, and then use that feedback to learn how to select the best items in the following rounds. Within computer science, economics,
Proc. of the 21st International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2022), P. Faliszewski, V. Mascardi, C. Pelachaud, M.E. Taylor (eds.), May 9â€“13, 2022, Online. Â© 2022 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.

John P. Dickerson
University of Maryland College Park, USA johnd@umd.edu
and operations research circles, this is typically modeled as a multiarmed bandit (MAB) problem [53]. Examples include algorithms for selecting what advertisements to display to users on a webpage [33], systems for dynamic pricing [34], and content recommendation services [29]. Indeed, such decision-making systems continue to expand in scope, making ever more important decisions in our lives such as setting bail [17], making hiring decisions [10, 43], and policing [41]. Thus, the study of the properties of these algorithms is of paramount importance as highlighted by Chouldechova and Roth [15] motivating priorities for fairness research in machine learning.
In the basic MAB setting, there are ğ‘› arms, each associated with a fixed but unknown reward probability distribution [3, 28]. At each time step ğ‘¡ âˆˆ ğ‘‡ , an agent pulls an arm and receives a reward that is independent of any previous action and follows the selected armâ€™s probability distribution. The goal of the agent is to maximize the total collected reward over time. A generalization of MAB is the contextual multi-armed bandit (CMAB) where the agent observes a ğ‘‘-dimensional context along with the observed rewards to choose a new arm. In the CMAB problem, the agent learns the relationship between contexts and rewards and selects the best arm [2].
Yet, the use of MAB- and CMAB-based systems often results in behavior that is societally repugnant. Sweeney [54] noted that queries for public records on Google resulted in different contextual advertisements based on whether the query target had a traditionally African American or Caucasian name; in the former case advertisements were more likely to contain text relating to criminal incidents. Following that initial report similar instances continue to be observed, both in the bandit setting and in the general machine learning world [37]. In lockstep, the academic community has begun developing approaches to tackling issues of (un)fairness in learning settings. We have an opportunity to identify and understand why the data we have may be causing the bias.
A Computing Community Consortium (CCC) report on fairness in ML identified that most studies of fairness are focused on classification problems [15]. These works define a statistical notion of fairness, typically a notion of equal treatment of equals [39], and propose algorithms to abide by these constraints. Two issues identified by Chouldechova and Roth [15] that we address in this paper are extensions to notions of group fairness and looking at fairness in online dynamic systems, e.g., CMABs. We address these gaps by formalizing and providing an algorithm for fairness with biased feedback when the arms of the bandit can be partitioned into groups. Direct applications of our work including scenarios

AAMAS â€™22, May 9â€“13, 2022, Online
discussed within the AAMAS community like aiding the allocation of human resources in talent sourcing [44].
The recent AI100 study [31], whose goal is to take a broad and long term look at the opportunities and pitfalls for AI researchers, has highlighted the need to develop systems that work with humans, providing oversight, transparency, and explanation. Our bandit formulation is one step towards creating more human-centered AI [47], a new area of study that seeks to understand and balance computer automation and autonomy with the level of human control in a given system. Many of the negative applications of MAB based systems we have discussed so far too often occur because there is too much autonomy given to the system, and it optimizes away from what humans or society considers desirable. By explicitly modeling the underlying bias term, we hope to improve computer aided decision making by understanding and mitigating the dangers that can occur when there are excessive levels of human control or excessive levels of computer autonomy; leading to systems that are more transparent, auditable, and trustworthy.
Running Example. As a running example throughout the paper, imagine the position of an agent at a bank or a lender on a micro-lending site. Here, the agent must sequentially pick loans to fund. In many cases, such as the micro-lending site Kiva, a user is presented with a slate of potential loans they may fund when they log in and this slate is generated by a recommender system [52]. Each of these loans, i.e. arms, has a context which includes attributes of the applicant (e.g., personal statement, repayment history, business plan). The loans can also be partitioned into sets of ğ‘š sensitive attributes, e.g. location, race, or gender. In the simplest case, assume we have two female applicants and two male applicants on the slate at a given time. We also assume that when pulling an arm from, for example, a female applicant, there is some societal bias introduced into the reward. Yet, in many settings (and, as we assume in this work), the average true (i.e., unbiased) reward across groups is equal. We want to balance the number of times the agent selects women versus men given this societal bias built into the feedback.
While we use loans as our running example, our notion of regret could be extended to a number of other areas including recent work in MAB problems on hiring situations [45], including the recent AAMAS Blue Sky Paper by Schumann et al. [44] specifically calling for the community to contribute to fair hiring. One could imagine a situation where hiring decisions are made w.r.t. a short-term reward signal that is biased,1 versus a longer-term reward of performance which is less biased, e.g., via an end-of-year review that is based on a more quantitative metric such as on-the-job performance. A similar argument can be made about school admissions or matching workers to online tasks in a crowdwork setting.
Contributions. We propose a novel formulation of group fairness in the contextual multi-armed bandit (CMAB) setting. In our model, arms are partitioned into two or more sensitive groups based on some protected feature, e.g., race. Despite the fact that there may be differences in expected payout between the groups, we may wish to ensure some form of fairness between picking arms from the
1
Class-based bias presents itself within seconds of an in-person interview; see https://news.yale.edu/2019/10/21/yale-study-shows-class-bias-hiring-based-fewseconds-speech.

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson
various groups. Our goal is to capture the phenomena where we want to balance the arms being pulled from both groups and (learn to) ignore societal bias generated by sensitive group membership. We define two novel notions of reward and regret to capture implicit societal bias: proportional parity and equal group parity. We provide a novel algorithm that can accommodate these notions of fairness for an arbitrary number of groups, learn the societal bias term itself, and provide bounds on the regret for our algorithm. We validate our algorithms using synthetic data and real-world datasets for intervention settings wherein we want to allocate resources fairly across protected groups.2
2 RELATED WORK
Fairness in machine learning has become one of the most active topics in computer science [15]. The idea of using formal notions of fairness, i.e. axioms or properties, to design decision schemes has a long history in economics and political economy [39, 61]. Typically within ML research, fairness is operationalized using the Rawlsian idea that similar individuals should be treated similarly; formally extended to the classification setting by Dwork et al. [19], who provided algorithms to ensure individual fairness at the cost of the utility of the overall system. Their work underscores that in many cases statistical parity is not sufficient to ensure individual fairness, as we may treat groups fairly but in doing so may be very unfair to some specific individual. Determining when, how, and if to define fairness is an ongoing discussion with roots well before the time of computer science [51]; indeed, it is known that many natural conditions for fairness cannot be achieved in tandem [26]. Still, group fairness is found in many fielded systems [6, 58], and we focus on it here.
The study of fairness in MAB was initiated by Joseph et al. [24], who showed for both MAB and CMAB one can implement a fairness definition where within a given pool of applicants, e.g., college admission or mortgages, a worse applicant is not favored over a better one, despite a learning algorithmâ€™s uncertainty over the true payoffs. However, Joseph et al. [24] only focus on individual fairness, and do not formally treat the idea of group fairness. Individual fairness is, in some sense, group fairness taken to an extreme, where every arm is its own singleton group; it offers strong guarantees, but under strong assumptions [9, 25].
Celis et al. [13] propose a bandit-based approach to personalization where arm pulls are constrained to fit some probability distribution defined by a fairness metric such as demographic parity. For example, when recommending news articles, their algorithm provides personalized articles from both left and right sources. Their formulation is perhaps closest in the literature to our formulation as it deals with group fairness, however it does not explicitly assume biased feedback. Instead, it enforces a fair probability distribution without learning about the bias present in the data.
There are a number of other recent studies of fairness in the MAB literature. Chen et al. [14] investigate a task allocation setting with a fairness constraint that captures a minimum rate at which
2
A full version of this paper, complete with an appendix containing proofs and additional experiments, can be found at https://arxiv.org/abs/1912.03802. We will also periodically update this work should typos or other errors be found; if you see any, please feel free to reach out! Code to reproduce the experiments is also available at https://github.com/candiceschumann/groupfairtreatment.

Group Fairness in Bandits with Biased Feedback
a task is assigned to a particular arm; their model is quite general and captures the adversarial and some non-stationary settings. Liu et al. [32] look at fairness between arms under the assumption that arm reward distributions are similar (another interpretation of equal treatment of equals). Patil et al. [38] define fairness such that each arm must be pulled for a predetermined required fraction over the total available rounds. Claure et al. [16] use the MAB framework to distribute resources amongst teammates in human-robot interaction settings; again, fairness is defined as a pre-configured minimum rate that each arm must be pulled. Hossain et al. [22] take a more theory-oriented approach to a similar setting, proposing a multi-agent varient of a stochastic MAB setting with a Nash social welfare definition of fairness.
Since preliminary versions of this work were presented [44, 46] there have been several papers that have investigated similar problems. Wang et al. [57] look at fairness of exposure in CMAB base systems, specifically focusing on similarity of merit, which is more in line with individual rather than the group fairness we consider here. Tang et al. [56] consider a setting inspired by liver transplantation where the objective is to trade off a more egalitarian, max-min, policy in allocating opportunities for surgeons to gain experience in liver transplant training. Finally, Ron et al. [40] investigate a setting of allocating opportunities to sub-populations in a corporate decision making setting where each arm needs to pulled at least a budgeted number of times, but where the cost of allocating an opportunity to a non-optimal arm is known in advance. Interestingly, their algorithmt also achieves a ğ‘‡ 2/3 regret, similar to our results.
One needs to be careful when appealing to purely statistical metrics for ensuring fairness. As argued by Corbett-Davies and Goel [17], simply setting our sights on a form of classification parity, i.e., forcing that some statistical measure be normalized across groups, we may miss bigger picture issues. Specifically, by only focusing on the statistics of the data we have, we miss an opportunity to identify and understand why the data we have may be causing the bias. Later, we will argue that our novel formalization of regret allows us to actually learn particular sources of bias that may exist in our data.

3 PRELIMINARIES

We follow the standard CMAB setting and assume that we are attempting to maximize a measure over a series of time steps ğ‘¡ âˆˆ ğ‘‡ .

We assume that there is a ğ‘‘-dimensional domain for the context

space,

X

=

ğ‘‘
R.

The

agent

is

presented

with

a

set

ğ´

of

arms

from

which to select and we have |ğ´| = ğ‘› total arms. Each of these

arms is associated with a, possibly disjoint, context space Xğ‘– âŠ†

X. Additionally, we assume that we have ğ‘š sensitive groups and

that the arms are partitioned into these sensitive groups such that
ğ‘ƒ1 âˆ© Â· Â· Â· âˆ© ğ‘ƒğ‘š = âˆ…, ğ‘ƒ1 âˆª Â· Â· Â· âˆª ğ‘ƒğ‘š = ğ´, and âˆ€ğ‘– âˆˆğ‘š |ğ‘ƒğ‘– | > 1. For expositionâ€™s sake, we assume a binary sensitive attribute with ğ‘š = 2

for the remaining of the paper. However, we show the generality

of our results to any number of groups in Section 4.
Each arm ğ‘– has a true linear reward function ğ‘“ğ‘– : X â†’ R such that ğ‘“ğ‘– (ğ‘¥) = ğ›½ğ‘– Â·ğ‘¥ where ğ›½ğ‘– is a vector of coefficients that is unknown to the agent. During each round ğ‘¡ âˆˆ ğ‘‡ , a context ğ‘¥ğ‘¡,ğ‘– âˆˆ Xğ‘– is given

for each arm ğ‘–. One arm is pulled per round. When arm ğ‘– is pulled during round ğ‘¡, a reward is returned: ğ‘Ÿğ‘¡,ğ‘– = ğ‘“ğ‘– (ğ‘¥ğ‘¡,ğ‘– ) + ğ‘’ğ‘¡,ğ‘– where

AAMAS â€™22, May 9â€“13, 2022, Online

ğ‘’ğ‘¡,ğ‘– âˆ¼ N (0, 1). The goal of the agent is to minimize the regret over all timesteps in ğ‘‡ . Formally, the regret of the agent at timestep ğ‘¡
is the difference between the arm selected and the best arm that could have been selected. Let ğ‘–âˆ— denote the optimal arm that could
be selected and ğ‘ be the selected arm. Then, the regret at ğ‘‡ is

ğ‘‡

âˆ‘ï¸

ğ‘… (ğ‘‡ ) = ğ‘“ (ğ‘¥ğ‘–âˆ—,ğ‘¡ ) âˆ’ ğ‘“ (ğ‘¥ğ‘,ğ‘¡ ).

(1)

ğ‘¡ =1

In this paper we compare our proposed algorithm against three other algorithms: TopInterval, a variation of LinUCB from Li et al. [29] with additional annotations to track group membership and treatment of arms, NaiveFair which randomly picks a sensitive group and then applies TopInterval to that group,3 and IntervalChaining, an individually fair algorithm from Joseph et al. [23]. All algorithms use ordinary least squares (OLS) estimators of the arm coefficients ğ›½Ë†ğ‘– with a confidence variable ğ‘¤ğ‘–,ğ‘¡ such that the true utility lies within [ğ›½Ë†ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ğ‘¤ğ‘–,ğ‘¡ , ğ›½Ë†ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ +ğ‘¤ğ‘–,ğ‘¡ ] with probability 1 âˆ’ ğ›¿. NaiveFair implements a naive version of demographic parity without explicitly looking at societal bias. TopInterval either explores by pulling an arm uniformly at random or exploits by pulling the arm with the highest upper confidence ğ›½Ë†ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ + ğ‘¤ğ‘–,ğ‘¡ . To ensure individual fairness, IntervalChaining either explores by choosing an arm uniformly at random or exploits by pulling arms that have overlapping confidence intervals with the arm with the highest upper confidence.

3.1 Regret with Societal Bias
As mentioned before, ground truth rewards for sensitive groups can be noisy due to societal or measurement bias. We now formalize this bias in terms of multi-armed bandits. For ease of exposition we assume two groups, but we generalize this in our results. Again, we assume that ğ‘› arms can be partitioned into two sets ğ‘ƒ1 and ğ‘ƒ2 such that ğ‘ƒ1 âˆ© ğ‘ƒ2 = âˆ… and ğ‘ƒ1 âˆª ğ‘ƒ2 = [ğ‘›]. We consider ğ‘ƒ1, with |ğ‘ƒ1| > 1 as the sensitive set, or set with some societal bias. In this situation, each arm ğ‘– has another true utility function ğ‘“ âˆ— (ğ‘¥ğ‘–,ğ‘¡ ) = ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ where ğ›½ğ‘– is a vector of coefficients; if arm ğ‘– is pulled at timestep ğ‘¡ the following reward is returned:

ğ‘Ÿğ‘–,ğ‘¡ = ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ + 1[ğ‘– âˆˆ ğ‘ƒ1]ğœ“ğ‘ƒ Â· ğ‘¥ğ‘–,ğ‘¡ + N (0, 1),

(2)

1

where 1[ğ‘– âˆˆ ğ‘ƒ1] = 1 when ğ‘– âˆˆ ğ‘ƒ1 and 0 otherwise, and ğœ“ğ‘ƒ 1
is a societal or systematic bias against group ğ‘ƒ1. Note that ğœ“ğ‘ƒ2 is a zero vector for the non-sensitive group. Hence, the underlying
biased utility function can be written as ğ‘“ (ğ‘¥ğ‘–,ğ‘¡ ) = ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ + 1[ğ‘– âˆˆ
ğ‘ƒ1]ğœ“ğ‘ƒ Â· ğ‘¥ğ‘–,ğ‘¡ . 1 Using our running example, letâ€™s assume that the down payment
reward received has some bias against the male applicants compared to the female applicants, while the final repayment does not. Note that the final repayment is not measured after accepting a loan and is only measured much later. The loan agency should then take the bias into account while learning what â€˜goodâ€™ applications look like. Or, in a hiring setting, an applicant may have a biased interview (initial reward) while their true performance is measured only after working for a year (later true reward).
3
See Section 5.1 for more information

AAMAS â€™22, May 9â€“13, 2022, Online

We define true regret for pulling an arm ğ‘ at time ğ‘‡ as

ğ‘‡
ğ‘…âˆ— (ğ‘‡ ) = âˆ‘ï¸ ğ‘“ âˆ— (ğ‘¥ğ‘–âˆ—,ğ‘¡ ) âˆ’ ğ‘“ âˆ— (ğ‘¥ğ‘,ğ‘¡ ) (3)
ğ‘¡ =1
where ğ‘–âˆ— is the optimal arm to pull at timestep ğ‘¡ and ğ‘“ âˆ— (ğ‘¥ğ‘–,ğ‘¡ ) is the true reward with no bias terms ğœ“ğ‘ƒ Â· ğ‘¥ğ‘–,ğ‘¡ . We also assume
1
that the average true reward (with no bias) for group ğ‘ƒ1 should be the same as the average reward for group ğ‘ƒ2. Compare this to Equation 1, which would return the regret on the biased reward function ğ‘“ (ğ‘¥ğ‘–,ğ‘¡ ). In the loan agency example, this real regret ğ‘“ âˆ— (ğ‘¥ğ‘–,ğ‘¡ ) would measure the regret of the final repayments instead of the biased down payment regret.
One can view the societal bias term ğœ“ğ‘– that we learn for some group ğ‘– as our algorithm learning how to automatically identify and adjust for anti-discrimination for group ğ‘– compared to all other groups. Anti-discrimination is the practice of identifying a relevant feature in data and adjusting it to provide fairness under that measure [17]. One example of this, discussed by Dwork et al. [19], Joseph et al. [24], and in the official White House algorithmic decision making statement [36], comes up in college admissions. Given other factors, specifically income level, some colleges weight SAT scores less in wealthy populations due to the presence of tutors while increasing the weight of working-class populations [5]. While in these admissions settings the adjustments may be ad-hoc, we learn our bias term from data. Past work has compared the vector ğ›½ learned for each arm as akin to adjusting for these biases [19]. While this is true at an individual level, our explicit modeling of bias allows us to discover these adjustments at a group level.

4 GROUP FAIR CONTEXTUAL BANDITS

In this section, given our new definition of reward (Equation 2) and

corresponding new definition of regret (Equation 3), we present

the algorithm GroupFairTopInterval (Algorithm 1) which takes

societal bias into account. We also give a bound on its regret in this

new reward and regret setting. Subsequently, we briefly describe

the algorithm.

In GroupFairTopInterval, each round ğ‘¡ is randomly chosen
1
with probability ğ‘¡1/3 to be an exploration round. The exploration round randomly chooses an arm to pull.

The remaining rounds become exploitation rounds, where linear

estimates are used to pull arms. GroupFairTopInterval learns

two different types of standard OLS linear estimators [27]. The

first is a coefficient vector ğ›½Ë†ğ‘–,ğ‘¡ for each arm ğ‘– (line 7). Additionally, GroupFairTopInterval learns a group coefficient vector ğœ“Ë† for
ğ‘ƒğ‘— ,ğ‘¡
each group ğ‘ƒğ‘— (lines 4 and 5). To calculate these coefficient vectors,

the algorithm keeps track of previous arm pull rewards for each arm

ğ‘– at every timestep ğ‘¡ in a vector ğ‘Œğ‘–,ğ‘¡ , and the corresponding contexts for each arm pull in a matrix ğ‘‹ğ‘–,ğ‘¡ . A similar vector Yğ‘ƒ ,ğ‘¡ and matrix
ğ‘—
Xğ‘ƒ ,ğ‘¡ is kept for both groups ğ‘ƒğ‘— . As mentioned previously, we treat ğ‘—
ğ‘ƒ1 as the sensitive group of arms. An arm ğ‘– in the non-sensitive group ğ‘ƒ2 has a reward estimation of ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ , while an arm ğ‘– in

the sensitive group ğ‘ƒ1 has a bias corrected reward estimation of

ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ ğœ“Ë†ğ‘ƒ ,ğ‘¡ + ğœ“Ë†ğ‘ƒ ,ğ‘¡ .

1

2

For each arm ğ‘–, the algorithm calculates confidence intervals

ğ‘¤ğ‘–,ğ‘¡ around the linear estimates ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ using a Quantile function

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson

ğ‘„ (line 9). This means that the true utility (including some bias)

falls within [ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ ğ‘¤ğ‘–, ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ + ğ‘¤ğ‘– ] with probability 1 âˆ’ ğ›¿ at

every arm ğ‘– and every timestep ğ‘¡. Similarly, for each group ğ‘ƒğ‘— and

context ğ‘¤ğ‘–,ğ‘¡ for a given arm ğ‘– at timestep ğ‘¡, the algorithm calculates

a confidence interval ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ using a Quantile function ğ‘„ (lines 4 ğ‘—
and 5). This means that the true group utility (or true average group

utility)

falls

within

[ğœ“Ë†ğ‘ƒ

,ğ‘–,ğ‘¡

Â·

ğ‘¥ğ‘–,ğ‘¡

âˆ’ ğ‘ğ‘ƒ

, ğœ“Ë†
,ğ‘–,ğ‘¡ ğ‘ƒ

,ğ‘–,ğ‘¡

Â·

ğ‘¥ğ‘–,ğ‘¡

+ ğ‘ğ‘ƒ

,ğ‘–,ğ‘¡ ]

with

ğ‘—

ğ‘—

ğ‘—

ğ‘—

probability [1 âˆ’ ğ›¿]. Using the confidence intervals ğ‘¤ğ‘–,ğ‘¡ and ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ , ğ‘—

and

the

linear

estimates

ğ›½Ë†ğ‘–,ğ‘¡

Â·

ğ‘¥ğ‘–,ğ‘¡

and ğœ“Ë†
ğ‘ƒ

,ğ‘–,ğ‘¡

Â·

ğ‘¥ğ‘–,ğ‘¡ ,

we

calculate

the

ğ‘—

upper bound of the estimated reward for each arm ğ‘– (lines 15 and

17), pulling the arm with the highest upper bound (line 18).

Algorithm 1 GroupFairTopInterval

Require: ğ›¿, ğ‘ƒ1, ğ‘ƒ2

1: for ğ‘¡ = 1 . . . ğ‘‡ do

2:

With probability

1
1/3 , play ğ‘–ğ‘¡

âˆˆğ‘…

{1, . . . , ğ‘›}

and observe re-

ğ‘¡

ward ğ‘¦ğ‘– ,ğ‘¡ ğ‘¡

3: otherwise:

âˆ’1

4:

ğœ“Ë†
ğ‘ƒ

,ğ‘¡

:=

Xğ‘‡

Xğ‘ƒ ,ğ‘¡

Xğ‘‡ Yğ‘ƒ ,ğ‘¡âˆ’1

1

ğ‘ƒ1,ğ‘¡ 1

ğ‘ƒ1,ğ‘¡ 1

âˆ’1

5:

ğœ“Ë†
ğ‘ƒ

,ğ‘¡

:=

Xğ‘‡

Xğ‘ƒ ,ğ‘¡

Xğ‘‡ Yğ‘ƒ ,ğ‘¡âˆ’1

2

ğ‘ƒ2,ğ‘¡ 2

ğ‘ƒ2,ğ‘¡ 2

6:

for ğ‘– = 1 . . . ğ‘› do

âˆ’1

7:

ğ›½Ë†ğ‘–,ğ‘¡ :=

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

ğ‘‡ğ‘‡
ğ‘‹ğ‘–,ğ‘¡ ğ‘Œğ‘–,ğ‘¡âˆ’1

âˆ’1

8:

ğ¹ğ‘–,ğ‘¡ := N

2
0, ğœ ğ‘¥ğ‘–,ğ‘¡

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘‡
ğ‘¥

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

9:

ğ‘¤ğ‘–,ğ‘¡ := ğ‘„ğ¹ ğ›¿

ğ‘–,ğ‘¡ 2ğ‘›ğ‘¡

10:

if ğ‘– âˆˆ ğ‘ƒ1 then

11:

Fğ‘ƒ ,ğ‘–,ğ‘¡ := N

2
0, ğœ ğ‘¥ğ‘–,ğ‘¡

Xğ‘‡

Xğ‘ƒ ,ğ‘¡

ğ‘‡
ğ‘¥

1

ğ‘ƒ1,ğ‘¡ 1 ğ‘–,ğ‘¡

12:

Fğ‘ƒ ,ğ‘–,ğ‘¡ := N

2
0, ğœ ğ‘¥ğ‘–,ğ‘¡

Xğ‘‡

Xğ‘ƒ ,ğ‘¡

ğ‘‡
ğ‘¥

2

ğ‘ƒ2,ğ‘¡ 2 ğ‘–,ğ‘¡

13:

ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ := ğ‘„ F

ğ›¿
ğ‘›

1 ğ‘ƒ1,ğ‘–,ğ‘¡ 2 |ğ‘ƒ1 | ğ‘‡

14:

ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ := ğ‘„ F

ğ›¿
ğ‘›

2 ğ‘ƒ2,ğ‘–,ğ‘¡ 2 |ğ‘ƒ2 | ğ‘‡

15:

ğ‘¢Ë†ğ‘–,ğ‘¡

:=

ğ›½Ë†ğ‘–,ğ‘¡

Â· ğ‘¥ğ‘–,ğ‘¡

+ ğ‘¤ğ‘–,ğ‘¡

âˆ’

ğœ“Ë†
ğ‘ƒ

,ğ‘¡

Â· ğ‘¥ğ‘–,ğ‘¡

+ ğ‘ğ‘ƒ

,ğ‘–,ğ‘¡

+

ğœ“Ë†
ğ‘ƒ

,ğ‘¡

Â·

1

1

2

ğ‘¥ğ‘–,ğ‘¡ + ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ 2

16:

else

17:

ğ‘¢Ë†ğ‘–,ğ‘¡ := ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ + ğ‘¤ğ‘–,ğ‘¡

18:

Play

argmax
ğ‘–

ğ‘¢Ë†ğ‘–,ğ‘¡

and

observe

reward ğ‘¦ğ‘–,ğ‘¡

Returning to our running example, using GroupFairTopInterval, the loan agency would learn a down payment reward function for each of the arms, i.e., a coefficient vector ğ›½ğ‘– where ğ‘– âˆˆ [young female arm, young male arm, older female arm, older male arm], as well as the group average coefficients for the gender-grouped arms, ğœ“ğ‘ƒ , for male and female. Using the gender-grouped coefficients,
ğ‘—
expected rewards for male arms are reweighted to account for the bias in down payment.
Standard algorithms like TopInterval4 would choose an arm ğ‘– = argmax(ğ›½Ë†Â·ğ‘¥ğ‘–,ğ‘¡ +ğ‘¤ğ‘–,ğ‘¡ ), ignoring societal bias (Equation 2, leading to a larger true regret (Equation 3)). Note that GroupFairTopInterval can be extended to multiple groups by defining an overall average reward.
4
A variant of the contextual bandit LinUCB by Li et al. [29]

Group Fairness in Bandits with Biased Feedback

GroupFairTopInterval is fairâ€”in the context of our group fairness definitionsâ€”and satisfies the following theorem. Appendix B of the full paper provides a detailed, complete proof.

Theorem 1. For two groups ğ‘ƒ1 and ğ‘ƒ2, where ğ‘ƒ1 has a bias offset in rewards, GroupFairTopInterval has regret

âˆšï¸‚

ğ‘…âˆ— (ğ‘‡ ) = ğ‘‚

2ğ‘›ğ‘‡

ğ‘‘ğ‘› ln

ğ›¿

ğ‘‡ 2/3 +

ğ‘‘ğ‘›ğ¿

2
ln

2ğ‘›ğ‘‡

+ ln ğ‘‘

2/3
.

ğ‘™

ğ‘™

ğ›¿

Proof Sketch. We start by proving two lemmas. The first of which states that with probability at least 1 âˆ’ ğ›¿:
ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ (ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ + 1[ğ‘– âˆˆ ğ‘ƒ1 ]ğœ“ğ‘ƒ1 Â· ğ‘¥ğ‘–,ğ‘¡ ) â‰¤ ğ‘¤ğ‘–,ğ‘¡ (4)

holds for any ğ‘– at time ğ‘¡. Similarly, the second states that with probability at last 1 âˆ’ ğ›¿:

ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ â‰¤ ğ‘¤ğ‘–,ğ‘¡

(5)

holds for any group ğ‘ƒğ‘— , any arm ğ‘–, and at any timestep ğ‘¡. By combining these two lemmas, we can see that arms should be treated

fairly.

The regret for GroupFairTopInterval can be broken down into

three terms:

ğ‘…âˆ— (ğ‘‡ ) =

âˆ‘ï¸

regret (ğ‘¡)

(6)

ğ‘¡ : ğ‘¡ is an explore round

âˆ‘ï¸

+

regret (ğ‘¡)

ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ <ğ‘‡1

âˆ‘ï¸

+

regret (ğ‘¡).

(7)

ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ â‰¥ğ‘‡1

First, for any ğ‘¡ we have:

âˆ‘ï¸ 1 = Î˜(ğ‘¡ 2/3) .

(8)

â€² ğ‘¡ 1/3

ğ‘¡ <ğ‘¡

We then show that the number of rounds ğ‘‡1 after which we have sufficient samples such that the estimators are well concentrated is:

3/2

ğ‘‘ğ‘›ğ¿

2

ğ‘‡1 = Î˜ min

2
ln

+ ln ğ‘‘

.

(9)

ğ‘ ğœ†ğ‘šğ‘–ğ‘›

ğ›¿

ğ‘,ğ‘‘

Finally, we bound the third term in Equation 6 as follows:

âˆ‘ï¸

ğ‘Ÿğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘¡ (ğ‘¡ )

(10)

ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ â‰¥ğ‘‡1

2ğ‘›ğ‘‡

ln

â‰¤ ğ‘‚ ğ‘‘ğ‘›

ğ›¿

ğ‘‡

2/3

+

â€²
ğ›¿ğ‘‡

.

(11)

minğ‘– ğœ†min
ğ‘–,ğ‘‘

Combining Equations 6, 8, 9, and 10, we have Theorem 1.

â–¡

Note that we can extend Algorithm 1 to ğ‘š groups. In this setting, we make the strong assumption that true rewards are centered about ğœŒ defined by the user.5 In this adaption of the algorithm, we set the upper bound radius for arm ğ‘– as:

ğ‘¢Ë†ğ‘–,ğ‘¡ = ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ + ğ‘¤ğ‘–,ğ‘¡ + ğœŒ âˆ’ ğœ“Ë†ğ‘ƒ ,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ + ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡

ğ‘—

ğ‘—

where ğ‘– âˆˆ ğ‘ƒğ‘— . We then have the following theorem for multiple groups:
5
See Appendix B.2 for further details.

AAMAS â€™22, May 9â€“13, 2022, Online

Theorem 2. For ğ‘š groups ğ‘ƒ1, . . . , ğ‘ƒğ‘š, GroupFairTopInterval (Multiple Groups) has regret

âˆšï¸‚

ğ‘…âˆ— (ğ‘‡ ) = ğ‘‚

2ğ‘›ğ‘‡

ğ‘‘ğ‘› ln

ğ›¿

ğ‘‡ 2/3 +

ğ‘‘ğ‘›ğ‘šğ¿

2
ln

2ğ‘›ğ‘‡

+ ln ğ‘‘

2/3
.

ğ‘™

ğ‘™

ğ›¿

where ğ‘™ = minğ‘– ğœ†ğ‘šğ‘–ğ‘›

, with ğœ†ğ‘šğ‘–ğ‘›

the

smallest

eigenvalue

of

ğ‘‡
ğ‘‹

ğ‘‹ğ‘–,ğ‘¡

;

ğ‘–,ğ‘‘

ğ‘–,ğ‘‘

ğ‘–,ğ‘¡

and ğ¿ > maxğ‘¡ ğœ†max (ğ‘¥ğ‘‡ ğ‘¥ğ‘–,ğ‘¡ ).
ğ‘–,ğ‘¡

5 EXPERIMENTS
To empirically evaluate GroupFairTopInterval, we perform experiments on synthetic data to demonstrate the effects of various parameters, and on real datasets to demonstrate how GroupFairTopInterval performs in the wild. In each of these sections we compare to TopInterval, due to Li et al. [29], NaiveFair (See Section 5.1), and IntervalChaining, due to Joseph et al. [24].

5.1 NaiveFair
One popular definition of group fairness in classification is the notion of demographic parity. Formally, given a protected demographic group ğ´, we want:

Pr(ğ‘ŒË† = 1|ğ´ = 0) = Pr(ğ‘ŒË† = 1|ğ´ = 1),

(12)

where the probability of assigning a classification label ğ‘ŒË† = 1 does not change based on the sensitive attribute class ğ´. Demographic parity is important when ground truth classes ğ‘Œ are extremely noisy for sensitive groups due to some societal or measurement bias. Assume that we have a classifier that predicts whether an individual should receive a loan where our sensitive attribute ğ´ is binary gender. Demographic parity states that the probability of getting a loan should be the same for males (ğ´ = 0) and females (ğ´ = 1).
In converting this definition of demographic parity to the the multi-armed bandit setting, we alter the definition to be that the probability of pulling an arm ğ‘ does not change based on group membership ğ‘ƒğ‘— :

Pr(pull ğ‘|ğ‘ âˆˆ ğ‘ƒ0) = Pr(pull ğ‘|ğ‘ âˆˆ ğ‘ƒ1).

(13)

Continuing our running example, assume we are a loan agency. The loan agency receives 4 applications at every timestep ğ‘¡: an applicant from a young female, an applicant from a young male, an applicant from a older female, an applicant from an older male; we must choose one application to grant at each timestep. After granting a loan the loan agency receives a down payment on that loan as reward. This reward is then used to update the estimates of whether or not a â€œgoodâ€ loan application was received for the pulled arm. Assume that the loan agency wants to act fairly using the binary sensitive attribute of gender. Then, the probability that the loan agency chooses a female applicant at timestep ğ‘¡ should be the same as the probability of choosing a male applicant.
A naive algorithm to enforce this definition of fairness is defined in Algorithm 2. We first pick from the groups uniformly at random, and then apply a regular CMAB algorithm like TopInterval6 or
6
TopInterval is a variant of LinUCB by Auer et al. [3].

AAMAS â€™22, May 9â€“13, 2022, Online

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson

(a) Increasing the total budget ğ‘‡ , for ğ‘› = 10, ğœ‡ = 10, and number of sensitive arms = 5

(b) Increasing the number of arms ğ‘›, for ğ‘‡ = 1000, ğœ‡ = 10, and number of sensitive
arms = 5

(c) Increasing ğœ‡, for ğ‘› = 10, ğ‘‡ = 1000, and number of sen-
sitive arms = 5

(d) Increasing the fraction of overall sensitive arms, for ğ‘› = 10, ğ‘‡ = 1000, ğœ‡ = 10

Figure 1: Percentage of total arm pulls that were pulled using sensitive arms.

(e) Legend

(a) ğ‘› = 10, ğœ‡ = 10, # of sensi- (b) ğ‘‡ = 1000, ğœ‡ = 10, # of sen- (c) ğ‘› = 10, ğ‘‡ = 1000, # of sen-

tive arms = 5

sitive arms = 5

sitive arms = 5

(d) ğ‘› = 10, ğ‘‡ = 1000, ğœ‡ = 10

(e) Legend

Figure 2: Regret for synthetic experiments. The solid lines are regret given the rewards received from pulling the arms (including the group bias). The dashed lines is the true regret (without the group bias).

Algorithm 2 NaiveGroupFair
Require: ğ›¿, ğ‘ƒ1, ğ‘ƒ2 1: for ğ‘¡ = 1 . . . ğ‘‡ do 2: ğ‘ƒ â† Randomly choose group ğ‘ƒ1 or ğ‘ƒ2. 3: Pull arm in ğ‘ƒ based on TopInterval
ContextualThompsonSampling [2] to choose which arm to pull within the group. Using our running example, NaiveGroupFair would randomly pick between male or female, and then choose the best applicant between the younger and older pair.
5.2 Synthetic Experiments
In each synthetic experiment, we generate true coefficient vectors ğ›½ğ‘– by choosing coefficients uniformly at random for each arm ğ‘–. Contexts at each timestep ğ‘¡ are chosen randomly for each arm ğ‘–. Bias coefficients ğœ“1 are set uniformly at random with mean ğœ‡ = 10. Seeds are set at the beginning of each experiment to keep arms consistent between algorithms.
We run four different types of experiments:7 a) Varying the total budget for pulling arms (ğ‘‡ ) while setting number of arms ğ‘› = 10, error mean ğœ‡ = 10, number of sensitive arms equal to 5, and context dimension ğ‘‘ = 2 (Figures 2a and 1a). b) Varying the total number of arms ğ‘› while setting total budget ğ‘‡ = 1000, error mean ğœ‡ = 10, ratio of sensitive arms to 50%, and context dimension ğ‘‘ = 2 (Figures 2b and 1b). c) Varying the error mean ğœ‡ while setting total budget ğ‘‡ = 1000, number of arms ğ‘› = 10, number of sensitive arms equal to 5, and context dimension ğ‘‘ = 5 (Figures 2c and 1c). d) Varying the number of sensitive arms while setting total budget ğ‘‡ = 1000, number of arms ğ‘› = 10, error mean ğœ‡ = 10, and context dimension ğ‘‘ = 2 (Figures 2d and 1d).
7
Additional experiments can be found in Appendix C.

The plots in Figure 1 show the percentage of times an algorithm pulled a sensitive arm over the full budget ğ‘‡ . In order to be fair, the percentage of sensitive arms pulled should be proportional to the number of sensitive arms, i.e., when there are 5 sensitive arms out of 10 total, the percentage of sensitive arms pulled is roughly 50%. Figure 2 shows the perceived regret that includes bias ğœ“ as solid lines, and real regret that corrects bias (see Equations 2 and 3) as dashed lines. Algorithms with low real regret are considered â€˜goodâ€™.
Figure 1a shows that once exploration is over, GroupFairTopInterval pulls sensitive arms roughly 50% of the time, matching the 50% of sensitive arms. Figure 2a shows that GroupFairTopInterval performs comparably on real regret as TopInterval performs on biased regret. This means GroupFairTopInterval should be used over TopInterval in contexts where bias is anticipated. NaiveFair performs poorly in the context of societal bias.
Figure 1b illustrates that IntervalChaining becomes more group fair as the number of arms increase. This is because many arms are chained together and therefore, arms are chosen uniformly at random. Figure 2b illustrates this random picking of arms as real regret and biased regret increases dramatically for IntervalChaining.
As expected, Figure 1c illustrates that when the error mean ğœ‡ is large, both IntervalChaining and TopInterval choose fewer sensitive arms. This leads to a high real regret as shown in Figure 2c. Following Kleinberg et al. [26], Figure 2c also suggests that one cannot have both individual and group fairness in a scenario with high mean error. The randomness in NaiveFair leads to a very high regret for both perceived regret and real regret.
Figure 1d demonstrates the fairness property of proportionality. The percentage of sensitive arms pulled by GroupFairTopInterval matches the number of sensitive arms. As shown in Figure 2d,

Group Fairness in Bandits with Biased Feedback

AAMAS â€™22, May 9â€“13, 2022, Online

(a) Sensitive arm pulls (%)

(b) Regret

(c) Sensitive arm pulls (%)

(d) Regret

Figure 3: Results of running contextual bandit algorithms on the family income and expenditure dataset (Figures 3a and 3b), as well as the COMPAS dataset (Figures 3c and 3d). Figures 3a and 3c show the percentage of pulls that were of sensitive arms. Figures 3b and 3d show the biased regret for each of the algorithms. Note that the â€œrealâ€ regret like that shown in the synthetic experiments cannot be calculated.

the number of sensitive arms does not affect the real regret of GroupFairTopInterval.
5.3 Experiments on Real-World Data
After exploring GroupFairTopInterval on synthetic data, we move on to using both the Philippines family income and expenditure dataset on Kaggle8 and the ProPublica COMPAS dataset.9 When one looks at the gender and age breakdown in the family income dataset, one can see that quite often female heads of households make more money than males in the Philippines. This is most likely due to the large number of Filipino women who work out of the country; it is estimated that up to 20% of the GDP of the Philippines is actually remittances from these overseasâ€”primarily femaleâ€”workers.10 In fact, almost 60% of overseas workers are women and 75% of these women are between the ages of 25 and 44.11 In the COMPAS dataset ProPublica observed a societal bias over recidivism risk scores for African-Americans.12
Experimental Setup. Given the skew of high income coming from female head of households in the family income dataset, we treat the binary â€˜Household Head Sexâ€™ feature as the sensitive attribute. To create arms, we split up households based on â€˜Household Head Ageâ€™ bucketed into the following five groups: (8, 27], (27, 45], (45, 63], (63, 81], and (81, 99]. We then have 10 different arms (for example, two arms would be Female head of household between 8 and 27, and Male head of household between 8 and 27).
Similarly, we treat African-American individuals from the COMPAS dataset as the sensitive attribute. We create arms by splitting up households based on the three age categories found in the data. We therefore have six different arms.
At each timestep ğ‘¡, we randomly select an individual from each arm. The context vector is the remaining features where any nominal features are transformed into integers. After an arm is pulled, a reward of the household income (for the family income dataset) or violent decile score (for COMPAS) is returned. We use these datasets for illustrative purposes.
8 https://www.kaggle.com/grosvenpaul/family-income-and-expenditure
9 https://www.kaggle.com/danofer/compass
10 https://www.nationalgeographic.com/magazine/2018/12/filipino-workers-return-from-overseas-philippines-
celebrates/ 11
https://psa.gov.ph/content/2017-survey-overseas-filipinos-results-2017-survey-overseas-filipinos 12
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing

Results. We see the same behavior of arm pulls in the real world data. Figures 3a and 3c show that after a period of exploration, the percentage of sensitive arms (male-grouped arms) pulled gets very close to 50%, matching the proportion of sensitive-grouped arms.
Figures 3b and 3d are perhaps more interesting. Since we cannot measure the â€œrealâ€ regret without the bias we assumed from the sensitive-grouped arms, we consider the gap between GroupFairTopInterval and TopInterval as the price of fairness. The gap in regret is small compared to the increase in percentage of sensitive arms pulled. However, the gap in regret for NaiveFair is large in comparison. This suggests that explicitly learning a societal bias term will help in biased settings with low price to perceived regret. Note that there is a difference between regret scales for the two different datasets. This is due to the family income and expenditure dataset reporting regret in income, while the COMPAS dataset reports regret in recidivism score.
6 GENERAL DISCUSSION & ETHICAL IMPLICATIONS OF THE WORK
This work was directly motivated by research into bias found in machine learning models. There have also been calls to action for more research to be done on bias mitigation in online learning settings [11, 15], specifically in multi-armed bandit settings [44] and related areas such as recommender systems [12, 49, 52]. Directly addressing these calls, in this work we propose a method of alleviating societal or measurement bias introduced into reward feedback. Using our CMAB model should help mitigate biased behaviors found in bandit systems currently in use [55].
Additionally, as noted by Oâ€™Neil [37], models can provide a biased feedback loop. We hope that by incorporating a societal bias term we can learn something about the bias that is being introduced. The coefficient vector will show which features are incorporating bias into the model. This allows users to address these features outside of the model and potentially find the sources of the societal bias. We do note that addressing societal bias and fixing the solution is a nontrivial task, the societal bias term provides the initial step of measurement.
On the other hand, as noted by Schumann et al. [44], Shneiderman [48], and many others, humans should still be active participants in decision making. If models such as our CMAB model are used to replace more and more human decision makers, this could

AAMAS â€™22, May 9â€“13, 2022, Online
have unintended and potentially negative medium- and long-term side effects. All models should be monitored for biased feedback loops in the given contexts that they are being used [37].
Choosing a particular definition of fairnessâ€”conditioned on deciding that it is even appropriate to formally define a notion of fairness in the first placeâ€”is a morally-laden decision. We note that, as machine learning practitioners, in many societally-relevant applications it is paramount that we maintain an open dialogue with stakeholders. In this work, we analyze a sequential decisionmaking system under one particular definition, group fairness; it is certainly not the case that this is a one-size-fits-all solution that would be deployable without receiving input from that larger set of stakeholders. Indeed, recent research [42] shows that non-expert users may have vastly varying degrees of comprehension of different definitions of fairness, and that the degree of comprehension may be a function in part of education level and other features that may correlate with measures of marginalization; this hints that the consequences of incorporating fairness definitions into machine-learning-based systems may not be uniformly understood by participants, and indeed that those participants who may be impacted the most by that change could comprehend that potential impact the least. In an allocative system like the one we describe in the main paper, nuanced considerations must be considered.
Our new definitions of reward (Equation 2) and regret (Equation 3) for the MAB setting provide an opportunity to look at biased data in a new light. In many cases, ground truths provided during learning are noisy with respect to sensitive groups. Additionally, debiased ground truths may be very expensive to receive or may take a long time to acquire. For instance, if looking at loans, true rewards of repayment may take years to receive. Or, for example, in hiringâ€”the true reward of hiring an individual may take over a year to estimate, while the initial estimate may be influenced by a hiring teamâ€™s unconscious bias over features such as ethnicity, gender, or orientation.
Our proposed algorithm, GroupFairTopInterval, learns societal bias in the data while still being able to differentiate between individual arms. Previous solutions relied on setting ad-hoc thresholds, requiring some form of quota, or choosing groups uniformly at random. While it is true that GroupFairTopInterval can easily be extended to a case where we know that the average of a group is a constant offset from the other group. That being said, defining such offsets raises a host of other ethical questions. For instance, in the US, the EEOC (Equal Employment Opportunity Commision) poposed that the ratio of the most favored group compared to the least favored group must not be less than 0.8. Meanwhile, this type of comparison is forbidden in some countries [30]. In any case, these prior solutions either lead to high regret, or require a large amount of domain knowledge for the chosen application.
7 CONCLUSION & FUTURE RESEARCH
This paper explores group fairness in the contextual multi-armed bandit (MAB) setting. Our main contributions are: (1) we provide a new definition of reward and regret which captures societal bias; (2) we provide an algorithm that learns and corrects for that definition of societal bias; and (3) we empirically explore the effects different CMAB algorithms have in the setting of societal bias.

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson
Future work could expand GroupFairTopInterval to enforce
individual fairness within groups. Intersectional group fairness is
also important to look at in the MAB setting where more than one
type of sensitive attribute needs to be protected. Additionally, other
group fairness definitions such as Equalized Opportunity should be
converted to the MAB setting [21]. Another interesting direction
for future work is to mix ideas from the study of budget constrained
bandits [18, 59] with our fairness definitions. We have also assumed
individual arms have fixed group membership; generalizing to a
setting where memberships in protected groups may change at
every timestep ğ‘¡ would fit more real world applications.
ACKNOWLEDGMENTS
Dickerson and Schumann were supported by NSF CAREER Award
IIS-1846237, NSF Award CCF-1852352, NSF Award SMA-2039862,
NIST MSE Award #20126334, DARPA GARD #HR00112020007,
DARPA SI3-CMD #S4761, DoD WHS Award #HQ003420F0035, and
ARPA-E DIFFERENTIATE Award #1257037. Mattei was supported
by NSF Awards IIS-RI-2007955 and IIS-III-2107505. We thank Aviva
Prins and Aravind Srinivasan for helpful comments on earlier ver-
sions of this paper, and the anonymous reviewers for helpful com-
ments and constructive critiques.
REFERENCES
[1] Himan Abdollahpouri and Robin Burke. 2019. Multi-stakeholder Recommendation and its Connection to Multi-sided Fairness. arXiv preprint arXiv:1907.13158 (2019).
[2] Shipra Agrawal and Navin Goyal. 2013. Thompson Sampling for Contextual Bandits with Linear Payoffs. In Proceedings of the 30th International Conference on Machine Learning (ICML). 127â€“135.
[3] Peter Auer, NicolÃ² Cesa-Bianchi, and Paul Fischer. 2002. Finite-time Analysis of the Multiarmed Bandit Problem. Machine Learning 47, 2-3 (2002), 235â€“256.
[4] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi. 2019. Incorporating Behavioral Constraints in Online AI Systems. In Proc. of the 33rd AAAI Conference on Artificial Intelligence (AAAI).
[5] Douglas Belkin. 2019. SAT to give students â€˜adversity scoreâ€™ to capture social and economic background. The Wall Street Journal (2019).
[6] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, A MojsiloviÄ‡, et al. 2019. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development (2019).
[7] Nawal Benabbou, Mithun Chakraborty, Edith Elkind, and Yair Zick. 2019. Fairness Towards Groups of Agents in the Allocation of Indivisible Items. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI). 95â€“101.
[8] Nawal Benabbou, Mithun Chakraborty, Xuan-Vinh Ho, Jakub Sliwinski, and Yair Zick. 2018. Diversity constraints in public housing allocation. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS). 973â€“981.
[9] Reuben Binns. 2020. On the apparent conflict between individual and group fairness. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT*). 514â€“524.
[10] Miranda Bogen. 2019. All the Ways Hiring Algorithms Can Introduce Bias. https: //hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias [2019-0822].
[11] M. Bogen and A. Rieke. 2018. Help Wanted: An Examination of Hiring Algorithms, Equity, and Bias. Technical Report. Upturn.
[12] Robin Burke, Amy Voida, Nicholas Mattei, and Nasim Sonboli. 2020. Algorithmic Fairness, Institutional Logics, and Social Choice. In Harvard CRCS Workshop on AI for Social Good at 29th International Joint Conference on Artificial Intelligence (IJCAI 2020).
[13] L. Elisa Celis, Sayash Kapoor, Farnood Salehi, and Nisheeth Vishnoi. 2019. Controlling Polarization in Personalization: An Algorithmic Framework. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT*). 160â€“169.
[14] Yifang Chen, Alex Cuellar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar, and Stefanos Nikolaidis. 2020. Fair contextual multi-armed bandits: Theory and experiments. In Conference on Uncertainty in Artificial Intelligence (UAI). PMLR, 181â€“190.

Group Fairness in Bandits with Biased Feedback
[15] Alexandra Chouldechova and Aaron Roth. 2018. The Frontiers of Fairness in Machine Learning. CoRR abs/1810.08810 (2018). arXiv:1810.08810 http://arxiv. org/abs/1810.08810
[16] Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, and Stefanos Niko-
laidis. 2019. Reinforcement Learning with Fairness Constraints for Resource Distribution in Human-Robot Teams. arXiv preprint arXiv:1907.00313 (2019). [17] Sam Corbett-Davies and Sharad Goel. 2018. The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023 (2018).
[18] Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. 2013. Multi-armed bandit with budget constraint and variable costs. In Twenty-Seventh AAAI Conference on Artificial Intelligence.
[19] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. ACM, 214â€“226.
[20] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and
Suresh Venkatasubramanian. 2015. Certifying and Removing Disparate Impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 259â€“268. [21] Moritz Hardt, Eric Price, , and Nati Srebro. 2016. Equality of Opportunity in Supervised Learning. In Advances in Neural Information Processing Systems (NeurIPS). 3315â€“3323.
[22] Safwan Hossain, Evi Micha, and Nisarg Shah. 2021. Fair Algorithms for MultiAgent Multi-Armed Bandits. In Neural Information Processing Systems (NeurIPS).
[23] Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. 2016. Rawlsian fairness for machine learning. arXiv preprint arXiv:1610.09559 (2016).
[24] Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. 2016. Fairness in Learning: Classic and Contextual Bandits. In Advances in Neural Information Processing Systems (NeurIPS) 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Eds.). 325â€“333.
[25] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018. Preventing
Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness. In International Conference on Machine Learning. 2569â€“2577. [26] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. Inherent trade-offs in the fair determination of risk scores. Proceesings of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems (2016).
[27] Chung-Ming Kuan. 2004. Classical least squares theory. http://homepage.ntu.
edu.tw/~ckuan/pdf/et_ch3_Fall2009.pdf
[28] Tallai Andherbertrobbins Lai and Herbert Robbins. 1985. Asymptotically Efficient Adaptive Allocation Rules. Advances in Applied Mathematics 6, 1 (1985), 4â€“22.
[29] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextualbandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web (WWW). ACM, 661â€“670.
[30] Robert Lieberman. 2001. A tale of two countries: the politics of color blindness in France and the United States. French Politics, Culture & Society 19, 3 (2001), 32â€“59.
[31] Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie,
Finale Doshi-Velez, Gillian Hadfield, Michael C. Horowitz, Charles Isbell, Hiroaki
Kitano, Karen Levy, Terah Lyons, Melanie Mitchell, Julie Shah, Steven Sloman,
Shannon Vallor, and Toby Walsh. 2021. Gathering Strength, Gathering Storms:
The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report. Stanford University (2021). [32] Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C Parkes. 2017. Calibrated fairness in bandits. In Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT-ML). [33] JÃ©rÃ©mie Mary, Romaric Gaudel, and Philippe Preux. 2015. Bandits and Recommender Systems. In Machine Learning, Optimization, and Big Data. 325â€“336. [34] Kanishka Misra, Eric M Schwartz, and Jacob Abernethy. 2019. Dynamic Online
Pricing with Incomplete Information Using Multiarmed Bandit Experiments. Marketing Science 38, 2 (2019), 226â€“252. [35] Michael Mitzenmacher and Eli Upfal. 2017. Probability and computing: randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press.
[36] Executive Office of the President, Cecilia Munoz, Domestic Policy Council Direc-
tor, Megan (US Chief Technology Officer Smith (Office of Science, Technology
Policy)), DJ (Deputy Chief Technology Officer for Data Policy, Chief Data Scientist Patil (Office of Science, and Technology Policy)). 2016. Big data: A report

AAMAS â€™22, May 9â€“13, 2022, Online
on algorithmic systems, opportunity, and civil rights. Executive Office of the President. [37] Cathy Oâ€™Neil. 2016. Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway Books. [38] Vishakha Patil, Ganesh Ghalme, Vineet Nair, and Y Narahari. 2019. Achieving fairness in the stochastic multi-armed bandit problem. arXiv preprint arXiv:1907.10516 (2019). [39] J. Rawls. 1971. A Theory of Justice. Harvard University Press. [40] Tom Ron, Omer Ben-Porat, and Uri Shalit. 2021. Corporate Social Responsibility via Multi-Armed Bandits. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT). 26â€“40. [41] Cynthia Rudin. 2013. Predictive policing using machine learning to detect patterns of crime. Wired Magazine (2013). [42] Debjani Saha, Candice Schumann, Duncan C. McElfresh, John P. Dickerson,
Michelle L Mazurek, and Michael Carl Tschantz. 2020. Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics. In Proceedings of the International Conference on Machine Learning (ICML). [43] Candice Schumann, Samsara N. Counts, Jeffrey S. Foster, and John P. Dickerson. 2019. The Diverse Cohort Selection Problem. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS). 601â€“609.
[44] Candice Schumann, Jeffrey S Foster, Nicholas Mattei, and John P Dickerson. 2020. We Need Fairness and Explainability in Algorithmic Hiring. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS). 1716â€“1720.
[45] Candice Schumann, Zhi Lang, Jeffrey S. Foster, and John P. Dickerson. 2019. Making the Cut: A Bandit-based Approach to Tiered Interviewing. In Proc. Advances in Neural Information Processing Systems (NeurIPS).
[46] Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson. 2019. Group Fairness in Bandit Arm Selection. In NeurIPS Workshop on Machine Learning and Causal Inference for Improved Decision Making.
[47] Ben Shneiderman. 2020. Human-centered artificial intelligence: Reliable, safe & trustworthy. International Journal of Humanâ€“Computer Interaction 36, 6 (2020), 495â€“504.
[48] Ben Shneiderman. 2020. Human-centered Artificial Intelligence: Reliable, safe & trustworthy. International Journal of Humanâ€“Computer Interaction 36, 6 (2020), 495â€“504.
[49] Ashudeep Singh, Yoni Halpern, Nithum Thain, Konstantina Christakopoulou,
Ed H. Chi, Jilin Chen, and Alex Beutel. 2020. Building Healthy Recommendation Sequences for Everyone: A Safe Reinforcement Learning Approach. In FAccTRec Workshop on Responsible Recommendation at RecSys-20. [50] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of exposure in rankings. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2219â€“2228. [51] Adam Smith. 1776. The Wealth of Nations. New York: The Modern Library (1776). [52] Nasim Sonboli, Robin Burke, Nicholas Mattei, Farzad Eskandanian, and Tian Gao.
2020. "And the Winner Is...": Dynamic Lotteries for Multi-group Fairness-Aware Recommendation. CoRR abs/2009.02590 (2020). arXiv:2009.02590 [53] Richard S. Sutton and Andrew Barto. 2017. Reinforcement Learning: An Introduction (2nd ed.). MIT Press. [54] Latanya Sweeney. 2013. Discrimination in online ad delivery. Commun. ACM 56, 5 (2013), 44â€“54. [55] Latanya Sweeney. 2013. Discrimination in online ad delivery. arXiv preprint arXiv:1301.6822 (2013). [56] Yanhan Savannah Tang, Alan Andrew Scheller-Wolf, and Sridhar R Tayur. 2021.
Generalized Bandits with Learning and Queueing in Split Liver Transplantation. Available at SSRN 3855206 (2021). [57] Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims. 2021. Fairness of Exposure in Stochastic Bandits. In International Conference on Machine Learning (ICML). [58] James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda
ViÃ©gas, and Jimbo Wilson. 2019. The What-If Tool: Interactive probing of machine learning models. IEEE transactions on visualization and computer graphics (2019). [59] Huasen Wu, Rayadurgam Srikant, Xin Liu, and Chong Jiang. 2015. Algorithms with logarithmic or sublinear regret for constrained contextual bandits. In Advances in Neural Information Processing Systems. 433â€“441. [60] Yifan Wu, Roshan Shariff, Tor Lattimore, and Csaba SzepesvÃ¡ri. 2016. Conservative bandits. In International Conference on Machine Learning. 1254â€“1262. [61] H Peyton Young. 1995. Equity: in theory and practice. Princeton University Press.

AAMAS â€™22, May 9â€“13, 2022, Online
A ADDITIONAL RELATED RESEARCH
We now discuss, and appropriately compare and contrast, additional related research in settings similar to some aspects of our own model. Fairness in ranking. A closely related area to our work is the research into fairness in rankings [50], multi-stakeholder recommender systems [1], and item allocation [7, 8]. When algorithms return rankings for an individual to select from, one must pay attention to the ordering and the positioning of various groups [50]. One can see this as an application of the group fairness concept to the slates that are chosen for display. A particular aspect of recommendation systems that one needs to keep in mind is that often there are different stakeholders: the person receiving the recommendation, the company giving the recommendation, and the businesses that are the subjects of recommendation [1]. Finally, when goods are allocated, such as housing or subsidies one may need to observe both individual and group fairness [8]. Indeed, group fairness is specifically important in, e.g., Singapore, which has specifically enforced notions of group fairness when allocating public housing [7]. Constrained reasoning in MAB. There is also significant recent work in constrained reasoning in the MAB setting. Balakrishnan et al. [4] study the idea of learning constraints over pulling arms by observation in a pre-training phase. Wu et al. [59] study constraints in both number of pulls per arm, as well as number of rounds where arms are available to be pulled. Wu et al. [60] study a different flavor of constrained bandits where the learned policy cannot fall below a certain threshold; modeling the case where one wants to explore, but not suffer too much of a penalty over a status-quo policy. A related and perhaps interesting direction for future work is the work on bandits that are budget-constrained (without fairness considerations). Ding et al. [18] study budget-constrained bandits where each arm also has an unknown cost distribution and one must learn a policy that maximizes reward and minimizes cost. Our formulation is not captured in the current literature on constrained and budgeted bandits and it is not obvious how to formalize a budget constraint as an inter-group fairness constraint. Indeed, a simplistic version of this would just lead to exhausting the budget of the â€œbetterâ€ arm pulls before moving to the next best. Legal motivation. Fairness in bandits is a particularly important area as the online, dynamic nature makes the task challenging and the use of bandits in a number of areas makes the problem particularly relevant. The motivating factor for group fairness is that one does not want to cause disparate impact, or the idea that groups should be treated differently based only on non-relevant aspects [20]. Indeed, discrimination in certain areas including housing, credit, and jobs is forbidden in the US by the Civil Rights Act of 1965. It is specifically in these areas where bandit algorithms are deployed: advertising (where discrimination has been found) [55], college admissions [43], and interviewing [45].
B PROOFS
B.1 Two Groups
In order to prove Theorem 1, we first prove two lemmas.

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson

Lemma 1. The following holds for any ğ‘– at any time ğ‘¡, with probability at least 1 âˆ’ ğ›¿:
ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ (ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ + 1[ğ‘– âˆˆ ğ‘ƒ1]ğœ“ğ‘ƒ Â· ğ‘¥ğ‘–,ğ‘¡ ) â‰¤ ğ‘¤ğ‘–,ğ‘¡ . (14) 1
Proof. There are two cases: ğ‘– âˆˆ ğ‘ƒ1 or ğ‘– âˆ‰ ğ‘ƒ1. Focusing on the first case, inequality 14 becomes:
ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ â‰¤ ğ‘¤ğ‘–,ğ‘¡ .

By the standard properties of OLS estimators [27],

ğ›½Ë†ğ‘–, ğ‘¡ âˆ¼ N ğ›½ğ‘–, ğœ2 (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ )âˆ’1 .

Then, for any fixed ğ‘¥ğ‘–,ğ‘¡ : ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ¼ N ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ , ğ‘¥ğ‘–ğ‘‡,ğ‘¡ ğœ2 (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ )âˆ’1ğ‘¥ğ‘–,ğ‘¡ .

Using the definition of the Quantile function and the symmetric property of the normal distribution, with probability at least 1 âˆ’ ğ›¿ ,
ğ‘›ğ‘‡

ğ›½Ë†ğ‘–,ğ‘¡ âˆ¼ N ğ›½ğ‘–, ğœ2 (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ )âˆ’1 .

Exploring the second case where ğ‘– âˆˆ ğ‘ƒ1, inequality 14 can be replaced with

ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ ğ¶ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ â‰¤ ğ‘¤ğ‘–,ğ‘¡

where ğ¶ğ‘– = ğ›½ğ‘– + ğœ“ğ‘ƒ . Again, by the standard properties of OLS 1
estimators ğ›½Ë†ğ‘–, ğ‘¡ âˆ¼ N ğ¶ğ‘–, ğœ2 (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ )âˆ’1 , we have for any fixed ğ‘¥ğ‘–,ğ‘¡ :
ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ¼ N ğ¶ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ , ğ‘¥ğ‘–ğ‘‡,ğ‘¡ ğœ2 (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ )âˆ’1ğ‘¥ğ‘–,ğ‘¡ .

This uses the definition of the Quantile function and the symmetric

property of the normal distribution, with probability at least 1 âˆ’ ğ›¿ .
ğ‘›ğ‘‡

Therefore, the probability that inequality 14 fails to hold for any

ğ‘– at any timestep ğ‘¡ is at most ğ‘›ğ‘‡ Â· ğ›¿ = ğ›¿.

â–¡

ğ‘›ğ‘‡

Lemma 2. The following holds for any group ğ‘ƒğ‘— , any arm ğ‘–, at any time ğ‘¡, with probability at least 1 âˆ’ ğ›¿:

ğœ“Ë†
ğ‘ƒ

,ğ‘¡

Â·

ğ‘¥ğ‘–,ğ‘¡

âˆ’ ğœ“Â¯ğ‘ƒ

Â· ğ‘¥ğ‘–,ğ‘¡

â‰¤ ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ .

(15)

ğ‘—

ğ‘—

ğ‘—

Proof.

By the standard properties of OLS estimators ğœ“Ë†
ğ‘ƒ

,ğ‘¡

âˆ¼

ğ‘—

N ğœ“Â¯ , ğœ2 (Xğ‘‡ , X )âˆ’1 . For any fixed ğ‘¥ ,
ğ‘ƒğ‘— ğ‘ƒğ‘— ,ğ‘¡ ğ‘ƒğ‘— ,ğ‘¡ ğ‘–,ğ‘¡

ğœ“Ë† Â· ğ‘¥

âˆ¼ N ğœ“Â¯

Â·ğ‘¥

ğ‘‡
,ğ‘¥

ğœ2 (Xğ‘‡

,X

)âˆ’1ğ‘¥

.

ğ‘–,ğ‘¡ ğ‘–,ğ‘¡

ğ‘ƒğ‘— ğ‘–,ğ‘¡ ğ‘–,ğ‘¡ ğ‘ƒğ‘— ,ğ‘¡ ğ‘ƒğ‘— ,ğ‘¡ ğ‘–,ğ‘¡

Using the definition of the quantile function and the symmetric

property of the normal distribution, with probability at least 1 âˆ’

ğ›¿
ğ‘› , inequality 15 holds. Therefore, the probability that this fails
|ğ‘ƒğ‘— | ğ‘‡

to hold for any ğ‘–

at any timestep ğ‘¡

is at most

ğ‘› |ğ‘ƒ

|ğ‘‡

Â·

ğ›¿
ğ‘›

= ğ›¿.

ğ‘‡

â–¡

ğ‘—

|ğ‘ƒğ‘— |

With Lemma 1 and Lemma 2, we can now prove Theorem 1.

Proof. Regret for GroupFairTopInterval can be grouped into

three terms for any ğ‘‡1 â‰¤ ğ‘‡ :

ğ‘…âˆ— (ğ‘‡ ) =

âˆ‘ï¸

regret (ğ‘¡)

ğ‘¡ : ğ‘¡ is an explore round

âˆ‘ï¸

+

regret (ğ‘¡)

ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ <ğ‘‡1

âˆ‘ï¸

+

regret (ğ‘¡)

(16)

ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ â‰¥ğ‘‡1

Group Fairness in Bandits with Biased Feedback

Starting with the first term, define ğ‘ğ‘¡ =

1
1/3 to be the probability

ğ‘¡

that timestep ğ‘¡ is an exploration round. Then, for any ğ‘¡,

âˆ‘ï¸ ğ‘ğ‘¡â€² = Î˜(ğ‘¡ 2/3).

(17)

ğ‘¡ â€² <ğ‘¡

We now focus on the third term of Equation 16, where ğ‘¡ is an

exploit round and ğ‘¡ > ğ‘‡1. Throughout the rest of the proof we assume Lemma 1 and Lemma 2. Fix a exploit timestep ğ‘¡ where arm ğ‘–ğ‘¡ is played. Then,

regret (ğ‘¡ ) â‰¤ 2ğ‘¤ğ‘–ğ‘¡ ,ğ‘¡ + 2ğ‘ğ‘ƒ ,ğ‘–ğ‘¡ ,ğ‘¡ + 2ğ‘ğ‘ƒ ,ğ‘–ğ‘¡ ,ğ‘¡

1

2

â‰¤ 2 max ğ‘¤ğ‘–,ğ‘¡ + ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ + ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡

1

2

ğ‘–

â‰¤ 2 max ğ‘¤ğ‘–,ğ‘¡ + max ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ + max ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ . (18)

1

2

ğ‘–

ğ‘–

ğ‘–

Note that:

ğ‘¤ğ‘–,ğ‘¡ = ğ‘„

âˆ’1

N

0,ğ‘¥ğ‘–,ğ‘¡

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘¥ğ‘‡

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

ğ›¿ .
2ğ‘›ğ‘‡

Similarly,

ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ = ğ‘„

âˆ’1

ğ‘—

N 0,ğ‘¥ğ‘–,ğ‘¡ Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ğ‘¥ğ‘‡

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘–,ğ‘¡

ğ›¿ .
ğ‘›
2 |ğ‘ƒğ‘— |ğ‘‡

We first bound

âˆ’1

ğ‘‡
ğ‘¥ğ‘–,ğ‘¡ ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘¥ğ‘–,ğ‘¡ â‰¤ ||ğ‘¥ğ‘–,ğ‘¡ ||ğœ†max

ğ‘–,ğ‘¡

âˆ’1 ğ‘‡
ğ‘‹ğ‘–,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡

1 = ||ğ‘¥ğ‘–,ğ‘¡ ||
ğ‘‡
ğœ†min ğ‘‹ğ‘–,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡

1

â‰¤

(19)

ğ‘‡
ğœ†min ğ‘‹ğ‘–,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡

where the last inequality holds since ||ğ‘¥ğ‘–,ğ‘¡ || â‰¤ 1 for all ğ‘– and ğ‘¡. Using similar logic,

âˆ’1

1

ğ‘¥ğ‘–,ğ‘¡ Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ğ‘¥ğ‘–,ğ‘¡ â‰¤

.

(20)

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğœ†min Xğ‘‡ Xğ‘ƒ ,ğ‘¡

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

Let ğºğ‘–,ğ‘¡ be the number of observations of arm ğ‘– with contexts drawn uniformly from the distribution for arm ğ‘– prior to timestep ğ‘¡. Similarly, let Gğ‘ƒ ,ğ‘¡ be the number of observations of group ğ‘ƒğ‘—
ğ‘—
with contexts drawn uniformly from the distribution for group ğ‘ƒğ‘— prior to timestep ğ‘¡. Let ğ¿ > maxğ‘¡ ğœ†max (ğ‘¥ğ‘‡ , ğ‘¥ğ‘–,ğ‘¡ ). For any ğ›¼ âˆˆ
ğ‘–,ğ‘¡
[0, 1], using the superaddivity of minimum eigenvectors for positive
semidefinite matrices, we get

E ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) â‰¥ ğºğ‘–,ğ‘¡ ğœ†minğ‘–,ğ‘‘ â‰¥ ğºğ‘–,ğ‘¡ ğœ†minğ‘–,ğ‘‘ . (21)

ğ‘‘

ğ‘‘

Similarly,

Gğ‘ƒ ,ğ‘¡

Gğ‘ƒ ,ğ‘¡

E ğœ†min (Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ) â‰¥

ğ‘—
ğœ†min

â‰¥

ğ‘—
ğœ†min . (22)

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘‘

ğ‘ƒ ğ‘— ,ğ‘‘

ğ‘‘

ğ‘ƒ ğ‘— ,ğ‘‘

Equation 21 implies that

Pr ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ ) â‰¤ ğ›¼ ğºğ‘–,ğ‘¡ ğœ†minğ‘–,ğ‘‘

ğ‘‹ğ‘–,ğ‘¡

ğ‘‘

â‰¤ Pr ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ ) â‰¤ ğ›¼E[ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ )] (23)
ğ‘‹ğ‘–,ğ‘¡

AAMAS â€™22, May 9â€“13, 2022, Online

â‰¤ Pr ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ , ğ‘‹ğ‘–,ğ‘¡ ) â‰¤ ğ›¼ğœ†min (E[ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ]) (24)
ğ‘‹ğ‘–,ğ‘¡

âˆ’(1 âˆ’ ğ›¼)2ğœ†min (E[ğ‘‹ğ‘‡ ğ‘‹ğ‘–,ğ‘¡ ])

â‰¤ ğ‘‘ exp

ğ‘–,ğ‘¡
(25)

2ğ¿

âˆ’(1 âˆ’ ğ›¼)2E[ğœ†min (ğ‘‹ğ‘‡ ğ‘‹ğ‘–,ğ‘¡ )]

â‰¤ ğ‘‘ exp

ğ‘–,ğ‘¡
(26)

2ğ¿

âˆ’(1 âˆ’ ğ›¼ )2 ğºğ‘‘ğ‘–,ğ‘¡ ğœ†minğ‘–,ğ‘‘

â‰¤ ğ‘‘ exp

(27)

2ğ¿

where Inequalities 23 and 27 are from equation 21, Inequalities 24 and 26 are from Jensenâ€™s inequality [35], and Inequality 25 uses a Matrix Chernoff Bound [35].
Using Inequality 27 after rearranging with probability 1 âˆ’ ğ›¿:

ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) â‰¥ ğ›¼ ğºğ‘–,ğ‘¡ ğœ†minğ‘–,ğ‘‘ (28) ğ‘‘

when

ğ¿

1

ğºğ‘–,ğ‘¡ â‰¥ ğ‘‘

ln + ln ğ‘‘ .

(29)

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘–,ğ‘‘

Using similar logic with probability 1 âˆ’ ğ›¿, we have

Gğ‘ƒ ,ğ‘¡

ğœ†min (Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ) â‰¥ ğ›¼

ğ‘—
ğœ†min

(30)

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘‘

ğ‘ƒ ğ‘— ,ğ‘‘

when

ğ¿

1

Gğ‘ƒ ,ğ‘¡ â‰¥ ğ‘‘

ln + ln ğ‘‘ .

(31)

ğ‘—

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘ƒ ğ‘— ,ğ‘‘

Using a multiplicative Chernoff bound [35] for a fixed timestep ğ‘¡ with probability 1 âˆ’ ğ›¿ â€², the number of exploitation rounds prior

to rounds ğ‘¡ will satisfy

âˆšï¸„

âˆ‘ï¸

2 âˆ‘ï¸

ğºğ‘¡ âˆ’ ğ‘ğ‘¡â€² â‰¤ ln â€² ğ‘ğ‘¡â€²

(32)

ğ‘¡â€²<ğ‘¡ ğ›¿ ğ‘¡ <ğ‘¡â€²

For a fixed ğ‘– and timestep ğ‘¡ using a multiplicative Chernoff bound, with probability 1 âˆ’ ğ›¿ â€², the number of exploitation rounds for arm
ğ‘– prior to round ğ‘¡ will satisfy

âˆšï¸‚

ğºğ‘¡

2 ğºğ‘¡

ğºğ‘–,ğ‘¡ âˆ’ â‰¤ ln â€² .

(33)

ğ‘›

ğ›¿ğ‘›

Similarly, for a fixed group ğ‘ƒğ‘— and timestep ğ‘¡ with probaility 1 âˆ’ ğ›¿ â€²,

the number of exploration rounds for group ğ‘ƒğ‘— prior to round ğ‘¡ will

satisfy

âˆšï¸„

ğºğ‘¡

2 ğºğ‘¡

Gğ‘–,ğ‘¡ âˆ’

â‰¤

|ğ‘ƒ |/ğ‘›

ln ğ›¿ â€² ğ‘›/|ğ‘ƒ |

(34)

ğ‘—

ğ‘—

where |ğ‘ƒğ‘— | is the size of group ğ‘ƒğ‘— . Combining equations 32 and 33 with probability at least 1 âˆ’ 2ğ›¿ â€²

for a fixed arm ğ‘– and timestep ğ‘¡, if

ğ‘¡â€²<ğ‘¡ ğ‘ƒğ‘¡â€²

â‰¥

36ğ‘›

2
ln

2
â€²

we have

ğ›¿

ğºğ‘–,ğ‘¡ âˆ’ ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² â‰¤ ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² . (35)

ğ‘›

2ğ‘›

Similarly, combining equations 32 and 34 with probability at least 1 âˆ’ 2ğ›¿ â€² for a fixed group ğ‘ƒğ‘— and timestep ğ‘¡:

Gâˆ’

ğ‘¡â€²<ğ‘¡ ğ‘ƒğ‘¡â€² â‰¤

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² .

(36)

ğ‘–,ğ‘¡ ğ‘›/|ğ‘ƒ ğ‘— |

2ğ‘›

AAMAS â€™22, May 9â€“13, 2022, Online

Therefore, equation 28 holds with probability 1 âˆ’ ğ›¿ â€² when

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡ â‰¥ ğ‘‘

ğ¿

1

ln + ln ğ‘‘ .

(37)

2ğ‘›

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘–,ğ‘‘

Similarly, equation 30 holds with probability 1 âˆ’ ğ›¿ â€² when

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡ â‰¥ ğ‘‘

ğ¿

1 ln + ln ğ‘‘ . (38)

2ğ‘›/|ğ‘ƒğ‘— |

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘ƒ ğ‘— ,ğ‘‘

Therefore, since ğ‘›/|ğ‘ƒğ‘— | < ğ‘›, the number of rounds after which we

have sufficient samples such that the estimators are well-concentrated

is

3/2

ğ‘‘ğ‘›ğ¿

2

ğ‘‡1 = Î˜ min

2
ln

+ ln ğ‘‘

(39)

ğ‘ğœ†

ğ›¿

ğ‘šğ‘–ğ‘›ğ‘,ğ‘‘

where ğ‘ âˆˆ [ğ‘›] âˆª ğ‘ƒ1 âˆª ğ‘ƒ2.

Also note that for any ğ‘¡ â‰¥ ğ‘‡1 we have

âˆ‘ï¸

ğ‘‘ğ‘›ğ¿

2

ğ‘ğ‘¡â€² = Î© min

2
ln

â€² + ln ğ‘‘

.

(40)

ğ‘¡â€²<ğ‘¡ ğ‘ ğœ†minğ‘,ğ‘‘ ğ›¿

We can now bound the third term in Equation 16.
âˆ‘ï¸ ğ‘Ÿğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘¡ (ğ‘¡)
ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ â‰¥ğ‘‡1

âˆ‘ï¸

â‰¤2

max ğ‘¤ğ‘–,ğ‘¡ + max ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ + max ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡

(41)

1

2

ğ‘–

ğ‘–

ğ‘–

ğ‘¡ â‰¥ğ‘‡1

âˆ‘ï¸

ğ›¿

â‰¤ 2 mğ‘–ax ğ‘„ N 0,ğœ†max ( (ğ‘‹ğ‘‡ ğ‘‹ğ‘–,ğ‘¡ ))âˆ’1 2ğ‘›ğ‘‡

ğ‘¡ â‰¥ğ‘‡1

ğ‘–,ğ‘¡

+ mğ‘–ax ğ‘„ N 0,ğœ†max ( ( Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ))âˆ’1 ğ‘ƒ1,ğ‘¡ 1

ğ›¿
ğ‘›
2 |ğ‘ƒ1 |ğ‘‡

+ mğ‘–ax ğ‘„ N 0,ğœ†max ( ( Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ))âˆ’1 ğ‘ƒ2,ğ‘¡ 2

ğ›¿
ğ‘›
2 |ğ‘ƒ2 |ğ‘‡

âˆ‘ï¸

ğ›¿

â‰¤2 ğ‘„

ğ‘¡ â‰¥ğ‘‡1 N 0, minğ‘– ğœ†min ( (ğ‘‹1 ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) )âˆ’1 2ğ‘›ğ‘‡

+ğ‘„

N 0,

1 ( (Xğ‘‡ X ) ) âˆ’1

minğ‘– ğœ†min

ğ‘ƒ ,ğ‘¡

ğ‘ƒ1,ğ‘¡ 1

ğ›¿
ğ‘›
2 |ğ‘ƒ1 |ğ‘‡

+ğ‘„

N 0,

1 ( (Xğ‘‡ X ) ) âˆ’1

minğ‘– ğœ†min

ğ‘ƒ ,ğ‘¡

ğ‘ƒ2,ğ‘¡ 2

ğ›¿
ğ‘›
2 |ğ‘ƒ2 |ğ‘‡

âˆ‘ï¸

ğ›¿

â‰¤2 ğ‘„

2ğ‘›ğ‘‡

ğ‘¡ â‰¥ğ‘‡1 N 0,

1

ğºğ‘–,ğ‘¡

minğ‘– ğ›¼

ğœ†min

ğ‘‘

ğ‘–,ğ‘‘

+ğ‘„

N 0, G 1

ğ‘ƒ1,ğ‘¡

ğ›¼

ğœ†min

ğ‘‘

ğ‘ƒ1,ğ‘‘

ğ›¿
ğ‘›
2 |ğ‘ƒ1 |ğ‘‡

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson

âˆ‘ï¸ â‰¤2
ğ‘¡ â‰¥ğ‘‡1

2ğ‘›ğ‘‡
ln
ğ›¿

ğºğ‘–,ğ‘¡

minğ‘– ğ›¼

ğœ†min

ğ‘‘

ğ‘–,ğ‘‘

ğ‘›
ln 2 |ğ‘ƒ1 | ğ‘‡

+

ğ›¿

Gğ‘ƒ ,ğ‘¡

minğ‘– ğ›¼ 1 ğœ†min

ğ‘‘

ğ‘ƒ1,ğ‘‘

ğ‘›

ln 2 |ğ‘ƒ2 | ğ‘‡

+

ğ›¿

+

6ğ›¿

â€²
ğ‘‡

(43)

Gğ‘ƒ ,ğ‘¡

minğ‘– ğ›¼ 2 ğœ†min

ğ‘‘

ğ‘ƒ2,ğ‘‘

2ğ‘›ğ‘‡

âˆ‘ï¸

ln

â‰¤2 3

ğ›¿

(44)

ğ‘¡ â‰¥ğ‘‡1

ğºğ‘–,ğ‘¡

minğ‘– ğ›¼

ğœ†min

ğ‘‘

ğ‘–,ğ‘‘

âˆ‘ï¸ =ğ‘‚
ğ‘¡ â‰¥ğ‘‡1

2ğ‘›ğ‘‡
ln
ğ›¿
ğ‘‘

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğºğ‘–,ğ‘¡ ğœ†min ğ‘–,ğ‘‘

2ğ‘›ğ‘‡
ln

=ğ‘‚ ğ‘‘

ğ›¿

âˆšï¸„

âˆ‘ï¸

1

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğœ†min ğ‘–,ğ‘‘

ğ‘¡ â‰¥ğ‘‡

minğ‘– ğºğ‘–,ğ‘¡

1

2ğ‘›ğ‘‡
ln

=ğ‘‚ ğ‘‘

ğ›¿

âˆšï¸‚

âˆ‘ï¸

ğ‘›

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğœ†min ğ‘–,ğ‘‘

ğ‘¡ â‰¥ğ‘‡

1

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€²

2ğ‘›ğ‘‡

âˆšï¸‚

ln

=ğ‘‚ ğ‘‘

ğ›¿

âˆ‘ï¸

ğ‘›

+

ğ›¿

â€²
ğ‘‡

(45)

minğ‘– ğœ†min

ğ‘¡ 2/3

ğ‘–,ğ‘‘ ğ‘¡ â‰¥ğ‘‡1

2ğ‘›ğ‘‡

ln

= ğ‘‚ ğ‘‘ğ‘›

ğ›¿

âˆ‘ï¸

1

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğœ†min

ğ‘¡ 1/3

ğ‘–,ğ‘‘ ğ‘¡ âˆˆ [ğ‘‡1,ğ‘‡ ]

2ğ‘›ğ‘‡

ln

= ğ‘‚ ğ‘‘ğ‘›

ğ›¿

ğ‘‡

2/3

+

ğ›¿

â€²
ğ‘‡

(46)

minğ‘– ğœ†min ğ‘–,ğ‘‘

where (41) is due to Equation 18, (42) is due to Equations 21 and 22,
ğ‘›
(43) is due to Chernoff bounds, (44) is due to the fact that |ğ‘ƒğ‘— | < ğ‘›
and ğºğ‘ƒ ,ğ‘¡ > minğ‘– ğºğ‘–,ğ‘¡ , and (45) is due to Equation 17. Theorem 1 ğ‘—
follows by combining Equations 16, 17, 39, and 46 and setting ğ›¿ â€² =
11
min 3ğ‘›ğ‘‡ , ğ‘‡ 1/3 . â–¡

B.2 Multiple Groups

In in order to prove Theorem 3, we first prove two lemmas.

Lemma 3. The following holds for any ğ‘– at any time ğ‘¡, with probability at least 1 âˆ’ ğ›¿

ğ‘š
ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ ğ›½ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ + âˆ‘ï¸ 1 ğ‘– âˆˆ ğ‘ƒ ğ‘— ğœ“ğ‘ƒ Â· ğ‘¥ğ‘–,ğ‘¡ ğ‘—
ğ‘— =1

â‰¤ ğ‘¤ğ‘–,ğ‘¡

(47)

+ğ‘„

ğ›¿

+

3ğ›¿

â€²
ğ‘‡

(42)

ğ‘›

2 |ğ‘ƒ |ğ‘‡

N 0, G 1

2

ğ‘ƒ2,ğ‘¡

ğ›¼

ğœ†min

ğ‘‘

ğ‘ƒ2,ğ‘‘

Group Fairness in Bandits with Biased Feedback

Algorithm 3 GroupFairTopInterval (Multiple Groups)

Require: ğ›¿, (ğ‘ƒ1, . . . , ğ‘ƒğ‘š), ğœŒ

1: for ğ‘¡ = 1 . . . ğ‘‡ do

2:

with probability

1
1/3 , play ğ‘–ğ‘¡

âˆˆğ‘…

{1, . . . , ğ‘›}

ğ‘¡

3: Else

4:

for ğ‘— = 1 . . . , ğ‘š do

âˆ’1

5:

Let ğœ“Ë†
ğ‘ƒ

,ğ‘¡

=

Xğ‘‡

Xğ‘ƒ ,ğ‘¡

Xğ‘‡ Yğ‘ƒ ,ğ‘¡

ğ‘—

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

6:

for ğ‘– = 1 . . . ğ‘› do

âˆ’1

7:

Let ğ›½Ë†ğ‘–,ğ‘¡ =

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘‡ğ‘‡
ğ‘‹ğ‘Œ

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡ ğ‘–,ğ‘¡

âˆ’1

8:

Let ğ¹ğ‘–,ğ‘¡ = N

2
0, ğœ ğ‘¥ğ‘–,ğ‘¡

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘‡
ğ‘¥

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

9:

Let ğ‘¤ğ‘–,ğ‘¡ = ğ‘„ğ¹ ğ›¿

ğ‘–,ğ‘¡ 2ğ‘›ğ‘¡

10:

for ğ‘— where ğ‘– âˆˆ ğ‘ƒğ‘— do

11:

Let Fğ‘ƒ ,ğ‘–,ğ‘¡ = N

2
0, ğœ ğ‘¥ğ‘–,ğ‘¡

Xğ‘‡

Xğ‘ƒ ,ğ‘¡

ğ‘‡
ğ‘¥

ğ‘—

ğ‘ƒğ‘— ,ğ‘¡ ğ‘— ğ‘–,ğ‘¡

12:

Let ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ = ğ‘„ F

ğ›¿
ğ‘›

ğ‘— ğ‘ƒ ğ‘— ,ğ‘–,ğ‘¡ 2 |ğ‘ƒ ğ‘— | ğ‘‡

13:

Let ğ‘¢Ë†ğ‘–,ğ‘¡ = ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ + ğ‘¤ğ‘–,ğ‘¡ + ğœŒ âˆ’ ğœ“Ë†ğ‘ƒ ,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ + ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡

ğ‘—

ğ‘—

14:

Play

argmax
ğ‘–

ğ‘¢Ë†ğ‘–,ğ‘¡

and

observe

reward ğ‘¦ğ‘–,ğ‘¡

Proof. Inequality 47 can be replaced with

ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ’ ğ¶ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ â‰¤ ğ‘¤ğ‘–,ğ‘¡

where ğ¶ğ‘– = ğ›½ğ‘– + ğœ“ğ‘ƒ and ğ‘– âˆˆ ğ‘ƒğ‘— . By the standard properties of OLS ğ‘—
estimators ğ›½Ë†ğ‘–,ğ‘¡ âˆ¼ ğ‘ ğ¶ğ‘–, ğœ2 (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ )âˆ’1 . For any fixed ğ‘¥ğ‘–,ğ‘¡ :

ğ›½Ë†ğ‘–,ğ‘¡ Â· ğ‘¥ğ‘–,ğ‘¡ âˆ¼ ğ‘ ğ¶ğ‘– Â· ğ‘¥ğ‘–,ğ‘¡ , ğ‘¥ğ‘–ğ‘‡,ğ‘¡ ğœ2 (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ )âˆ’1ğ‘¥ğ‘–,ğ‘¡

Using the definition of the quantile function and the symmetric

property of the normal distribution, with probability at least 1 âˆ’ ğ›¿ ,
ğ‘›ğ‘‡

Inequality 47 holds. Therefore, the probability that inequality 47

ğ›¿
fails to hold for any ğ‘– at any timestep ğ‘¡ is at most ğ‘›ğ‘‡

= ğ›¿.

â–¡

ğ‘›ğ‘‡

Lemma 4. The following holds for any group ğ‘ƒğ‘— , any arm ğ‘–, at any timestep ğ‘¡, with probability at least 1 âˆ’ ğ›¿:

ğœ“Ë†
ğ‘ƒ

,ğ‘¡

Â·

ğ‘¥ğ‘–,ğ‘¡

âˆ’ ğœ“ğ‘ƒ

,ğ‘¡

Â·

ğ‘¥ğ‘–,ğ‘¡

â‰¤ ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ .

(48)

ğ‘—

ğ‘—

ğ‘—

Proof. By the standard properties of OLS estimators,

ğœ“Ë† âˆ¼ ğ‘ ğœ“ , ğœ2 (Xğ‘‡ X )âˆ’1 .
ğ‘ƒğ‘— ,ğ‘¡ ğ‘ƒğ‘— ğ‘ƒğ‘— ,ğ‘¡ ğ‘ƒğ‘— ,ğ‘¡

For any fixed ğ‘¥ğ‘–,ğ‘¡ :

ğœ“Ë†

Â·ğ‘¥

âˆ¼ğ‘ ğœ“

Â·ğ‘¥

ğ‘‡
,ğ‘¥

ğœ2 (Xğ‘‡

X

)âˆ’1ğ‘¥

.

ğ‘ƒğ‘— ,ğ‘¡ ğ‘–,ğ‘¡

ğ‘ƒğ‘— ğ‘–,ğ‘¡ ğ‘–,ğ‘¡ ğ‘ƒğ‘— ,ğ‘¡ ğ‘ƒğ‘— ,ğ‘¡ ğ‘–,ğ‘¡

Using the definition of the quantile function and the symmetric

property of the normal distribution, with probability of at least

1âˆ’

ğ›¿
ğ‘›

inequality 48 holds. Therefore the probability this fails to

|ğ‘ƒğ‘— | ğ‘‡

ğ‘›
hold for any ğ‘– at timestep ğ‘¡ is at most |ğ‘ƒ |ğ‘‡

ğ›¿
ğ‘›

= ğ›¿.

ğ‘‡

â–¡

ğ‘— |ğ‘ƒğ‘— |

Theorem 3. For ğ‘š groups ğ‘ƒ1, . . . , ğ‘ƒğ‘š, where ğœŒ is the expected
average reward, GroupFairTopInterval (Multiple Groups) has regret

âˆšï¸„

ğ‘…âˆ— (ğ‘‡ ) = ğ‘‚

2ğ‘›ğ‘‡
ğ‘‘ğ‘› ln
ğ›¿ 2/3
ğ‘‡

ğ‘™

AAMAS â€™22, May 9â€“13, 2022, Online

2/3

ğ‘‘ğ‘›ğ‘šğ¿ 2ğ‘›ğ‘‡

+

2
ln

+ ln ğ‘‘

(49)

ğ‘™

ğ›¿

where ğ‘™ = minğ‘– ğœ†ğ‘šğ‘–ğ‘› and ğ¿ > maxğ‘¡ ğœ†max (ğ‘¥ğ‘‡ ğ‘¥ğ‘–,ğ‘¡ ).

ğ‘–,ğ‘‘

ğ‘–,ğ‘¡

We can now prove Theorem 3.

Proof. Assume that both Lemma 3 and Lemma 4 hold for all

arms ğ‘– and all timesteps ğ‘¡.

Regret for GroupFairTopInterval (Multiple Groups) can be

grouped into three terms for any ğ‘‡1 â‰¤ ğ‘‡ :

ğ‘…âˆ— (ğ‘‡ ) =

âˆ‘ï¸

regret (ğ‘¡)

ğ‘¡ : ğ‘¡ is an explore round

âˆ‘ï¸

+

regret (ğ‘¡)

ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ <ğ‘‡1

âˆ‘ï¸

+

regret (ğ‘¡)

(50)

ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ â‰¥ğ‘‡1

Starting with the first term in Equation 50, define ğ‘ğ‘¡ =

1
1/3 to be

ğ‘¡

the probability that timestep ğ‘¡ is an exploration round. Then, for

any ğ‘¡,

âˆ‘ï¸ ğ‘ğ‘¡â€² = Î˜(ğ‘¡ 2/3)

(51)

ğ‘¡ â€² <ğ‘¡

Focusing on the third term of Equation 50, fix an exploit timestep

ğ‘¡ where arm ğ‘–ğ‘¡ is played. Then,

regret (ğ‘¡ ) â‰¤ 2ğ‘¤ğ‘– ,ğ‘¡ + max(2ğ‘ğ‘ƒ ,ğ‘– ,ğ‘¡ )

ğ‘¡

ğ‘—ğ‘¡

ğ‘—

â‰¤ 2 max(ğ‘¤ğ‘–,ğ‘¡ + ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ ) ğ‘— ğ‘–,ğ‘—

â‰¤ 2 max ğ‘¤ğ‘–,ğ‘¡ + max ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡

(52)

ğ‘—

ğ‘–

ğ‘–,ğ‘—

From Algorithm 3, note that

ğ‘¤ğ‘–,ğ‘¡ = ğ‘„

âˆ’1

ğ‘‡

ğ‘‡

ğ‘ 0,ğ‘¥ğ‘–,ğ‘¡ ğ‘‹ ğ‘‹ğ‘–,ğ‘¡ ğ‘¥

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

ğ›¿ .
2ğ‘›ğ‘‡

Similarly,

ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡ = ğ‘„

âˆ’1

ğ‘—

ğ‘ 0,ğ‘¥ğ‘–,ğ‘¡ Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ğ‘¥ğ‘‡

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘–,ğ‘¡

ğ›¿
ğ‘›
2 |ğ‘ƒğ‘— |ğ‘‡

âˆ’1

ğ‘‡

ğ‘‡

We will first bound ğ‘¥ğ‘–,ğ‘¡ ğ‘‹ ğ‘‹ğ‘–,ğ‘¡ ğ‘¥ .

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

âˆ’1

âˆ’1

ğ‘‡
ğ‘¥ğ‘–,ğ‘¡ ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘‡
ğ‘¥

â‰¤ ||ğ‘¥ğ‘–,ğ‘¡ ||ğœ†max

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

1 = ||ğ‘¥ğ‘–,ğ‘¡ ||
ğœ†min (ğ‘‹ğ‘‡ ğ‘‹ğ‘–,ğ‘¡ )
ğ‘–,ğ‘¡

1

â‰¤

(53)

ğœ†min (ğ‘‹ğ‘‡ ğ‘‹ğ‘–,ğ‘¡ )

ğ‘–,ğ‘¡

where inequality 53 is due to ||ğ‘¥ğ‘–,ğ‘¡ || â‰¤ 1 for all arms ğ‘– and all

timesteps ğ‘¡.

Using similar logic:

âˆ’1

1

ğ‘¥ğ‘–,ğ‘¡ Xğ‘‡ Xğ‘ƒ ,ğ‘¡

ğ‘‡
ğ‘¥

â‰¤

.

(54)

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘–,ğ‘¡

ğœ†min Xğ‘‡ Xğ‘ƒ ,ğ‘¡

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

AAMAS â€™22, May 9â€“13, 2022, Online

Let ğºğ‘–,ğ‘¡ be the number of observations of arm ğ‘– with context ğ‘– drawn uniformly from the distribution for arm ğ‘– prior to timestep ğ‘¡. Similarly, let Gğ‘ƒ ,ğ‘¡ be the number of observations of group ğ‘ƒğ‘— with
ğ‘—
context drawn uniformly from the distribution for group ğ‘ƒğ‘— prior
ğ‘‡
to timestep ğ‘¡. Let ğ¿ > maxğ‘¡ ğœ†max ğ‘¥ ğ‘¥ğ‘–,ğ‘¡ .
ğ‘–,ğ‘¡
For any ğ›¼ âˆˆ [0, 1], using the superadditivity of minimum eugenvectors for positive semi-definite matrices, we get:

E ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) â‰¥ ğºğ‘–,ğ‘¡ ğœ†minğ‘–,ğ‘‘ ğ‘‘

ğºğ‘–,ğ‘¡

â‰¥

.

(55)

ğ‘‘

Similarly,

ğºğ‘ƒ ,ğ‘¡

E ğœ†ğ‘šğ‘–ğ‘› (Xğ‘‡ Xğ‘ƒ ,ğ‘¡ ) â‰¥

ğ‘—
ğœ†min .

(56)

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘‘

ğ‘ƒ ğ‘— .ğ‘‘

Equation 55 implies that:

ğºğ‘–,ğ‘¡

Pr ğœ†min (ğ‘‹ğ‘–,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) â‰¤ ğ›¼

ğœ†min

ğ‘–,ğ‘‘

ğ‘¥ğ‘–,ğ‘¡

ğ‘‘

â‰¤ Pr ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) â‰¤ ğ›¼E ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) (57)
ğ‘¥ğ‘–,ğ‘¡

â‰¤ Pr ğœ†min (ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ ) â‰¤ ğ›¼ğœ†min E ğ‘‹ğ‘–ğ‘‡,ğ‘¡ ğ‘‹ğ‘–,ğ‘¡ (58)
ğ‘¥ğ‘–,ğ‘¡

âˆ’(1 âˆ’ ğ›¼)2ğœ†min

E

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

â‰¤ ğ‘‘ exp

(59)

2ğ¿

âˆ’(1 âˆ’ ğ›¼)2E

ğœ†min

ğ‘‡
ğ‘‹ ğ‘‹ğ‘–,ğ‘¡

ğ‘–,ğ‘¡

â‰¤ ğ‘‘ exp

(60)

2ğ¿

âˆ’(1 âˆ’ ğ›¼ )2 ğºğ‘‘ğ‘–,ğ‘¡ ğœ†minğ‘–,ğ‘‘

â‰¤ ğ‘‘ exp

(61)

2ğ¿

where inequality 57 comes from inequality 55, inequality 58 is due to

Jensenâ€™s inequality, inequality 59 is due to a matrix Chernoff Bound,

inequality 60 is due to Jensenâ€™s inequality, and inequality 61 is due

to inequality 55. After rearranging inequality 61, with probability

1 âˆ’ ğ›¿,

ğœ† (ğ‘‹ğ‘‡ ğ‘‹ ) â‰¥ ğ›¼ ğºğ‘–,ğ‘¡ ğœ†

(62)

ğ‘šğ‘–ğ‘› ğ‘–,ğ‘¡ ğ‘–,ğ‘¡

minğ‘–,ğ‘‘

ğ‘‘

when

ğ¿

1

ğºğ‘–,ğ‘¡ â‰¥ ğ‘‘

ln + ln ğ‘‘ .

(63)

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘–,ğ‘‘

Using similar logic with probability 1 âˆ’ ğ›¿, we have

ğºğ‘ƒ ,ğ‘¡

ğœ†min Xğ‘‡ Xğ‘ƒ ,ğ‘¡ â‰¥ ğ›¼

ğ‘—
ğœ†min

(64)

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ‘‘

ğ‘ƒ ğ‘— ,ğ‘‘

when

ğ¿

1

Gğ‘ƒ ,ğ‘¡ â‰¥ ğ‘‘

ln + ln ğ‘‘ .

(65)

ğ‘—

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘ƒ ğ‘— ,ğ‘‘

Using a multiplicative Chernoff bound for a fixed timestep ğ‘¡ with probability 1 âˆ’ ğ›¿ â€², the number of exploitation rounds prior to rount ğ‘¡ will satisfy

âˆšï¸„

âˆ‘ï¸

2 âˆ‘ï¸

ğºğ‘¡ âˆ’ ğ‘ğ‘¡â€² â‰¤ ln â€² ğ‘ğ‘¡â€² .

(66)

ğ‘¡â€²<ğ‘¡ ğ›¿ ğ‘¡â€²<ğ‘¡

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson

For a fixed ğ‘– and timestep ğ‘¡, using a multiplicative Chernoff bound for a fixed timestep ğ‘¡ with probability 1 âˆ’ ğ›¿ â€², the number of ex-
ploitation rounds for arm ğ‘– prior to round ğ‘¡ will satisfy

âˆšï¸‚

ğºğ‘¡

2 ğºğ‘¡

ğºğ‘–,ğ‘¡ âˆ’ â‰¤ ln â€²

(67)

ğ‘›

ğ›¿ğ‘›

Similarly, for a fixed group ğ‘ƒğ‘— and timestep ğ‘¡ with probability 1âˆ’ğ›¿ â€², the number of exploration rounds for group ğ‘ƒğ‘— prior to round
ğ‘¡ will satisfy

âˆšï¸„

ğºğ‘¡

2 ğºğ‘¡

ğºğ‘ƒ ,ğ‘¡ âˆ’

â‰¤

ğ‘— ğ‘›/|ğ‘ƒ |

ln ğ›¿ â€² ğ‘›/|ğ‘ƒ |

(68)

ğ‘—

ğ‘—

where |ğ‘ƒğ‘— | is the size of group ğ‘ƒğ‘— .

Combining inequality 66 and inequality 67, with probability

1 âˆ’ 2ğ›¿ â€² for a fixed arm ğ‘– and timestep ğ‘¡ , if ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² â‰¥ 36ğ‘› ln2 ğ›¿2â€² we

have

ğºğ‘–,ğ‘¡ âˆ’ ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² â‰¤ ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² . (69)

ğ‘›

2ğ‘›

Similarly, combining inequality 66 and inequality 68 with probability at least 1 âˆ’ 2ğ›¿ â€² for a fixed group ğ‘ƒğ‘— and fixed timestep ğ‘¡:

ğº

âˆ’

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² â‰¤

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² .

(70)

ğ‘–,ğ‘¡ ğ‘›/|ğ‘ƒ ğ‘— |

2ğ‘›/|ğ‘ƒğ‘— |

Therefore inequality 62 holds with probability 1 âˆ’ ğ›¿ â€² when

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² â‰¥ ğ‘‘

ğ¿

1 ln + ln ğ‘‘ . (71)

2ğ‘›

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘–,ğ‘‘

Similarly, inequality 64 holds with probability 1 âˆ’ ğ›¿ â€² when

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€² â‰¥ ğ‘‘

ğ¿

1 ln + ln ğ‘‘ . (72)

2ğ‘›/|ğ‘ƒğ‘— |

(1 âˆ’ ğ›¼)2ğœ†min

ğ›¿

ğ‘–,ğ‘‘

ğ‘›
Therefore, since |ğ‘ƒğ‘— | < ğ‘›, the number of rounds after which we have sufficient samples such that the estimators are well-concentrated

is

3/2

ğ‘‘ğ‘›ğ‘šğ¿

2

ğ‘‡1 = Î˜ min

2
ln

+ ln ğ‘‘

(73)

ğ‘ ğœ†min

ğ›¿

ğ‘,ğ‘‘

where ğ‘ âˆˆ [ğ‘›] âˆª ğ‘ƒ1 âˆª Â· Â· Â· âˆª ğ‘ƒğ‘š.

Also note that for any ğ‘¡ > ğ‘‡1 we have:

âˆ‘ï¸

ğ‘‘ğ‘›ğ‘šğ¿

2

ğ‘ğ‘¡â€² = Î© min

2
ln

â€² + ln ğ‘‘

.

(74)

ğ‘¡â€²<ğ‘¡ ğ‘ 2 minğ‘,ğ‘‘ ğ›¿

Now we can bound the third term in equation 50.
âˆ‘ï¸ regret (ğ‘¡)
ğ‘¡ : ğ‘¡ is an exploit round and ğ‘¡ >ğ‘‡1

âˆ‘ï¸

â‰¤2

max ğ‘¤ğ‘–,ğ‘¡ + max ğ‘ğ‘ƒ ,ğ‘–,ğ‘¡

(75)

ğ‘—

ğ‘–

ğ‘–,ğ‘—

ğ‘¡ >ğ‘‡1

âˆ‘ï¸

ğ›¿

â‰¤ 2 mğ‘–ax ğ‘„ğ‘ 0,ğœ†max (ğ‘‹ğ‘‡ ğ‘‹ğ‘–,ğ‘¡ )âˆ’1 2ğ‘›ğ‘‡

ğ‘–,ğ‘¡

ğ‘¡ >ğ‘‡1

+ mğ‘—ax ğ‘„ğ‘ 0,ğœ†max ( Xğ‘‡ Xğ‘ƒ ,ğ‘¡ )âˆ’1 ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ›¿
ğ‘›
2 |ğ‘ƒğ‘— |ğ‘‡

Group Fairness in Bandits with Biased Feedback

AAMAS â€™22, May 9â€“13, 2022, Online

âˆ‘ï¸

ğ›¿

â‰¤2 ğ‘„

ğ‘¡ >ğ‘‡1

1
ğ‘ 0,
minğ‘– ğœ†min (ğ‘‹ğ‘‡ ğ‘‹ğ‘–,ğ‘¡ ) ğ‘–,ğ‘¡

2ğ‘›ğ‘‡

+ğ‘„
ğ‘

1

0,

minğ‘— ğœ†min (Xğ‘‡

X) ğ‘ƒ ,ğ‘¡

ğ‘ƒ ğ‘— ,ğ‘¡ ğ‘—

ğ›¿
ğ‘›
2 |ğ‘ƒğ‘— |ğ‘‡

âˆ‘ï¸

ğ›¿

â‰¤2 ğ‘„

2ğ‘›ğ‘‡

ğ‘¡ >ğ‘‡1 ğ‘ 0,

1

ğºğ‘–,ğ‘¡

minğ‘– ğ›¼

ğœ†min

ğ‘‘

ğ‘–,ğ‘‘

2ğ‘›ğ‘‡
ln

=ğ‘‚ ğ‘‘

ğ›¿

âˆšï¸‚

âˆ‘ï¸

ğ‘›

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğœ†min
ğ‘–,ğ‘‘ ğ‘¡ >ğ‘‡1

ğ‘¡â€²<ğ‘¡ ğ‘ğ‘¡â€²

2ğ‘›ğ‘‡

âˆšï¸‚

ln

=ğ‘‚ ğ‘‘

ğ›¿

âˆ‘ï¸

ğ‘›

+

ğ›¿

â€²
ğ‘‡

(79)

minğ‘– ğœ†min

ğ‘¡ 2/3

ğ‘–,ğ‘‘ ğ‘¡ >ğ‘‡1

2ğ‘›ğ‘‡

ln

= ğ‘‚ ğ‘‘ğ‘›

ğ›¿

âˆ‘ï¸

1

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğœ†min

ğ‘¡ 1/3

ğ‘–,ğ‘‘ ğ‘¡ âˆˆ [ğ‘‡1,ğ‘‡ ]

2ğ‘›ğ‘‡

ln

= ğ‘‚ ğ‘‘ğ‘›

ğ›¿

ğ‘‡

2/3

+

ğ›¿

â€²
ğ‘‡

(80)

minğ‘– ğœ†min ğ‘–,ğ‘‘

where inequality 75 is due to equation 52, inequality 76 is due

to equation 55 and equation 56, inequality 77 is due to a Cher-

+ğ‘„

ğ›¿

+

3ğ›¿

â€²
ğ‘‡

(76)

ğ‘›

noff bound, inequality 78 is due to the fact that

< ğ‘› and

ğ‘›
2ğ‘‡

minğ‘— |ğ‘ƒğ‘— |

|ğ‘ƒğ‘— |
1
ğ‘ 0,

minğ‘— Gğ‘ƒ ,ğ‘¡ â‰¥ ğ‘šğ‘–ğ‘›ğ‘–ğºğ‘–,ğ‘¡ , and equation 79 is due to equation 51. ğ‘—

G

ğ‘ƒ ğ‘— ,ğ‘¡

minğ‘— ğ›¼

ğœ†min

Combining equation 50, equation 51, equation 74, and equa-

ğ‘‘ ğ‘ƒ ğ‘— ,ğ‘‘

tion 80 and setting ğ›¿ â€² = min( 3ğ‘›1ğ‘‡ , ğ‘‡ 11/3 ) we get Theorem 3. â–¡

âˆ‘ï¸ â‰¤2

2ğ‘›ğ‘‡
ln
ğ›¿

C ADDITIONAL EXPERIMENTS

ğ‘¡ >ğ‘‡1

ğºğ‘–,ğ‘¡

minğ‘– ğ›¼

ğœ†min

ğ‘‘

ğ‘–,ğ‘‘

Additionally to the experiments found in Section 5.2, we ran the

ğ‘›
2 ğ‘šğ‘–ğ‘› ğ‘— |ğ‘ƒ ğ‘— | ğ‘‡

ln

+

ğ›¿

Gğ‘ƒ ğ‘— ,ğ‘¡

+

6ğ›¿

â€²
ğ‘‡

minğ‘– ğ›¼

ğœ†min

ğ‘‘

ğ‘–,ğ‘‘

âˆ‘ï¸ â‰¤2 2
ğ‘¡ >ğ‘‡1

2ğ‘›ğ‘‡
ln
ğ›¿

ğºğ‘–,ğ‘¡

minğ‘– ğ›¼

ğœ†min

ğ‘‘

ğ‘–,ğ‘‘

+

6ğ›¿

â€²
ğ‘‡

following experiments and found no interesting effects:

(a) Varying the range in which coefficients are chosen (between

(77)

[0,c]) while setting the total budget ğ‘‡ = 1000, the number of

arms ğ‘› = 10, the error mean ğœ‡ = 10, the number of sensitive

arms equal to 5, and the context dimension ğ‘‘ = 2 (Figures 4a

and 5a).

(78)

(b) Varying the context dimension while setting the total budget

ğ‘‡ = 1000, the number of arms ğ‘› = 10, the error mean ğœ‡ = 10,

âˆ‘ï¸ =ğ‘‚
ğ‘¡ >ğ‘‡1

2ğ‘›ğ‘‡
ln
ğ›¿
ğ‘‘

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğºğ‘–,ğ‘¡ ğœ†min ğ‘–,ğ‘‘

2ğ‘›ğ‘‡
ln

=ğ‘‚ ğ‘‘

ğ›¿

âˆšï¸„

âˆ‘ï¸

1

+

ğ›¿

â€²
ğ‘‡

minğ‘– ğœ†min

minğ‘– ğºğ‘–,ğ‘¡

ğ‘–,ğ‘‘ ğ‘¡ >ğ‘‡1

and the number of sensitive arms equal to 5 (Figures 4b and 5b). (c) Varying probability ğ›¿ while setting the total budget ğ‘‡ = 1000,
the number of arms ğ‘› = 10, the error mean ğœ‡ = 10, the number of sensitive arms equal to 5, and the context dimension ğ‘‘ = 2
(Figures 4c and 5c).

AAMAS â€™22, May 9â€“13, 2022, Online

Candice Schumann, Zhi Lang, Nicholas Mattei, and John P. Dickerson

(a) ğ‘› = 10, ğœ‡ = 10, number of sensitive arms = 5

(b) ğ‘‡ = 1000, ğœ‡ = 10, number of sensitive arms = 5

(c) ğ‘› = 10, ğ‘‡ = 1000, number of sensitive arms = 5

(d) legend

Figure 4: Percentage of total arm pulls that were pulled using sensitive arms.

(a) ğ‘› = 10, ğœ‡ = 10, number of sensitive arms = 5

(b) ğ‘‡ = 1000, ğœ‡ = 10, number of sensitive arms = 5

(c) ğ‘› = 10, ğ‘‡ = 1000, number of sensitive arms = 5

(d) legend

Figure 5: Regret for synthetic experiments. The solid lines are regret given the rewards received from pulling the arms (including the group bias). The dashed lines is the true regret (without the group bias).

