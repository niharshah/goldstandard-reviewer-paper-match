1
Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring
Hirofumi Inaguma, Student Member, IEEE, Yosuke Higuchi, Student Member, IEEE, Kevin Duh, Member, IEEE Tatsuya Kawahara, Fellow, IEEE and Shinji Watanabe, Senior Member, IEEE

arXiv:2109.04411v1 [eess.AS] 9 Sep 2021

Abstract‚ÄîThis article describes an efÔ¨Åcient end-to-end speech translation (E2E-ST) framework based on non-autoregressive (NAR) models. End-to-end speech translation models have several advantages over traditional cascade systems such as inference latency reduction. However, conventional AR decoding methods are not fast enough because each token is generated incrementally. NAR models, however, can accelerate the decoding speed by generating multiple tokens in parallel on the basis of the token-wise conditional independence assumption. We propose a uniÔ¨Åed NAR E2E-ST framework called Orthros, which has an NAR decoder and an auxiliary shallow AR decoder on top of the shared encoder. The auxiliary shallow AR decoder selects the best hypothesis by rescoring multiple candidates generated from the NAR decoder in parallel (parallel AR rescoring). We adopt conditional masked language model (CMLM) and a connectionist temporal classiÔ¨Åcation (CTC)-based model as NAR decoders for Orthros, referred to as Orthros-CMLM and Orthros-CTC, respectively. We also propose two training methods to enhance the CMLM decoder. Experimental evaluations on three benchmark datasets with six language directions demonstrated that Orthros achieved large improvements in translation quality with a very small overhead compared with the baseline NAR model. Moreover, the Conformer encoder architecture enabled large quality improvements, especially for CTC-based models. Orthros-CTC with the Conformer encoder increased decoding speed by 3.63√ó on CPU with translation quality comparable to that of an AR model.
Index Terms‚ÄîEnd-to-end speech translation, nonautoregressive decoding, rescoring
I. INTRODUCTION
B REAKING language barriers by using machines is an ultimate goal for international communications. Automatic speech translation (ST) has been studied for this purpose for decades [1]‚Äì[4]. Cascade approaches combining automatic speech recognition (ASR) and machine translation (MT) systems have been the de facto standard, but the research paradigm is shifting to end-to-end speech translation (E2E-ST) models thanks to advances in deep learning [5]‚Äì[7]. End-toend models have several attractive properties, such as avoiding ASR error propagation and low-latency decoding. However, the translation quality of E2E models still lags behind that
H. Inaguma and T. Kawahara are with the Graduate School of Informatics, Kyoto University, Kyoto 606-8501, Japan (e-mail: {inaguma, kawahara}@sap.ist.i.kyoto-u.ac.jp).
Y. Higuchi is with Waseda University, Tokyo 162-0042, Japan (e-mail: higuchi@pcl.cs.waseda.ac.jp).
K. Duh is with HLTCOE, Johns Hopkins University, Baltimore, MD 212112840, USA (e-mail: kevinduh@cs.jhu.edu).
S. Watanabe is with Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213-3891, USA (e-mail: shinjiw@cmu.edu).

of cascade systems when additional resources are available, although the gap is closing [8]‚Äì[10].
Various solutions have been proposed to bridge the quality gap between cascade and E2E models, such as multi-task learning with auxiliary tasks [6], [7], pre-training [7], [11]‚Äì [13], knowledge distillation [14], [15], and semi-supervised training [16], [17]. Most studies on E2E systems have focused on autoregressive (AR) models, which generate a target sequence from left to right. However, the decoding speed is not fast enough for real-world applications because of the incremental update of decoder states. This slows the decoding speed, especially when the Transformer architecture [18] is adopted because of the self-attention operation to all past tokens in the decoder at each generation step.
Recently, non-AR (NAR) models have attracted attention to increase the decoding speed by discarding the conditional dependency of outputs in AR models. Gu et al. [19] proposed the Ô¨Årst single-step NAR MT model, but it sacriÔ¨Åces translation quality. Various models have been proposed to address this issue; improved single-step NAR models [20]‚Äì[28], iterativereÔ¨Ånement models [29]‚Äì[32], latent alignment models [33]‚Äì [36], insertion-based models [37]‚Äì[39], and latent variable models [40]‚Äì[43]. NAR models have been successfully extended to other tasks such as text-to-speech synthesis [44], [45], ASR [46]‚Äì[48] and ST [49], [50].
However, what differentiates the NAR ST task from other NAR tasks is many-to-many non-monotonic mapping. In other words, source inputs, even though they correspond to the same word sequence, can vary signiÔ¨Åcantly depending on speaker attributes, speaking rate, recording conditions and so on. These problems do not exist in text-based tasks because discrete tokens represent the input instead of continuous signals. It is also challenging to determine the target length from the speech in advance because it includes many silent frames and the input length is much longer than that of text.
To increase the decoding speed for the E2E-ST task, we propose an efÔ¨Åcient uniÔ¨Åed NAR framework called Orthros, which introduces an auxiliary shallow AR decoder on top of the shared speech encoder (see Fig. 1). The AR decoder is jointly trained with the NAR decoder and is used to rescore multiple candidates of different lengths generated from the NAR decoder, referred to as parallel AR rescoring. This is based on an observation that sequence-level scores from the NAR decoder are not sufÔ¨Åcient for accurately selecting the best candidate. Because outputs from the NAR decoder can be fed to the AR decoder in parallel, the NAR decoding is still maintained. Because a shallow AR decoder works for

2

    

















   

       

















  

















   

















    

   

      

      





  

  



 

   





  

  



 

   





  

  



 

   

 

 

Fig. 1: Overview of Orthros

the rescoring purpose only, additional computation cost is minimal.
We investigate the conditional masked language model (CMLM) [30] and connectionist temporal classiÔ¨Åcation (CTC)-based model [51] as NAR decoders for Orthros, referred to as Orthros-CMLM and Orthros-CTC, respectively. The CMLM is an iterative reÔ¨Ånement model while the CTCbased model is a single-step latent alignment model. However, any other NAR decoder topology can be used for Orthros in theory as long as multiple candidates can be generated. To enhance the training of the CMLM decoder, we further propose multi-mask training (MMT) and joint training with an auxiliary text-input NAR MT task. With these methods, the CMLM decoder can take decoder inputs from multiple views per sample for improving translation quality. To enhance the encoder representation, we also investigated the Conformer encoder [52].
Experimental evaluations on three benchmark corpora, including six language directions, are conducted to show that Orthros signiÔ¨Åcantly improves the BLEU scores with small additional latency. We show that the rescoring is more effective than reÔ¨Åning predictions through further iterations. We also argue that joint training with the auxiliary shallow AR decoder improves the BLEU scores in most cases. MMT and joint training with the NAR MT task also boosts the BLEU score for the Transformer encoder while maintaining the decoding speed. The Conformer encoder improves BLEU scores across models, especially for CTC-based models. We show that Orthros-CTC with the Conformer encoder achieved the best BLEU scores among NAR models. Compared with the AR model, it achieved 3.63√ó faster decoding speed on a CPU with comparable BLEU scores.
The contributions of this article are summarized as follows:1
‚Ä¢ We propose Orthros and show its effectiveness with two NAR models, i.e., CMLM and CTC-based model as examples.
‚Ä¢ We propose two enhanced training methods for the CMLM.
‚Ä¢ We thoroughly compare AR and NAR models on the basis of both Transformer and Conformer encoder architectures.
‚Ä¢ We present strong AR baselines with and without sequence-level knowledge distillation (SeqKD) [53],
1This study is an extension of our previous study [49], in which only the Ô¨Årst item was investigated.

which will be helpful for future E2E-ST studies. ‚Ä¢ We conducted various analyses to evaluate the effective-
ness of Orthros in terms of model capacity, robustness against long-form speech, and searchability.

II. BACKGROUND
In this section, we review AR and NAR E2E-ST models. Let X = (x1, . . . , xU ) denote input speech features in a source language, Y = (y1, . . . , yN ) denote the target translation text, where U and N denote the input length and output length, respectively.

A. AR sequence model

Sequence-to-sequence models, so-called encoder-decoder models, are typically using AR sequence models such as the Transformer [18]. An AR model factorizes a conditional probability of Y given X into a chain of conditional probabilities from left to right as follows:

N

P (Y |X) = PAR(yi|y<i, X),

(1)

i=1

where PAR is a probability density function of the AR model, and y<i are all previous tokens before the i-th token. The parameters are updated with a cross-entropy (CE) loss LAR formulated as

LAR = ‚àí log PAR(Y |X)
N
= ‚àí log PAR(yi|y<i, X).
i=1
When using the Transformer architecture, training can be carried out efÔ¨Åciently by feeding all ground-truth tokens to the decoder in parallel, i.e., teacher-forcing [54]. During inference, however, beam search is used as a heuristic to Ô¨Ånd the most plausible sequence. Because this incrementally expands a preÔ¨Åx of each hypothesis token by token, the decoding speed is typically not fast enough. SpeciÔ¨Åcally, the Transformer decoder slows the speed because it performs self-attention to all past tokens generated thus far. The decoding complexity is O(U N ).

B. NAR sequence model

To increase the decoding of AR models by generating all tokens in a target sequence in parallel, the conditional independence for each token position in the output probability is assumed with NAR models. The conditional probability in Eq. (1) is decomposed as

N

P (Y |X) = PNAR(yi|X),

(2)

i=1

where PNAR is a probability density function of the NAR model. Because PNAR is not conditioned on y<i, this formulation enables the generation of a sequence with a single iteration, which can achieve large increases in decoding speed. However, such a strong assumption degrades translation quality because of the multimodality problem, in which multiple

3

correct translations are predicted given the same source sentence [19]. SeqKD is an effective method for mitigating this problem by transforming reference translations in the training data into a more deterministic form by using a teacher AR model [55].
To relax the conditional independence assumption, iterative reÔ¨Ånement methods [29] have also been studied by modifying Eq. (2) to a chain of T iterations as

T
P (Y |X) = PNAR(Y (t)|Y (t‚àí1), X)

t=1

T
=

N (t)
PNAR(yi(t)|Y (t‚àí1), X) ,

t=1 i=1

where Y (t) = (y1(t), ¬∑ ¬∑ ¬∑ , yN(t)(t) ) is a sequence of tokens at the tth iteration, where N (t) is the output length at t-th iteration and can be changed in some models [37]‚Äì[39]. Although iterative reÔ¨Ånement methods slow the decoding speed of pure NAR models, they can achieve better translation quality in general and Ô¨Çexibly control the speed by changing T .

1) CTC: CTC is a loss function for a single-step latent alignment model, which eliminates the necessity of framelevel supervision for learning input-output mapping [51]. A linear projection layer is stacked on the encoder to generate a probability distribution PCTC so the model is typically a decoder-free architecture. The conditional independence per encoder frame is assumed with CTC, and CTC marginalizes posterior probabilities of all possible alignment paths efÔ¨Åciently with the forward-backward algorithm. The gap in the sequence lengths between the input and output is Ô¨Ålled by introducing blanks labels. The CTC loss LCTC is deÔ¨Åned as the negative log-likelihood:

LCTC = ‚àí log PCTC(Y |X).

During inference, CTC-based models can be regarded as fully NAR models if the best label class is taken from the vocabulary at every encoder frame, which is known as greedy search. The decoding complexity is O(U ). CTC assumes the length of X to be longer than that of Y (i.e., |X| ‚â• |Y |), so previous studies using CTC for the text-input NAR MT task adopted the upsampling technique to expand the input sequence length [34]‚Äì[36]. However, this is not necessary for speech-to-text generation tasks because |X| is generally much longer than |Y |. Instead, we usually compress input sequence lengths for the E2E-ST task [56]‚Äì[59]. Another advantage of CTC for NAR models is that an explicit target length prediction is not necessary. It can be obtained as a by-product after collapsing frame-level outputs.
2) CMLM: The CMLM [30] is an iterative reÔ¨Ånementbased model with a token in-Ô¨Ålling adopted in Bidirectional Encoder Representations from Transformers (BERT) [60]. During inference, CMLM adopts the Mask-Predict algorithm, which alternates Mask and Predict steps at every iteration and reÔ¨Ånes predictions through T iterations. Therefore, the decoding complexity is O(U T ).

Inference Let YÀÜ (t) and YÀÜ (t) be masked and observed to-

mask

obs

kens in the prediction YÀÜ (t) at the t-th iteration (0 ‚â§ t ‚â§ T ),

respectively. Given a target length NÀÜ estimated using a length

predictor stacked on the encoder, the CMLM starts generation from a placeholder Ô¨Ålled with a [MASK] token at all NÀÜ

positions (t = 0). In the Predict step, the most plausible token

is selected from the vocabulary according to the probability

of

the

CMLM

PCMLM

at

each

masked

position

i

in

YÀÜ (t‚àí1)
mask

as

yÀÜi(t)

=

argmax PCMLM(yi

= w|YÀÜ (t), X),
obs

w‚ààV

p(i,tC)MLM

‚Üê

max PCMLM(yi

=

w|YÀÜ (t), X),
obs

w‚ààV

where V is the output vocabulary, and p(i,tC)MLM is a score of the i-th token at the t-th iteration. Note that p(i,tC)MLM is updated for masked positions only.
In the Mask step, k(t) tokens having the lowest conÔ¨Ådence scores in the previous prediction YÀÜ (t) are replaced with
[MASK], where k(t) is a linear decay function deÔ¨Åned as

k(t) = NÀÜ ¬∑ T ‚àí t . t

As the target length must be determined before starting the

token generation, length parallel decoding (LPD) [23], [30]

is typically used by generating multiple length candidates in

parallel. The l length candidates are used by selecting top-l

classes from a length predictor.2 After T iterations, a candidate

having

the

highest

sequence-level

scores

1 NÀÜ

selected as the best translation output.

i log Pi(,tC)MLM is

Training The training objective of the CMLM is formulated as a CE loss calculated at masked positions as

LCMLM = ‚àí

log PCMLM(y|Yobs, X),

(3)

y‚ààYmask

where Ymask ‚äÇ Y are partially masked ground-truth tokens, and Yobs = Y \ Ymask. The number of masked tokens is sampled from a uniform distribution U(1, N ), and the positions are also determined randomly.
A length predictor is implemented as a linear classiÔ¨Åer and trained to predict the ground-truth target sequence length N given X as

Llp = ‚àí log Plp(N |X),

where Llp is a length prediction loss. Unlike the text-input MT task, where the encoder output corresponding to a spacial token [LENGTH] is used as an input to the linear classiÔ¨Åer [30], time-averaged encoder outputs are used for the E2E-ST task. The total objective Ltotal is formulated as follows:

Ltotal = LCMLM + ŒªlpLlp,

where Œªlp is a weight for the length-prediction loss.

2We use top-l classes instead of [ÀÜl ‚àí ‚àÜ, ÀÜl + ‚àÜ] centering on the best class ÀÜl as in [23].

4

3) Semi-autoregressive Training: Semi-AR training (SMART) is an improved training method to mitigate the gap in the behaviors of the CMLM between training and test [31]. To resemble the test-time behavior based on the Mask-Predict algorithm during training, the decoder input is replaced with the model prediction at each training step. To obtain the prediction, the most plausible token is taken at all N positions by adopting the same masking as the original CMLM training while the gradients are truncated. Part of the resultant tokens YÀÜ = (yÀÜ1, . . . , yÀÜN ) are masked out to generate a new decoder input YÀÜobs, which is fed to the decoder again to calculate the CE loss at all positions as
LCMLM = ‚àí log PCMLM(y|YÀÜobs, X).
y‚ààY
During inference, the Mask-Predict algorithm is used, but tokens at all positions are updated at every iteration, unlike the original CMLM training. Although SMART slightly slows the training speed by doubling the forward pass, it does not increase the decoding cost during inference.
4) Mask-CTC: Since the CMLM is designed for iterative reÔ¨Ånement starting from [MASK] tokens at all positions, the translation quality at the early decoding iterations is likely to be poor. To provide useful contexts from a single-step NAR E2E-ST model based on CTC, Mask-CTC initializes the decoder input of the CMLM with a greedy CTC prediction [47]. The CTC module is attached to the top encoder layer and trained jointly with the CMLM decoder. The less conÔ¨Ådent CTC predictions, the probabilities of which are smaller than pthres, are replaced with [MASK] tokens. However, unlike Mask-CTC for the ASR task, masked positions change depending on the score at each iteration because of the nonmonotonic sequence generation in the ST task [48]. To keep from modifying most CTC outputs by using the original MaskPredict algorithm, the restricted Mask-Predict algorithm was proposed [48], where k(t) is truncated by the number of masked tokens at t = 0. We refer to this type of CMLM as CTC-CMLM in this paper.
Because CTC can also be used as a length predictor, a separate length-prediction layer or LPD is not necessary. The total training objective is formulated by interpolating LCMLM and LCTC as
Ltotal = (1 ‚àí ŒªCTC)LCMLM + ŒªCTCLCTC,
where ŒªCTC is a CTC loss weight.
C. Conformer
The Conformer is a Transformer-based encoder architecture augmented by the convolution module in each block [52]. It was introduced in the ASR task to capture global and local features by self-attention and convolution, respectively. Because this property is compatible with speech, its effectiveness has been demonstrated in various speech-related tasks [61], including E2E-ST. Each Conformer block introduces an additional convolution module right after the self-attention module. An additional position-wise feed-forward network (FFN) is introduced right before the self-attention module, following the

Macaron-Net [62]. Relative positional encoding is also used in each self-attention module. In our study, we investigated how the Conformer impacts the translation quality of both AR and NAR models. Although the Conformer encoder introduces additional parameters, it is not a bottleneck of decoding speed.

III. PROPOSED FRAMEWORK: ORTHROS
In this section, we propose a uniÔ¨Åed NAR E2E-ST framework, Orthros, which enhances the NAR decoder by joint training and parallel rescoring with an auxiliary shallow AR decoder.

A. Model architecture
An overview of Orthros is presented in Fig. 1. Orthros has three main components: speech encoder, NAR decoder, and auxiliary shallow AR decoder. The motivation to introduce the AR decoder is based on an observation that sequencelevel scores obtained from the NAR decoder are not suitable for selecting the best translation from multiple candidates. This is because of the conditional independence assumption made with the NAR decoder, which makes the NAR decoder not fully leverage the effectiveness of generating multiple candidates.
In Orthros, the speech encoder is shared between the NAR and AR decoders, which greatly reduces the model size and computational overhead compared with using another AR model such as that proposed by Gu et al. [19]. For models except for a CTC-based one, a length predictor is also stacked on the speech encoder. The entire architecture is jointly trained.
For Orthros, we mainly focus on the CMLM as the NAR decoder because we can control the decoding speed by changing the number of iterations, and it is widely used in the MT literature [26], [27], [30], [31], [63]‚Äì[68]. We refer to this model as Orthros-CMLM. Because of powerful ConformerCTC, we also investigated equipping the AR decoder of with CTC, referred to as Orthros-CTC. Orthros can be applied to any NAR decoder topology as long as multiple candidates can be generated.

B. Inference: parallel AR rescoring

During inference, we use sequence-level scores from the AR

decoder to select the most plausible translation after generating

multiple candidates with the NAR decoder. Because we can

feed tokens at all positions to the AR decoder simultaneously

in Eq. (1), we refer to this as parallel AR rescoring.

When using the CMLM as the NAR decoder, we use

the Mask-Predict algorithm described in Section II-B2 to

generate l candidates through T iterations. When using a

CTC-based model as the NAR decoder, we use left-to-right

beam search with a beam width of l. Although this is not

purely NAR decoding, there is no matrix multiplication with

weight parameters. After the NAR decoding, we immediately

feed the resulting tokens to the auxiliary AR decoder. We use

the

sequence-level

log

probabilities

1 NÀÜ

i log PAR(yi|y<i, X)

obtained from the AR decoder to select the best translation

5

among the l candidates. Note that scores from the NAR decoder are not used for the rescoring.3
For Orthros-CMLM, this rescoring step corresponds to performing one more decoding iteration. However, as shown in Section V, parallel AR rescoring with T ‚àí 1 is more effective than the Mask-Predict with T without the rescoring. A shallow AR decoder is used for rescoring purpose, so the additional decoding cost is much smaller than a single iteration of the CMLM decoder.

C. Orthros-CMLM
In this section, we present Orthros-CMLM, which uses the CMLM as the NAR decoder in Orthros. We also propose two enhanced training methods for the CMLM.

1) MMT: In the training of the CMLM, the decoder observes a single set of tokens per sample, and the CE loss is calculated at only masked positions. This is not as data-efÔ¨Åcient as AR models, especially when training data are sparse such as in the E2E-ST task. To mitigate this problem, we introduce MMT by calculating CE losses for M forward passes with different Yobs = Y \ Ymask. At each forward pass, a random mask for Ymask is sampled independently. Accordingly, the CE loss in Eq. (3) is modiÔ¨Åed to the average CE losses in all forward passes as

1M

(m) (m)

LCMLM = M

LCMLM(Ymask|Yobs , X)

m=1

1M

(m)

= M

‚àí log PCMLM(y|Yobs , X), (4)

m=1 y‚ààYm(m as)k

where Ym(mas)k and Yo(bms) are masked and observed tokens in the m-th forward pass, respectively. This method corresponds
to augmenting the training data by a factor of M in total.
However, the number of parameter updates does not increase.

2) Auxiliary NAR MT objective: To further enhance the CMLM decoder further, we train the NAR E2E-ST model jointly with an auxiliary text-input NAR MT task by sharing the parameters of the CMLM decoder. The effectiveness of an auxiliary MT task has been studied to improve the translation quality of AR E2E-ST models by leveraging source transcriptions Z [7], [56], [69]. However, the task is still AR sequence generation, and none of the previous studies investigated the effectiveness of the auxiliary MT task for the NAR E2E-ST task. The CE loss for the NAR MT task LMT is formulated as

1M

(m)

LMT = M

‚àí log PCMLM(y|Yobs , Z). (5)

m=1 y‚ààYm(m as)k

To encode source transcriptions, an additional text encoder is introduced. However, this encoder can be removed during inference, so the decoding cost does not change.
Regarding masking strategies, we use separate masks for the NAR MT task by randomly sampling them independently, which is more effective than reusing the same mask used in the

3We also investigated linearly interpolating scores from the AR and NAR decoders in the log domain, but it did not lead to improvement.

NAR E2E-ST task. This also has an effect of augmenting the training data twice similar to MMT. However, we show that these two methods are complementary and their combination4 is effective.

3) Training objective: We optimize an entire network with an end-to-end training objective as

Ltotal = LCMLM + ŒªlpLlp + ŒªARLAR + ŒªMTLMT,

(6)

where Œª‚àó are the corresponding loss weights.5

D. Orthros-CTC
In this section, we describe Orthros-CTC, which uses a CTC-based model as the NAR decoder. Unlike OrthrosCMLM, Orthros-CTC does not have any NAR decoder parameters except for the linear projection layer, so it is more parameter-efÔ¨Åcient. Note again that CTC does not require a separate length predictor. As discussed in Section V, translation quality of CTC-based models greatly improves using the Conformer encoder. Therefore, we focus only on the Conformer encoder. The total objective Ltotal is formulated as

Ltotal = LCTC + ŒªARLAR.

(7)

To generate multiple candidates, left-to-right preÔ¨Åx beam search can be used in the CTC branch. Although this is no longer NAR decoding, we can use an efÔ¨Åcient implementation with C++6 as done in a previous study [36] because there is no matrix multiplication.

IV. EXPERIMENTAL SETTING
A. Dataset
We used Must-C [70] En‚Üí{De, Nl, Fr, Es, It}, Libritrans En‚ÜíFr [71], and Fisher-CallHome Spanish Es‚ÜíEn [72] corpora for the experimental evaluations. We used the recipes provided in the ESPnet-ST toolkit [73].
1) Must-C En‚Üí{De, Nl, Fr, Es, It}: We used En‚ÜíDe (399 h), En‚ÜíNl (433 h), En‚ÜíFr (483 h), En‚ÜíEs (495 h), and En‚ÜíIt (456 h) directions on Must-C. This corpus contains spontaneous English lecture speech extracted from TED talks, the corresponding source transcriptions, and the target translations. Non-verbal speech labels, such as ‚Äú(Applause)‚Äù and ‚Äú(Laughter)‚Äù, were kept during training but removed during evaluation as post-processing. We report case-sensitive detokenized BLEU scores [74] on the tst-COMMON set.
2) Libri-trans (En‚ÜíFr): This corpus contains 100 h of English read speech, the corresponding English transcriptions, and the French translations. The French translation in the training set is augmented with Google Translate for each utterance, and we used both references following the standard practice. We report case-insensitive detokenized BLEU scores on the test set.

4This corresponds to augmenting the training data by a factor of 4. 5Unlike our previous study [49], we removed the auxiliary CTC-based ASR objective because it can be removed without quality degradation when pretraining the encoder with the ASR task. Instead, an auxiliary NAR MT task was newly introduced in this article. 6https://github.com/parlance/ctcdecode

6

3) Fisher-CallHome Spanish (Es‚ÜíEn): This corpus contains 171 h of Spanish conversational telephone speech, the corresponding Spanish transcriptions as well as the English translations. Following the standard practice of this corpus, all punctuation marks except for apostrophes were removed from both transcriptions and translations. We report case-insensitive BLEU scores on Fisher-{dev, dev2, test}, and CallHome{devtest, evltest}. We report case-insensitive detokenized BLEU scores on these Ô¨Åve sets. Note that BLEU scores on the Fisher splits were evaluated with four references. We used the Fisher-dev set as the validation set.
B. Pre-processing
We extracted 80-channel log-mel Ô¨Ålterbank coefÔ¨Åcients computed with 25-ms window size and shifted every 10ms with three-dimensional pitch features with the Kaldi toolkit [75]. We augmented speech data with speed perturbation [76] and SpecAugment [77] for both ASR and E2E-ST task. Utterances having more than 3000 speech frames or more than 400 characters were removed from the training data to Ô¨Åt the GPU memory.
We tokenized all sentences with the tokenizer.perl script in the Moses toolkit [78]. Source transcriptions were lowercased, and the punctuation marks except for apostrophes were removed. We constructed shared source and target vocabularies on the basis of the byte pair encoding (BPE) algorithm [79] with the Sentencepiece toolkit [80]. ASR vocabularies were built only on source transcriptions. For AR models, we used 1k units for all tasks on the FisherCallHome Spanish and Libri-trans corpora, while 5k and 8k units were used for ASR and E2E-ST/MT tasks on the MustC corpus, respectively. For NAR models, we used 16k units on all corpora, unless otherwise noted. These vocabulary sizes were selected to achieve the best performance for each model following [49].
C. Architecture
We implemented models based on the ESPnet-ST toolkit. All models in the ASR and E2E-ST tasks consisted of 12 encoder blocks and 6 decoder blocks, while MT models used 6 Transformer encoder blocks. We also used the Transformer decoder when using the Conformer encoder. The speech encoder in the ASR and E2E-ST tasks had two convolutional neural network (CNN) blocks before the Ô¨Årst encoder block, which had a kernel size of 3 and channel size of 256. Each CNN block down-sampled features on both time and frequent axes with a stride of two, which resulted in the four-fold time reduction. The dimensions of the self-attention layer dmodel and FFN dÔ¨Ä were set to 256 and 2048, respectively, and the number of attention heads H was set to four. The kernel size of depthwise separable convolution in each Conformer block was set to 15. The CTC-based models had the same encoder architecture while the decoder was replaced with a linear projection layer.
D. Training
The Adam optimizer [83] was used for training with Œ≤1 = 0.9, Œ≤2 = 0.98, and = 10‚àí9. We used the Noam learning

rate schedule [18] with warmup steps of 25k and learningrate constant of 5.0. The effective batch size was set to 256 utterances for the NAR models. We used dropout and label smoothing [84] with a probability of 0.1 and 0.1, respectively. We set (Œªlp, ŒªAR, ŒªMT) to (0.1, 0.3, 0.3) in Eq. (6) throughout the experiments and empirically conÔ¨Årmed that they work well under various conditions. The number of masks for MMT M was set to 2. We trained AR models for 30 epochs, CTC-based models and CTC-CMLM for 100 epochs, the other models for 50 epochs. The last Ô¨Åve best checkpoints based on the validation score were used for model averaging, except that the last ten best checkpoints were used for the CTC-based models.
We initialized encoder parameters of all E2E-ST models with those of the pre-trained ASR model trained on the same speech data. We did not use any external resources for pretraining. The decoder parameters of all E2E-ST models were initialized on the basis of a strategy in BERT [60], where weight parameters were sampled from N (0, 0.02), biases were set to zero, and layer normalization parameters were set to Œ≤ = 0, Œ≥ = 1.7 This technique was used for the CMLM models in the MT task [30].
Following the standard practice in NAR models [19], [30], we used SeqKD with an AR Transformer MT model as a teacher, except for Libri-trans.8 The teacher MT models used a beam width of 5.

E. Decoding
For the AR models, we used a beam width bST ‚àà {1, 4, 10}. The ASR models used the joint CTC/Attention decoding [85] with shallow fusion of an external long short-term memory (LSTM) LM. For the CMLM decoders, we used a length beam size l = 5 and number of iterations T ‚àà {4, 10} as default settings. We mainly used a relatively small l to avoid slowing the decoding speed on the CPU. We also used a de-duplication technique [27], [29], [86] for the CMLM decoders, where repeated tokens were collapsed to a single token as postprocessing, except for the Fisher-CallHome Spanish corpus.9 For Orthros-CTC, we used l = 20 without de-duplication. The decoding speed was measured with a batch size of 1 on an NVIDIA TITAN RTX GPU and Intel(R) Xeon(R) Gold 6128 CPU @ 3.4GHz by averaging on Ô¨Åve runs. We calculated BLEU scores with SacreBLEU10 [87].

A. Must-C

V. MAIN RESULTS

1) Transformer encoder: Results with the Transformer encoder on Must-C are shown in Table I. The single step NAR decoding with CTC showed poor BLEU scores although

7Unlike [49], we did not use a pre-trained MT model for initialization of decoder parameters.
8We did not observe any improvement in BLEU scores with SeqKD for both AR and NAR models on this corpus. This is probably because the teacher MT model was too weak due to a small amount of the training bitext.
9De-duplication was effective for corpora having long-form speech, such as Must-C and Libri-trans.
10case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1

7

TABLE I: BLEU scores with Transformer encoder on Must-C tst-COMMON. Decoding speed was measured on En‚ÜíDe with batch size of 1. NAR was set to 1. MMT: multi-mask training. NAR MT: auxiliary text-input NAR MT objective.

Model

T

BLEU (‚Üë)

Speedup (‚Üë)

De Nl Fr Es

It Avg GPU

CPU

E2E AR E2E NAR Cascade AR

ESPnet-ST [73] (bST = 10)

22.9 27.4 32.8 28.0 23.8 27.0

‚Äì

‚Äì

Fairseq S2T [81]

N 22.7 27.3 32.9 27.2 22.7 26.6

‚Äì

‚Äì

NeurST [82]

22.8 27.2 33.3 27.4 22.9 26.7

‚Äì

‚Äì

Transformer (bST = 1)

Transformer (bST = 4)

N

Transformer + SeqKD (bST = 1)

Transformer + SeqKD (bST = 4)

21.0 26.1 31.6 26.2 22.1 25.4 22.8 27.3 33.3 27.8 23.3 26.9 23.8 27.7 33.7 28.0 23.3 27.3 24.3 28.4 34.5 28.9 24.2 28.1

1.64√ó 1.00√ó 1.64√ó 1.00√ó

3.26√ó 1.00√ó 3.26√ó 1.00√ó

CTC

1 19.8 23.9 28.9 22.8 19.5 23.0 24.51√ó 14.62√ó

CMLM SMART Orthros-CMLM + MMT + NAR MT

20.1 23.6 28.9 24.0 20.5 23.4 6.45√ó 5.43√ó 4 20.4 24.3 29.7 24.9 21.0 24.1 6.45√ó 5.43√ó
21.8 25.3 30.0 25.3 21.5 24.8 5.69√ó 5.10√ó 22.5 26.1 31.5 26.0 22.7 25.8 5.69√ó 5.10√ó

CMLM SMART Orthros-CMLM + MMT + NAR MT

21.5 25.1 30.7 25.3 21.8 24.9 10 21.4 25.2 31.0 25.4 22.1 25.0
22.9 26.4 31.8 26.1 22.5 26.0 23.5 27.1 33.1 27.1 23.3 26.8

3.13√ó 3.13√ó 2.99√ó 2.99√ó

2.75√ó 2.75√ó 2.64√ó 2.64√ó

Trf ASR ‚Üí Trf MT

2N 23.5 28.5 33.9 28.6 24.3 27.8 0.45√ó 0.68√ó

TABLE II: BLEU scores with Conformer encoder on Must-C tst-COMMON

E2E AR E2E NAR Cascade AR

Model

T

BLEU (‚Üë)

Speedup (‚Üë)

De Nl Fr Es

It Avg GPU

CPU

Conformer (bST = 1)

Conformer (bST = 4)

N

Conformer + SeqKD (bST = 1)

Conformer + SeqKD (bST = 4)

23.2 28.3 34.5 29.7 24.6 28.1 25.0 29.7 35.5 30.5 25.4 29.2 25.7 29.7 35.8 30.6 25.5 29.5 26.3 30.6 36.4 31.0 25.9 30.0

1.59√ó 1.00√ó 1.59√ó 1.00√ó

2.82√ó 1.00√ó 2.82√ó 1.00√ó

CTC (l = 1) Orthros-CTC (l = 20)

1 24.1 28.5 34.6 29.0 24.3 28.1 13.83√ó 8.32√ó 25.3 29.9 36.2 30.4 25.4 29.4 1.14√ó 3.63√ó

CMLM CTC-CMLM Orthros-CMLM + MMT + NAR MT

22.6 26.3 31.3 26.5 21.6 25.7 5.44√ó 4.20√ó 4 24.1 27.8 34.7 29.1 24.5 28.0 5.07√ó 5.97√ó
23.4 27.6 33.3 28.3 23.2 27.2 4.92√ó 4.03√ó 23.3 27.5 34.0 28.7 23.7 27.4 4.92√ó 4.03√ó

CMLM CTC-CMLM Orthros-CMLM + MMT + NAR MT

23.5 27.6 33.0 27.5 22.7 26.9 10 24.2 28.2 34.8 29.3 24.6 28.1
24.5 28.5 34.6 29.1 24.0 28.1 24.1 28.6 35.1 29.2 24.4 28.3

2.89√ó 3.06√ó 2.73√ó 2.73√ó

2.33√ó 4.43√ó 2.31√ó 2.31√ó

Cfm ASR ‚Üí Trf MT

2N 24.1 29.4 35.0 29.5 24.7 28.5 0.46√ó 0.67√ó

the speed increases were very large; 24.51√ó and 14.62√ó on the GPU and CPU, respectively. The iterative NAR models, however, improved BLEU scores at the cost of speed. We Ô¨Årst compared the CMLM and SMART without the auxiliary AR decoder, and the SMART outperformed the CMLM when T = 4. However, the gains were diminished when T = 10. Therefore, we did not use SMART for Orthros. OrthrosCMLM consistently improved the CMLM for all language pairs regardless of T . The additional latency with the parallel AR rescoring was about 5 and 100 ms on the GPU and CPU, respectively, which are negligible when we take the gains of BLEU scores into account. The enhanced training with MMT and multi-task with the auxiliary NAR-MT task further improved the BLEU scores for all language pairs and T . Compared with the baseline CMLM, the enhanced OrthrosCMLM brought the gains of 2.4 and 1.9 BLEU scores on average for T = 4 and T = 10, respectively. The enhanced Orthros-CMLM with T = 10 also achieved BLEU scores

comparable to those of the AR Transformer (bST = 4) with 2.99√ó and 2.64√ó increase in speed on the GPU and CPU, respectively. However, we found that SeqKD improved the AR model and the gains were much larger than those reported in the MT literature [36]. Compared with the cascade system, the enhanced Orthros-CMLM with T = 10 achieved 6.44√ó and 3.88√ó faster decoding on GPU and CPU while the average BLEU score underperformed by 1.0.
2) Conformer encoder: Results with the Conformer encoder on Must-C are shown in Table II. We observed that the Conformer encoder consistently improved BLEU scores across models. The CTC-based models showed the largest gains of BLEU scores by more than 4 BLEU. One limitation of the CTC-based models was that it took more training steps to converge. We trained them for 100 epochs while we trained the CMLM models for 50 epochs. We evaluated Orthros-CTC considering the strong BLEU scores of Conformer-CTC. It resulted in further gains by parallel AR

8

TABLE III: BLEU scores with Transformer encoder on Libri-trans En‚ÜíFr test. NAR was set to 1.

Model

T BLEU (‚Üë) Speedup (‚Üë) GPU CPU

E2E AR E2E NAR

WordKD [14] TCEN-LSTM [12] NeurST [82] Curriculum PT [13] N LUT [88] STAST [56] COSTT [58] SATE [89]

Transformer (bST = 1) N Transformer (bST = 4)

CTC (l = 1)

1

CMLM

SMART

4

Orthros-CMLM

+ MMT + NAR MT

CMLM

SMART

10

Orthros-CMLM

+ MMT + NAR MT

17.02 17.05 17.2 17.66 17.75 17.81 17.83 18.3
16.6 16.7
13.4
14.3 13.6 15.0 15.9
15.5 14.6 16.2 16.7

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

1.64√ó 4.29√ó 1.00√ó 1.00√ó

35.03√ó 26.61√ó

9.40√ó 10.09√ó 9.40√ó 10.09√ó 8.47√ó 9.52√ó 8.47√ó 9.52√ó

4.61√ó 4.61√ó 4.39√ó 4.39√ó

5.18√ó 5.18√ó 4.99√ó 4.99√ó

Cascade AR Trf ASR ‚Üí Trf MT 2N 17.0 0.43√ó 0.77√ó

rescoring and addressed the slow convergence issue. In terms of decoding speed, Orthros-CTC achieved a large gain for the CPU (3.63√ó) while that for the GPU was small (1.14√ó). We also investigated CTC-CMLM, which reÔ¨Åned a CTC output through multiple iterations. Although it outperformed Orthros-CMLM in speed with similar BLEU scores, we did not observe any BLEU improvement from the pure CTCbased model. Therefore, increasing the number of candidates was more effective than reÔ¨Åning the single output multiple times. Similar to the Transformer encoder, Orthros-CMLM improved the CMLM by 1.5 and 1.2 BLEU scores on average for T = 4 and T = 10, respectively. The enhanced training slightly improved the BLEU scores (+0.2) except for En‚ÜíDe and En‚ÜíNl. We reason that the Conformer encoder already enhanced the CMLM decoder by providing better encoder representations and augmenting masks for the CMLM training was not complementary. Comparing the enhanced OrthrosCMLM (T = 10) with the cascade system, the gap in the average BLEU score was reduced from 1.0 to 0.2 by using the Conformer encoder. To summarize, Orthros-CTC showed the best BLEU scores in all language pairs.
B. Libri-trans
The results from the Transformer and Conformer encoders on Libri-trans are listed in Tables III and IV, respectively. We did not use SeqKD for this corpus because it was not effective in our preliminary experiments. We also referred to the results of previous studies that did not leverage additional resources.
For Transformer-based models, we conÔ¨Årmed that SMART was not effective and Orthros outperformed both CMLM and SMART regardless of T . The enhanced training was also helpful, boosting the BLEU score by 0.9 for T = 4 and 0.5 for T = 10. The enhanced Orthros with T = 10 matched the AR model in terms of BLEU score with 4.39√ó and 4.99√ó

TABLE IV: BLEU scores with Conformer encoder on Libri-trans En‚ÜíFr test

Model

T BLEU (‚Üë) Speedup (‚Üë) GPU CPU

E2E AR Conformer (bST = 1) N 18.8

Conformer (bST = 4)

19.2

1.61√ó 3.90√ó 1.00√ó 1.00√ó

CTC (l = 1)

1 17.5 19.24√ó 15.53√ó

Orthros-CTC (l = 20)

18.5 1.37√ó 3.52√ó

CMLM

15.3

CTC-CMLM E2E NAR Orthros-CMLM

4 17.0 16.7

+ MMT + NAR MT

16.7

7.92√ó 8.23√ó 7.03√ó 11.34√ó 7.11√ó 7.82√ó 7.11√ó 7.82√ó

CMLM

15.8

CTC-CMLM Orthros-CMLM

10 17.2 17.6

+ MMT + NAR MT

17.4

4.32√ó 4.49√ó 4.12√ó 4.12√ó

4.77√ó 8.59√ó 4.61√ó 4.61√ó

Cascade AR Cfm ASR ‚Üí Trf MT 2N 17.3 0.42√ó 0.79√ó

increase in decoding speed on the GPU and CPU, respectively. The relative increase in speed were much larger than those on Must-C because the test set on Libri-trans had many longform utterances up to about 50 s.
For Conformer-based models, we observed large improvements across all models. The AR model achieved the state-ofthe-art BLEU score of 19.2. The other trend was consistent with that on Must-C. Conformer-CTC improved upon the Transformer-CTC by 4.1 BLEU, and Orthros-CTC further boosted it by 1.0 BLEU. Orthros-CMLM outperformed the CMLM by 1.4 BLEU for T = 4 and 1.8 BLEU for T = 10, while the enhanced training did not improve it further.
C. Fisher-CallHome Spanish
The results with the Transformer and Conformer encoders on Fisher-CallHome Spanish are listed in Tables V and VI, respectively. Note that the CallHome sets are out-of-domain because the models were trained in the Fisher domain. We conÔ¨Årmed similar trends for Must-C and Libri-trans. However, unlike previous experiments, we observed that Conformerbased Orthros-CMLM outperformed Conformer-CTC on the CallHome test sets by a large margin. Considering the fact that the word error rates on the CallHome test sets were much larger than those on the Fisher test sets (40% vs. 20% [61], [90]), we can conclude that Orthros is more robust in noisy acoustic conditions.
VI. ANALYSIS
A. Effective number of auxiliary AR decoder layers
We Ô¨Årst investigated the effective number of auxiliary AR decoder layers NAR for Orthros-CMLM. We increased NAR from zero to six.11 We used the Conformer encoder without any enhanced training method. The results on the dev set of Must-C En‚ÜíDe, Libri-trans, and Fisher-CallHome Spanish in Table VII indicate that a shallow AR decoder works
11In our previous study [49], we used NAR = 6 and did not study its impact on BLEU scores.

9

TABLE V: BLEU scores with Transformer encoder on Fisher-CallHome Spanish Es‚ÜíEn. NAR was set to 3. Decoding speed was measured on Fisher-test set.

Model

BLEU (‚Üë)

Speedup (‚Üë)

T Fisher

CallHome

GPU

CPU

dev dev2 test devtest evltest

E2E AR

Transformer (bST = 1)

47.3 48.5 47.5 18.0 17.3 1.84√ó 3.07√ó

Transformer (bST = 4)

N 49.4 50.6 49.4 18.6 18.4 1.00√ó 1.00√ó

Transformer + SeqKD (bST = 1)

49.6 49.9 49.4 18.5 18.6 1.84√ó 3.07√ó

Transformer + SeqKD (bST = 4)

51.1 51.4 50.8 19.6 19.2 1.00√ó 1.00√ó

CTC (l = 1)

1 45.8 46.4 46.2 15.6 16.0 22.11√ó 11.55√ó

E2E NAR

CMLM SMART Orthros-CMLM + MMT + NAR MT

45.3 46.2 45.4 17.5 16.8 5.76√ó 4.50√ó 4 45.3 46.1 45.9 17.2 17.0 5.76√ó 4.50√ó
47.4 48.2 47.4 18.6 17.8 4.95√ó 4.05√ó 49.1 49.9 48.5 19.0 18.9 4.95√ó 4.05√ó

CMLM SMART Orthros-CMLM + MMT + NAR MT

47.7 48.5 48.0 18.5 18.6 3.19√ó 2.47√ó 10 46.3 47.1 47.0 17.9 17.8 3.19√ó 2.47√ó
49.9 50.4 49.6 19.3 18.9 2.93√ó 2.34√ó 50.8 51.3 50.1 19.5 19.6 2.93√ó 2.34√ó

Cascade AR Trf ASR ‚Üí Trf MT

2N 41.4 43.3 42.3 20.4 19.9 0.50√ó 0.64√ó

TABLE VI: BLEU scores with Conformer encoder on Fisher-CallHome Spanish Es‚ÜíEn

Model

BLEU (‚Üë)

Speedup (‚Üë)

T Fisher

CallHome

GPU

CPU

dev dev2 test devtest evltest

E2E AR

Conformer (bST = 1)

53.3 53.9 52.5 20.6 20.8 1.82√ó 2.71√ó

Conformer (bST = 4)

N 54.4 55.1 53.6 21.1 21.1 1.00√ó 1.00√ó

Conformer + SeqKD (bST = 1)

54.1 54.6 53.9 21.4 21.5 1.82√ó 2.71√ó

Conformer + SeqKD (bST = 4)

54.7 55.4 54.1 21.5 21.0 1.00√ó 1.00√ó

CTC (l = 1) Orthros-CTC (l = 20)

1 51.0 51.6 50.8 18.0 54.0 54.8 54.1 21.0

18.7 11.80√ó 6.91√ó 20.8 1.09√ó 2.80√ó

E2E NAR

CMLM CTC-CMLM Orthros-CMLM + MMT + NAR MT

47.7 48.1 46.9 18.8 18.5 4.70√ó 3.68√ó 4 50.9 51.4 50.5 17.7 17.9 4.79√ó 4.96√ó
50.3 50.7 48.9 20.0 18.9 4.18√ó 3.44√ó 50.3 50.7 49.0 20.3 20.0 4.18√ó 3.44√ó

CMLM CTC-CMLM Orthros-CMLM + MMT + NAR MT

49.8 49.7 48.9 19.7 19.4 2.89√ó 2.30√ó 10 51.3 51.7 50.7 17.9 18.3 3.40√ó 3.89√ó
51.8 52.3 50.9 20.7 20.0 2.70√ó 2.20√ó 51.3 52.2 51.2 20.9 20.4 2.70√ó 2.20√ó

Cascade AR Cfm ASR ‚Üí Trf MT

2N 42.6 44.4 43.2 21.7 21.0 0.50√ó 0.66√ó

TABLE VII: Effective number of auxiliary AR decoders NAR for Orthros-CMLM on dev sets of Must-C En‚ÜíDe, Libritrans, and Fisher-CallHome Spanish. Conformer encoder was used. All models used parallel AR rescoring. No enhanced training method, such as MMT, was used.

NAR Must-C

BLEU (‚Üë) Libri-trans

Fisher

T = 4 T = 10 T = 4 T = 10 T = 4 T = 10

0

21.4

22.7

16.2

16.9

47.7

49.8

1

22.2

23.2

17.4

18.4

48.7

50.1

2

21.9

23.1

17.3

18.2

49.2

51.0

3

22.0

23.2

16.8

18.1

50.3

51.8

4

21.0

22.4

17.6

18.5

49.5

51.6

5

21.8

23.1

17.3

18.4

47.9

49.5

6

21.9

22.8

16.9

17.8

49.1

51.1

well across corpora. SpeciÔ¨Åcally, NAR = 1 was best on MustC, NAR = 4 was best on Libri-trans while NAR = 1 showed

comparable BLEU scores. However, Fisher-CallHome Spanish reached a peak at NAR = 3. This was probably because the input speech was much noisy compared with other corpora; therefore, stacking multiple AR decoder layers was more effective for providing better sequence-level scores. Otherwise, a single layer was enough, which minimized the additional decoding cost by the rescoring. Hence, we used NAR = 3 on Fisher-CallHome Spanish and NAR = 1 on the other corpora in the other experiments, regardless of the encoder type. We also used the same NAR for Orthros-CTC.
B. Ablation study
We conducted an ablation study of Orthros-CMLM in terms of parallel AR rescoring and the enhanced training methods on the dev sets of Must-C En‚ÜíDe, Libri-trans, and Fisher-CallHome Spanish shown in Table VIII. We used the Transformer encoder and Ô¨Årst observed that joint training with an auxiliary shallow AR decoder improved the BLEU scores

10

TABLE VIII: Ablation study of parallel AR rescoring and enhanced training methods on dev sets of Must-C En‚ÜíDe, Libritrans, and Fisher-CallHome Spanish. Transformer encoder was used.

ID Model

Must-C T = 4 T = 10

BLEU (‚Üë) Libri-trans T = 4 T = 10

Fisher T = 4 T = 10

A1 CMLM

19.8

21.4

14.6

15.8

45.3

47.7

A2 Orthros-CMLM A3 - parallel AR rescoring

21.1

22.0

15.7

16.9

47.4

49.9

20.4

21.3

15.1

16.3

45.7

48.3

A4 Orthros-CMLM + MMT + NAR MT 21.9

22.8

16.3

17.3

49.1

50.8

A5 - parallel AR rescoring

21.5

22.4

15.7

16.7

47.2

49.0

A6 - MMT (M = 2)

21.8

23.0

15.6

16.3

48.6

50.2

A7 - NAR MT

21.2

22.4

16.4

17.2

47.5

49.9

A8

+ MMT (M = 3)

21.8

22.6

16.1

17.2

47.9

50.1

A9

+ MMT (M = 4)

21.4

22.4

16.1

17.0

48.7

50.4

TABLE IX: BLEU scores of large models with (dmodel, H) = (512, 8) on Must-C En‚ÜíDe tst-COMMON. Transformer encoder was used.

Model

T BLEU (‚Üë) Speedup (‚Üë) GPU CPU

Transformer AR + SeqKD (bST = 1) N 24.0 1.62√ó 3.30√ó

Transformer AR + SeqKD (bST = 4)

24.7 1.00√ó 1.00√ó

Orthros-CMLM + MMT + NAR MT

4 23.0 5.89√ó 5.63√ó 23.8

Orthros-CMLM + MMT + NAR MT

10 24.0 3.02√ó 2.92√ó 24.4

Fig. 2: Robustness against long-form speech on IWSLT tst2019 set. X-axis denotes maximum input length threshold in segment-merging algorithm [91].

(A1 vs. A3), except for T = 10 on Must-C. A3 was trained jointly with the auxiliary AR decoder but did not use it during inference. Parallel AR rescoring further improved the BLEU scores in all settings (A2 vs. A3).
The effectiveness of parallel AR rescoring remained even when using the enhanced training combining MMT (M = 2) and the auxiliary NAR-MT task (A4 vs. A5). Each of these training methods was beneÔ¨Åcial (A4 vs. A6 vs. A7). Because A4 increased the number of masks for the CMLM training by a factor of 4 per sample, we also investigated increasing M in MMT without the auxiliary NAR-MT task (A9). However, we conÔ¨Årmed that this was less effective, indicating that providing source information from different modalities was helpful to some extent.
C. Large model
In the above experiments, we used (dmodel, H) = (256, 4) in all models. They are much smaller than those used in the NAR MT literature [36], where (dmodel, H) = (512, 8) was typically used. Therefore, our baseline AR model was relatively lightweight. This was because using a large AR model did not lead to a notable BLEU improvement in our preliminary experiments due to the limited parallel data in the ST corpora. However, when additional data are available, increasing model capacity would be typically beneÔ¨Åcial. Therefore, we also compared decoding speeds of AR and NAR models based on a large Transformer architecture. We used (dmodel, H) = (512, 8) in both the encoder and decoder architectures of AR and NAR models, except that the size

of the auxiliary AR decoder in Orthros-CMLM was kept to (dmodel, H) = (256, 4). The results on Must-C En‚ÜíDe tst-COMMON in Table IX indicate that using the large architecture improved the BLUE score of the AR model by only 0.4. The inference speed was not so different from that in Table I, but we observed a large BLEU improvement in Orthros-CMLM by increasing the model capacity. Compared with Table I, it improved BLEU scores by 1.2 and 1.1 when T = 4 and T = 10, respectively. The quality was further boosted by the enhanced training by 0.8 and 0.4 for T = 4 and T = 10, respectively. Finally, the enhanced Orthros-CMLM almost matched the strong AR model trained with SeqKD in quality with 3√ó faster decoding.
D. Robustness against long-form speech
In speech translation, audio segmentation has a large impact on the Ô¨Ånal translation quality [91]‚Äì[95] because acousticdriven segmentation does not necessarily correspond to sentence segmentation based on punctuation marks. Therefore, it is important to translate long-form speech robustly by incorporating long context [91]. Therefore, we investigated the robustness of E2E-ST models against long-form data on the International Conference on Spoken Language Translation (IWSLT) tst2019 set (En‚ÜíDe).12 Following the segmentmerging algorithm [91], we split the entire speech using a neural voice activity detection model [96] then concatenated multiple adjacent segments until reaching the desired input
12The IWSLT test sets have reference translation over the entire session, unlike Must-C.

11

Fig. 3: Effectiveness of length beam size l with T = 10 on Must-C En‚ÜíDe tst-COMMON. Conformer encoder was used.
length.13 Figure 2 shows BLEU scores as a function of the maximum input length threshold for the segment merging. We evaluated models trained on Must-C En‚ÜíDe. Note that we did not change any of the decoding hyperparameters of the E2EST models for this experiment. We observed that both the AR model and Orthros-CMLM were not robust against long-form speech, regardless of the encoder architecture. In contrast, Orthros-CTC could robustly translate long-form speech up to 60 s, outperforming the Conformer AR model on speech longer than 30 s.

TABLE X: Oracle BLEU scores on Must-C En‚ÜíDe tst-COMMON. Numbers inside brackets denote gains from parallel AR rescoring.

Model

T l Oracle BLEU (‚àÜ) (‚Üë)

Transformer encoder

Orthros-CMLM

45

49

10 5

10 9

24.6 (+2.8) 26.1 (+3.9) 25.9 (+3.0) 27.2 (+4.2)

+ MMT + NAR MT 4 5 49 10 5 10 9

25.7 (+3.2) 27.2 (+4.4) 26.9 (+3.4) 28.4 (+4.6)

+ large

45 49 10 5 10 9

26.1 (+3.1) 27.5 (+4.4) 27.4 (+3.4) 28.8 (+4.6)

Conformer encoder Orthros-CTC

1 20

28.6 (+3.3)

Orthros-CMLM

45 49 10 5 10 9

26.2 (+2.4) 27.5 (+4.1) 27.1 (+2.5) 28.5 (+4.0)

E. Searchability
In this section, we analyze the searchability of Orthros by changing a length beam size l. We investigate how it impacts the quality of hypotheses.
1) Length beam size: Figure 3 shows BLEU scores as a function of length beam size l with T = 10 on Must-C En‚ÜíDe tst-COMMON. We used the Conformer encoder and conÔ¨Årmed that parallel AR rescoring consistently improved the quality of NAR models when l > 1 and the gain increased using a larger l. The baseline CMLM and CTC-based model did not improve by increasing l because the NAR decoders could not provide effective sequence-level scores. Because parallel AR rescoring introduces an additional iteration in terms of T , we also investigated reducing T in Orthros-CMLM by 1. We conÔ¨Årmed that BLEU scores of Orthros-CMLM with T = 9 was much better than those of CMLM with T regardless of l and the gap was enlarged as l increased. Therefore, we conclude that parallel AR rescoring improves the searchability of NAR models.
2) Oracle BLEU: We next investigated oracle BLEU scores, where the best hypothesis was selected from candidates generated from the NAR decoder based on the sentence-level BLEU score with the reference translation. It is regarded as the upper bound that can be achieved by parallel AR rescoring. We conducted this experiment to identify a room for improvement of translation quality of NAR models. Table X indicates that increasing the number of length candidates can generate translations of higher quality than reÔ¨Åning the predictions for more iterations. However, NAR models were still affected by selecting a better translation even with parallel AR rescoring. This was more severe when using a large l. Better training and architecture also led to higher oracle BLEU scores.
13We did not merge adjacent segments with interval above 100ms.

VII. CONCLUSIONS
To increase the decoding speed of E2E-ST models, we proposed Orthros, which introduces an auxiliary shallow AR decoder on top of the shared encoder to assist the NAR decoder and optimizes the entire network end-to-end. The AR decoder is used for rescoring outputs from the NAR decoder with a very small additional decoding cost. We investigated the CMLM and CTC as the NAR decoder for Orthros. We also compared Transformer and Conformer encoder architectures. We introduced MMT and joint training with an auxiliary textinput NAR-MT task to enhance the CMLM decoder. Experimental evaluations on three benchmark corpora including six language pairs conÔ¨Årmed the consistent effectiveness of Orthros; parallel AR rescoring improved the BLUE scores of the NAR model regardless of the encoder and decoder topologies. Enhanced training methods improved the quality of OrthrosCMLM by a large margin when the Transformer encoder was used. When increasing the model capacity, the enhanced Orthros-CMLM matched the strong large AR model trained with SeqKD in terms of BLEU score with 3√ó faster decoding. The Conformer encoder improved the overall quality across models while the effectiveness of the parallel AR rescoring was maintained. Orthros-CTC showed the best BLEU score while achieving a 3.63√ó increase in decoding speed on a CPU compared with the AR model.
For future work, we will further improve the translation quality of Conformer-CTC by adopting a deep encoder architecture [97]. We believe that improving the selection of a better hypothesis in NAR models would bridge the quality gap between AR and NAR models.
REFERENCES
[1] F. W. Stentiford and M. G. Steer, ‚ÄúMachine translation of speech,‚Äù British Telecom technology journal, vol. 6, no. 2, pp. 116‚Äì122, 1988.

12

[2] A. Waibel, A. N. Jain, A. E. McNair, H. Saito, A. G. Hauptmann, and J. Tebelskis, ‚ÄúJANUS: a speech-to-speech translation system using connectionist and symbolic processing strategies,‚Äù in Acoustics, Speech, and Signal Processing, IEEE International Conference on. IEEE Computer Society, 1991, pp. 793‚Äì796.
[3] H. Ney, ‚ÄúSpeech translation: Coupling of recognition and translation,‚Äù in Proceedings of ICASSP. IEEE, 1999, pp. 517‚Äì520.
[4] C. Fu¬®gen, ‚ÄúA system for simultaneous translation of lectures and speeches,‚Äù Ph.D. dissertation, Karlsruhe Institute of Technology, 2009.
[5] A. Be¬¥rard, O. Pietquin, C. Servan, and L. Besacier, ‚ÄúListen and translate: A proof of concept for end-to-end speech-to-text translation,‚Äù in Proceedings of NeurIPS 2016 End-to-end Learning for Speech and Audio Processing Workshop, 2016.
[6] R. J. Weiss, J. Chorowski, N. Jaitly, Y. Wu, and Z. Chen, ‚ÄúSequence-tosequence models can directly translate foreign speech,‚Äù in Proceedings of Interspeech, 2017, pp. 2625‚Äì2629.
[7] A. Be¬¥rard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin, ‚ÄúEndto-end automatic speech translation of audiobooks,‚Äù in Proceedings of ICASSP. IEEE, 2018, pp. 6224‚Äì6228.
[8] E. Ansari, A. Axelrod, N. Bach, O. Bojar, R. Cattoni, F. Dalvi, N. Durrani, M. Federico, C. Federmann, J. Gu et al., ‚ÄúFindings of the IWSLT 2020 evaluation campaign,‚Äù in Proceedings of IWSLT, 2020, pp. 1‚Äì34.
[9] M. Sperber and M. Paulik, ‚ÄúSpeech translation and the end-to-end promise: Taking stock of where we are,‚Äù in Proceedings of ACL, 2020.
[10] L. Bentivogli, M. Cettolo, M. Gaido, A. Karakanta, A. Martinelli, M. Negri, and M. Turchi, ‚ÄúCascade versus direct speech translation: Do the differences still make a difference?‚Äù in Proceedings of ACL, 2021, pp. 2873‚Äì2887.
[11] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Goldwater, ‚ÄúPretraining on high-resource speech recognition improves low-resource speech-to-text translation,‚Äù in Proceedings of NAACL-HLT, 2019, pp. 58‚Äì68.
[12] C. Wang, Y. Wu, S. Liu, Z. Yang, and M. Zhou, ‚ÄúBridging the gap between pre-training and Ô¨Åne-tuning for end-to-end speech translation,‚Äù in Proceedings of AAAI, 2020, pp. 9161‚Äì9168.
[13] C. Wang, Y. Wu, S. Liu, M. Zhou, and Z. Yang, ‚ÄúCurriculum pre-training for end-to-end speech translation,‚Äù in Proceedings of ACL, 2020, pp. 3728‚Äì3738.
[14] Y. Liu, H. Xiong, Z. He, J. Zhang, H. Wu, H. Wang, and C. Zong, ‚ÄúEndto-end speech translation with knowledge distillation,‚Äù in Proceedings of Interspeech, 2019, pp. 1128‚Äì1132.
[15] H. Inaguma, T. Kawahara, and S. Watanabe, ‚ÄúSource and target bidirectional knowledge distillation for end-to-end speech translation,‚Äù in Proceedings of NAACL-HLT, 2021, pp. 1872‚Äì1881.
[16] Y. Jia, M. Johnson, W. Macherey, R. J. Weiss, Y. Cao, C.-C. Chiu, N. Ari, S. Laurenzo, and Y. Wu, ‚ÄúLeveraging weakly supervised data to improve end-to-end speech-to-text translation,‚Äù in Proceedings of ICASSP. IEEE, 2019, pp. 7180‚Äì7184.
[17] J. Pino, L. Puzon, J. Gu, X. Ma, A. D. McCarthy, and D. Gopinath, ‚ÄúHarnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade,‚Äù in Proceedings of IWSLT, 2019.
[18] A. Vaswani et al., ‚ÄúAttention is all you need,‚Äù in Proceedings of NIPS, 2017, pp. 5998‚Äì6008.
[19] J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher, ‚ÄúNonautoregressive neural machine translation,‚Äù in Proceedings of ICLR, 2018.
[20] J. Guo, X. Tan, D. He, T. Qin, L. Xu, and T.-Y. Liu, ‚ÄúNon-autoregressive neural machine translation with enhanced decoder input,‚Äù in Proceedings of AAAI, 2019, pp. 3723‚Äì3730.
[21] Z. Li, Z. Lin, D. He, F. Tian, T. Qin, L. Wang, and T.-Y. Liu, ‚ÄúHint-based training for non-autoregressive machine translation,‚Äù in Proceedings of EMNLP, 2019, pp. 5708‚Äì5713.
[22] Y. Wang, F. Tian, D. He, T. Qin, C. Zhai, and T.-Y. Liu, ‚ÄúNonautoregressive machine translation with auxiliary regularization,‚Äù in Proceedings of AAAI, 2019, pp. 5377‚Äì5384.
[23] B. Wei, M. Wang, H. Zhou, J. Lin, and X. Sun, ‚ÄúImitation learning for non-autoregressive neural machine translation,‚Äù in Proceedings of ACL, 2019, pp. 1304‚Äì1312.
[24] C. Shao, J. Zhang, Y. Feng, F. Meng, and J. Zhou, ‚ÄúMinimizing the bagof-ngrams difference for non-autoregressive neural machine translation,‚Äù in Proceedings of AAAI, 2020, pp. 198‚Äì205.
[25] J. Liu, Y. Ren, X. Tan, C. Zhang, T. Qin, Z. Zhao, and T.-Y. Liu, ‚ÄúTask-level curriculum learning for non-autoregressive neural machine translation,‚Äù in Proceedings of IJCAI, 2020, pp. 3861‚Äì3867.

[26] M. Ghazvininejad, O. Levy, and L. Zettlemoyer, ‚ÄúAligned cross entropy for non-autoregressive machine translation,‚Äù in Proceedings of ICML, 2020.
[27] C. Du, Z. Tu, and J. Jiang, ‚ÄúOrder-agnostic cross entropy for nonautoregressive machine translation,‚Äù in Proceedings of ICML, 2021.
[28] L. Qian, H. Zhou, Y. Bao, M. Wang, L. Qiu, W. Zhang, Y. Yu, and L. Li, ‚ÄúGlancing Transformer for non-autoregressive neural machine translation,‚Äù in Proceedings of ACL, 2021, pp. 1993‚Äì2003.
[29] J. Lee, E. Mansimov, and K. Cho, ‚ÄúDeterministic non-autoregressive neural sequence modeling by iterative reÔ¨Ånement,‚Äù in Proceedings of EMNLP, 2018, pp. 1173‚Äì1182.
[30] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, ‚ÄúMask-predict: Parallel decoding of conditional masked language models,‚Äù in Proceedings of EMNLP, 2019, pp. 6112‚Äì6121.
[31] M. Ghazvininejad, V. Karpukhin, L. Zettlemoyer, and O. Levy, ‚ÄúSemiautoregressive training improves mask-predict decoding,‚Äù arXiv preprint arXiv:2001.08785, 2020.
[32] J. Kasai, J. Cross, M. Ghazvininejad, and J. Gu, ‚ÄúNon-autoregressive machine translation with disentangled context Transformer,‚Äù in Proceedings of ICML, 2020, pp. 5144‚Äì5155.
[33] Z. Sun, Z. Li, H. Wang, D. He, Z. Lin, and Z. Deng, ‚ÄúFast structured decoding for sequence models,‚Äù in Proceedings of NeurIPS, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche¬¥-Buc, E. Fox, and R. Garnett, Eds., 2019.
[34] J. Libovicky` and J. Helcl, ‚ÄúEnd-to-end non-autoregressive neural machine translation with connectionist temporal classiÔ¨Åcation,‚Äù in Proceedings of EMNLP, 2018, pp. 3016‚Äì3021.
[35] C. Saharia, W. Chan, S. Saxena, and M. Norouzi, ‚ÄúNon-autoregressive machine translation with latent alignments,‚Äù in Proceedings of EMNLP, 2020, pp. 1098‚Äì1108.
[36] J. Gu and X. Kong, ‚ÄúFully non-autoregressive neural machine translation: Tricks of the trade,‚Äù in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 120‚Äì133.
[37] J. Gu, C. Wang, and J. Zhao, ‚ÄúLevenshtein Transformer,‚Äù in Proceedings of NeurIPS, 2019, pp. 11 181‚Äì11 191.
[38] M. Stern, W. Chan, J. Kiros, and J. Uszkoreit, ‚ÄúInsertion Transformer: Flexible sequence generation via insertion operations,‚Äù in Proceedings of ICML, 2019, pp. 5976‚Äì5985.
[39] W. Chan, N. Kitaev, K. Guu, M. Stern, and J. Uszkoreit, ‚ÄúKERMIT: Generative insertion-based modeling for sequences,‚Äù arXiv preprint arXiv:1906.01604, 2019.
[40] L. Kaiser, S. Bengio, A. Roy, A. Vaswani, N. Parmar, J. Uszkoreit, and N. Shazeer, ‚ÄúFast decoding in sequence models using discrete latent variables,‚Äù in Proceedings of ICML, 2018, pp. 2390‚Äì2399.
[41] X. Ma, C. Zhou, X. Li, G. Neubig, and E. Hovy, ‚ÄúFlowSeq: Nonautoregressive conditional sequence generation with generative Ô¨Çow,‚Äù in Proceedings of EMNLP, 2019, pp. 4282‚Äì4292.
[42] R. Shu, J. Lee, H. Nakayama, and K. Cho, ‚ÄúLatent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior,‚Äù in Proceedings of AAAI, 2020, pp. 8846‚Äì8853.
[43] L. Tu, R. Y. Pang, S. Wiseman, and K. Gimpel, ‚ÄúENGINE: Energybased inference networks for non-autoregressive machine translation,‚Äù in Proceedings of ACL, 2020, pp. 2819‚Äì2826.
[44] A. Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu, G. Driessche, E. Lockhart, L. Cobo, F. Stimberg et al., ‚ÄúParallel WaveNet: Fast high-Ô¨Ådelity speech synthesis,‚Äù in Proceedings of ICML, 2018, pp. 3918‚Äì3926.
[45] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, ‚ÄúFastSpeech: Fast, robust and controllable text to speech,‚Äù in Proceedings of NeurIPS, 2019, pp. 3171‚Äì3180.
[46] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, ‚ÄúImputer: Sequence modelling via imputation and dynamic programming,‚Äù in Proceedings of ICML, 2020, pp. 1403‚Äì1413.
[47] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, ‚ÄúMask CTC: Non-autoregressive end-to-end ASR with CTC and mask predict,‚Äù in Proceedings of Interspeech, 2020, pp. 3655‚Äì3659.
[48] Y. Higuchi, H. Inaguma, S. Watanabe, T. Ogawa, and T. Kobayashi, ‚ÄúImproved Mask-CTC for non-autoregressive end-to-end ASR,‚Äù in Proceedings of ICASSP. IEEE, 2021, pp. 8363‚Äì8367.
[49] H. Inaguma, Y. Higuchi, K. Duh, T. Kawahara, and S. Watanabe, ‚ÄúOrthros: Non-autoregressive end-to-end speech translation with dualdecoder,‚Äù in Proceedings of ICASSP. IEEE, 2021, pp. 7503‚Äì7507.
[50] S.-P. Chuang, Y.-S. Chuang, C.-C. Chang, and H.-y. Lee, ‚ÄúInvestigating the reordering capability in CTC-based non-autoregressive end-to-end speech translation,‚Äù in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 1068‚Äì1077.

13

[51] A. Graves, S. Ferna¬¥ndez, F. Gomez, and J. Schmidhuber, ‚ÄúConnectionist temporal classiÔ¨Åcation: Labelling unsegmented sequence data with recurrent neural networks,‚Äù in Proceedings of ICML, 2006, pp. 369‚Äì376.
[52] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, ‚ÄúConformer: Convolutionaugmented Transformer for speech recognition,‚Äù in Proceedings of Interspeech, 2020, pp. 5036‚Äì5040.
[53] Y. Kim and A. M. Rush, ‚ÄúSequence-level knowledge distillation,‚Äù in Proceedings of EMNLP, 2016, pp. 1317‚Äì1327.
[54] R. J. Williams and D. Zipser, ‚ÄúA learning algorithm for continually running fully recurrent neural networks,‚Äù Neural computation, vol. 1, no. 2, pp. 270‚Äì280, 1989.
[55] C. Zhou, J. Gu, and G. Neubig, ‚ÄúUnderstanding knowledge distillation in non-autoregressive machine translation,‚Äù in Proceedings of ICLR, 2019.
[56] Y. Liu, J. Zhu, J. Zhang, and C. Zong, ‚ÄúBridging the modality gap for speech-to-text translation,‚Äù arXiv preprint arXiv:2010.14920, 2020.
[57] M. Gaido, M. Cettolo, M. Negri, and M. Turchi, ‚ÄúCTC-based compression for direct speech translation,‚Äù in Proceedings of EACL, 2021, pp. 690‚Äì696.
[58] Q. Dong, M. Wang, H. Zhou, S. Xu, B. Xu, and L. Li, ‚ÄúConsecutive decoding for speech-to-text translation,‚Äù in Proceedings of AAAI, 2021.
[59] X. Zeng, L. Li, and Q. Liu, ‚ÄúRealTranS: End-to-end simultaneous speech translation with convolutional weighted-shrinking Transformer,‚Äù in Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, 2021, pp. 2461‚Äì2474.
[60] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pretraining of deep bidirectional Transformers for language understanding,‚Äù in Proceedings of NAACL-HLT, 2019, pp. 4171‚Äì4186.
[61] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma, N. Kamo, C. Li, D. Garcia-Romero, J. Shi et al., ‚ÄúRecent developments on ESPnet toolkit boosted by Conformer,‚Äù in Proceedings of ICASSP. IEEE, 2021, pp. 5874‚Äì5878.
[62] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and T.-Y. Liu, ‚ÄúUnderstanding and improving Transformer from a multi-particle dynamic system point of view,‚Äù arXiv preprint arXiv:1906.02762, 2019.
[63] J. Guo, L. Xu, and E. Chen, ‚ÄúJointly masked sequence-to-sequence model for non-autoregressive neural machine translation,‚Äù in Proceedings of ACL, 2020, pp. 376‚Äì385.
[64] X. Kong, Z. Zhang, and E. Hovy, ‚ÄúIncorporating a local translation mechanism into non-autoregressive translation,‚Äù in Proceedings of EMNLP, 2020, pp. 1067‚Äì1073.
[65] L. Ding, L. Wang, D. Wu, D. Tao, and Z. Tu, ‚ÄúContext-aware crossattention for non-autoregressive translation,‚Äù in Proceedings of COLING, 2020, pp. 4396‚Äì4402.
[66] P. Xie, Z. Cui, X. Chen, X. Hu, J. Cui, and B. Wang, ‚ÄúInfusing sequential information into conditional masked translation model with self-review mechanism,‚Äù in Proceedings of COLING, 2020, pp. 15‚Äì25.
[67] Y. Hao, S. He, W. Jiao, Z. Tu, M. Lyu, and X. Wang, ‚ÄúMulti-task learning with shared encoder for non-autoregressive machine translation,‚Äù in Proceedings of NAACL-HLT, 2021, pp. 3989‚Äì3996.
[68] Q. Wang, H. Yu, S. Kuang, and W. Luo, ‚ÄúHybrid-regressive neural machine translation,‚Äù 2021. [Online]. Available: https://openreview.net/ forum?id=jYVY piet7m
[69] P. Bahar, T. Bieschke, and H. Ney, ‚ÄúA comparative study on end-to-end speech to text translation,‚Äù in Proceedings of ASRU. IEEE, 2019, pp. 792‚Äì799.
[70] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, ‚ÄúMuST-C: a Multilingual Speech Translation Corpus,‚Äù in Proceedings of NAACL-HLT, 2019, pp. 2012‚Äì2017.
[71] A. C. Kocabiyikoglu, L. Besacier, and O. Kraif, ‚ÄúAugmenting Librispeech with French translations: A multimodal corpus for direct speech translation evaluation,‚Äù in Proceedings of LREC, 2018.
[72] M. Post, G. Kumar, A. Lopez, D. Karakos, C. Callison-Burch, and S. Khudanpur, ‚ÄúImproved speech-to-text translation with the Fisher and Callhome Spanish‚ÄìEnglish speech translation corpus,‚Äù in Proceedings of IWSLT, 2013.
[73] H. Inaguma, S. Kiyono, K. Duh, S. Karita, N. Yalta, T. Hayashi, and S. Watanabe, ‚ÄúESPnet-ST: All-in-one speech translation toolkit,‚Äù in Proceedings of ACL: System Demonstrations, 2020, pp. 302‚Äì311.
[74] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for automatic evaluation of machine translation,‚Äù in Proceedings of ACL, 2002, pp. 311‚Äì318.
[75] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., ‚ÄúThe kaldi speech recognition toolkit,‚Äù in Proceedings of ASRU. IEEE, 2011.

[76] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, ‚ÄúAudio augmentation for speech recognition,‚Äù in Proceedings of Interspeech, 2015, pp. 3586‚Äì 3589.
[77] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, ‚ÄúSpecAugment: A simple data augmentation method for automatic speech recognition,‚Äù in Proceedings of Interspeech, 2019, pp. 2613‚Äì2617.
[78] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst, ‚ÄúMoses: Open source toolkit for statistical machine translation,‚Äù in Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, 2007, pp. 177‚Äì180.
[79] R. Sennrich, B. Haddow, and A. Birch, ‚ÄúNeural machine translation of rare words with subword units,‚Äù in Proceedings of ACL, 2016, pp. 1715‚Äì1725.
[80] T. Kudo and J. Richardson, ‚ÄúSentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,‚Äù in Proceedings of EMNLP: System Demonstrations, 2018, pp. 66‚Äì71.
[81] C. Wang, Y. Tang, X. Ma, A. Wu, D. Okhonko, and J. Pino, ‚ÄúFairseq S2T: Fast speech-to-text modeling with fairseq,‚Äù in Proceedings of AACL: System Demonstrations, 2020, pp. 33‚Äì39.
[82] C. Zhao, M. Wang, Q. Dong, R. Ye, and L. Li, ‚ÄúNeurST: Neural speech translation toolkit,‚Äù in Proceedings of ACL: System Demonstrations, 2021, pp. 55‚Äì62.
[83] D. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù Proceedings of ICLR, 2015.
[84] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄúRethinking the inception architecture for computer vision,‚Äù in Proceedings of CVPR, 2016, pp. 2818‚Äì2826.
[85] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, ‚ÄúHybrid CTC/attention architecture for end-to-end speech recognition,‚Äù IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240‚Äì1253, 2017.
[86] Z. Sun and Y. Yang, ‚ÄúAn EM approach to non-autoregressive conditional sequence generation,‚Äù in Proceedings of ICML, 2020, pp. 9249‚Äì9258.
[87] M. Post, ‚ÄúA call for clarity in reporting BLEU scores,‚Äù in Proceedings of the Third Conference on Machine Translation: Research Papers, 2018, pp. 186‚Äì191.
[88] Q. Dong, R. Ye, M. Wang, H. Zhou, S. Xu, B. Xu, and L. Li, ‚Äú‚ÄúListen, Understand and Translate‚Äù: Triple supervision decouples end-to-end speech-to-text translation,‚Äù in Proceedings of AAAI, 2021.
[89] C. Xu, B. Hu, Y. Li, Y. Zhang, S. Huang, Q. Ju, T. Xiao, and J. Zhu, ‚ÄúStacked acoustic-and-textual encoding: Integrating the pretrained models into speech translation encoders,‚Äù in Proceedings of ACL, 2021, pp. 2619‚Äì2630.
[90] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., ‚ÄúA comparative study on Transformer vs RNN in speech applications,‚Äù in Proceedings of ASRU. IEEE, 2019, pp. 499‚Äì456.
[91] H. Inaguma, B. Yan, S. Dalmia, P. Guo, J. Shi, K. Duh, and S. Watanabe, ‚ÄúESPnet-ST IWSLT 2021 ofÔ¨Çine speech translation system,‚Äù in Proceedings of IWSLT, 2021, pp. 100‚Äì109.
[92] M. Gaido, M. A. D. Gangi, M. Negri, M. Cettolo, and M. Turchi, ‚ÄúContextualized translation of automatically segmented speech,‚Äù in Proceedings of Interspeech, 2020, pp. 1471‚Äì1475.
[93] N.-Q. Pham, T.-L. Ha, T.-N. Nguyen, T.-S. Nguyen, E. Salesky, S. Stu¬®ker, J. Niehues, and A. Waibel, ‚ÄúRelative positional encoding for speech recognition and direct translation,‚Äù in Proceedings of Interspeech, 2020, pp. 31‚Äì35.
[94] T. Potapczyk and P. Przybysz, ‚ÄúSRPOL‚Äôs system for the IWSLT 2020 end-to-end speech translation task,‚Äù in Proceedings of IWSLT, 2020, pp. 89‚Äì94.
[95] M. Gaido, M. Negri, M. Cettolo, and M. Turchi, ‚ÄúBeyond voice activity detection: Hybrid audio segmentation for direct speech translation,‚Äù arXiv preprint arXiv:2104.11710, 2021.
[96] H. Bredin, R. Yin, J. M. Coria, G. Gelly, P. Korshunov, M. Lavechin, D. Fustes, H. Titeux, W. Bouaziz, and M.-P. Gill, ‚ÄúPyannote.audio: neural building blocks for speaker diarization,‚Äù in Proceedings of ICASSP. IEEE, 2020, pp. 7124‚Äì7128.
[97] J. Kasai, N. Pappas, H. Peng, J. Cross, and N. Smith, ‚ÄúDeep encoder, shallow decoder: Reevaluating non-autoregressive machine translation,‚Äù in Proceedings of ICLR, 2021.

